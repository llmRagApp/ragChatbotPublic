{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d384b692-218a-490b-89d3-d1c97f6cfce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /usr/anaconda3/envs/llm-app/lib/python3.11/site-packages (3.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tiktoken in /usr/anaconda3/envs/llm-app/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/anaconda3/envs/llm-app/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/anaconda3/envs/llm-app/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/anaconda3/envs/llm-app/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/anaconda3/envs/llm-app/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/anaconda3/envs/llm-app/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/anaconda3/envs/llm-app/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6698fa-a5ce-4c56-8f33-3dc52cd9f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "principles_of_ds = ''\n",
    "with open('/home/devuser/pds2.pdf', 'rb') as file:\n",
    "\n",
    "  reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "  for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    principles_of_ds += '\\n\\n' + text[text.find(' ]')+2:]\n",
    "  \n",
    "principles_of_ds = principles_of_ds.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a75472-9991-44a8-981d-f8336c0248a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def overlapping_chunks(text,max_tokens=500, overlapping_factor=5):\n",
    "    sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "    \n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks, tokens_so_far, chunk = [], 0, []\n",
    "\n",
    " # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "        print(sentence, token)\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            if overlapping_factor > 0:\n",
    "                chunk = chunk[-overlapping_factor:]\n",
    "                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])\n",
    "            else:\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of\n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40590ec3-95f7-448f-813a-4d99d54ef28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rinciples of Data Science\n",
      "Second Edition\n",
      "A beginner's guide to statistical techniques and theory to\n",
      "build eﬀective data-driven applications\n",
      "Sinan Ozdemir\n",
      "Sunil Kakade\n",
      "BIRMINGHAM - MUMBAI\n",
      "\n",
      "rinciples of Data Science\n",
      "Second Edition\n",
      "Copyright © 2018 Packt Publishing\n",
      "All rights reserved 74\n",
      "No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form\n",
      "or by any means, without the prior written permission of the publisher, except in the case of brief quotations\n",
      "embedded in critical articles or reviews 49\n",
      "\n",
      "Every effort has been made in the preparation of this book to ensure the accuracy of the information presented 20\n",
      "\n",
      "However, the information contained in this book is sold without warranty, either express or implied 18\n",
      "Neither the\n",
      "authors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to\n",
      "have been caused directly or indirectly by this book 36\n",
      "\n",
      "Packt Publishing has endeavored to provide trademark information about all of the companies and products\n",
      "mentioned in this book by the appropriate use of capitals 29\n",
      "However, Packt Publishing cannot guarantee the accuracy\n",
      "of this information 13\n",
      "\n",
      "Commissioning Editor:  Amey Varangoankar\n",
      "Acquisition Editor:  Dayne Castelino\n",
      "Content Development Editor:  Chris D'cruz\n",
      "Technical Editor:  Sneha Hanchate\n",
      "Copy Editor:  Safis Editing\n",
      "Project Coordinator:  Namrata Swetta\n",
      "Proofreader:  Safis Editing\n",
      "Indexer:  Pratik Shirodkar\n",
      "Graphics:  Tom Scaria\n",
      "Production Coordinator:  Nilesh Mohite\n",
      "First published: December 2016\n",
      "Second edition: November 2018\n",
      "Production reference: 1111218\n",
      "Published by Packt Publishing Ltd 129\n",
      "\n",
      "Livery Place\n",
      "35 Livery Street\n",
      "Birmingham\n",
      "B3 2PB, UK 20\n",
      "\n",
      "ISBN 978-1-78980-454-6\n",
      "www 15\n",
      "packtpub 3\n",
      "com\n",
      "\n",
      "\n",
      "mapt 4\n",
      "io\n",
      "Mapt is an online digital library that gives you full access to over 5,000 books and videos, as\n",
      "well as industry leading tools to help you plan your personal development and advance\n",
      "your career 43\n",
      "For more information, please visit our website 8\n",
      "\n",
      "Why subscribe 3\n",
      "\n",
      "Spend less time learning and more time coding with practical eBooks and Videos\n",
      "from over 4,000 industry professionals\n",
      "Improve your learning with Skill Plans built especially for you\n",
      "Get a free eBook or video every month\n",
      "Mapt is fully searchable\n",
      "Copy and paste, print, and bookmark content\n",
      "Packt 64\n",
      "com\n",
      "Did you know that Packt offers eBook versions of every book published, with PDF and\n",
      "ePub files available 24\n",
      "You can upgrade to the eBook version at www 9\n",
      "packt 2\n",
      "com  and as a print\n",
      "book customer, you are entitled to a discount on the eBook copy 20\n",
      "Get in touch with us at\n",
      "customercare@packtpub 14\n",
      "com  for more details 5\n",
      "\n",
      "At www 3\n",
      "packt 2\n",
      "com , you can also read a collection of free technical articles, sign up for a\n",
      "range of free newsletters, and receive exclusive discounts and offers on Packt books and\n",
      "eBooks 37\n",
      "\n",
      "\n",
      "ontributors\n",
      "About the authors\n",
      "Sinan Ozdemir  is a data scientist, start-up founder, and educator living in the San\n",
      "Francisco Bay Area 35\n",
      "He studied pure mathematics at Johns Hopkins University 8\n",
      "He then\n",
      "spent several years conducting lectures on data science at Johns Hopkins University before\n",
      "founding his own start-up, Kylie 25\n",
      "ai, which uses artificial intelligence to clone brand\n",
      "personalities and automate customer service communications 17\n",
      "\n",
      "Sinan is also the author of Principles of Data Science, First Edition  available through Packt 20\n",
      "\n",
      "Sunil Kakade  is a technologist, educator, and senior leader with expertise in creating data-\n",
      "and AI-driven organizations 26\n",
      "He is in the adjunct faculty at Northwestern University,\n",
      "Evanston, IL, where he teaches graduate courses of data science and big data 27\n",
      "He has\n",
      "several research papers to his credit and has presented his work in big data applications at\n",
      "reputable conferences 25\n",
      "He has US patents in areas of big data and retail processes 12\n",
      "He is\n",
      "passionate about applying data science to improve business outcomes and save patients'\n",
      "lives 19\n",
      "At present, Sunil leads the information architecture and analytics team for a large\n",
      "healthcare organization focused on improving healthcare outcomes and lives with his wife,\n",
      "Pratibha, and daughter, Preeti, in Scottsdale, Arizona 47\n",
      "\n",
      "I would like to thank my mother, Subhadra, wife; Pratibha; and daughter, Preeti, for\n",
      "supporting me during my education and career and for supporting my passion for\n",
      "learning 45\n",
      "Many thanks to my mentors, Prof 7\n",
      "Faisal Akkawi, Northwestern University; Bill\n",
      "Guise, Sr 15\n",
      "Director, Dr 3\n",
      "Joseph Colorafi, CMIO, and Deanna Wise, CIO at Dignity\n",
      "Health for supporting my passion for big data, data science, and artificial intelligence 34\n",
      "\n",
      "Special thanks to Sinan Ozdemir and Packt Publishing for giving me the opportunity to\n",
      "co-author this book 24\n",
      "I appreciate the incredible support of my team at Dignity Health\n",
      "Insights in my journey in data science 22\n",
      "Finally, I'd like to thank my friend, Anand\n",
      "Deshpande, who inspired me to take on this project 26\n",
      "\n",
      "\n",
      "bout the reviewers\n",
      "Oleg Okun  got his PhD from the Institute of Engineering Cybernetics, National Academy\n",
      "of Sciences (Minsk, Belarus) in 1996 37\n",
      "Since 1998 he has worked abroad, doing both academic\n",
      "research (in Belarus and Finland) and industrial research (in Sweden and Germany) 29\n",
      "His\n",
      "research experience includes document image analysis, cancer prediction by analyzing\n",
      "gene expression profiles (bioinformatics), fingerprint verification and identification\n",
      "(biometrics), online and offline marketing analytics, credit scoring (microfinance), and text\n",
      "search and summarization (natural language processing) 54\n",
      "He has 80 publications, including\n",
      "one IGI Global-published book and three co-edited books published by Springer-Verlag, as\n",
      "well as book chapters, journal articles, and numerous conference papers 42\n",
      "He has also been a\n",
      "reviewer of several books published by Packt Publishing 16\n",
      "\n",
      "Jared James Thompson , PhD, is a graduate of Purdue University and has held both\n",
      "academic and industrial appointments teaching programming, algorithms, and big data\n",
      "technology 33\n",
      "He is a machine learning enthusiast and has a particular love of optimization 13\n",
      "\n",
      "Jared is currently employed as a machine learning engineer at Atomwise, a start-up that\n",
      "leverages artificial intelligence to design better drugs faster 29\n",
      "\n",
      "Packt is searching for authors like you\n",
      "If you're interested in becoming an author for Packt, please visit authors 25\n",
      "packtpub 3\n",
      "com\n",
      "and apply today 5\n",
      "We have worked with thousands of developers and tech professionals,\n",
      "just like you, to help them share their insight with the global tech community 26\n",
      "You can\n",
      "make a general application, apply for a specific hot topic that we are recruiting an author\n",
      "for, or submit your own idea 28\n",
      "\n",
      "\n",
      "able of Contents\n",
      "Preface 1\n",
      "Chapter 1: How to Sound Like a Data Scientist 6\n",
      "What is data science 28\n",
      "9\n",
      "Basic terminology 9\n",
      "Why data science 11\n",
      "10\n",
      "Example – xyz123 Technologies 11\n",
      "The data science Venn diagram 12\n",
      "The math 13\n",
      "Example – spawner-recruit models 14\n",
      "Computer programming 16\n",
      "Why Python 42\n",
      "16\n",
      "Python practices 17\n",
      "Example of basic Python 18\n",
      "Example – parsing a single tweet 19\n",
      "Domain knowledge 20\n",
      "Some more terminology 21\n",
      "Data science case studies 22\n",
      "Case study – automating government paper pushing 23\n",
      "Fire all humans, right 58\n",
      "24\n",
      "Case study – marketing dollars 25\n",
      "Case study – what's in a job description 20\n",
      "27\n",
      "Summary 30\n",
      "Chapter 2: Types of Data 32\n",
      "Flavors of data 32\n",
      "Why look at these distinctions 29\n",
      "33\n",
      "Structured versus unstructured data 33\n",
      "Example of data pre-processing 34\n",
      "Word/phrase counts 35\n",
      "Presence of certain special characters 35\n",
      "The relative length of text 35\n",
      "Picking out topics 36\n",
      "Quantitative versus qualitative data 36\n",
      "Example – coffee shop data 37\n",
      "Example – world alcohol consumption data 39\n",
      "Digging deeper 41\n",
      "The road thus far 42\n",
      "The four levels of data 42\n",
      "The nominal level 43\n",
      "Mathematical operations allowed 43\n",
      "\n",
      "Measures of center 43\n",
      "What data is like at the nominal level 44\n",
      "The ordinal level 44\n",
      "Examples 44\n",
      "Mathematical operations allowed 44\n",
      "Measures of center 45\n",
      "Quick recap and check 46\n",
      "The interval level 47\n",
      "Example 47\n",
      "Mathematical operations allowed 48\n",
      "Measures of center 48\n",
      "Measures of variation 49\n",
      "Standard deviation 49\n",
      "The ratio level 51\n",
      "Examples 51\n",
      "Measures of center 51\n",
      "Problems with the ratio level 52\n",
      "Data is in the eye of the beholder 53\n",
      "Summary 53\n",
      "Chapter 3: The Five Steps of Data Science 55\n",
      "Introduction to data science 55\n",
      "Overview of the five steps 56\n",
      "Asking an interesting question 56\n",
      "Obtaining the data 56\n",
      "Exploring the data 56\n",
      "Modeling the data 57\n",
      "Communicating and visualizing the results 57\n",
      "Exploring the data 57\n",
      "Basic questions for data exploration 58\n",
      "Dataset 1 – Yelp 59\n",
      "DataFrames 61\n",
      "Series 62\n",
      "Exploration tips for qualitative data 62\n",
      "Nominal level columns 62\n",
      "Filtering in pandas 64\n",
      "Ordinal level columns 67\n",
      "Dataset 2 – Titanic 68\n",
      "Summary 73\n",
      "Chapter 4: Basic Mathematics 74\n",
      "Mathematics as a discipline 74\n",
      "Basic symbols and terminology 75\n",
      "Vectors and matrices 75\n",
      "Quick exercises 78\n",
      "Answers 78\n",
      "Arithmetic symbols 78\n",
      "Summation 78\n",
      "\n",
      "Proportional 79\n",
      "Dot product 79\n",
      "Graphs 82\n",
      "Logarithms/exponents 83\n",
      "Set theory 86\n",
      "Linear algebra 90\n",
      "Matrix multiplication 90\n",
      "How to multiply matrices 91\n",
      "Summary 94\n",
      "Chapter 5: Impossible or Improbable – A Gentle Introduction to\n",
      "Probability 95\n",
      "Basic definitions 95\n",
      "Probability 96\n",
      "Bayesian versus Frequentist 97\n",
      "Frequentist approach 98\n",
      "The law of large numbers 99\n",
      "Compound events 101\n",
      "Conditional probability 104\n",
      "The rules of probability 104\n",
      "The addition rule 104\n",
      "Mutual exclusivity 106\n",
      "The multiplication rule 107\n",
      "Independence 108\n",
      "Complementary events 108\n",
      "A bit deeper 110\n",
      "Summary 111\n",
      "Chapter 6: Advanced Probability 112\n",
      "Collectively exhaustive events 112\n",
      "Bayesian ideas revisited 113\n",
      "Bayes' theorem 113\n",
      "More applications of Bayes' theorem 117\n",
      "Example – Titanic 117\n",
      "Example  – medical studies 119\n",
      "Random variables 120\n",
      "Discrete random variables 121\n",
      "Types of discrete random variables 127\n",
      "Binomial random variables 127\n",
      "Geometric random variables 129\n",
      "Poisson random variable 131\n",
      "Continuous random variables 133\n",
      "Summary 136\n",
      "Chapter 7: Basic Statistics 137\n",
      "What are statistics 708\n",
      "137\n",
      "How do we obtain and sample data 10\n",
      "139\n",
      "\n",
      "Obtaining data 139\n",
      "Observational 139\n",
      "Experimental 139\n",
      "Sampling data 141\n",
      "Probability sampling 142\n",
      "Random sampling 142\n",
      "Unequal probability sampling 143\n",
      "How do we measure statistics 46\n",
      "144\n",
      "Measures of center 144\n",
      "Measures of variation 145\n",
      "Definition 149\n",
      "Example – employee salaries 150\n",
      "Measures of relative standing 150\n",
      "The insightful part – correlations in data 156\n",
      "The empirical rule 159\n",
      "Summary 161\n",
      "Chapter 8: Advanced Statistics 162\n",
      "Point estimates 162\n",
      "Sampling distributions 167\n",
      "Confidence intervals 170\n",
      "Hypothesis tests 175\n",
      "Conducting a hypothesis test 176\n",
      "One sample t-tests 177\n",
      "Example of a one-sample t-test 178\n",
      "Assumptions of the one-sample t-test 178\n",
      "Type I and type II errors 181\n",
      "Hypothesis testing for categorical variables 182\n",
      "Chi-square goodness of fit test 182\n",
      "Assumptions of the chi-square goodness of fit test 182\n",
      "Example of a chi-square test for goodness of fit 183\n",
      "Chi-square test for association/independence 185\n",
      "Assumptions of the chi-square independence test 185\n",
      "Summary 187\n",
      "Chapter 9: Communicating Data 188\n",
      "Why does communication matter 221\n",
      "189\n",
      "Identifying effective and ineffective visualizations 189\n",
      "Scatter plots 190\n",
      "Line graphs 191\n",
      "Bar charts 193\n",
      "Histograms 195\n",
      "Box plots 197\n",
      "When graphs and statistics lie 201\n",
      "Correlation versus causation 201\n",
      "Simpson's paradox 204\n",
      "If correlation doesn't imply causation, then what does 74\n",
      "206\n",
      "\n",
      "Verbal communication 206\n",
      "It's about telling a story 206\n",
      "On the more formal side of things 207\n",
      "The why/how/what strategy of presenting 208\n",
      "Summary 208\n",
      "Chapter 10: How to Tell If Your Toaster Is Learning – Machine Learning\n",
      "Essentials 210\n",
      "What is machine learning 69\n",
      "211\n",
      "Example – facial recognition 211\n",
      "Machine learning isn't perfect 213\n",
      "How does machine learning work 23\n",
      "214\n",
      "Types of machine learning 214\n",
      "Supervised learning 215\n",
      "Example – heart attack prediction 216\n",
      "It's not only about predictions 218\n",
      "Types of supervised learning 219\n",
      "Regression 219\n",
      "Classification 219\n",
      "Data is in the eyes of the beholder 220\n",
      "Unsupervised learning 221\n",
      "Reinforcement learning 223\n",
      "Overview of the types of machine learning 224\n",
      "How does statistical modeling fit into all of this 93\n",
      "226\n",
      "Linear regression 226\n",
      "Adding more predictors 231\n",
      "Regression metrics 234\n",
      "Logistic regression 240\n",
      "Probability, odds, and log odds 242\n",
      "The math of logistic regression 245\n",
      "Dummy variables 248\n",
      "Summary 253\n",
      "Chapter 11: Predictions Don't Grow on Trees – or Do They 67\n",
      "255\n",
      "Naive Bayes classification 255\n",
      "Decision trees 264\n",
      "How does a computer build a regression tree 24\n",
      "266\n",
      "How does a computer fit a classification tree 11\n",
      "266\n",
      "Unsupervised learning 271\n",
      "When to use unsupervised learning 271\n",
      "k-means clustering 272\n",
      "Illustrative example – data points 274\n",
      "Illustrative example – beer 41\n",
      "279\n",
      "Choosing an optimal number for K and cluster validation 282\n",
      "The Silhouette Coefficient 282\n",
      "Feature extraction and principal component analysis 284\n",
      "\n",
      "Summary 296\n",
      "Chapter 12: Beyond the Essentials 297\n",
      "The bias/variance tradeoff 298\n",
      "Errors due to bias 298\n",
      "Error due to variance 298\n",
      "Example – comparing body and brain weight of mammals 299\n",
      "Two extreme cases of bias/variance tradeoff 308\n",
      "Underfitting 308\n",
      "Overfitting 308\n",
      "How bias/variance play into error functions 309\n",
      "K folds cross-validation 310\n",
      "Grid searching 315\n",
      "Visualizing training error versus cross-validation error 318\n",
      "Ensembling techniques 320\n",
      "Random forests 323\n",
      "Comparing random forests with decision trees 329\n",
      "Neural networks 329\n",
      "Basic structure 330\n",
      "Summary 337\n",
      "Chapter 13: Case Studies 338\n",
      "Case study 1 – Predicting stock prices based on social media 338\n",
      "Text sentiment analysis 338\n",
      "Exploratory data analysis 339\n",
      "Regression route 351\n",
      "Classification route 353\n",
      "Going beyond with this example 355\n",
      "Case study 2 – Why do some people cheat on their spouses 245\n",
      "356\n",
      "Case study 3 – Using TensorFlow 363\n",
      "TensorFlow and neural networks 368\n",
      "Summary 374\n",
      "Chapter 14: Microsoft Azure Databricks 375\n",
      "The Microsoft data science environment 375\n",
      "What exactly are Spark and PySpark 52\n",
      "376\n",
      "Basic Azure Databricks use 376\n",
      "Setting up our first cluster 377\n",
      "Case study 1 – bike-sharing usage prediction using parallelization in Azure\n",
      "Databricks 378\n",
      "Case study 2 – Using MLlib in Azure Databricks to predict credit card fraud 391\n",
      "Using the MLlib Grid Search module to tune hyperparameters 399\n",
      "Case study 3 – Using Azure Databricks to optimize our hyperparameter\n",
      "tuning 403\n",
      "How to add Python libraries to your cluster 403\n",
      "Using spark_sklearn to build an MNIST classifier 404\n",
      "Summary 406\n",
      "\n",
      "Other Books You May Enjoy 407\n",
      "Index 410\n",
      "\n",
      "reface\n",
      "The topic of this book is data science, which is a field of study that has been growing\n",
      "rapidly for the past few decades 167\n",
      "Today, more companies than ever before are investing in\n",
      "big data and data science to improve business performance, drive innovation, and create\n",
      "new revenue streams by building data products 34\n",
      "According to LinkedIn's 2017 US\n",
      "Emerging Jobs Report, machine learning engineer, data scientist, and big data engineer\n",
      "rank among the top emerging jobs, and companies in a wide range of industries are seeking\n",
      "people with the requisite skills for those roles 52\n",
      "\n",
      "We will dive into topics from all three areas  and solve complex problems 15\n",
      "We will clean,\n",
      "explore, and analyze data in order to derive scientific and accurate conclusions 18\n",
      "Machine\n",
      "learning and deep learning techniques will be applied to solve complex data tasks 15\n",
      "\n",
      "Who this book is for\n",
      "This book is for people who are looking to understand and utilize the basic practices of data\n",
      "science for any domain 29\n",
      "The reader should be fairly well acquainted with basic mathematics\n",
      "(algebra, and perhaps probability) and should feel comfortable reading snippets in\n",
      "R/Python as well as pseudo code 34\n",
      "The reader is not expected to have worked in a data field;\n",
      "however, they should have the urge to learn and apply the techniques put forth in this book\n",
      "to either their own datasets or those provided to them 42\n",
      "\n",
      "What this book covers\n",
      "Chapter 1 , How to Sound Like a Data Scientist , introduces the basic terminology used by data\n",
      "scientists and looks at the types of problem we will be solving throughout this book 42\n",
      "\n",
      "Chapter 2 , Types of Data , looks at the different levels and types of data out there and shows\n",
      "how to manipulate each type 28\n",
      "This chapter will begin to deal with the mathematics needed\n",
      "for data science 14\n",
      "\n",
      "Chapter 3 , The Five Steps of Data Science , uncovers the five basic steps of performing data\n",
      "science, including data manipulation and cleaning, and shows examples of each step in\n",
      "detail 39\n",
      "\n",
      "\n",
      "Chapter 4 , Basic Mathematics , explains the basic mathematical principles that guide the\n",
      "actions of data scientists by presenting and solving examples in calculus, linear algebra, and\n",
      "more 35\n",
      "\n",
      "Chapter 5 , Impossible or Improbable – a Gentle Introduction to Probability , is a beginner's guide\n",
      "to probability theory and how it is used to gain an understanding of our random universe 39\n",
      "\n",
      "Chapter 6 , Advanced Probability , uses principles from the previous chapter and introduces\n",
      "and applies theorems, such as Bayes' Theorem, in the hope of uncovering the hidden\n",
      "meaning in our world 44\n",
      "\n",
      "Chapter 7 , Basic Statistics , deals with the types of problem that statistical inference attempts\n",
      "to explain, using the basics of experimentation, normalization, and random sampling 33\n",
      "\n",
      "Chapter 8 , Advanced Statistics , uses hypothesis testing and confidence intervals to gain\n",
      "insight from our experiments 22\n",
      "Being able to pick which test is appropriate and how to\n",
      "interpret p-values and other results is very important as well 23\n",
      "\n",
      "Chapter 9 , Communicating Data , explains how correlation and causation affect our\n",
      "interpretation of data 22\n",
      "We will also be using visualizations in order to share our results with\n",
      "the world 17\n",
      "\n",
      "Chapter 10 , How to Tell Whether Your Toaster Is Learning – Machine Learning Essentials ,\n",
      "focuses on the definition of machine learning and looks at real-life examples of how and\n",
      "when machine learning is applied 42\n",
      "A basic understanding of the relevance of model\n",
      "evaluation is introduced 12\n",
      "\n",
      "Chapter 11 , Predictions Don't Grow on Trees, or Do They 16\n",
      ", looks at more complicated\n",
      "machine learning models, such as decision trees and Bayesian predictions, in order to solve\n",
      "more complex data-related tasks 28\n",
      "\n",
      "Chapter 12 , Beyond the Essentials , introduces some of the mysterious forces guiding data\n",
      "science, including bias and variance 24\n",
      "Neural networks are introduced as a modern deep\n",
      "learning technique 11\n",
      "\n",
      "Chapter 13 , Case Studies , uses an array of case studies in order to solidify the ideas of data\n",
      "science 25\n",
      "We will be following the entire data science workflow from start to finish multiple\n",
      "times for different examples, including stock price prediction and handwriting detection 27\n",
      "\n",
      "Chapter 14 , Microsoft Databricks Case Studies , will harness the power of the Microsoft data\n",
      "environment as well as Apache Spark to put our machine learning in high gear 35\n",
      "This\n",
      "chapter makes use of parallelization and advanced visualization software to get the most\n",
      "out of our data 21\n",
      "\n",
      "\n",
      "To get the most out of this book\n",
      "This book will attempt to bridge the gap between math, programming, and domain\n",
      "expertise 28\n",
      "Most people today have expertise in at least one of these (maybe two), but proper\n",
      "data science requires a little bit of all three 27\n",
      "\n",
      "Download the example code files\n",
      "You can download the example code files for this book from your account at\n",
      "www 23\n",
      "packt 2\n",
      "com 1\n",
      "If you purchased this book elsewhere, you can visit\n",
      "www 12\n",
      "packt 2\n",
      "com/support  and register to have the files emailed directly to you 13\n",
      "\n",
      "You can download the code files by following these steps:\n",
      "Log in or register at www 18\n",
      "packt 2\n",
      "com 1\n",
      "1 2\n",
      "\n",
      "Select the SUPPORT  tab 6\n",
      "2 2\n",
      "\n",
      "Click on Code Downloads & Errata 8\n",
      "3 2\n",
      "\n",
      "Enter the name of the book in the Search  box and follow the onscreen 4 19\n",
      "\n",
      "instructions 2\n",
      "\n",
      "Once the file is downloaded, please make sure that you unzip or extract the folder using the\n",
      "latest version of:\n",
      "WinRAR/7-Zip for Windows\n",
      "Zipeg/iZip/UnRarX for Mac\n",
      "7-Zip/PeaZip for Linux\n",
      "The code bundle for the book is also hosted on GitHub\n",
      "at https://github 71\n",
      "com/PacktPublishing/ Principles-of-Data-Science-Second-\n",
      "Edition 17\n",
      "In case there's an update to the code, it will be updated on the existing GitHub\n",
      "repository 20\n",
      "\n",
      "We also have other code bundles from our rich catalog of books and videos available\n",
      "at https:/ ​/​github 24\n",
      "​com/ ​PacktPublishing/ ​ 10\n",
      "Check them out 3\n",
      "\n",
      "Download the color images\n",
      "We also provide a PDF file that has color images of the screenshots/diagrams used in this\n",
      "book 27\n",
      "You can download it here: https:/ ​/​www 12\n",
      "​packtpub 4\n",
      "​com/ ​sites/ ​default/ ​files/\n",
      "downloads/ ​9781789804546_ ​ColorImages 24\n",
      "​pdf 2\n",
      "\n",
      "\n",
      "Conventions used\n",
      "There are a number of text conventions used throughout this book 16\n",
      "\n",
      "CodeInText : Indicates c ode words in text, database table names, folder names, filenames,\n",
      "file extensions, pathnames, dummy URLs, user input, and Twitter handles 36\n",
      "Here is an\n",
      "example:  \"Mount the downloaded WebStorm-10* 16\n",
      "dmg  disk image file as another disk in\n",
      "your system 12\n",
      "\n",
      "A block of code is set as follows:\n",
      "dict = {\"dog\": \"human's best friend\", \"cat\": \"destroyer of world\"}\n",
      "dict[\"dog\"]# == \"human's best friend\"\n",
      "len(dict[\"cat\"]) # == 18\n",
      "# but if we try to create a pair with the same key as an existing key\n",
      "dict[\"dog\"] = \"Arf\"\n",
      "When we wish to draw your attention to a particular part of a code block, the relevant lines\n",
      "or items are set in bold:\n",
      "def jaccard(user1, user2):\n",
      "  stores_in_common = len(user1 & user2)\n",
      "  stores_all_together = len(user1 | user2)\n",
      "  return stores / float(stores_all_together)\n",
      "Any command-line input or output is written as follows:\n",
      "import numpy as np\n",
      "Bold : Indicates a new term, an important word, or w ords that you see onscreen 188\n",
      "For\n",
      "example, words in menus or dialog boxes appear in the text like this 16\n",
      "Here is an example:\n",
      "\"Select System info  from the Administration  panel 15\n",
      "\"\n",
      "Warnings or important notes appear like this 8\n",
      "\n",
      "Tips and tricks appear like this 7\n",
      "\n",
      "\n",
      "Get in touch\n",
      "Feedback from our readers is always welcome 12\n",
      "\n",
      "General feedback : If you have questions about any aspect of this book, mention the book\n",
      "title in the subject of your message and  email us at customercare@packtpub 38\n",
      "com 1\n",
      "\n",
      "Errata : Although we have taken every care to ensure the accuracy of our content, mistakes\n",
      "do happen 22\n",
      "If you have found a mistake in this book, we would be grateful if you would\n",
      "report this to us 22\n",
      "Please visit www 3\n",
      "packt 2\n",
      "com/submit-errata , selecting your book, clicking\n",
      "on the Errata Submission Form link, and entering the details 25\n",
      "\n",
      "Piracy : If you come across any illegal copies of our works in any form on the Internet, we\n",
      "would be grateful if you would provide us with the location address or website name 39\n",
      "\n",
      "Please contact us at copyright@packt 9\n",
      "com  with a link to the material 8\n",
      "\n",
      "If you are interested in becoming an author : If there is a topic that you have expertise in\n",
      "and you are interested in either writing or contributing to a book, please visit\n",
      "authors 38\n",
      "packtpub 3\n",
      "com 1\n",
      "\n",
      "Reviews\n",
      "Please leave a review 7\n",
      "Once you have read and used this book, why not leave a review on\n",
      "the site that you purchased it from 23\n",
      "Potential readers can then see and use your unbiased\n",
      "opinion to make purchase decisions, we at Packt can understand what you think about our\n",
      "products, and our authors can see your feedback on their book 41\n",
      "Thank you 2\n",
      "\n",
      "For more information about Packt, please visit packt 12\n",
      "com 1\n",
      "\n",
      "\n",
      "\n",
      "How to Sound Like a Data\n",
      "Scientist\n",
      "No matter which industry you work in —IT, fashion, food, or finance —there is no doubt\n",
      "that data affects your life and work 40\n",
      "At some point this week, you will either have or hear a\n",
      "conversation about data 17\n",
      "News outlets are covering more and more stories about data leaks,\n",
      "cybercrimes, and how data can give us a glimpse into our lives 28\n",
      "But why now 3\n",
      "What makes\n",
      "this era such a hotbed of data-related industries 13\n",
      "\n",
      "In the nineteenth century, the world was in the grip of the I ndustrial Age 18\n",
      "Mankind was\n",
      "exploring its place in the industrial world, working with giant mechanical inventions 18\n",
      "\n",
      "Captains of industry, such as Henry Ford, recognized that using these machines could open\n",
      "major market opportunities, enabling industries to achieve previously unimaginable\n",
      "profits 32\n",
      "Of course, the Industrial Age had its pros and cons 11\n",
      "While mass production placed\n",
      "goods in the hands of more consumers, our battle with pollution also began at around this\n",
      "time 24\n",
      "\n",
      "By the twentie th century, we were quite skilled at making huge machines; the goal now\n",
      "was to make them smaller and faster 29\n",
      "The Industrial Age was over and was replaced by\n",
      "what we now refer to as the I nformation Age 21\n",
      "We started using machines to gather and\n",
      "store information (data) about ourselves and our environment for the purpose of\n",
      "understanding our universe 27\n",
      "\n",
      "\n",
      "Beginning in the 1940s, machines such as ENIAC  (considered one of the first —if not the\n",
      "first—computers) were computing math equations and running models and simulations\n",
      "like never before 46\n",
      "The following photograph shows ENIAC:\n",
      "ENIAC —The world's ﬁrst electronic digital computer (Ref: http://ftp 27\n",
      "arl 2\n",
      "mil/ftp/historic-computers/)\n",
      "We finally had a decent lab assistant who could run the numbers better than we could 26\n",
      "As\n",
      "with the Industrial Age, the Information Age brought us both the good and the bad 18\n",
      "The\n",
      "good was the extraordinary works of technology, including mobile phones and televisions 16\n",
      "\n",
      "The bad was not as bad as worldwide pollution, but still left us with a problem in the\n",
      "twenty-fir st century —so much data 30\n",
      "\n",
      "\n",
      "That's right —the Information Age, in its quest to procure data, has exploded the production\n",
      "of electronic data 24\n",
      "Estimates show that we created about 1 8\n",
      "8 trillion gigabytes of data in\n",
      "2011 (take a moment to just think about how much that is) 24\n",
      "Just one year later, in 2012, we\n",
      "created over 2 16\n",
      "8 trillion gigabytes of data 7\n",
      "This number is only going to explode further to\n",
      "hit an estimated 40 trillion gigabytes of created data in just one year by 2020 29\n",
      "People\n",
      "contribute to this every time they tweet, post on Facebook, save a new resume on Microsoft\n",
      "Word, or just send their mom a picture by text message 34\n",
      "\n",
      "Not only are we creating data at an unprecedented rate, but we are also consuming it at an\n",
      "accelerated pace as well 27\n",
      "Just five years ago, in 2013, the average cell phone user used under\n",
      "1 GB of data a month 24\n",
      "Today, that number is estimated to be well over 2 GB a month 15\n",
      "We\n",
      "aren't just looking for the next personality quiz —what we are looking for is insight 19\n",
      "With all\n",
      "of this data out there, some of it has to be useful to me 18\n",
      "And it can be 4\n",
      "\n",
      "So we, in the twenty-fir st century, are left with a problem 17\n",
      "We have so much data and we\n",
      "keep making more 11\n",
      "We have built insanely tiny machines that collect data 24/7, and it's our\n",
      "job to make sense of it all 26\n",
      "Enter the D ata Age 5\n",
      "This is the age when we take machines\n",
      "dreamed up by our nineteenth century ancestors and the data created by our twentieth\n",
      "century counterparts and create insights and sources of knowledge that every human on\n",
      "Earth can benefit from 44\n",
      "The United States created an entirely new role in the government of\n",
      "chief data scientist 16\n",
      "Many companies are now investing in data science departments and\n",
      "hiring data scientists 15\n",
      "The benefit is quite obvious —using data to make accurate predictions\n",
      "and simulations gives us insight into our world like never before 24\n",
      "\n",
      "Sounds great, but what's the catch 9\n",
      "\n",
      "This chapter will explore the terminology and vocabulary of the modern data scientist 14\n",
      "We\n",
      "will learn keywords and phrases that will be essential in our discussion of data science\n",
      "throughout this book 22\n",
      "We will also learn why we use data science and learn about the three\n",
      "key domains that data science is derived from before we begin to look at the code in\n",
      "Python, the primary language used in this book 42\n",
      "This chapter will cover the following\n",
      "topics:\n",
      "The basic terminology of data science\n",
      "The three domains of data science\n",
      "The basic Python syntax\n",
      "\n",
      "What is data science 32\n",
      "\n",
      "Before we go any further, let's look at some  basic definitions that we will use throughout\n",
      "this book 23\n",
      "The great/awful thing about this field is that it is so young that these definitions\n",
      "can differ from textbook to newspaper to whitepaper 28\n",
      "\n",
      "Basic terminology\n",
      "The definitions that follow are general  enough to be used in daily conversations, and work\n",
      "to serve the purpose of this book, an introduction to the principles of data science 38\n",
      "\n",
      "Let's start by defining what data is 9\n",
      "This might seem like a silly first definition to look at,\n",
      "but it is very important 17\n",
      "Whenever we use the word \"data,\" we refer to a collection of\n",
      "information in either an organized  or unorganized  format 26\n",
      "These formats have the\n",
      "following qualities:\n",
      "Organized data : This refers to data  that is sorted into a row/column structure,\n",
      "where every row represents a single observation  and the columns represent the\n",
      "characteristics  of that observation 46\n",
      "\n",
      "Unorganized data : This is the type of data  that is in a free form, usually text or\n",
      "raw audio/signals that must be parsed further to become organized 35\n",
      "\n",
      "Whenever you open Excel (or any other spreadsheet program), you are looking at a blank\n",
      "row/column structure waiting for organized data 26\n",
      "These programs don't do well with\n",
      "unorganized data 11\n",
      "For the most part, we will deal with organized data as it is the easiest to\n",
      "glean insights from, but we will not shy away from looking at raw text and methods of\n",
      "processing unorganized forms of data 44\n",
      "\n",
      "Data science is the art and science of acquiring knowledge through data 13\n",
      "\n",
      "What a small definition for such a big topic, and rightfully so 14\n",
      "Data science covers so many\n",
      "things that it would take pages to list it all out (I should know —I tried and got told to edit it\n",
      "down) 33\n",
      "\n",
      "Data science is all about how we take data, use it to acquire knowledge, and then use that\n",
      "knowledge to do the following:\n",
      "Make decisions\n",
      "Predict the future\n",
      "Understand the past/present\n",
      "Create new industries/products\n",
      "\n",
      "This book is all about the methods of data science, including how to process data, gather\n",
      "insights, and use those insights to make informed decisions and predictions 79\n",
      "\n",
      "Data science is about using data in order to gain new insights that you would otherwise\n",
      "have missed 20\n",
      "\n",
      "As an example, using data science, clinics can identify patients who are likely to not show\n",
      "up for an appointment 24\n",
      "This can help improve margins, and providers can give other\n",
      "patients available slots 15\n",
      "\n",
      "That's why data science won't replace the human brain, but complement it, working\n",
      "alongside it 22\n",
      "Data science should not be thought of as an end-all solution to our data woes;\n",
      "it is merely an opinion —a very informed opinion, but an opinion nonetheless 32\n",
      "It deserves a\n",
      "seat at the table 8\n",
      "\n",
      "Why data science 4\n",
      "\n",
      "In this Data Age, it's clear that we have  a surplus of data 17\n",
      "But why should that necessitate\n",
      "an entirely new set of vocabulary 13\n",
      "What was wrong with our previous forms of analysis 9\n",
      "\n",
      "For one, the sheer volume of data makes it literally impossible for a human to parse it in a\n",
      "reasonable time frame 25\n",
      "Data is collected in various forms and from different sources, and\n",
      "often comes in a very unorganized format 21\n",
      "\n",
      "Data can be missing, incomplete, or just flat out wrong 13\n",
      "Oftentimes, we will have data on\n",
      "very different scales, and that makes it tough to compare it 22\n",
      "Say that we are looking at data\n",
      "in relation to pricing used cars 14\n",
      "One characteristic of a car is the year it was made, and\n",
      "another might be the number of miles on that car 24\n",
      "Once we clean our data (which we will\n",
      "spend a great deal of time looking at in this book), the relationships between the data\n",
      "become more obvious, and the knowledge that was once buried deep in millions of rows of\n",
      "data simply pops out 52\n",
      "One of the main goals of data science is to make explicit practices and\n",
      "procedures to discover and apply these relationships in the data 26\n",
      "\n",
      "Earlier, we looked at data science in a more historical perspective, but let's take a minute to\n",
      "discuss its role in business today using a very simple example 34\n",
      "\n",
      "\n",
      "Example – xyz123 Technologies\n",
      "Ben Runkle, the CEO of xyz123 Technologies,  is trying to solve a huge problem 27\n",
      "The\n",
      "company is consistently losing long-time customers 9\n",
      "He does not know why they are\n",
      "leaving, but he must do something fast 17\n",
      "He is convinced that in order to reduce  his churn,\n",
      "he must create new products and features, and consolidate existing technologies 24\n",
      "To be\n",
      "safe, he calls in his chief data scientist, Dr 14\n",
      "Hughan 2\n",
      "However, she is not convinced that new\n",
      "products and features alone will save the company 17\n",
      "Instead, she turns to the transcripts of\n",
      "recent customer service tickets 13\n",
      "She shows Ben the most recent transcripts and finds\n",
      "something surprising:\n",
      "\" 14\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "Not sure how to export this; are you 9\n",
      "\n",
      "\"Where is the button that makes a new list 11\n",
      "\n",
      "\"Wait, do you even know where the slider is 12\n",
      "\n",
      "\"If I can't figure this out today, it's a real problem 15\n",
      " 1\n",
      " 1\n",
      "\n",
      "It is clear that customers were having problems with the existing UI/UX, and weren't upset\n",
      "because of a lack of features 27\n",
      "Runkle and Hughan organized a mass UI/UX overhaul and\n",
      "their sales have never been better 21\n",
      "\n",
      "Of course, the science  used in the last example was minimal, but it makes a point 20\n",
      "We tend\n",
      "to call people like Runkle drivers 11\n",
      "Today's common stick-to-your-gut CEO wants to make\n",
      "all decisions quickly and iterate over solutions until something works 23\n",
      "Dr 1\n",
      "Hughan is much\n",
      "more analytical 7\n",
      "She wants to solve the problem just as much as Runkle, but she turns to\n",
      "user-generated data instead of her gut feeling for answers 29\n",
      "Data science is about applying\n",
      "the skills of the analytical mind and using them as a driver would 19\n",
      "\n",
      "Both of these mentalities have their place in today's enterprises; however, it is Hughan's\n",
      "way of thinking that dominates the ideas of data science —using data generated by the\n",
      "company as her source of information, rather than just picking up a solution and going\n",
      "with it 58\n",
      "\n",
      "\n",
      "The data science Venn diagram\n",
      "It is a common misconception that only those with a PhD or geniuses can understand the\n",
      "math/programming behind data science 33\n",
      "This is absolutely false 4\n",
      "Understanding data\n",
      "science begins with three basic areas:\n",
      "Math/statistics : This is the use of equations and formulas to perform analysis 25\n",
      "\n",
      "Computer programming : This is the ability to use code to create outcomes on a\n",
      "computer 18\n",
      "\n",
      "Domain knowledge : This refers  to understanding the problem domain\n",
      "(medicine, finance, social science, and so on) 25\n",
      "\n",
      "The following Venn diagram provides a visual representation of how these three areas of\n",
      "data science intersect:\n",
      "The Venn diagram of data science\n",
      "Those with hacking skills can conceptualize and program complicated algorithms using\n",
      "computer languages 44\n",
      "Having a math and statistics background allows you to theorize and\n",
      "evaluate algorithms and tweak the existing procedures to fit specific situations 24\n",
      "Having\n",
      "substantive expertise (domain expertise) allows you to apply concepts and results in a\n",
      "meaningful and effective way 25\n",
      "\n",
      "\n",
      "While having only two of these three qualities can make you intelligent, it will also leave a\n",
      "gap 21\n",
      "Let's say that you are very skilled in coding and have formal training in day trading 17\n",
      "\n",
      "You might create an automated system to trade in your place, but lack the math skills to\n",
      "evaluate your algorithms 23\n",
      "This will mean that you end up losing money in the long run 13\n",
      "It\n",
      "is only when you boost your skills in coding, math, and domain knowledge that you can\n",
      "truly perform data science 26\n",
      "\n",
      "The quality that was probably a surprise  for you was domain knowledge 14\n",
      "It is really just\n",
      "knowledge of the area you are working in 13\n",
      "If a financial analyst started analyzing data\n",
      "about heart attacks, they might need the help of a cardiologist to make sense of a lot of the\n",
      "numbers 31\n",
      "\n",
      "Data science  is the intersection of the three key areas mentioned earlier 14\n",
      "In order to gain\n",
      "knowledge from data, we must be able to utilize computer programming to access the data,\n",
      "understand the mathematics behind the models we derive, and, above all, understand our\n",
      "analyses' place in the domain we are in 50\n",
      "This includes the presentation of data 6\n",
      "If we are\n",
      "creating a model to predict heart attacks in patients, is it better to create a PDF of\n",
      "information, or an app where you can type in numbers and get a quick prediction 39\n",
      "All\n",
      "these decisions must be made by the data scientist 11\n",
      "\n",
      "The intersection of math and coding is machine learning 10\n",
      "This book will\n",
      "look at machine learning in great detail later on, but it is important to note\n",
      "that without the explicit ability to generalize any models or results to a\n",
      "domain, machine learning algorithms remain just that —algorithms sitting\n",
      "on your computer 51\n",
      "You might have the best algorithm to predict cancer 9\n",
      "\n",
      "You could be able to predict cancer with over 99% accuracy based on past\n",
      "cancer patient data, but if you don't understand how to apply this model\n",
      "in a practical sense so that doctors and nurses can easily use it, your\n",
      "model might be useless 55\n",
      "\n",
      "Both computer programming and math are covered extensively in this book 12\n",
      "Domain\n",
      "knowledge comes with both the practice of data science and reading examples of other\n",
      "people's analyses 20\n",
      "\n",
      "The math\n",
      "Most people stop listening once  someone says the word \" math 16\n",
      "They'll nod along in an\n",
      "attempt to hide their utter disdain for the topic 16\n",
      "This book will guide you through the math\n",
      "needed for data science, specifically statistics and probability 18\n",
      "We will use these\n",
      "subdomains of mathematics to create what are called models 15\n",
      "\n",
      "\n",
      "A data model  refers to an organized and formal relationship between elements of data,\n",
      "usually meant to simulate a real-world phenomenon 25\n",
      "\n",
      "Essentially, we will use math  in order to formalize relationships between variables 17\n",
      "As a\n",
      "former pure mathematician and current math teacher, I know how difficult this can be 19\n",
      "I\n",
      "will do my best to explain everything as clearly as I can 14\n",
      "Between the three areas of data\n",
      "science, math is what allows us to move from domain to domain 20\n",
      "Understanding the theory\n",
      "allows us to apply a model that we built for the fashion industry to a financial domain 21\n",
      "\n",
      "The math covered in this book ranges from basic algebra to advanced probabilistic and\n",
      "statistical modeling 20\n",
      "Do not skip over these chapters, even if you already know these topics\n",
      "or you're afraid of them 21\n",
      "Every mathematical concept that I will introduce will be\n",
      "introduced with care and purpose, using examples 19\n",
      "The math in this book is essential for\n",
      "data scientists 11\n",
      "\n",
      "Example – spawner-recruit models\n",
      "In biology, we use, among many  other models, a model known as the spawner-recruit\n",
      "model to judge the biological health of a species 40\n",
      "It is a basic relationship between the\n",
      "number of healthy parental units of a species and the number of new units in the group of\n",
      "animals 28\n",
      "In a public dataset of the number of salmon spawners and recruits, the graph\n",
      "further down (titled spawner-recruit model)  was formed to visualize the relationship\n",
      "between the two 41\n",
      "We can see that there definitely is some sort of positive relationship (as\n",
      "one goes up, so does the other) 24\n",
      "But how can we formalize this relationship 8\n",
      "For example, if\n",
      "we knew the number of spawners in a population, could we predict the number of recruits\n",
      "that the group would obtain, and vice versa 34\n",
      "\n",
      "Essentially, models allow us to plug in one variable to get the other 16\n",
      "Consider the following\n",
      "example :\n",
      "In this example, let's say we knew that a group of salmon had 1 23\n",
      "15 (in thousands) spawners 9\n",
      "\n",
      "Then, we would have the following:\n",
      " (in thousands)\n",
      "\n",
      "\n",
      "This result can be very beneficial to estimate how the health of a population is changing 29\n",
      "If\n",
      "we can create these models, we can visually observe how the relationship between the two\n",
      "variables can change 22\n",
      "\n",
      "There are many types of data models, including probabilistic and statistical models 15\n",
      "Both of\n",
      "these are subsets  of a larger paradigm, called machine learning 15\n",
      "The essential idea behind\n",
      "these three topics is that we use data in order to come up with the best model possible 23\n",
      "We\n",
      "no longer rely on human instincts —rather, we rely on data, such as that displayed in the\n",
      "following graph:\n",
      "The spawner-recruit model visualized\n",
      "The purpose of this example is to show how we can define relationships between data\n",
      "elements using mathematical equations 55\n",
      "The fact that I used salmon health data was\n",
      "irrelevant 12\n",
      "Throughout this book, we will look at relationships involving marketing dollars,\n",
      "sentiment data, restaurant reviews, and much more 23\n",
      "The main reason for this is that I\n",
      "would like you (the reader) to be exposed to as many domains as possible 25\n",
      "\n",
      "Math and coding are vehicles that allow data scientists to step back and apply their skills\n",
      "virtually anywhere 21\n",
      "\n",
      "\n",
      "Computer programming\n",
      "Let's be honest: you probably think  computer science is way cooler than math 20\n",
      "That's ok, I\n",
      "don't blame you 10\n",
      "The news isn't filled with math news like it is with news on technology 15\n",
      "\n",
      "You don't turn on the TV to see a new theory on primes —rather, you will see investigative\n",
      "reports on how the latest smartphone can take better  photos of cats, or something 39\n",
      "\n",
      "Computer languages are how we communicate with machines and tell them to do our\n",
      "bidding 18\n",
      "A computer speaks many languages and, like a book, can be written in many\n",
      "languages; similarly, data science can also be done in many languages 30\n",
      "Python,  Julia,  and R\n",
      "are some of the many languages that are available to us 20\n",
      "This book will focus exclusively on\n",
      "using Python 9\n",
      "\n",
      "Why Python 3\n",
      "\n",
      "We will use Python  for a variety  of reasons, listed as follows:\n",
      "Python is an extremely simple language to read and write, even if you've never\n",
      "coded before, which will make future examples easy to understand and read\n",
      "later on, even after you have read this book 58\n",
      "\n",
      "It is one of the most common languages, both in production and in the academic\n",
      "setting (one of the fastest growing, as a matter of fact) 32\n",
      "\n",
      "The language's online community is vast and friendly 10\n",
      "This means that a quick\n",
      "search for the solution to a problem should yield many people who have faced\n",
      "and solved similar (if not exactly the same) situations\n",
      "Python has prebuilt data science modules that both the novice and the veteran\n",
      "data scientist can utilize 52\n",
      "\n",
      "The last point is probably the biggest reason we will focus on Python 14\n",
      "These prebuilt\n",
      "modules are not only powerful, but also easy to pick up 16\n",
      "By the end of the first few\n",
      "chapters, you will be very comfortable with these modules 19\n",
      "Some of these modules include\n",
      "the following:\n",
      "pandas\n",
      "scikit-learn\n",
      "seaborn\n",
      "numpy/scipy\n",
      "requests  (to mine data from the web)\n",
      "BeautifulSoup  (for web –HTML parsing)\n",
      "\n",
      "Python practices\n",
      "Before we move on, it is important  to formalize many of the requisite coding skills in\n",
      "Python 69\n",
      "\n",
      "In Python, we have variables  that are placeholders for objects 13\n",
      "We will focus on just a few\n",
      "types of basic objects at first, as shown in the following table :\n",
      "Object Type Example\n",
      "int  (an integer) 3, 6, 99, -34, 34, 11111111\n",
      "float  (a decimal) 3 60\n",
      "14159, 2 6\n",
      "71, -0 5\n",
      "34567\n",
      "boolean  (either  True  or False )• The statement \"Sunday is a weekend\" is True\n",
      "• The statement \"Friday is a weekend\" is False\n",
      "• The statement \"pi is exactly the ratio of a circle's circumference to its\n",
      "diameter\" is True  (crazy, right 66\n",
      "\n",
      "string  (text or words made up of\n",
      "characters)\"I love hamburgers\" ( by the way, who doesn't 26\n",
      ")\n",
      "\"Matt is awesome\"\n",
      "A tweet is a string\n",
      "list  (a collection of objects) [1, 5 25\n",
      "4, True, \"apple\"]\n",
      "We will also have to understand some basic logistical operators 18\n",
      "For these operators, keep\n",
      "the Boolean datatype in mind 11\n",
      "Every operator will evaluate to either True  or False 10\n",
      "Let's\n",
      "take a look at the following operators :\n",
      "Operators Example\n",
      "==Evaluates to  True  if both sides are equal; otherwise, it evaluates to\n",
      "False , as shown in the following examples:\n",
      "• 3 + 4 == 7 (will evaluate to True )\n",
      "• 3 - 2 == 7 (will evaluate to False )\n",
      "< (less than)• 3 < 5 (True )\n",
      "• 5 < 3 (False )\n",
      "\n",
      "<= (less than or equal to)• 3 <= 3 (True )\n",
      "• 5 <= 3 (False )\n",
      "> (greater than)• 3 > 5 (Fals e)\n",
      "• 5 > 3 (True )\n",
      ">= (greater than or equal to)• 3 >= 3 (True )\n",
      "• 5 >= 7 (False )\n",
      "When coding in Python, I will use a pound sign ( #) to create a \"comment,\" which will not\n",
      "be processed as code, but is merely there to communicate with the reader 211\n",
      "Anything to the\n",
      "right of a  # sign is a comment on the code being executed 18\n",
      "\n",
      "Example of basic Python\n",
      "In Python, we use spaces/tabs to denote  operations that belong to other lines of code 25\n",
      "\n",
      "The print True  statement belongs to the if x + y == 15 16\n",
      "3:  line\n",
      "preceding it because it is tabbed right under it 17\n",
      "This means that the print\n",
      "statement will be executed if, and only if, x + y  equals 15 23\n",
      "3 2\n",
      "\n",
      "Note that the following list variable, my_list , can hold multiple types of objects 17\n",
      "This one\n",
      "has an int, a float , a boolean , and string  inputs (in that order):\n",
      "my_list = [1, 5 30\n",
      "7, True, \"apples\"]\n",
      "len(my_list) == 4  # 4 objects in the list\n",
      "my_list[0] == 1    # the first object\n",
      "my_list[1] == 5 47\n",
      "7    # the second object\n",
      "\n",
      "In the preceding code, I used the  len command to get the length of the list (which was 4) 32\n",
      "\n",
      "Also, n ote the zero-indexing of Python 12\n",
      "Most computer languages start counting at zero\n",
      "instead of one 11\n",
      "So if I want the first element, I call index 0, and if I want the 95th  element, I\n",
      "call index 94 31\n",
      "\n",
      "Example – parsing a single tweet\n",
      "Here is some more Python code 14\n",
      "In this example, I will be parsing some tweets about stock\n",
      "prices (one of the important case studies in this book will be trying to predict market\n",
      "movements based  on popular sentiment regarding stocks on social media):\n",
      "tweet = \"RT @j_o_n_dnger: $TWTR now top holding for Andor, unseating $AAPL\"\n",
      "words_in_tweet = tweet 77\n",
      "split(' ') # list of words in tweet\n",
      "for word in words_in_tweet:             # for each word in list\n",
      "  if \"$\" in word:                       # if word has a \"cashtag\"\n",
      "  print(\"THIS TWEET IS ABOUT\", word)  # alert the user\n",
      "I will point out a few things about this code snippet line by line, as follows:\n",
      "First, we set a variable to hold some text (known as a string in Python) 96\n",
      "In this\n",
      "example, the tweet in question is \"RT @robdv: $TWTR now top holding for\n",
      "Andor, unseating $AAPL \" 35\n",
      "\n",
      "The words_in_tweet  variable tokenizes  the tweet (separates it by word) 20\n",
      "If you\n",
      "were to print this variable, you would see the following:\n",
      "['RT',\n",
      "'@robdv:',\n",
      "'$TWTR',\n",
      "'now',\n",
      "'top',\n",
      "'holding',\n",
      "'for',\n",
      "'Andor,',\n",
      "'unseating',\n",
      "'$AAPL']\n",
      "We iterate through this list of words; this is called a for loop 67\n",
      "It just means that\n",
      "we go through a list one by one 13\n",
      "\n",
      "\n",
      "Here, we have another if statement 8\n",
      "For each word in this tweet, if the word\n",
      "contains the $ character which represents stock tickers on Twitter 22\n",
      "\n",
      "If the preceding if statement is True  (that is, if the tweet contains a cashtag),\n",
      "print it and show it to the user 29\n",
      "\n",
      "The output of this code will be as follows:\n",
      "THIS TWEET IS ABOUT $TWTR\n",
      "THIS TWEET IS ABOUT $AAPL\n",
      "We get this output as these are the only words in the tweet that use the cashtag 49\n",
      "Whenever I\n",
      "use Python in this book, I will ensure that I am as explicit as possible about what I am\n",
      "doing in each line of code 30\n",
      "\n",
      "Domain knowledge\n",
      "As I mentioned earlier, domain knowledge focuses  mainly on having knowledge of the\n",
      "particular topic you are working on 27\n",
      "For example, if you are a financial analyst working on\n",
      "stock market data, you have a lot of domain knowledge 23\n",
      "If you are a journalist looking at\n",
      "worldwide adoption rates, you might benefit from  consulting an expert in the field 24\n",
      "This\n",
      "book will attempt to show examples from several problem domains, including medicine,\n",
      "marketing, finance, and even UFO sightings 24\n",
      "\n",
      "Does this mean that if you're not a doctor, you can't work with medical data 19\n",
      "Of course not 3\n",
      "\n",
      "Great data scientists can apply their skills to any area, even if they aren't fluent in it 20\n",
      "Data\n",
      "scientists can adapt to the field and contribute meaningfully when their analysis is\n",
      "complete 19\n",
      "\n",
      "A big part of domain knowledge is a presentation 10\n",
      "Depending on your audience, it can\n",
      "matter greatly on how you present your findings 16\n",
      "Your results are only as good as your\n",
      "vehicle of communication 12\n",
      "You can predict the movement of the market with 99 11\n",
      "99%\n",
      "accuracy, but if your program is impossible to execute, your results will go unused 19\n",
      "\n",
      "Likewise, if your vehicle is inappropriate for the field, your results will go equally unused 19\n",
      "\n",
      "\n",
      "Some more terminology\n",
      "This is a good time to define some more vocabulary 15\n",
      "By this point, you're probably\n",
      "excitedly looking up a lot of data  science material and seeing words and phrases I haven't\n",
      "used yet 31\n",
      "Here are some common terms that you are likely to encounter 11\n",
      "\n",
      "Machine learning : This refers to giving computers the ability to learn from data\n",
      "without explicit \"rules\" being given by a programmer 26\n",
      "We have seen the concept\n",
      "of machine learning earlier in this chapter as the union of someone who has both\n",
      "coding and math skills 26\n",
      "Here, we are attempting to formalize this definition 10\n",
      "\n",
      "Machine learning combines the power of computers with intelligent learning\n",
      "algorithms in order to automate the discovery of relationships in data and create\n",
      "powerful data models 31\n",
      "Speaking of data models, in this book, we will concern\n",
      "ourselves with the following two basic types of data model:\n",
      "Probabilistic model : This refers to using  probability to find a\n",
      "relationship between elements that includes a degree of\n",
      "randomness 51\n",
      "\n",
      "Statistical model : This refers to taking  advantage of statistical\n",
      "theorems to formalize relationships between data elements in a\n",
      "(usually) simple mathematical formula 33\n",
      "\n",
      "While both the statistical and probabilistic models\n",
      "can be run on computers and might be considered\n",
      "machine learning in that regard, we will keep these\n",
      "definitions separate, since machine learning\n",
      "algorithms generally attempt to learn  relationships\n",
      "in different ways 50\n",
      "We will take a look at the\n",
      "statistical and probabilistic models in later chapters 17\n",
      "\n",
      "Exploratory data analysis  (EDA ): This refers to preparing data in order to\n",
      "standardize results and gain quick insights 26\n",
      "EDA is concerned with data\n",
      "visualization and preparation 10\n",
      "This is where we turn unorganized data into\n",
      "organized data and clean up missing/incorrect data points 20\n",
      "During EDA, we will\n",
      "create many types of plots and use these plots to identify key features and\n",
      "relationships to exploit in our data models 29\n",
      "\n",
      "Data mining:  This is the process of finding  relationships between elements of\n",
      "data 18\n",
      "Data mining is the part of data science where we try to find relationships\n",
      "between variables (think the spawn-recruit model) 25\n",
      "\n",
      "I have tried pretty hard  not to use the term big data  up until now 18\n",
      "This is because I think\n",
      "this term is misused, a lot 14\n",
      "Big data is data that is too large to be processed by a single\n",
      "machine (if your laptop crashed, it might be suffering from a case of big data) 33\n",
      "\n",
      "\n",
      "The following diagram shows the relationship between these data science concepts:\n",
      "The state of data science (so far)\n",
      "The preceding diagram is incomplete and is meant for visualization purposes only 34\n",
      "\n",
      "Data science case studies\n",
      "The combination of math, computer programming, and domain knowledge is what makes\n",
      "data science so powerful 25\n",
      "Oftentimes, it is difficult for a single person to master all three  of\n",
      "these areas 20\n",
      "That's why it's very common for companies to hire teams of data scientists\n",
      "instead of a single person 21\n",
      "Let's look at a few powerful examples of data science in action\n",
      "and their outcomes 17\n",
      "\n",
      "\n",
      "Case study – automating government paper\n",
      "pushing\n",
      "Social security claims are known  to be a major hassle for both the agent reading it and the\n",
      "person who wrote the claim 37\n",
      "Some claims take over two years to get resolved in their\n",
      "entirety, and that's absurd 20\n",
      "Let's look at the following diagram, which shows what goes into\n",
      "a claim:\n",
      "Sample social security form\n",
      "\n",
      "Not bad 24\n",
      "It's mostly just text, though 7\n",
      "Fill this in, then that, then this, and so on 13\n",
      "You can see\n",
      "how it would be difficult for an agent to read these all day, form after form 21\n",
      "There must be a\n",
      "better way 7\n",
      "\n",
      "Well, there is 5\n",
      "Elder Research Inc 3\n",
      "parsed this unorganized data and was able to automate\n",
      "20% of all disability social security forms 19\n",
      "This means that a computer could look at 20% of\n",
      "these written forms and give its opinion on the approval 23\n",
      "\n",
      "Not only that —the third-party company that is hired to rate the approvals of the forms\n",
      "actually gave the machine-graded forms a higher grade than the human forms 34\n",
      "So not only\n",
      "did the computer handle 20% of the load on average, it also did better than a human 24\n",
      "\n",
      "Fire all humans, right 6\n",
      "\n",
      "Before I get a load of angry emails  claiming that data science is bringing about the end of\n",
      "human workers, keep in mind that the computer was only  able to handle 20% of the load 42\n",
      "\n",
      "This means that it probably performed terribly on 80% of the forms 15\n",
      "This is because the\n",
      "computer was probably great at simple forms 12\n",
      "The claims that would have taken a human\n",
      "minutes to compute took the computer seconds 16\n",
      "But these minutes add up, and before you\n",
      "know it, each human is being saved over an hour a day 23\n",
      "\n",
      "Forms that might be easy for a human to read are also likely easy for the computer 18\n",
      "It's\n",
      "when the forms are very terse, or when the writer starts deviating from the usual grammar,\n",
      "that the computer starts to fail 28\n",
      "This model is great because it lets the humans spend more\n",
      "time on those difficult claims and gives them more attention without getting distracted by\n",
      "the sheer volume of papers 32\n",
      "\n",
      "Note that I used the word \"model 9\n",
      "Remember that a model is a\n",
      "relationship between elements 10\n",
      "In this case, the relationship is between\n",
      "written words and the approval status of a claim 18\n",
      "\n",
      "\n",
      "Case study – marketing dollars\n",
      "A dataset shows the relationships between TV, radio, and newspaper sales 20\n",
      "The goal is to\n",
      "analyze the relationships between the three different marketing mediums and how they \n",
      "affect  the sale of a product 26\n",
      "In this case, our data is displayed in the form of a table 14\n",
      "Each\n",
      "row represents a sales region, and the columns tell us how much money was spent on each\n",
      "medium, as well as the profit that was gained in that region 34\n",
      "F or example, from the\n",
      "following table, we can see that in the third region, we spent $17,200 on TV advertising and\n",
      "sold 9,300 widgets:\n",
      "Usually, the data scientist must ask for units and the scale 49\n",
      "In this case, I\n",
      "will tell you that the TV, radio, and newspaper categories are measured in\n",
      "\"thousands of dollars\" and the sales in \"thousands of widgets sold 38\n",
      "This\n",
      "means that in the first region, $230,100 was spent on TV advertising,\n",
      "$37,800 on radio advertising, and $69,200 on newspaper advertising 35\n",
      "In\n",
      "the same region, 22,100 items were sold 13\n",
      "\n",
      "Advertising budgets' data\n",
      "If we plot each variable against the sales, we get the following graph:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "%matplotlib inline\n",
      "data = pd 38\n",
      "read_csv('http://www-bcf 8\n",
      "usc 2\n",
      "edu/~gareth/ISL/Advertising 9\n",
      "csv',\n",
      "index_col=0)\n",
      "data 8\n",
      "head()\n",
      "sns 3\n",
      "pairplot(data, x_vars=['TV','radio','newspaper'], y_vars='sales',\n",
      "height=4 23\n",
      "5, aspect=0 6\n",
      "7)\n",
      "\n",
      "\n",
      "Results – Graphs of advertising budgets\n",
      "Note how none of these variables form a very strong line, and that therefore they might not\n",
      "work well in predicting sales on their own 38\n",
      "TV comes closest in forming an obvious\n",
      "relationship, but even that isn't great 16\n",
      "In this case, we will have to create a more complex\n",
      "model than the one we used in the spawner-recruiter model and combine all three variables\n",
      "in order to model sales 38\n",
      "\n",
      "This type of problem is very common in data science 11\n",
      "In this example, we are attempting to\n",
      "identify key features that are associated with the sales of a product 21\n",
      "If we can isolate these\n",
      "key features, then we can exploit these relationships and change how much we spend on\n",
      "advertising in different places with the hope of increasing our sales 35\n",
      "\n",
      "\n",
      "Case study – what's in a job description 10\n",
      "\n",
      "Looking for a job in data  science 9\n",
      "Great 1\n",
      "Let me help 3\n",
      "In this case study, I have \"scraped\"\n",
      "(taken from the web) 1,000 job descriptions for companies that are actively hiring data\n",
      "scientists 34\n",
      "The goal here is to look at some of the most common keywords that people use in\n",
      "their job descriptions, as shown in the following screenshot:\n",
      "An example of data scientist job listings 36\n",
      "\n",
      "Note the second one asking for core Python libraries; we will talk about\n",
      "these later on in this book 22\n",
      "\n",
      "\n",
      "In the following Python code, the first two imports are used to grab web data from the\n",
      "website http:/ ​/​indeed 28\n",
      "​com/ ​, and the third import is meant to simply count the number of\n",
      "times a word or phrase appears, as shown in the following code:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from sklearn 43\n",
      "feature_extraction 2\n",
      "text import CountVectorizer\n",
      "# grab postings from the web\n",
      "texts = []\n",
      "for i in range(0,1000,10): # cycle through 100 pages  of indeed job\n",
      "resources\n",
      " soup =\n",
      "BeautifulSoup(requests 46\n",
      "get('http://www 5\n",
      "indeed 1\n",
      "com/jobs 3\n",
      "q=data+scientist&sta\n",
      "rt='+str(i)) 13\n",
      "text)\n",
      " texts += [a 6\n",
      "text for a in soup 5\n",
      "findAll('span', {'class':'summary'})]\n",
      "print(type(texts))\n",
      "print(texts[0]) # first job description\n",
      "Okay, before I lose you, all that this loop is doing is going through 100 pages  of job\n",
      "descriptions, and for each page, grabbing each job description 61\n",
      "The important variable here\n",
      "is texts , which is a list of over 1,000 job descriptions, as shown in the following code:\n",
      "type(texts) # == list\n",
      "vect = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
      "# Get basic counts of one and two word phrases\n",
      "matrix = vect 67\n",
      "fit_transform(texts)\n",
      "# fit and learn to the vocabulary in the corpus\n",
      "print len(vect 20\n",
      "get_feature_names())  # how many features are there\n",
      "# There are 10,587 total one and two words phrases in my case 28\n",
      " 1\n",
      "\n",
      "Since web pages are scraped in real-time and these pages may change since\n",
      "this code is run, you may get different number than 10587 30\n",
      "\n",
      "\n",
      "I have omitted some code here, but it exists in the GitHub repository for this book 18\n",
      "The\n",
      "results are as follows (represented as the phrase and then the number of times it occurred ):\n",
      "\\\n",
      "The following list shows some  things that we should mention:\n",
      "\"Machine learning\" and \"experience\" are at the top of the list 48\n",
      "Experience comes\n",
      "with practice 5\n",
      "A basic idea of machine learning comes with this book 10\n",
      "\n",
      "These words are followed closely by statistical words implying a knowledge of\n",
      "math and theory 17\n",
      "\n",
      "The word \" team\"  is very high up, implying that you will need to work with a\n",
      "team of data scientists; you won't be a lone wolf 34\n",
      "\n",
      "Computer science words such as \" algorithms\"  and \" programming\"  are prevalent 17\n",
      "\n",
      "The words \" techniques\" , \"understanding\" , and \" methods\"  imply a more\n",
      "theoretical approach, unrelated to any single domain 30\n",
      "\n",
      "The word \" business\"  implies a particular problem domain 12\n",
      "\n",
      "\n",
      "There are many interesting things to note about this case study, but the biggest take away is\n",
      "that there are many keywords and phrases that make up a data science role 34\n",
      "It isn't just\n",
      "math, coding, or domain knowledge; it truly is a combination of these three ideas (whether\n",
      "exemplified in a single person or across a multiperson team) that makes data science\n",
      "possible and powerful 47\n",
      "\n",
      "Summary\n",
      "At the beginning of this chapter, I posted a simple question: what's the catch of data\n",
      "science 24\n",
      "Well, there is one 5\n",
      "It isn't all fun, games, and modeling 10\n",
      "There must be a price for\n",
      "our quest to create ever-smarter machines and algorithms 17\n",
      "As we seek new and innovative\n",
      "ways to discover data trends, a beast lurks in the shadows 20\n",
      "I'm not talking about the\n",
      "learning curve of mathematics or programming, nor am I referring to the surplus of data 23\n",
      "\n",
      "The Industrial Age left us with an ongoing battle against pollution 12\n",
      "The subsequent\n",
      "Information Age left behind a trail of big data 12\n",
      "So, what dangers might the Data Age bring\n",
      "us 11\n",
      "\n",
      "The Data Age can lead to something much more sinister —the dehumanization of the\n",
      "individual through mass data 23\n",
      "\n",
      "More and more people are jumping head first into the field of data science, most with no\n",
      "prior experience of math or CS, which, on the surface, is great 35\n",
      "Average data scientists have\n",
      "access to millions of dating profiles' data, tweets, online reviews, and much more in order\n",
      "to jump start their education 30\n",
      "\n",
      "However, if you jump into data science without the proper exposure to theory or coding\n",
      "practices, and without respect for the domain you are working in, you face the risk of\n",
      "oversimplifying the very phenomenon you are trying to model 49\n",
      "\n",
      "For example, let's say you want to automate your sales pipeline by building a simplistic\n",
      "program that looks at LinkedIn for very specific keywords in a person's LinkedIn profile 34\n",
      "\n",
      "You could use the following code to do this:\n",
      "keywords = [\"Saas\", \"Sales\", \"Enterprise\"]\n",
      "Great 24\n",
      "Now you can scan LinkedIn quickly to find people who match your criteria 13\n",
      "But\n",
      "what about that person who spells out \"Software as a Service\", instead of \"SaaS,\" or\n",
      "misspells \"enterprise\" (it happens to the best of us; I bet someone will find a typo in my\n",
      "book) 50\n",
      "How will your model figure out that these people are also a good match 14\n",
      "They\n",
      "should not be left behind just because the cut-corners data scientist has overgeneralized\n",
      "people in such an easy way 26\n",
      "\n",
      "\n",
      "The programmer chose to simplify their search for another human by looking for three\n",
      "basic keywords and ended up with a lot of missed opportunities left on the table 31\n",
      "\n",
      "In the next chapter, we will explore the different types of data that exist in the world,\n",
      "ranging from free-form text to highly structured row/column files 32\n",
      "We will also look at the\n",
      "mathematical operations that are allowed for different types of data, as well as deduce\n",
      "insights based on the form of the data that comes in 38\n",
      "\n",
      "\n",
      "\n",
      "Types of Data\n",
      "Now that we have a basic introduction to the world of data science and understand why\n",
      "the field is so important, let's take a look at the various ways in which data can be formed 43\n",
      "\n",
      "Specifically, in this chapter, we will look at the following topics:\n",
      "Structured versus unstructured data\n",
      "Quantitative versus qualitative data\n",
      "The four levels of data\n",
      "We will dive further into each of these topics by showing examples of how data scientists\n",
      "look at and work with data 57\n",
      "The aim of this chapter is to familiarize ourselves with the\n",
      "fundamental ideas underpinning data science 21\n",
      "\n",
      "Flavors of data\n",
      "In the field, it is important to understand the different flavors of data for several reasons 23\n",
      "\n",
      "Not only will the type of data  dictate the methods used to analyze and extract results, but\n",
      "knowing whether the data is unstructured, or perhaps quantitative, can also tell you a lot\n",
      "about the real-world phenomenon being measured 48\n",
      "\n",
      "The first thing to note is my use of the word data 13\n",
      "In the last chapter, I defined data as\n",
      "merely being a collection of information 17\n",
      "This vague definition exists because we may\n",
      "separate data into different categories and need our definition to be loose 21\n",
      "\n",
      "The next thing to remember while we go through this chapter is that for the most part,\n",
      "when I talk about the type of data I will refer to either a specific characteristic  of a dataset or\n",
      "to the entire dataset  as a whole 49\n",
      "I will be very clear about which one I refer to at any given\n",
      "time 16\n",
      "\n",
      "\n",
      "Why look at these distinctions 6\n",
      "\n",
      "It might seem worthless to stop and think about what type of data we have before getting\n",
      "into the fun stuff, such as statistics and machine learning, but this is arguably one of the\n",
      "most important steps you need to take to perform data science 50\n",
      "\n",
      "The same principle applies to data  science 9\n",
      "When given a dataset, it is tempting to jump\n",
      "right into exploring, applying statistical models, and researching the applications of\n",
      "machine learning in order to get results faster 33\n",
      "However, if you don't understand the type\n",
      "of data that you are working with, then you might waste a lot of time applying models that\n",
      "are known to be ineffective with that specific type of data 41\n",
      "\n",
      "When given a new dataset, I always recommend taking about an hour (usually less) to\n",
      "make the distinctions mentioned in the following sections 28\n",
      "\n",
      "Structured versus unstructured data\n",
      "The distinction between structured  and unstructured data  is usually the first question you\n",
      "want to ask yourself about the entire  dataset 33\n",
      "The answer to this question can mean the\n",
      "difference between needing three days or three weeks of time to perform a proper analysis 24\n",
      "\n",
      "The basic breakdown is as follows (this is a rehashed definition of organized and\n",
      "unorganized data in the first chapter):\n",
      "Structured (organized) data : This is data that can be thought of as observations\n",
      "and characteristics 45\n",
      "It is usually organized using a table method (rows and\n",
      "columns) 14\n",
      "\n",
      "Unstructured (unorganized) data : This data exists  as a free entity and does not\n",
      "follow any standard organization hierarchy 26\n",
      "\n",
      "Here are a few examples that could help you differentiate between the two:\n",
      "Most data that exists in text form, including server logs and Facebook posts, is\n",
      "unstructured\n",
      "Scientific observations, as recorded by careful scientists, are kept in a very neat\n",
      "and organized ( structured ) format\n",
      "A genetic sequence of chemical nucleotides (for example, ACGTATTGCA) is\n",
      "unstructured,  even if the order of the nucleotides matters, as we cannot form\n",
      "descriptors of the sequence using a row/column format without taking a further\n",
      "look\n",
      "\n",
      "Structured data is generally thought of as being much easier to work with and analyze 132\n",
      "\n",
      "Most statistical and machine learning models were built with structured data in mind and\n",
      "cannot work on the loose interpretation of unstructured data 26\n",
      "The natural row and column\n",
      "structure is easy to digest for human and machine eyes 16\n",
      "So, why even talk about\n",
      "unstructured data 10\n",
      "Because it is so common 5\n",
      "Most estimates place unstructured data as\n",
      "80-90% of the world's data 17\n",
      "This data exists in many forms and, for the most part, goes\n",
      "unnoticed by humans as a potential source of data 25\n",
      "Tweets, emails, literature, and server\n",
      "logs are generally unstructured forms of data 17\n",
      "\n",
      "While a data scientist likely prefers structured data, they must be able to deal with the\n",
      "world's massive amounts of unstructured data 27\n",
      "If 90% of the world's data is unstructured,\n",
      "that implies that about 90% of the world's information is trapped in a difficult format 31\n",
      "\n",
      "So, with most of our data  existing in this free-form format, we must turn to pre-analysis\n",
      "techniques, called pre-processing , in order to apply structure to at least a part of the data\n",
      "for further analysis 47\n",
      "The next chapter will deal with pre-processing in great detail; for now,\n",
      "we will consider the part of pre-processing wherein we attempt to apply transformations to\n",
      "convert unstructured data into a structured counterpart 39\n",
      "\n",
      "Example of data pre-processing\n",
      "When looking at text data (which is almost always considered unstructured), we have\n",
      "many options to transform the set into a structured format 34\n",
      "We may do this by applying\n",
      "new characteristics that describe the data 13\n",
      "A few such characteristics are as follows:\n",
      "Word/phrase count\n",
      "The existence of certain special characters\n",
      "The relative length of text\n",
      "Picking out topics\n",
      "I will use the following tweet as a quick example of unstructured data, but you may use\n",
      "any unstructured free-form text that you like, including tweets and Facebook posts:\n",
      "\"This Wednesday morn, are you early to rise 77\n",
      "Then look East 3\n",
      "The Crescent Moon joins Venus\n",
      "& Saturn 8\n",
      "Afloat in the dawn skies 6\n",
      "\n",
      "It is important to reiterate that pre-processing is necessary for this tweet because a vast\n",
      "majority of learning algorithms require numerical data (which we will get into after this\n",
      "example) 38\n",
      "\n",
      "\n",
      "More than requiring a certain type of data, pre-processing allows us to explore features that\n",
      "have been created from the existing features 26\n",
      "For example, we can extract features such as\n",
      "word count and special characters from the mentioned tweet 19\n",
      "Now, let's take a look at a few\n",
      "features that we can extract from the text 19\n",
      "\n",
      "Word/phrase counts\n",
      "We may break down a tweet  into its word/phrase count 19\n",
      "The word this appears in the tweet\n",
      "once, as does every other word 15\n",
      "We can represent this tweet in a structured format as\n",
      "follows, thereby converting the unstructured set of words into a row/column format:\n",
      "this wednesday morn are you\n",
      "Word count 1 1 1 1 1\n",
      "Note that to obtain this format, we can utilize scikit-learn's CountVectorizer , which we\n",
      "saw in the previous chapter 76\n",
      "\n",
      "Presence of certain special characters\n",
      "We may also look at the presence of special characters, such as the question mark and\n",
      "exclamation mark 28\n",
      "The appearance of these characters might imply certain ideas about the\n",
      "data that are otherwise difficult to know 19\n",
      "For example, the fact that this tweet contains a\n",
      "question mark might strongly imply that this tweet contains a question for the reader 25\n",
      "We\n",
      "might append the preceding table with a new column, as shown:\n",
      "this wednesday morn are you 22\n",
      "\n",
      "Word Count 1 1 1 1 1 1\n",
      "The relative length of text\n",
      "This tweet is 125 characters long:\n",
      "len(\"This Wednesday morn, are you early to rise 42\n",
      "Then look East 3\n",
      "The\n",
      "Crescent Moon joins Venus & Saturn 10\n",
      "Afloat in the dawn skies 6\n",
      "\n",
      "# get the length of this text (number of characters for a string)\n",
      "# 125\n",
      "\n",
      "The average tweet, as discovered by analysts, is about 30 characters in length 36\n",
      "So, we might\n",
      "impose a new  characteristic, called relative length  (which is the length of the tweet divided\n",
      "by the average length), telling us the length of this tweet as compared with the average\n",
      "tweet 45\n",
      "This tweet is actually 4 6\n",
      "03 times longer than the average tweet, as shown:\n",
      "We can add yet another column to our table using this method:\n",
      "this wednesday morn are you 32\n",
      "Relative length\n",
      "Word count 1 1 1 1 1 1 4 19\n",
      "03\n",
      "Picking out topics\n",
      "We can pick out some topics of the tweet  to add as columns 22\n",
      "This tweet is about astronomy,\n",
      "so we can add another column, as illustrated:\n",
      "this wednesday morn are you 23\n",
      "Relative length Topic\n",
      "Word count 1 1 1 1 1 1 4 20\n",
      "03 astronomy\n",
      "And just like that, we can convert a piece of text into structured/organized data ready for\n",
      "use in our models and exploratory analysis 32\n",
      "\n",
      "The topic is the only extracted feature we looked at that is not automatically derivable from\n",
      "the tweet 21\n",
      "Looking at word count and tweet length in Python is easy 11\n",
      "However, more\n",
      "advanced models (called topic models) are able to derive and predict topics of natural text\n",
      "as well 24\n",
      "\n",
      "Being able to quickly recognize whether your data is structured or unstructured can save\n",
      "hours or even days of work in the future 26\n",
      "Once you are able to discern the organization of\n",
      "the data presented to you, the next question is aimed at the individual characteristics of the\n",
      "dataset 29\n",
      "\n",
      "Quantitative versus qualitative data\n",
      "When you ask a data scientist, \"what type of data is this 21\n",
      ", they will usually assume that you\n",
      "are asking them whether or not it is mostly quantitative or qualitative 20\n",
      "It is likely the most\n",
      "common way of describing the specific  characteristics of a dataset 17\n",
      "\n",
      "\n",
      "For the most part, when talking about quantitative data, you are usually  (not always)\n",
      "talking about a structured dataset with a strict row/column structure (because we don't\n",
      "assume unstructured data even has any characteristics) 47\n",
      "All the more reason why the pre-\n",
      "processing step is so important 13\n",
      "\n",
      "These two data types can be defined as follows:\n",
      "Quantitative data : This data  can be described using numbers, and basic\n",
      "mathematical procedures, including addition, are possible on the set 40\n",
      "\n",
      "Qualitative data : This data  cannot be described using numbers and basic\n",
      "mathematics 18\n",
      "This data is generally thought of as being described using natural\n",
      "categories and language 15\n",
      "\n",
      "Example – coffee shop data\n",
      "Say that we were processing  observations of coffee  shops in a major city using the\n",
      "following five descriptors (characteristics):\n",
      "Data: Coffee Shop\n",
      "Name of coffee shop\n",
      "Revenue (in thousands of dollars)\n",
      "Zip code\n",
      "Average monthly customers\n",
      "Country of coffee origin\n",
      "Each of these characteristics can be classified as either quantitative or qualitative, and that\n",
      "simple distinction can change everything 82\n",
      "Let's take a look at each one:\n",
      "Name of a coffee shop : Qualitative\n",
      "The name of a coffee shop is not expressed as a number and we cannot perform\n",
      "mathematical operations on the name of the shop 45\n",
      "\n",
      "Revenue : Quantitative\n",
      "How much money a coffee shop brings in can definitely be described using a\n",
      "number 22\n",
      "Also, we can do basic operations, such as adding up the revenue for 12\n",
      "months to get a year's worth of revenue 27\n",
      "\n",
      "\n",
      "Zip code : Qualitative\n",
      "This one is tricky 11\n",
      "A zip code is always represented using numbers, but what\n",
      "makes it qualitative is that it does not fit the second part of the definition of\n",
      "quantitative —we cannot perform basic mathematical operations on a zip code 42\n",
      "If\n",
      "we add together two zip codes, it is a nonsensical measurement 16\n",
      "We don't\n",
      "necessarily get a new zip code and we definitely don't get \"double the zip code 22\n",
      "\n",
      "Average monthly customers : Quantitative\n",
      "Again, describing this factor using numbers and addition makes sense 19\n",
      "Add up\n",
      "all of your monthly customers and you get your yearly customers 14\n",
      "\n",
      "Country of coffee origin : Qualitative\n",
      "We will assume this is a very small coffee shop with coffee from a single origin 25\n",
      "\n",
      "This country is described using a name (Ethiopian, Colombian), and not\n",
      "numbers 19\n",
      "\n",
      "A couple of important things to note:\n",
      "Even though a zip code is being described using numbers, it is not quantitative 24\n",
      "\n",
      "This is because you can't talk about the sum of all zip codes or an average  zip\n",
      "code 22\n",
      "These are nonsensical descriptions 6\n",
      "\n",
      "Pretty much whenever a word is used to describe a characteristic, it is a\n",
      "qualitative factor 20\n",
      "\n",
      "If you are having trouble identifying which is which, basically, when trying to decide\n",
      "whether or not the data is qualitative or quantitative, ask yourself a few basic questions\n",
      "about the data characteristics:\n",
      "Can you describe it using numbers 46\n",
      "\n",
      "No 2\n",
      "It is qualitative\n",
      "Yes 5\n",
      "Move on to the next question\n",
      "Does it still make sense after you add them together 17\n",
      "\n",
      "No 2\n",
      "They are qualitative\n",
      "Yes 5\n",
      "You probably have quantitative  data\n",
      "This method will help you to classify most, if not all, data into one of these two categories 27\n",
      "\n",
      "\n",
      "The difference between these two categories defines the types of questions you may ask\n",
      "about each column 19\n",
      "For a quantitative column, you may ask questions such as the\n",
      "following:\n",
      "What is the average value 20\n",
      "\n",
      "Does this quantity increase or decrease over time (if time is a factor) 16\n",
      "\n",
      "Is there a threshold where if this number became too high or too low, it would\n",
      "signal trouble for the company 24\n",
      "\n",
      "For a qualitative column, none of the preceding questions can be answered 14\n",
      "However, the\n",
      "following questions only apply to qualitative values:\n",
      "Which value occurs the most and the least 20\n",
      "\n",
      "How many unique values are there 7\n",
      "\n",
      "What are these unique values 6\n",
      "\n",
      "Example – world alcohol consumption data\n",
      "The World Health Organization  (WHO ) released a dataset describing  the average drinking\n",
      "habits of people  in countries across  the world 36\n",
      "We will use Python and the data exploration\n",
      "tool, pandas, in order to gain a better look:\n",
      "import pandas as pd\n",
      "# read in the CSV file from a URL\n",
      "drinks =\n",
      "pd 40\n",
      "read_csv('https://raw 6\n",
      "githubusercontent 2\n",
      "com/sinanuozdemir/principles_of_\n",
      "data_science/master/data/chapter_2/drinks 23\n",
      "csv')\n",
      "# examine the data's first five rows\n",
      "drinks 13\n",
      "head()           # print the first 5 rows\n",
      "These three lines have  done the following:\n",
      "Imported pandas , which will be referred to as pd in the future\n",
      "Read in a comma separated value  (CSV ) file as a variable called drinks\n",
      "Called a method, head , that reveals the first five rows of the dataset\n",
      "Note the neat row/column structure a CSV comes in 78\n",
      "\n",
      "\n",
      "\n",
      "The preceding table lists the first five rows of data from the drink 14\n",
      "csv  file 3\n",
      "We have six\n",
      "different columns that we are working within this example:\n",
      "country : Qualitative\n",
      "beer_servings : Quantitative\n",
      "spirit_servings : Quantitative\n",
      "wine_servings : Quantitative\n",
      "total_litres_of_pure_alcohol : Quantitative\n",
      "continent : Qualitative\n",
      "Let's look at the qualitative column continent 65\n",
      "We can use Pandas in order to get some\n",
      "basic summary statistics about this non-numerical characteristic 21\n",
      "The describe()  method\n",
      "is being used here, which first identifies whether the column is likely to be quantitative or\n",
      "qualitative, and then gives basic information about the column as a whole 38\n",
      "This is done as\n",
      "follows:\n",
      "drinks['continent'] 13\n",
      "describe()\n",
      ">> count     170\n",
      ">> unique      5\n",
      ">> top        AF\n",
      ">> freq       53\n",
      "It reveals that the WHO has gathered data about five unique continents, the most frequent\n",
      "being AF (Africa), which occurred 53 times in the 193 observations 57\n",
      "\n",
      "\n",
      "If we take a look at one of the quantitative columns and call the same method, we can see\n",
      "the difference in output, as shown:\n",
      "drinks['beer_servings'] 37\n",
      "describe()\n",
      "The output is as follows:\n",
      "count    193 12\n",
      "000000\n",
      "mean     106 8\n",
      "160622\n",
      "std      101 8\n",
      "143103\n",
      "min        0 8\n",
      "000000\n",
      "25%       20 9\n",
      "000000\n",
      "50%       76 9\n",
      "000000\n",
      "75%      188 9\n",
      "000000\n",
      "max      376 8\n",
      "000000\n",
      "Now, we can look at the mean (average) beer serving per person per country (106 23\n",
      "2\n",
      "servings), as well as the lowest beer serving, zero, and the highest beer serving recorded,\n",
      "376 (that's more than a beer a day) 34\n",
      "\n",
      "Digging deeper\n",
      "Quantitative data can be broken down  one step further into discrete  and continuous\n",
      "quantities 24\n",
      "\n",
      "These can be defined as follows:\n",
      "Discrete data : This describes data  that is counted 19\n",
      "It can only take on certain\n",
      "values 8\n",
      "\n",
      "Examples of discrete quantitative data include a dice roll, because it can only take\n",
      "on six values, and the number of customers in a coffee shop, because you can't\n",
      "have a real range of people 42\n",
      "\n",
      "Continuous data : This describes data  that is measured 11\n",
      "It exists on an infinite\n",
      "range of values 9\n",
      "\n",
      "A good example of continuous data would be a person's weight, because it can\n",
      "be 150 pounds or 197 25\n",
      "66 pounds (note the decimals) 8\n",
      "The height of a person or\n",
      "building is a continuous number because an infinite scale of decimals is possible 20\n",
      "\n",
      "Other examples of continuous data would be time and temperature 11\n",
      "\n",
      "\n",
      "The road thus far\n",
      "So far in this chapter, we have looked  at the differences between structured and\n",
      "unstructured data, as well as between qualitative and quantitative characteristics 35\n",
      "These\n",
      "two simple distinctions can have drastic effects on the analysis that is performed 15\n",
      "Allow me\n",
      "to summarize before moving on the second half of the chapter 14\n",
      "\n",
      "Data as a whole can either be structured  or unstructured , meaning that the data can either\n",
      "take on an organized row/column structure with distinct features that describe each row of\n",
      "the dataset, or exist in a free-form state that usually must be pre-processed into a form that\n",
      "is easily digestible 63\n",
      "\n",
      "If data is structured, we can look at each column (feature) of the dataset as being either\n",
      "quantitative  or qualitative 27\n",
      "Basically, can the column be described using mathematics and\n",
      "numbers or not 14\n",
      "The next part of this chapter will break down data into four very specific\n",
      "and detailed levels 18\n",
      "At each order, we will apply more complicated rules of mathematics,\n",
      "and in turn, we can gain a more intuitive and quantifiable understanding of the data 30\n",
      "\n",
      "The four levels of data\n",
      "It is generally understood that a specific characteristic (feature/column) of structured data\n",
      "can be broken down into one of four levels of data 34\n",
      "The levels are as follows:\n",
      "The nominal level\n",
      "The ordinal level\n",
      "The interval level\n",
      "The ratio level\n",
      "As we move down the list, we gain more structure and, therefore, more returns from our\n",
      "analysis 43\n",
      "Each level comes with its own accepted practice in measuring the center  of the\n",
      "data 17\n",
      "We usually think of the mean/average as being an acceptable form of a center 16\n",
      "\n",
      "However, this is only true for a specific type of data 13\n",
      "\n",
      "\n",
      "The nominal level\n",
      "The first level of data, the nominal  level, consists of data  that is described purely by name\n",
      "or category 29\n",
      "Basic examples include gender, nationality, species, or yeast strain  in a beer 16\n",
      "\n",
      "They are not described by numbers and are therefore qualitative 11\n",
      "The following are some\n",
      "examples:\n",
      "A type of animal is on the nominal level of data 18\n",
      "We may also say that if you are\n",
      "a chimpanzee, then you belong to the mammalian class as well 23\n",
      "\n",
      "A part of speech is also considered on the nominal level of data 14\n",
      "The word she is a\n",
      "pronoun, and it is also a noun 15\n",
      "\n",
      "Of course, being qualitative, we cannot perform any quantitative mathematical operations,\n",
      "such as addition or division 20\n",
      "These would not make any sense 6\n",
      "\n",
      "Mathematical operations allowed\n",
      "We cannot perform mathematics on the nominal  level of data except the basic equality  and\n",
      "set membership  functions, as shown in the following two examples:\n",
      "Being a tech entrepreneur  is the same as being in the tech industry , but not the other\n",
      "way around\n",
      "A figure described as a square falls under the description of being a rectangle,\n",
      "but not the other way around\n",
      "Measures of center\n",
      "A measure of center  is a number that describes what the data tends to 103\n",
      "It is sometimes\n",
      "referred to as the balance point  of the data 15\n",
      "Common examples include the mean, median,\n",
      "and mode 10\n",
      "\n",
      "In order to find the center  of nominal data, we generally turn to the mode  (the most\n",
      "common element) of the dataset 29\n",
      "For example, look back at the WHO alcohol consumption\n",
      "data 12\n",
      "The most common  continent surveyed was Africa, making that a possible choice for\n",
      "the center of the continent column 22\n",
      "\n",
      "Measures of the center, such as the mean and median, do not make sense at this level as we\n",
      "cannot order the observations or even add them together 33\n",
      "\n",
      "\n",
      "What data is like at the nominal level\n",
      "Data at the nominal level is mostly  categorical in nature 21\n",
      "Because we generally can only use\n",
      "words to describe the data, it can be lost in translation between countries, or can even be\n",
      "misspelled 30\n",
      "\n",
      "While data at this level can certainly be useful, we must be careful about what insights we\n",
      "may draw from them 24\n",
      "With only the mode as a basic measure of center, we are unable to \n",
      "draw  conclusions about an average  observation 24\n",
      "This concept does not exist at this level 8\n",
      "It\n",
      "is only at the next level that we may begin to perform true mathematics on our\n",
      "observations 20\n",
      "\n",
      "The ordinal level\n",
      "The nominal level did not provide us with much flexibility in terms of mathematical\n",
      "operations due to one seemingly unimportant fact: we could not order the observations in\n",
      "any natural way 40\n",
      "Data in the ordinal  level provides us with a rank order, or the means to\n",
      "place one observation before the other 24\n",
      "However, it does not provide us with relative\n",
      "differences between observations, meaning that while we may order the observations from\n",
      "first to last, we cannot add or subtract them to get any real meaning 40\n",
      "\n",
      "Examples\n",
      "The Likert  is among the most common  ordinal level scales 16\n",
      "Whenever you are given a \n",
      "survey  asking you to rate your satisfaction on a scale from 1 to 10, you are providing data at\n",
      "the ordinal level 33\n",
      "Your answer, which must fall between 1 and 10, can be ordered: eight is\n",
      "better than seven while three is worse than nine 29\n",
      "\n",
      "However, differences between the numbers do not make much sense 12\n",
      "The difference\n",
      "between a seven and a six might be different from the difference between a two and a one 21\n",
      "\n",
      "Mathematical operations allowed\n",
      "We are allowed much more freedom  on this level in mathematical operations 20\n",
      "We inherit all\n",
      "mathematics from the ordinal level (equality and set membership) and we can also add the\n",
      "following to the list of operations allowed in the nominal level:\n",
      "Ordering\n",
      "Comparison\n",
      "\n",
      "Ordering refers to the natural order provided to us by the data 53\n",
      "However, this can be tricky\n",
      "to figure out sometimes 11\n",
      "When speaking about the spectrum of visible light, we can refer to\n",
      "the names of colors —Red, Orange , Yellow , Green , Blue , Indigo , and Violet 34\n",
      "Naturally, as\n",
      "we move from left to right, the light is gaining energy and other properties 19\n",
      "We may refer to\n",
      "this as a natural order:\n",
      "The natural order of color\n",
      "However, if needed, an artist may impose another order on the data, such as sorting the\n",
      "colors based on the cost of the material to make said color 49\n",
      "This could change the order of\n",
      "the data, but as long as we are consistent in what defines the order, it does not matter what\n",
      "defines it 31\n",
      "\n",
      "Comparisons are another new operation allowed at this level 11\n",
      "At the ordinal level, it would\n",
      "not make sense to say that one country was naturally  better than another or that one part of\n",
      "speech is worse than another 33\n",
      "At the ordinal level, we can make these comparisons 10\n",
      "For\n",
      "example, we can talk about how putting a \"7\" on a survey is worse than putting a \"10 24\n",
      "\n",
      "Measures of center\n",
      "At the ordinal level, the median  is usually an appropriate  way of defining the center of the\n",
      "data 28\n",
      "The mean, however, would be impossible because the division is not allowed  at this\n",
      "level 19\n",
      "We can also use the mode as we could at the nominal level 13\n",
      "\n",
      "We will now look at an example of using the median 12\n",
      "\n",
      "Imagine you have conducted a survey among your employees asking \" how happy are you to\n",
      "be working here on a scale from 1-5 29\n",
      ",\" and your results are as follows:\n",
      "5, 4, 3, 4, 5, 3, 2, 5, 3, 2, 1, 4, 5, 3, 4, 4, 5, 4, 2, 1, 4, 5, 4, 3, 2,\n",
      "4, 4, 5, 4, 3, 2, 1\n",
      "\n",
      "Let's use Python to find the median of this data 113\n",
      "It is worth noting that most people would\n",
      "argue that the mean of these scores would work just fine 21\n",
      "The reason that the mean would\n",
      "not be as mathematically viable is that if we subtract/add two scores, say a score of four\n",
      "minus a score of two, the difference of two does not really mean anything 44\n",
      "If\n",
      "addition/subtraction among the scores doesn't make sense, the mean won't make sense\n",
      "either:\n",
      "import numpy\n",
      "results = [5, 4, 3, 4, 5, 3, 2, 5, 3, 2, 1, 4, 5, 3, 4, 4, 5, 4, 2, 1, 4,\n",
      "5, 4, 3, 2, 4, 4, 5, 4, 3, 2, 1]\n",
      "sorted_results = sorted(results)\n",
      "print(sorted_results)\n",
      "'''\n",
      "[1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "5, 5, 5, 5, 5, 5, 5]\n",
      "'''\n",
      "print(numpy 232\n",
      "mean(results)) # == 3 7\n",
      "4375\n",
      "print(numpy 6\n",
      "median(results)) # == 4 7\n",
      "0\n",
      "The ''' (triple apostrophe) denotes a longer (over two lines) comment 20\n",
      "It\n",
      "acts in a way similar to # 9\n",
      "\n",
      "It turns out that the median is not only more sound but makes the survey results look much\n",
      "better 21\n",
      "\n",
      "Quick recap and check\n",
      "So far, we have seen half of the levels of data:\n",
      "The nominal level\n",
      "The ordinal level\n",
      "\n",
      "At the nominal level, we deal with data usually described using vocabulary (but sometimes\n",
      "with numbers), with no order and little use of mathematics 55\n",
      "\n",
      "At the ordinal level, we have data that can be described with numbers and we also have a\n",
      "\"natural\" order, allowing us to put one in front of the other 36\n",
      "\n",
      "Let's try to classify the following example as either ordinal or nominal ( answers are at the\n",
      "end of the chapter ):\n",
      "The origin of the beans in your cup of coffee\n",
      "The place someone receives after completing a foot race\n",
      "The metal used to make the medal that they receive after placing in said race\n",
      "The telephone number of a client\n",
      "How many cups of coffee you drink in a day\n",
      "The interval level\n",
      "Now, we are getting somewhere interesting 91\n",
      "At the interval  level, we are beginning to look\n",
      "at data that can be expressed through very quantifiable means, and where much more\n",
      "complicated mathematical  formulas are allowed 36\n",
      "The basic difference between the ordinal\n",
      "level and the interval level is, well, just that difference 19\n",
      "\n",
      "Data at the interval level allows meaningful subtraction between data points 12\n",
      "\n",
      "Example\n",
      "Temperature is a great example of data  at the interval level 15\n",
      "If it is 100 degrees Fahrenheit in\n",
      "Texas and 80 degrees Fahrenheit in Istanbul, Turkey, then Texas is 20 degrees warmer than\n",
      "Istanbul 31\n",
      "This simple example allows for so much more manipulation at this level than\n",
      "previous examples 16\n",
      "\n",
      "(Non) Example\n",
      "It seems as though the example in the ordinal level (using the one to five survey) fits the bill\n",
      "of the interval level 32\n",
      "However, remember that the difference  between the scores (when you\n",
      "subtract them) does not make sense; therefore, this data cannot be called at the interval\n",
      "level 34\n",
      "\n",
      "\n",
      "Mathematical operations allowed\n",
      "We can use all the operations allowed  on the lower levels (ordering, comparisons, and so\n",
      "on), along with two other notable operations:\n",
      "Addition\n",
      "Subtraction\n",
      "The allowance of these two operations allows us to talk about data at this level in a whole\n",
      "new way 63\n",
      "\n",
      "Measures of center\n",
      "At this level, we can use the median  and mode to describe this data 22\n",
      "However, usually the\n",
      "most accurate description of the center of data would be the arithmetic mean , more\n",
      "commonly referred to as simply the mean 29\n",
      "Recall that the definition of the mean  requires us\n",
      "to add together all the measurements 17\n",
      "At the previous levels, an addition was meaningless 9\n",
      "\n",
      "Therefore, the mean would have lost extreme value 10\n",
      "It is only at the interval level and\n",
      "above that the arithmetic mean makes sense 16\n",
      "\n",
      "We will now look at an example of using the mean 12\n",
      "\n",
      "Suppose we look at the temperature of a fridge containing a pharmaceutical company's\n",
      "new vaccine 19\n",
      "We measure the temperature every hour with the following data points (in\n",
      "Fahrenheit):\n",
      "31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26\n",
      "Using Python again, let's find the mean and median of the data:\n",
      "import numpy\n",
      "temps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\n",
      "print(numpy 128\n",
      "mean(temps))    # == 30 9\n",
      "73\n",
      "print(numpy 5\n",
      "median(temps))  # == 31 9\n",
      "0\n",
      "\n",
      "Note how the mean and median are quite close to each other and both are around 31\n",
      "degrees 23\n",
      "The question, on average, how cold is the fridge 11\n",
      ",  has an answer of about 31 9\n",
      "\n",
      "However, the vaccine comes with a warning:\n",
      "\"Do not keep this vaccine at a temperature under 29 degrees 22\n",
      "\n",
      "Note that at least twice the temperature dropped below 29 degrees, but you ended up\n",
      "assuming that it isn't enough for it to be detrimental 30\n",
      "\n",
      "This is where the measure of variation can help us understand how bad the fridge situation\n",
      "can be 20\n",
      "\n",
      "Measures of variation\n",
      "This is something new that we have  not yet discussed 17\n",
      "It is one thing to talk about the\n",
      "center of the data but, in data science, it is also very important to mention how \"spread out\"\n",
      "the data is 34\n",
      "The measures that describe this phenomenon are called measures of variation 11\n",
      "\n",
      "You have  likely heard of \"standard deviation\" from your statistics classes 15\n",
      "This idea is\n",
      "extremely important and I would like to address it briefly 15\n",
      "\n",
      "A measure of variation (such as the standard deviation) is a number that attempts to\n",
      "describe how spread out the data is 26\n",
      "\n",
      "Along with a measure of center, a measure of variation can almost entirely describe a\n",
      "dataset with only two numbers 23\n",
      "\n",
      "Standard deviation\n",
      "Arguably, the standard deviation is the most common measure  of variation of data at the\n",
      "interval level and beyond 27\n",
      "The standard deviation can be thought of as the \"average\n",
      "distance a data point is at from the mean 21\n",
      "While this description is technically and\n",
      "mathematically incorrect, it is a good way to think about it 21\n",
      "The formula for standard\n",
      "deviation can be broken down into the following steps:\n",
      "Find the mean of the data1 23\n",
      "\n",
      "For each number in the dataset, subtract it from the mean and then square it2 18\n",
      "\n",
      "Find the average of each square difference3 9\n",
      "\n",
      "Take the square root of the number obtained in Step 3  and this is the standard 4 21\n",
      "\n",
      "deviation\n",
      "Notice how, in the steps, we do actually take an arithmetic mean as one of the steps 23\n",
      "\n",
      "\n",
      "For example, look back at the temperature dataset 10\n",
      "Let's find the standard deviation of the\n",
      "dataset using Python:\n",
      "import numpy\n",
      "temps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\n",
      "mean = numpy 66\n",
      "mean(temps)    # == 30 9\n",
      "73\n",
      "squared_differences = []\n",
      "# empty list o squared differences\n",
      "for temperature in temps:\n",
      "    difference = temperature - mean\n",
      " # how far is the point from the mean\n",
      "    squared_difference = difference**2\n",
      "    # square the difference\n",
      "    squared_differences 56\n",
      "append(squared_difference)\n",
      "    # add it to our list\n",
      "average_squared_difference = numpy 18\n",
      "mean(squared_differences)\n",
      "# This number is also called the \"Variance\"\n",
      "standard_deviation = numpy 22\n",
      "sqrt(average_squared_difference)\n",
      "# We did it 10\n",
      "\n",
      "print(standard_deviation)  # == 2 12\n",
      "5157\n",
      "All of this code led to us find out that the standard deviation of the dataset is around 2 24\n",
      "5,\n",
      "meaning that \"on average,\" a data point is 2 15\n",
      "5 degrees off from the average temperature of\n",
      "around 31 degrees, meaning that the temperature could likely dip below 29 degrees again\n",
      "in the near future 32\n",
      "\n",
      "The reason we want the \"square difference\" between each point and the\n",
      "mean and not the \"actual difference\" is because squaring the value\n",
      "actually puts emphasis on outliers —data points that are abnormally far\n",
      "away 46\n",
      "\n",
      "Measures of variation give us a very clear picture of how spread out or dispersed our data\n",
      "is 21\n",
      "This is especially important when we are concerned with ranges of data and how data\n",
      "can fluctuate (think percentage return on stocks) 26\n",
      "\n",
      "\n",
      "The big difference between data at this level and at the next level lies in something that is\n",
      "not obvious 22\n",
      "\n",
      "Data at the interval level does not have a \"natural starting point or a natural zero 18\n",
      "\n",
      "However, being at zero degrees Celsius does not mean that you have \"no temperature\" 18\n",
      "\n",
      "The ratio level\n",
      "Finally, we will take a look  at the ratio level 17\n",
      "After moving through three different levels\n",
      "with differing levels of allowed mathematical operations, the ratio level proves to be the\n",
      "strongest of the four 28\n",
      "\n",
      "Not only can we define order  and difference, but the ratio level also allows us to multiply\n",
      "and divide  as well 26\n",
      "This might seem like not much to make a fuss over but it changes almost\n",
      "everything about the way we view data at this level 26\n",
      "\n",
      "Examples\n",
      "While Fahrenheit and Celsius are stuck  in the interval level, the Kelvin scale of temperature\n",
      "boasts a natural zero 26\n",
      "A measurement of zero Kelvin literally means the absence of heat 11\n",
      "It\n",
      "is a non-arbitrary starting zero 9\n",
      "We can actually scientifically say that 200 Kelvin is twice as\n",
      "much heat as 100 Kelvin 19\n",
      "\n",
      "Money in the bank is at the ratio level 10\n",
      "You can have \"no money in the bank\" and it makes\n",
      "sense that $200,000 is \"twice as much as\" $100,000 32\n",
      "\n",
      "Many people may argue that Celsius and Fahrenheit also have a starting\n",
      "point (mainly because we can convert from Kelvin to either of the two) 30\n",
      "\n",
      "The real difference here might seem silly, but because the conversion to\n",
      "Celsius and Fahrenheit make the calculations go into the negative, it does\n",
      "not define a clear and \"natural\" zero 39\n",
      "\n",
      "Measures of center\n",
      "The arithmetic mean still holds meaning  at this level, as does a new type of mean called the\n",
      "geometric mean 30\n",
      "This measure is generally not used as much, even at the ratio  level, but is\n",
      "worth mentioning 21\n",
      "It is the square root of the product of all the values 12\n",
      "\n",
      "\n",
      "For example, in our fridge temperature data, we can calculate the geometric mean as shown\n",
      "here:\n",
      "import numpy\n",
      "temps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\n",
      "num_items = len(temps)\n",
      "product = 1 82\n",
      "\n",
      "for temperature in temps:\n",
      "    product *= temperature\n",
      "geometric_mean = product**(1 18\n",
      "/num_items)\n",
      "print(geometric_mean)   # == 30 15\n",
      "634\n",
      "Note again how it is close to the arithmetic mean and median as calculated before 18\n",
      "This is\n",
      "not always the case and will be talked about at great length in the statistics chapter of this\n",
      "book 23\n",
      "\n",
      "Problems with the ratio level\n",
      "Even with all of this added  functionality at this level, we must generally also make a very\n",
      "large assumption that actually makes the ratio level a bit restrictive 39\n",
      "\n",
      "Data at the ratio level is usually non-negative 10\n",
      "\n",
      "For this reason alone, many data scientists prefer the interval level to the ratio level 17\n",
      "The\n",
      "reason for this restrictive property is because if we allowed negative values, the ratio might\n",
      "not always make sense 23\n",
      "\n",
      "Consider that we allowed debt to occur in our money in the bank example 15\n",
      "If we had a\n",
      "balance of $50,000, the following ratio would not really make sense at all:\n",
      "\n",
      "\n",
      "Data is in the eye of the beholder\n",
      "It is possible to impose structure  on data 42\n",
      "For example, while I said that you technically\n",
      "cannot use a mean for the one to five data at the ordinal scale, many statisticians would not\n",
      "have a problem using this number as a descriptor of the dataset 43\n",
      "\n",
      "The level at which you are interpreting data is a huge  assumption that should be made at\n",
      "the beginning of any analysis 25\n",
      "If you are looking at data that is generally thought of at the\n",
      "ordinal level and applying tools such as the arithmetic mean and standard deviation, this is\n",
      "something that data scientists must be aware of 39\n",
      "This is mainly because if you continue to\n",
      "hold these assumptions as valid in your analysis, you may encounter problems 22\n",
      "For\n",
      "example, if you also assume divisibility at the ordinal level by mistake, you are imposing a\n",
      "structure where the structure may not exist 29\n",
      "\n",
      "Summary\n",
      "The type of data that you are working with is a very large piece of data science 20\n",
      "It must\n",
      "precede most of your analysis because the type of data you have impacts the type of\n",
      "analysis that is even possible 26\n",
      "\n",
      "Whenever you are faced with a new dataset, the first three questions you should ask about\n",
      "it are the following:\n",
      "Is the data organized or unorganized 31\n",
      "For example, does our data exist in a nice,\n",
      "clean row/column structure 15\n",
      "\n",
      "Is each column quantitative or qualitative 7\n",
      "For example, are the values\n",
      "numbers, strings, or do they represent quantities 16\n",
      "\n",
      "At what level is the data in each column 10\n",
      "For example, are the values at the\n",
      "nominal, ordinal, interval, or ratio level 19\n",
      "\n",
      "The answers to these questions will not only impact your knowledge of the data at the end\n",
      "but will also dictate the next steps of your analysis 29\n",
      "They will dictate the types of graphs\n",
      "you are able to use and how you interpret them in your upcoming data models 23\n",
      "Sometimes,\n",
      "we will have to convert from one level to another in order to gain more perspective 18\n",
      "In the\n",
      "coming chapters, we will take a much deeper look at how to deal with and explore data at\n",
      "different levels 25\n",
      "\n",
      "\n",
      "By the end of this book, we will be able to not only recognize data at different levels, but\n",
      "will also know how to deal with it at these levels 34\n",
      "In the next chapter, we will review how\n",
      "types of data are used by data scientists to do data discovery and visualization 24\n",
      "\n",
      "Answers for classification of the following example as either ordinal or nominal:\n",
      "The origin of the beans in your cup of coffee : Nominal\n",
      "The place someone receives after completing a foot race : Ordinal\n",
      "The metal used to make the medal that they receive after placing in the race :\n",
      "Nominal\n",
      "The telephone number of a client : Nominal\n",
      "How many cups of coffee you drink in a day : Ordinal\n",
      "\n",
      "\n",
      "The Five Steps of Data Science\n",
      "We have spent much time looking at the preliminaries of data science, including outlining\n",
      "the types of data and how to approach datasets depending on their type 122\n",
      "\n",
      "In this chapter, in addition to the introduction of data science, we will focus on the\n",
      "following topics:\n",
      "Steps to perform data science\n",
      "Data exploration\n",
      "Data visualization\n",
      "We will use the Python packages pandas  and matplotlib  to explore different datasets 50\n",
      "\n",
      "Introduction to data science\n",
      "Many people ask me the biggest difference between data science and data analytics 19\n",
      "While\n",
      "some can argue  that there is no difference between the two, many will argue that there are\n",
      "hundreds 24\n",
      "I believe that regardless of how many differences there are between the two\n",
      "terms, the biggest is that data science follows a structured, step-by-step process that, when\n",
      "followed, preserves the integrity of the results 43\n",
      "\n",
      "Like any other scientific endeavor, this process must be adhered to, or else the analysis and\n",
      "the results are in danger of scrutiny 28\n",
      "On a simpler level, following a strict process can make\n",
      "it much easier for amateur data scientists to obtain results faster than if they were exploring\n",
      "data with no clear vision 34\n",
      "\n",
      "While these steps are a guiding lesson for amateur analysts, they also provide the\n",
      "foundation for all data scientists, even those in the highest levels of business and academia 33\n",
      "\n",
      "Every data scientist recognizes the value of these steps and follows them in some way or\n",
      "another 19\n",
      "\n",
      "\n",
      "Overview of the five steps\n",
      "The five essential steps to perform data  science are as follows:\n",
      "Asking an interesting question1 26\n",
      "\n",
      "Obtaining the data2 7\n",
      "\n",
      "Exploring the data3 6\n",
      "\n",
      "Modeling the data4 6\n",
      "\n",
      "Communicating and visualizing the results5 9\n",
      "\n",
      "First, let's look at the five steps with reference to the big picture 16\n",
      "\n",
      "Asking an interesting question\n",
      "This is probably my favorite step 13\n",
      "As an entrepreneur, I ask myself (and others) interesting\n",
      "questions every day 16\n",
      "I would treat this step as you would treat a brainstorming session 13\n",
      "\n",
      "Start writing down questions regardless of whether or not you think the data  to answer\n",
      "these questions even exists 22\n",
      "The reason for this is twofold 8\n",
      "First off, you don't want to start\n",
      "biasing yourself even before searching for data 18\n",
      "Secondly, obtaining data might involve\n",
      "searching in both public and private locations and, therefore, might not be very\n",
      "straightforward 26\n",
      "You might ask a question and immediately tell yourself \"Oh, but I bet\n",
      "there's no data out there that can help me\" and cross it off your list 33\n",
      "Don't do that 4\n",
      "Leave it\n",
      "on your list 6\n",
      "\n",
      "Obtaining the data\n",
      "Once you have selected the question  you want to focus on, it is time to scour the world for\n",
      "the data that might be able to answer that question 39\n",
      "As mentioned before, the data can\n",
      "come from a variety of sources; so, this step can be very creative 23\n",
      "\n",
      "Exploring the data\n",
      "Once we have the data, we use the lessons  learned in Chapter 2 , Types of Data , and begin to\n",
      "break down the types of data that we are dealing with 42\n",
      "This is a pivotal step in the process 8\n",
      "\n",
      "Once this step is completed, the analyst has generally  spent several hours learning about\n",
      "the domain, using code or other tools to manipulate and explore the data, and has a very\n",
      "good sense of what the data might be trying to tell them 50\n",
      "\n",
      "\n",
      "Modeling the data\n",
      "This step involves the use of statistical  and machine learning models 18\n",
      "In this step, we are\n",
      "not only fitting and choosing models, but we are also implanting mathematical validation\n",
      "metrics in order to quantify the models and their effectiveness 33\n",
      "\n",
      "Communicating and visualizing the results\n",
      "This is arguably the most important step 16\n",
      "While it might seem obvious and simple, the\n",
      "ability to conclude your  results in a digestible format is much more difficult than it seems 28\n",
      "\n",
      "We will look at different  examples of cases when results were communicated poorly and\n",
      "when they were displayed very well 23\n",
      "\n",
      "In this book, we will focus mainly on steps 3, 4, and 5 20\n",
      "\n",
      "Why are we skipping steps 1 and 2 in this book 14\n",
      "\n",
      "While the first two steps are undoubtedly imperative to the process, they\n",
      "generally precede statistical and programmatic systems 24\n",
      "Later in this\n",
      "book, we will touch upon the different ways to obtain data; however, for\n",
      "the purpose of focusing on the more scientific aspects of the process, we\n",
      "will begin with exploration right away 42\n",
      "\n",
      "Exploring the data\n",
      "The process of exploring data is not simply defined 15\n",
      "It involves the ability to recognize the\n",
      "different types of data, transform data types, and use code to systemically improve the \n",
      "quality  of the entire dataset to prepare it for the modeling stage 39\n",
      "In order to best represent\n",
      "and teach the art of exploration, I will present several different datasets and use the Python\n",
      "package pandas  to explore the data 31\n",
      "Along the way, we will run into different tips and\n",
      "tricks on how to handle data 19\n",
      "\n",
      "There are three basic questions we should ask ourselves when dealing with a new dataset\n",
      "that we have not seen before 23\n",
      "Keep in mind that these questions are not the beginning and\n",
      "the end of data science; they are some guidelines that should be followed when exploring a\n",
      "newly obtained set of data 36\n",
      "\n",
      "\n",
      "Basic questions for data exploration\n",
      "When looking at a new dataset, whether it is familiar to you or not, it is important to use\n",
      "the following questions  as guidelines for your preliminary analysis:\n",
      "Is the data organized or not 46\n",
      ": We are checking for whether or not the data is\n",
      "presented in a row/column structure 19\n",
      "For the most part, data will be presented in\n",
      "an organized fashion 14\n",
      "In this book, over 90% of our examples will begin with\n",
      "organized data 17\n",
      "Nevertheless, this is the most basic question that we can answer\n",
      "before diving any deeper into our analysis 20\n",
      "A general rule of thumb is that if we\n",
      "have unorganized data, we want to transform it into a row/column structure 25\n",
      "For\n",
      "example, earlier in this book, we looked at ways to transform text into a\n",
      "row/column structure by counting the number of words/phrases 30\n",
      "\n",
      "What does each row represent 6\n",
      ": Once we have an answer to how the data is\n",
      "organized and are looking at a nice row/column-based dataset, we should\n",
      "identify what each row actually represents 33\n",
      "This step is usually very quick and\n",
      "can help put things into perspective much more quickly 17\n",
      "\n",
      "What does each column represent 6\n",
      ": We should identify each column by the level\n",
      "of data and whether or not it is quantitative/qualitative, and so on 26\n",
      "This\n",
      "categorization might change as our analysis progresses, but it is important to\n",
      "begin this step as early as possible 25\n",
      "\n",
      "Are there any missing data points 7\n",
      ": Data isn't perfect 5\n",
      "Sometimes, we might be\n",
      "missing data because of human or mechanical error 14\n",
      "When this happens, we, as\n",
      "data scientists, must make decisions about how to deal with these discrepancies 21\n",
      "\n",
      "Do we need to perform any transformations on the columns 11\n",
      ": Depending on\n",
      "the level/type of data in each column, we might need to perform certain types of\n",
      "transformation 24\n",
      "For example, generally speaking, for the sake of statistical\n",
      "modeling and machine learning, we would like each column to be numerical, so\n",
      "would use Python to make any transformations 36\n",
      "\n",
      "All the while, we are asking ourselves the overall question, what can we infer from the\n",
      "preliminary inferential statistics 26\n",
      "We want to be able to understand our data better than when\n",
      "we first found it 17\n",
      "\n",
      "Enough talk, let's see an example in the following section 13\n",
      "\n",
      "\n",
      "Dataset 1 – Yelp\n",
      "The first dataset we will look at is a public  dataset made available by the restaurant review\n",
      "site, Yelp 29\n",
      "All personally identifiable information has been removed 7\n",
      "Let's read  in the data\n",
      "first, as shown here:\n",
      "import pandas as pd\n",
      "yelp_raw_data = pd 25\n",
      "read_csv(\"yelp 5\n",
      "csv\")\n",
      "yelp_raw_data 6\n",
      "head()\n",
      "A quick recap of what the preceding code does:\n",
      "Imports the pandas  package and nickname it pd 1 23\n",
      "\n",
      "Reads in the 5\n",
      "csv  from the web; call is yelp_raw_data 2 14\n",
      "\n",
      "Looks at the head  of the data (just the first few rows) 3 18\n",
      "\n",
      "We get the following:\n",
      "Is the data organized or not 12\n",
      ":\n",
      "Because we have a nice row/column structure, we can conclude that this data\n",
      "seems pretty organized 21\n",
      "\n",
      "\n",
      "What does each row represent 6\n",
      ":\n",
      "It seems pretty obvious that each row represents a user giving a review of a\n",
      "business 18\n",
      "The next thing we should do is examine each row and label it by the\n",
      "type of data it contains 21\n",
      "At this point, we can also use Python to figure out just\n",
      "how big our dataset is 19\n",
      "We can use the shape  quality of a DataFrame to find this\n",
      "out, as shown:\n",
      "yelp_raw_data 23\n",
      "shape\n",
      "# (10000,10)\n",
      "It tells us that this dataset has 10000  rows and 10 columns 25\n",
      "Another way to say\n",
      "this is that this dataset has 10,000 observations and 10 characteristics 20\n",
      "\n",
      "What does each column represent 6\n",
      "(Note that we have 10 columns) :\n",
      "business_id : This is likely to be a unique identifier for the business the review\n",
      "is for 29\n",
      "This would  be at the nominal level  because there is no natural order to this\n",
      "identifier 19\n",
      "\n",
      "date : This is probably the date on which the review was posted 14\n",
      "Note that it\n",
      "seems to be only specific to the day, month, and year 18\n",
      "Even though time is \n",
      "usually  considered continuous, this column would likely be considered discrete\n",
      "and at the ordinal level  because of the natural order that dates have 32\n",
      "\n",
      "review_id : This is likely to be a unique identifier for the review that each post\n",
      "represents 21\n",
      "This would be at the nominal level  because, again, there is no natural\n",
      "order to this identifier 21\n",
      "\n",
      "stars : From a quick look (don't worry; we will perform some further analysis\n",
      "soon), we can see that this is an ordered column that represents what the\n",
      "reviewer gave the restaurant as a final score 44\n",
      "This is ordered and qualitative, so is\n",
      "at the ordinal level 13\n",
      "\n",
      "text : This is probably the raw text that each reviewer wrote 13\n",
      "As with most text,\n",
      "we place this at the nominal level 12\n",
      "\n",
      "type : In the first five columns, all we see is the word review 16\n",
      "This might be a\n",
      "column that identifies that each row is a review, implying that there might be\n",
      "another type of row other than a review 29\n",
      "We will take a look at this later 8\n",
      "We\n",
      "place this at the nominal level 8\n",
      "\n",
      "user_id : This is likely to be a unique identifier for the user who is writing the\n",
      "review 21\n",
      "Just like the other unique IDs, we place this data at the nominal level 15\n",
      "\n",
      "\n",
      "Note that after we have looked at all of the columns, and found that all of\n",
      "the data is either at the ordinal or nominal level, we have to look at the\n",
      "following things 39\n",
      "This is not uncommon, but it is worth mentioning 10\n",
      "\n",
      "Are there any missing data points 7\n",
      ":\n",
      "Perform an isnull  operation 7\n",
      "For example, if your DataFrame is called\n",
      "awesome_dataframe , then try the Python command\n",
      "awesome_dataframe 20\n",
      "isnull() 3\n",
      "sum() , which will show the number of\n",
      "missing values in each column 15\n",
      "\n",
      "Do we need to perform any transformations on the columns 11\n",
      ":\n",
      "At this point, we are looking for a few things 12\n",
      "For example, will we need to\n",
      "change the scale of some of the quantitative data, or do we need to create dummy\n",
      "variables for the qualitative variables 31\n",
      "As this dataset only has qualitative\n",
      "columns, we can only focus on transformations at the ordinal and nominal scale 21\n",
      "\n",
      "Before starting, let's go over some quick terminology for pandas, the Python data\n",
      "exploration module 21\n",
      "\n",
      "DataFrames\n",
      "When we read in a dataset, pandas creates a custom  object called a DataFrame 20\n",
      "Think of\n",
      "this as the Python version of a spreadsheet (but way better) 16\n",
      "In this case, the variable,\n",
      "yelp_raw_data , is a DataFrame 15\n",
      "\n",
      "To check whether this is true in Python, type in the following code:\n",
      "type(yelp_raw_data)\n",
      "# pandas 24\n",
      "core 1\n",
      "frame 1\n",
      "DataFrame\n",
      "DataFrames are two-dimensional in nature, meaning that they are organized in a\n",
      "row/column structure just as spreadsheets are 26\n",
      "The main benefits of using DataFrames over,\n",
      "say, spreadsheet software would be that a DataFrame can handle much larger data than\n",
      "most common spreadsheet software 29\n",
      "If you are familiar with the R language, you might\n",
      "recognize the word DataFrame 17\n",
      "This is because the name was actually borrowed from the\n",
      "language 12\n",
      "\n",
      "As most of the data that we will deal with is organized, DataFrames are likely to be the\n",
      "most used object in pandas, second only to the Series  object 35\n",
      "\n",
      "\n",
      "Series\n",
      "The Series  object is simply a DataFrame, but only with one dimension 17\n",
      "Essentially, it is a\n",
      "list of data  points 11\n",
      "Each column of a DataFrame is considered to be a Series  object 13\n",
      "Let's\n",
      "check this —the first thing we need to do is grab a single column from our DataFrame; we\n",
      "generally use what is known as bracket notation 33\n",
      "The following is an example:\n",
      "yelp_raw_data['business_id'] # grabs a single column of the Dataframe\n",
      "We will list the first and last few rows:\n",
      "0     9yKzy9PApeiPPOUJEtnvkg\n",
      "1     ZRJwVLyzEJq1VAihDhYiow\n",
      "2     6oRAC4uyJCsJl1X0WZpVSA\n",
      "3     _1QQZuf4zZOyFCvXc0o6Vg\n",
      "4     6ozycU1RpktNG2-1BroVtw\n",
      "5     -yxfBYGB6SEqszmxJxd97A\n",
      "6     zp713qNhx8d9KCJJnrw1xA\n",
      "Let's use the type function to check that this column is a Series :\n",
      "type(yelp_raw_data['business_id'])\n",
      "# pandas 194\n",
      "core 1\n",
      "series 1\n",
      "Series\n",
      "Exploration tips for qualitative data\n",
      "Using these two pandas objects, let's start performing  some preliminary data exploration 24\n",
      "\n",
      "For qualitative  data, we will specifically look at the nominal and ordinal levels 16\n",
      "\n",
      "Nominal level columns\n",
      "As we are at the nominal level, let's recall that at this level, data is qualitative and is\n",
      "described purely by name 33\n",
      "In this dataset, this refers to business_id , review_id , text ,\n",
      "type , and user_id 20\n",
      "Let's use pandas in order to dive a bit deeper, as shown here:\n",
      "yelp_raw_data['business_id'] 24\n",
      "describe()\n",
      "The output is as follows:\n",
      "# count                      10000\n",
      "# unique                      4174\n",
      "# top       ntN85eu27C04nwyPa8IHtw\n",
      "# freq                          37\n",
      "\n",
      "The describe  function will give us some quick stats about the column whose name we\n",
      "enter into the quotation marks 67\n",
      "Note how pandas automatically recognized that\n",
      "business_id  was a qualitative column and gave us stats that make sense 21\n",
      "When\n",
      "describe  is called on a qualitative column, we will always get the following four items:\n",
      "count : How many values are filled in\n",
      "unique : How many unique values are filled in\n",
      "top: The name of the most common item in the dataset\n",
      "freq : How often the most common item appears in the dataset\n",
      "At the nominal level, we are usually looking for a few things, which would signal a\n",
      "transformation:\n",
      "Do we have a reasonable number (usually under 20) of unique items 102\n",
      "\n",
      "Is this column free text 6\n",
      "\n",
      "Is this column completely unique across all rows 9\n",
      "\n",
      "So, for the business_id  column, we have a count of 10000 18\n",
      "Don't be fooled though 5\n",
      "This\n",
      "does not mean that we have 10,000 businesses being reviewed here 16\n",
      "It just means that of the\n",
      "10,000 rows of reviews, the business_id  column is filled in all 10,000 times 28\n",
      "The next\n",
      "qualifier, unique , tells us that we have 4174  unique businesses being reviewed in this\n",
      "dataset 25\n",
      "The most reviewed business is business JokKtdXU7zXHcr20Lrk29A , which was\n",
      "reviewed 37 times:\n",
      "yelp_raw_data['review_id'] 40\n",
      "describe()\n",
      "The output is as follows:\n",
      "# count                      10000\n",
      "# unique                     10000\n",
      "# top       M3jTv5NIipi_N4mgmZiIEg\n",
      "# freq                           1\n",
      "We have a count  of 10000  and a unique  of 10000 65\n",
      "Think for a second, does this make\n",
      "sense 10\n",
      "Think about what each row represents and what this column represents 11\n",
      "\n",
      "(Insert Jeopardy theme song here)\n",
      "\n",
      "Of course, it does 15\n",
      "Each row of this dataset is supposed to represent a single, unique review\n",
      "of a business and this column is meant to serve as a unique identifier for a review; so, it\n",
      "makes sense that the review_id  column has 10000  unique items in it 54\n",
      "So, why is eTa5KD-\n",
      "LTgQv6UT1Zmijmw  the most common  review 26\n",
      "This is just a random choice from the 10,000\n",
      "and means nothing:\n",
      "yelp_raw_data['text'] 24\n",
      "describe()\n",
      "The output is as follows:\n",
      "count                                                 10000\n",
      "unique                                                 9998\n",
      "top       This review is for the chain in general 30\n",
      "The l 2\n",
      " 1\n",
      " 1\n",
      "\n",
      "freq                                                      2\n",
      "This column, which represents the actual text people wrote, is interesting 19\n",
      "We would\n",
      "imagine that this should also be similar to review_id  in that each one should contain\n",
      "unique text, because it would be weird if two people wrote exactly the same thing; but we\n",
      "have two reviews with the exact same text 50\n",
      "Let's take a second to learn about DataFrame\n",
      "filtering to examine this further 16\n",
      "\n",
      "Filtering in pandas\n",
      "Let's talk a bit about how  filtering works 16\n",
      "Filtering rows based on certain criteria is quite\n",
      "easy in pandas 12\n",
      "In a DataFrame, if we wish to filter out rows based on some search criteria,\n",
      "we will need to go row by row and check whether or not a row satisfies that particular\n",
      "condition; pandas handles this by passing in a Series  of True  and False  (Booleans) 57\n",
      "\n",
      "We literally pass into the DataFrame a list of True  and False  data that mean the following:\n",
      "True : This row satisfies the condition\n",
      "False : This row does not satisfy the condition\n",
      "So, first, let's make the conditions 48\n",
      "In the following lines of code, I will grab the text that\n",
      "occurs twice:\n",
      "yelp_raw_data['text'] 25\n",
      "describe()['top']\n",
      "\n",
      "Here is a snippet of the text:\n",
      "\"This review is for the chain in general 20\n",
      "The location we went to is new so\n",
      "it isn't in Yelp yet 15\n",
      "Once it is I will put this review there as\n",
      "well 12\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "Right off the bat, we can guess that this might actually be one person who went to review\n",
      "two businesses that belong to the same chain and wrote the exact same review 35\n",
      "However,\n",
      "this is just a guess right now 9\n",
      "\n",
      "The duplicate_text  variable is of  string  type 12\n",
      "\n",
      "Now that we have this text, let's use some magic to create that Series  of True  and False :\n",
      "duplicate_text = yelp_raw_data['text'] 34\n",
      "describe()['top']\n",
      "text_is_the_duplicate = yelp_raw_data['text'] == duplicate_text\n",
      "Right away you might be confused 26\n",
      "What we have done here is take the text column of the\n",
      "DataFrame and compared it to the string, duplicate_text 23\n",
      "This is strange because we\n",
      "seem to be comparing a list of 10,000 elements to a single string 23\n",
      "Of course, the answer\n",
      "should be a straight false, right 13\n",
      "\n",
      "Series has a very interesting feature in that if you compare the Series  to an object, it will\n",
      "return another Series of Booleans of the same length where each True  and False  is the\n",
      "answer to the question is this element the same as the element you are comparing it to 59\n",
      "Very\n",
      "handy 4\n",
      "\n",
      "type(text_is_the_duplicate) # it is a Series of Trues and Falses\n",
      "text_is_the_duplicate 24\n",
      "head() # shows a few Falses out of the Series\n",
      "In Python, we can add and subtract true and false as if they were 1 and 0, respectively 36\n",
      "For\n",
      "example, True + False - True + False + True == 1 16\n",
      "So, we can verify that this Series  is correct\n",
      "by adding up all of the values 19\n",
      "As only two of these rows should contain the duplicate text,\n",
      "the sum of the Series should only be 2, which it is 26\n",
      "This is as follows:\n",
      "sum(text_is_the_duplicate) # == 2\n",
      "\n",
      "Now that we have our Series  of Booleans, we can pass it directly into our DataFrame,\n",
      "using bracket notation, and get our filtered rows, as illustrated:\n",
      "filtered_dataframe = yelp_raw_data[text_is_the_duplicate]\n",
      "# the filtered Dataframe\n",
      "filtered_dataframe\n",
      "It seems that our suspicions were correct and one person, on the same day, gave the exact\n",
      "same review to two different business_id , presumably a part of the same chain 105\n",
      "Let's\n",
      "keep moving along to the rest of our columns:\n",
      "yelp_raw_data['type'] 20\n",
      "describe()\n",
      "# count      10000\n",
      "# unique         1\n",
      "# top       review\n",
      "# freq       10000\n",
      "Remember this column 30\n",
      "Turns out they are all the exact same type, namely review :\n",
      "yelp_raw_data['user_id'] 21\n",
      "describe()\n",
      "# count                      10000\n",
      "# unique                      6403\n",
      "# top       fczQCSmaWF78toLEmb0Zsw\n",
      "# freq                          38\n",
      "\n",
      "Similar to the business_id  column, all the 10000  values are filled in with 6403  unique\n",
      "users and one user reviewing 38 times 72\n",
      "\n",
      "In this example, we won't have to perform any transformations 13\n",
      "\n",
      "Ordinal level columns\n",
      "As far as ordinal columns go, we are looking  at date and stars 20\n",
      "For each of these columns,\n",
      "let's look at what the described method brings back:\n",
      "yelp_raw_data['stars'] 24\n",
      "describe()\n",
      "# count    10000 8\n",
      "000000\n",
      "# mean         3 9\n",
      "777500\n",
      "# std          1 9\n",
      "214636\n",
      "# min          1 9\n",
      "000000\n",
      "# 25%          3 11\n",
      "000000\n",
      "# 50%          4 11\n",
      "000000\n",
      "# 75%          5 11\n",
      "000000\n",
      "# max          5 9\n",
      "000000\n",
      "Woah 6\n",
      "Even though this column is ordinal, the describe  method returned stats that we\n",
      "might expect for a quantitative column 22\n",
      "This is because the software saw a bunch of\n",
      "numbers and just assumed that we wanted stats like the mean  or the min and max 27\n",
      "This is\n",
      "not a problem 6\n",
      "Let's use a method called value_counts  to see the count distribution, as\n",
      "shown here:\n",
      "yelp_raw_data['stars'] 27\n",
      "value_counts()\n",
      "# 4    3526\n",
      "# 5    3337\n",
      "# 3    1461\n",
      "# 2     927\n",
      "# 1     749\n",
      "\n",
      "The value_counts  method will return the distribution of values for any column 55\n",
      "In this\n",
      "case, we see that the star rating 4 is the most common, with 3526  values, followed closely\n",
      "by rating 5 32\n",
      "We can also plot this data to get a nice visual 11\n",
      "First, let's sort by star rating, and\n",
      "then use the prebuilt plot  method to make a bar chart:\n",
      "import datetime\n",
      "dates = yelp_raw_data['stars'] 37\n",
      "value_counts()\n",
      "dates 4\n",
      "sort_values\n",
      "dates 4\n",
      "plot(kind='bar')\n",
      "From this graph, we can conclude that people are definitely more likely to give good star\n",
      "ratings over bad ones 27\n",
      "We can follow this procedure for the date column 9\n",
      "I will leave you to\n",
      "try it on your own 11\n",
      "For now, let's look at a new dataset 10\n",
      "\n",
      "Dataset 2 – Titanic\n",
      "The titanic  dataset contains a sample  of people who were  on the Titanic when it struck\n",
      "an iceberg in 1912 34\n",
      "Let's go ahead and import it, as shown here:\n",
      "titanic = pd 17\n",
      "read_csv('short_titanic 7\n",
      "csv')\n",
      "titanic 5\n",
      "head()\n",
      "\n",
      "\n",
      "This table represents the DataFrame for the dataset short_titanic 14\n",
      "csv 1\n",
      "This data is\n",
      "definitely organized in a row/column structure, as is most spreadsheet data 18\n",
      "Let's take a\n",
      "quick peek at its size, as shown here:\n",
      "titanic 18\n",
      "shape\n",
      "# (891, 5)\n",
      "So, we have 891 rows and 5 columns 20\n",
      "Each row seems to represent a single passenger on\n",
      "the ship and as far as columns are concerned, the following list tells us what they indicate:\n",
      "Survived : This is a binary variable that indicates whether or not the passenger\n",
      "survived the accident ( 1 if they survived, 0 if they died) 63\n",
      "This is likely to be at the\n",
      "nominal level because there are only two options 17\n",
      "\n",
      "Pclass : This is the class that the passenger was traveling in ( 3 for third class,\n",
      "and so on) 25\n",
      "This is at the ordinal level 6\n",
      "\n",
      "Name : This is the name of the passenger, and it is definitely at the nominal level 19\n",
      "\n",
      "Sex: This indicates the gender of the passenger 10\n",
      "It is at the nominal level 6\n",
      "\n",
      "Age: This one is a bit tricky 9\n",
      "Arguably, you may place age at either a qualitative\n",
      "or quantitative level; however, I think that age belongs to a quantitative state,\n",
      "and thus, to the ratio level 35\n",
      "\n",
      "As far as transformations are concerned, usually, we want all columns to be numerical,\n",
      "regardless of their qualitative state 24\n",
      "This means that Name  and Sex will have to be\n",
      "converted into numerical columns somehow 17\n",
      "For Sex, we can change the column to hold 1\n",
      "if the passenger was female and 0 if they were male 25\n",
      "Let's use pandas  to make the change 9\n",
      "\n",
      "We will have to import another Python module, called numpy  or numerical Python, as\n",
      "illustrated:\n",
      "import numpy as np\n",
      "titanic['Sex'] = np 35\n",
      "where(titanic['Sex']=='female', 1, 0)\n",
      "\n",
      "The np 17\n",
      "where  method takes in three things:\n",
      "A list of Booleans ( True  or False )\n",
      "A new value\n",
      "A backup value\n",
      "The method will replace all true  with the first value (in this case 1) and the false  with the\n",
      "second value (in this case 0), leaving us with a new numerical column that represents the\n",
      "same thing as the original Sex column:\n",
      "titanic['Sex']\n",
      "# 0     0\n",
      "# 1     1\n",
      "# 2     1\n",
      "# 3     1\n",
      "# 4     0\n",
      "# 5     0\n",
      "# 6     0\n",
      "# 7     0\n",
      "Let's use a shortcut and describe all the columns at once, as shown:\n",
      "titanic 161\n",
      "describe()\n",
      "\n",
      "\n",
      "This table lists descriptive statistics of the titanic  dataset 13\n",
      "Note how our qualitative\n",
      "columns are being treated as quantitative; however, I'm looking for something irrelevant to\n",
      "the data type 25\n",
      "Note the count row: Survived , Pclass , and Sex all have 891 values (the\n",
      "number of rows), but Age only has 714 values 32\n",
      "Some are missing 3\n",
      "To double verify, let's use\n",
      "the pandas functions, called isnull  and sum, as shown:\n",
      "titanic 25\n",
      "isnull() 3\n",
      "sum()\n",
      "Survived      0\n",
      "Pclass        0\n",
      "Name          0\n",
      "Sex           0\n",
      "Age         177\n",
      "This will show us the number of missing values in each column 41\n",
      "So, Age is the only column\n",
      "with missing values to deal with 14\n",
      "\n",
      "When dealing with missing values, you usually have the following two options:\n",
      "Drop the row with the missing value\n",
      "Try to fill it in\n",
      "Dropping the row is the easy choice; however, you run the risk of losing valuable data 48\n",
      "For\n",
      "example, in this case, we have 177 missing age values (891-714), which are nearly 20% of\n",
      "the data 30\n",
      "To fill in the data, we could either go back to the history books, find each person\n",
      "one by one, and fill in their age, or we can fill in the age with a placeholder value 41\n",
      "\n",
      "Let's fill in each missing value of the Age column with the overall average age of the people\n",
      "in the dataset 24\n",
      "For this, we will use two new methods, called mean  and fillna 16\n",
      "We use\n",
      "isnull  to tell us which values are null and the mean  function to give us the average value\n",
      "of the Age column 29\n",
      "The fillna  method is a pandas method that replaces null values with a\n",
      "given value:\n",
      "print sum(titanic['Age'] 27\n",
      "isnull()) # == 177 missing values\n",
      "average_age = titanic['Age'] 18\n",
      "mean() # get the average age\n",
      "titanic['Age'] 14\n",
      "fillna(average_age, inplace = True) #use the fillna method\n",
      "to remove null values\n",
      "print sum(titanic['Age'] 30\n",
      "isnull()) # == 0 missing values\n",
      "\n",
      "We're done 13\n",
      "We have replaced each value with 26 8\n",
      "69 , the average age in the dataset 9\n",
      "The\n",
      "following code now confirms that no null values exist:\n",
      "titanic 15\n",
      "isnull() 3\n",
      "sum()\n",
      "Survived      0\n",
      "Pclass        0\n",
      "Name          0\n",
      "Sex           0\n",
      "Age           0\n",
      "Great 30\n",
      "Nothing is missing, and we did not have to remove any rows:\n",
      "titanic 17\n",
      "head()\n",
      "At this point, we could start getting a bit more complicated with our questions, for\n",
      "example, what is the average age for a female or a male 33\n",
      "To answer this, we can filter by each\n",
      "gender and take the mean age; pandas has a built-in function for this, called groupby , as\n",
      "illustrated here:\n",
      "titanic 39\n",
      "groupby('Sex')['Age'] 7\n",
      "mean()\n",
      "This means group the data by the Sex column, and then give me the mean age for each group 22\n",
      "This\n",
      "gives us the following output:\n",
      "Sex\n",
      "0      30 15\n",
      "505824\n",
      "1      28 8\n",
      "216730\n",
      "We will ask more of these difficult and complex questions and will be able to answer them\n",
      "with Python and statistics 26\n",
      "\n",
      "\n",
      "Summary\n",
      "Although this is only our first look at data exploration, don't worry, this is definitely not the\n",
      "last time we will follow these steps for data science and exploration 36\n",
      "From now on, every\n",
      "time we look at a new piece of data, we will use our steps of exploration to transform, break\n",
      "down, and standardize our data 35\n",
      "The steps outlined in this chapter, while they are only\n",
      "guidelines, form a standard practice that any data scientist can follow in their work 28\n",
      "The\n",
      "steps can also be applied to any dataset that requires analysis 13\n",
      "\n",
      "We are rapidly approaching the section of the book that deals with statistical, probabilistic,\n",
      "and machine learning models 22\n",
      "Before we can truly jump into these models, we have to look\n",
      "at some of the basics of mathematics 21\n",
      "In the next chapter, we will take a look at some of the\n",
      "math necessary to perform some of the more complicated operations in modeling, but don't\n",
      "worry, the math required for this process is minimal, and we will go through it step by\n",
      "step 54\n",
      "\n",
      "\n",
      "\n",
      "Basic Mathematics\n",
      "It's time to start looking at some basic mathematics principles that are handy when dealing\n",
      "with data science 24\n",
      "The word math  tends to strike fear in the hearts of many, but I aim to \n",
      "make  this as enjoyable as possible 26\n",
      "In this chapter, we will go over the basics of the\n",
      "following topics:\n",
      "Basic symbols/terminology\n",
      "Logarithms/exponents\n",
      "Set theory\n",
      "Calculus\n",
      "Matrix (linear) algebra\n",
      "We will also cover other fields of mathematics 48\n",
      "Moreover, we will see how to apply each of\n",
      "these to various aspects of data science, as well as other scientific endeavors 25\n",
      "\n",
      "Recall that, in a previous chapter, we identified math as being one of the three key\n",
      "components of data science 25\n",
      "In this chapter, I will introduce concepts that will become\n",
      "important later on in this book – when looking at probabilistic and statistical models – and\n",
      "we will also be looking at concepts that will be useful in this chapter 44\n",
      "Regardless of this, all\n",
      "of the concepts in this chapter should be considered fundamental to your quest to become a\n",
      "data scientist 25\n",
      "\n",
      "Mathematics as a discipline\n",
      "Mathematics, as a science, is one of the oldest known forms of logical thinking 24\n",
      "Since\n",
      "ancient Mesopotamia (3,000 BCE), and probably before, humans have been relying on \n",
      "arithmetic  and more challenging forms of math to answer life's biggest questions 39\n",
      "\n",
      "\n",
      "Today, we rely on math for most  aspects of our daily lives; yes, I know that sounds like a\n",
      "cliche, but I mean it 32\n",
      "Whether you are watering your plants or feeding your dog, your\n",
      "internal mathematical engine is constantly working —calculating how much water the plant\n",
      "had per day over the last week and predicting the next time your dog will be hungry given\n",
      "that it is eating right now 53\n",
      "Whether or not you are consciously using the principles of math,\n",
      "the concepts live deep inside everyone's brains 20\n",
      "It's my job as a math teacher to get you to\n",
      "realize it 16\n",
      "\n",
      "Basic symbols and terminology\n",
      "In the following section, we will review  the mathematical concepts of vectors, matrices,\n",
      "arithmetic symbols, and linear algebra, as well as some more subtle notations used  by data\n",
      "scientists 46\n",
      "\n",
      "Vectors and matrices\n",
      "A vector  is defined as an object with  both magnitude and direction 19\n",
      "This definition,\n",
      "however, is a bit complicated 9\n",
      "For our purpose, a vector is simply a 1-dimensional array\n",
      "representing a series of numbers 20\n",
      "Put another way, a vector is a list of numbers 11\n",
      "\n",
      "It is generally represented using an arrow or bold font, shown as follows:\n",
      "Vectors are broken into components, which are individual members of the vector 29\n",
      "We use\n",
      "index notations to denote the element that we are referring to, illustrated as follows:\n",
      "In math, we generally refer to the first element as index  1, as opposed to\n",
      "computer science, where we generally refer to the first element as index  0 56\n",
      "\n",
      "It is important to remember which index system you are using 12\n",
      "\n",
      "\n",
      "In Python, we can represent arrays in many ways 11\n",
      "We could simply use a Python list to\n",
      "represent the preceding array:\n",
      "x = [3, 6, 8]\n",
      "However, it is better to use the numpy  array type to represent arrays, as shown, because it\n",
      "gives us much more utility when performing vector operations:\n",
      "import numpy as np\n",
      "x = np 66\n",
      "array([3, 6, 8])\n",
      "Regardless of the Python representation, vectors give us a simple way of storing multiple\n",
      "dimensions  of a single data point/observation 35\n",
      "\n",
      "If we measure the average satisfaction rating (0-100) of employees in three departments of a\n",
      "company as being 57 for HR, 89 for engineering, and 94 for management 39\n",
      "We can represent\n",
      "this as a vector with the following formula:\n",
      "This vector holds three different bits of information about our data 24\n",
      "This is the perfect use\n",
      "of a vector in data science 12\n",
      "\n",
      "You can also think of a vector as being the theoretical generalization of the pandas Series\n",
      "object 20\n",
      "So, naturally, we need something to represent the DataFrame 11\n",
      "\n",
      "We can extend our notion of an array to move beyond a single dimension and represent\n",
      "data in multiple dimensions 22\n",
      "\n",
      "A matrix  is a two-dimensional representation of arrays of numbers 13\n",
      "Matrices (plural) have\n",
      "two main characteristics that we need to be aware of 17\n",
      "The dimension of a matrix, denoted\n",
      "by n x m (n by m ), tells us that the matrix has n rows and m columns 29\n",
      "Matrices are generally\n",
      "denoted by a capital, bold-faced letter, such as X 18\n",
      "Consider the following example:\n",
      "\n",
      "\n",
      "This is a 3 x 2 (3 by 2 ) matrix because it has three rows and two columns 28\n",
      "\n",
      "If a matrix has the same number of rows and columns, it is called a square\n",
      "matrix 20\n",
      "\n",
      "The matrix is our generalization of the pandas DataFrame 11\n",
      "It is arguably one of the most\n",
      "important mathematical objects in our toolkit 14\n",
      "It is used to hold organized information, in\n",
      "our case, data 14\n",
      "\n",
      "Revisiting our previous example, let's say we have three offices in different locations, each\n",
      "with the same three departments: HR, engineering, and management 33\n",
      "We could make three\n",
      "different vectors, each holding a different office's satisfaction scores, as shown:\n",
      "However, this is not only cumbersome, but also unscalable 33\n",
      "What if you have 100 different\n",
      "offices 10\n",
      "Then you would need to have 100 different one-dimensional arrays to hold this\n",
      "information 17\n",
      "\n",
      "This is where a matrix alleviates this problem 10\n",
      "Let's make a matrix where each row\n",
      "represents a different department and each column represents a different office, as shown :\n",
      "This is much more natural 30\n",
      "Now, let's strip away the labels, and we are left with a matrix 16\n",
      "\n",
      "\n",
      "\n",
      "Quick exercises\n",
      "If we added a fourth office, would we need  a new row or column 20\n",
      "1 2\n",
      "\n",
      "What would the dimension of the matrix  be after we added the fourth office 16\n",
      "2 2\n",
      "\n",
      "If we eliminate the management department from the original X matrix, what 3 16\n",
      "\n",
      "would the dimension of the new matrix be 9\n",
      "\n",
      "What is the general formula to find out the number of elements in the matrix 16\n",
      "4 2\n",
      "\n",
      "Answers\n",
      "Column1 5\n",
      "\n",
      "3 x 4 2 7\n",
      "\n",
      "2 x 3 3 7\n",
      "\n",
      "n × m (n being the number  of rows and m being the number  of columns) 4 23\n",
      "\n",
      "Arithmetic symbols\n",
      "In this section, we will go over some  symbols associated with basic arithmetic that appear\n",
      "in most, if not all, data science tutorials and books 35\n",
      "\n",
      "Summation\n",
      "The uppercase sigma ∑ symbol is a universal symbol for addition 17\n",
      "Whatever is to the right\n",
      "of the sigma symbol is usually  something iterable, meaning that we can go over it one by\n",
      "one (for example, a vector) 34\n",
      "\n",
      "For example, let's create the representation of a vector:\n",
      "X = [1, 2, 3, 4, 5]\n",
      "To find the sum of the content, we can use the following formula:\n",
      "In Python, we can use the following formula:\n",
      "sum(x) # == 15\n",
      "\n",
      "For example, the formula for calculating the mean of a series of numbers is quite common 80\n",
      "\n",
      "If we have a vector ( x) of length n, the mean of the vector can be calculated as follows:\n",
      "This means that we will add up each element of x, denoted by xi, and then multiply the sum\n",
      "by 1/n, otherwise known as dividing by n, (the length of the vector) 66\n",
      "\n",
      "Proportional\n",
      "The lowercase alpha symbol, α, represents values  that are proportional to each other 21\n",
      "This\n",
      "means that as one value changes, so does the other 13\n",
      "The direction in which the values move\n",
      "depends on how the values are proportional 15\n",
      "Values can either vary directly or indirectly 7\n",
      "If\n",
      "values vary directly, they both move in the same direction (as one goes up, so does the\n",
      "other) 25\n",
      "If they vary indirectly, they move in opposite directions (if one goes down, the other\n",
      "goes up) 23\n",
      "\n",
      "Consider the following examples:\n",
      "The sales of a company vary directly with the number of customers 18\n",
      "This can be\n",
      "written as Sales α Customers 9\n",
      "\n",
      "Gas prices vary (usually) indirectly with oil availability, meaning that as the\n",
      "availability of oil goes down (it's more scarce), gas prices go up 32\n",
      "This can be\n",
      "denoted as Gas α Availability 10\n",
      "\n",
      "Later on, we will see a very  important formula called the Bayes' formula , which includes a\n",
      "variation symbol 25\n",
      "\n",
      "Dot product\n",
      "The dot product is an operator like addition and multiplication 14\n",
      "It is used to combine two\n",
      "vectors, as shown:\n",
      "\n",
      "\n",
      "So, what does this mean 19\n",
      "Let's say we have a vector that represents a customer's sentiments\n",
      "toward three genres of movies: comedy, romance, and action 27\n",
      "\n",
      "When using a dot product, note that an answer is a single number, known\n",
      "as a scalar 21\n",
      "\n",
      "On a scale of 1-5, a customer loves  comedies, hates romantic movies, and is alright with\n",
      "action movies 28\n",
      "We might represent this as follows:\n",
      "Here, 5 denotes their love for comedies, 1 their hatred of romantic movies, and 3 the\n",
      "customer's indifference toward action movies 37\n",
      "\n",
      "Now, let's assume that we have two new movies, one of which is a romantic comedy and\n",
      "the other is a funny action movie 29\n",
      "The movies would have their own vector of qualities, as\n",
      "shown:\n",
      "Here, m1 is our romantic comedy and m2 is our funny action movie 30\n",
      "\n",
      "In order to make a recommendation, we will apply the dot product between the customer's\n",
      "preferences for each movie 23\n",
      "The higher value will win and, therefore, will be recommended\n",
      "to the user 16\n",
      "\n",
      "Let's compute the recommendation score for each movie 10\n",
      "For movie 1, we want to compute\n",
      "the following:\n",
      "\n",
      "\n",
      "We can think of this problem like as follows:\n",
      "The answer we obtain is 28, but what does this number mean 37\n",
      "On what scale is it 5\n",
      "Well, the\n",
      "best score anyone can ever get is when all values are 5, making the outcome as follows:\n",
      "The lowest possible score is when all values are 1, as shown:\n",
      "So, we must think about 28 on a scale from 3 to 75 56\n",
      "To do this, imagine a number line from 3\n",
      "to 75 and where 28 would be on it 23\n",
      "This is illustrated as follows:\n",
      "Not that far 9\n",
      "Let's try for movie 2:\n",
      "\n",
      "\n",
      "This is higher than 28 14\n",
      "Putting this number on the same timeline as before, we can also\n",
      "visually observe that it is a much better score, as shown:\n",
      "So, between movie 1 and movie 2, we would definitely recommend movie 2 to our user 49\n",
      "\n",
      "This is, in essence, how most movie prediction engines work 13\n",
      "They build a customer profile,\n",
      "which is represented as a vector 12\n",
      "They then take a vector representation of each movie they\n",
      "have to offer, combine them with the customer profile (perhaps with a dot product), and\n",
      "make recommendations from there 34\n",
      "Of course, most companies must do this on a much\n",
      "larger scale, which is where  a particular field of mathematics, called linear algebra , can be\n",
      "very useful; we will look at it later in the chapter 45\n",
      "\n",
      "Graphs\n",
      "No doubt you have encountered dozens, if not hundreds, of graphs in your life so far 22\n",
      "I'd\n",
      "like to mostly talk about conventions with regard to graphs and notations 16\n",
      "\n",
      "The following is a basic Cartesian graph  (x and y coordinates) 15\n",
      "The x and y notations are\n",
      "very standard but sometimes do not entirely explain the big picture 19\n",
      "We sometimes refer  to\n",
      "the x variable as being the independent variable and the y as the dependent variable 21\n",
      "This is\n",
      "because when we write functions, we tend to speak about them as being y is a function of x ,\n",
      "meaning that the value of y is dependent on the value of x 37\n",
      "This is what a graph is trying to\n",
      "show 10\n",
      "\n",
      "Suppose we have two points on a graph, as shown:\n",
      "We refer to the points as (x1, y1) and (x2, y2) 35\n",
      "\n",
      "\n",
      "The slope  between these two points is defined as follows:\n",
      "You have probably seen this formula before, but it is worth mentioning, if only for its\n",
      "significance 34\n",
      "The slope defines the rate of change between the two points 11\n",
      "Rates of change\n",
      "can be very important in data science, specifically in areas involving differential equations\n",
      "and calculus 21\n",
      "\n",
      "Rates of change are a way of representing how variables move together and to what degree 17\n",
      "\n",
      "Imagine we are modeling the temperature of your coffee in relation to the time that it has\n",
      "been sitting outside 22\n",
      "Perhaps we have a rate of change as follows:\n",
      "This rate of change is telling us that for every single minute, our coffee's temperature is\n",
      "dropping by two degrees Fahrenheit 35\n",
      "\n",
      "Later on in this book,  we will look at a machine learning algorithm called linear regression 19\n",
      "\n",
      "In linear regression, we are concerned with the rates of change between variables, as they\n",
      "allow us to exploit this relationship for predictive purposes 28\n",
      "\n",
      "Think of the Cartesian plane as being an infinite plane of vectors with two\n",
      "elements 17\n",
      "When people refer to higher dimensions, such as 3D or 4D, they\n",
      "are merely referring to an infinite space that holds vectors with more\n",
      "elements 33\n",
      "A 3D space holds vectors of length three while a 7D space holds\n",
      "vectors with seven elements in them 25\n",
      "\n",
      "Logarithms/exponents\n",
      "An exponent  tells you how many  times you have  to multiply a number by itself, as\n",
      "illustrated:\n",
      "\n",
      "\n",
      "A logarithm  is the number that answers the question \"what exponent gets me from the base\n",
      "to this other number 55\n",
      "This can be denoted as follows:\n",
      "If these two concepts seem similar, then you are correct 19\n",
      "Exponents and logarithms are\n",
      "heavily related 11\n",
      "In fact, the words exponent and logarithm actually mean the same thing 14\n",
      "A\n",
      "logarithm is an exponent 8\n",
      "The preceding two equations are actually two versions of the\n",
      "same thing 13\n",
      "The basic idea is that 2 times 2 times 2 times 2 is 16 19\n",
      "\n",
      "The following is a depiction of how we can use both versions to say the same thing 18\n",
      "Note\n",
      "how I use arrows to move from the log formula to the exponent formula:\n",
      "Consider the following examples:\n",
      "Note something interesting 25\n",
      "Let's rewrite the first equation:\n",
      "We then replace 81 with the equivalent statement,  , as follows:\n",
      "Something interesting to note : the 3s seem to cancel out 35\n",
      "This is actually very important\n",
      "when dealing with numbers more difficult to work with than 3s and 4s 23\n",
      "\n",
      "Exponents and logarithms  are most important when  dealing with growth 15\n",
      "More often than\n",
      "not, if a quantity is growing (or declining in growth), an exponent/logarithm can help\n",
      "model this behavior 28\n",
      "\n",
      "\n",
      "For example, the number e is around 2 11\n",
      "718 and has many practical applications 7\n",
      "A very\n",
      "common application is interest  calculation for saving 11\n",
      "Suppose you have $5,000 deposited\n",
      "in a bank with continuously compounded interest at the rate of 3%, then we can use the\n",
      "following formula to model the growth of your deposit:\n",
      "In this formula:\n",
      "A denotes the final amount\n",
      "P denotes the principal investment ( 5000 )\n",
      "e denotes a constant ( 2 66\n",
      "718 )\n",
      "r denotes the rate of growth ( 10\n",
      "03)\n",
      "t denotes the time (in years)\n",
      "We are curious; when will our investment double 20\n",
      "How long would I have to have my\n",
      "money in this investment to achieve 100% growth 19\n",
      "We can write this in mathematical form,\n",
      "as follows:\n",
      " (divided by 5000 on both sides)\n",
      "At this point, we have a variable in the exponent that we want to solve 38\n",
      "When this happens,\n",
      "we can use the logarithm notation to figure it out:\n",
      "This leaves us with  \n",
      "When we take the logarithm of a number with a base of e, it is called a natural logarithm 43\n",
      "We\n",
      "rewrite the logarithm as follows:\n",
      "\n",
      "\n",
      "Using a calculator (or Python), we find that \n",
      "This means that it would take 2 28\n",
      "31 years to double our money 7\n",
      "\n",
      "Set theory\n",
      "Set theory involves mathematical operations at the set level 13\n",
      "It is sometimes thought of as a\n",
      "basic fundamental group  of theorems that governs the rest of mathematics 23\n",
      "For our\n",
      "purpose, we'll use set theory to manipulate groups of elements 15\n",
      "\n",
      "A set is a collection of distinct objects 9\n",
      "\n",
      "That's it 4\n",
      "A set can be thought of as a list in Python, but with no repeat objects 17\n",
      "In fact, there\n",
      "is even a set of objects in Python:\n",
      "s = set()\n",
      "s = set([1, 2, 2, 3, 2, 1, 2, 2, 3, 2])\n",
      "# will remove duplicates from a list\n",
      "s == {1, 2, 3}\n",
      "Note that, in Python, curly braces {, } can denote either a set or a\n",
      "dictionary 90\n",
      "\n",
      "Remember that a dictionary in Python is a set of key-value pairs, for\n",
      "example:\n",
      "dict = {\"dog\": \"human's best friend\", \"cat\": \"destroyer of world\"}\n",
      "dict[\"dog\"]# == \"human's best friend\"\n",
      "len(dict[\"cat\"]) # == 18\n",
      "# but if we try to create a pair with the same key as an existing key\n",
      "dict[\"dog\"] = \"Arf\"\n",
      "dict\n",
      "{\"dog\": \"Arf\", \"cat\": \"destroyer of world\"}\n",
      "# It will override the previous value\n",
      "# dictionaries cannot have two values for one key 123\n",
      "\n",
      "\n",
      "They share this notation because they share a quality in that sets cannot have duplicate\n",
      "elements, just as dictionaries cannot have duplicate keys 26\n",
      "\n",
      "The magnitude  of a set is the number of elements in the set and is represented as follows:\n",
      "s  # == {1,2,3}\n",
      "len(s) == 3 # magnitude of s\n",
      "The concept of an empty set exists and is denoted by the character  {} 59\n",
      "\n",
      "This null  set is said to have a magnitude of 0 14\n",
      "\n",
      "If we wish to denote that an element is within a set, we use the epsilon notation, as shown:\n",
      "2 ∈ {1,2,3}\n",
      "This means that the  2 element exists in the set of 1, 2, and 3 54\n",
      "If one set is entirely inside\n",
      "another set, we say that it is a subset  of its larger counterpart:\n",
      "A= {1,5,6}, B={1,5,6,7,8}\n",
      "A ⊆ B                                                                                                   \n",
      " \n",
      "So, A is a subset of B and B is called the superset  of A 69\n",
      "If A is a subset of B but A does not\n",
      "equal B (meaning that there is at least one element in B that is not in A), then A is called a\n",
      "proper subset  of B 42\n",
      "\n",
      "Consider the following examples:\n",
      "A set of even numbers is a subset of all integers\n",
      "Every set is a subset, but not a proper subset, of itself\n",
      "A set of all tweets is a superset of English tweets\n",
      "In data science, we use sets (and lists) to represent a list of objects and, often, to generalize\n",
      "the behavior of consumers 74\n",
      "It is common to reduce a customer to a set of characteristics 12\n",
      "\n",
      "\n",
      "Imagine we are a marketing firm  trying to predict where a person wants to shop for clothes 19\n",
      "\n",
      "We are given a set of clothing brands the user has previously visited, and our goal is to\n",
      "predict a new store that they would also enjoy 30\n",
      "Suppose a specific user has previously\n",
      "shopped at the following stores:\n",
      "user1 = {\"Target\",\"Banana Republic\",\"Old Navy\"}\n",
      "# note that we use {} notation to create a set\n",
      "# compare that to using [] to make a list\n",
      "So, user1  has previously shopped at Target , Banana Republic , and Old Navy 68\n",
      "Let's\n",
      "also look at a different user, called user2 , as shown:\n",
      "user2 = {\"Banana Republic\",\"Gap\",\"Kohl's\"}\n",
      "Suppose we are wondering how similar these users are 41\n",
      "With the limited information we\n",
      "have, one way to define similarity is to see how many stores there are that they both shop\n",
      "at 27\n",
      "This is called an intersection :\n",
      "The intersection of two sets is a set whose elements appear in both sets 20\n",
      "It is denoted using\n",
      "the ∩ symbo l, as shown:\n",
      "The intersection of the two users is just one store 26\n",
      "So, right away, that doesn't seem great 10\n",
      "\n",
      "However, each user only has three elements in their set, so having 1/3 does not seem as bad 24\n",
      "\n",
      "Suppose we are curious about how many stores are represented between the two of them;\n",
      "this is called a union 23\n",
      "\n",
      "The union of two sets is a set whose elements appear in either set 15\n",
      "It is denoted using the\n",
      "symbol ∪, as shown:\n",
      "\n",
      "\n",
      "When looking at the similarities between user1  and user2 , we should use a combination of\n",
      "the union and the intersection of their sets 42\n",
      "user1  and user2  have one element in common\n",
      "out of a total of five distinct elements between them 23\n",
      "So, we can define the similarity\n",
      "between the two users as follows:\n",
      "                                                     \n",
      "          \n",
      "In fact, this has a name in set theory 28\n",
      "It is called the Jaccard measure 8\n",
      "In general, for the A\n",
      "and B sets, the Jaccard  measure (Jaccard similarity) between the two sets is defined as\n",
      "follows:\n",
      "                                                          \n",
      "It can also be defined as the magnitude of the intersection of the two sets divided by the\n",
      "magnitude of the union of the two sets 63\n",
      "\n",
      "This gives us a way to quantify similarities between elements represented with sets 14\n",
      "\n",
      "Intuitively, the Jaccard measure is a number between 0 and 1, such that when the number is\n",
      "closer to 0, people are more dissimilar and when the measure is closer to 1, people are\n",
      "considered similar to each other 57\n",
      "\n",
      "If we think about the definition, then it actually makes sense 13\n",
      "Take a look at the measure\n",
      "once more:\n",
      "Here, the numerator represents the number of stores that the users have in common (in the\n",
      "sense that they like shopping there), while the denominator represents the unique number\n",
      "of stores that they like put together 51\n",
      "\n",
      "We can represent this in Python using some simple code, as shown:\n",
      "user1 = {\"Target\",\"Banana Republic\",\"Old Navy\"}\n",
      "user2 = {\"Banana Republic\",\"Gap\",\"Kohl's\"}\n",
      "def jaccard(user1, user2):\n",
      "  stores_in_common = len(user1 & user2)\n",
      "  stores_all_together = len(user1 | user2)\n",
      "\n",
      "  return stores / float(stores_all_together)\n",
      "# I cast stores_all_together as a float to return a decimal answer instead\n",
      "of python's default integer division\n",
      "# so\n",
      "jaccard(user1, user2) == # 0 128\n",
      "2 or 1/5\n",
      "Set theory becomes highly prevalent when we enter the world of probability and also when\n",
      "dealing with high-dimensional data 30\n",
      "We will use sets to represent real-world events taking\n",
      "place, and probability becomes set theory with vocabulary on top of it 24\n",
      "\n",
      "Linear algebra\n",
      "Remember the movie recommendation engine  we looked at earlier 14\n",
      "What if we had 10,000\n",
      "movies to recommend and we had to choose only 10 to give to the user 25\n",
      "We'd have to take\n",
      "a dot product between the user profile and each of the 10,000 movies 22\n",
      "Linear algebra\n",
      "provides the tools to make these calculations much more efficient 14\n",
      "\n",
      "It is an area of mathematics that deals with the math of matrices and vectors 16\n",
      "It has the aim\n",
      "of breaking down these objects and reconstructing them in order to provide practical\n",
      "applications 21\n",
      "Let's look at a few linear algebra rules before proceeding 11\n",
      "\n",
      "Matrix multiplication\n",
      "Like numbers, we can multiply  matrices together 13\n",
      "Multiplying matrices is, in essence, a\n",
      "mass-produced way of taking several dot products at once 20\n",
      "Let's, for example, try to \n",
      "multiply  the following matrices:\n",
      "We need to consider a couple of things:\n",
      "Unlike numbers, multiplication of matrices is not commutative , meaning that the\n",
      "order in which you multiply matrices matters a great deal 50\n",
      "\n",
      "In order to multiply matrices, their dimensions must match up 12\n",
      "This means that\n",
      "the first matrix must have the same number of columns as the second matrix has\n",
      "rows 21\n",
      "\n",
      "\n",
      "To remember this, write out the dimensions of the matrices 12\n",
      "In this case, we have a 3 x 2\n",
      "times a 2 x 2 matrix 21\n",
      "You can multiply matrices together if the second number in the first\n",
      "dimension pair is the same as the first number in the second dimension pair:\n",
      "                                                                          \n",
      "The resulting matrix will always have dimensions equal to the outer numbers in the\n",
      "dimension pairs (the ones you did not circle) 55\n",
      "In this case, the resulting matrix will have a\n",
      "dimension of 3 x 2 18\n",
      "\n",
      "How to multiply matrices\n",
      "To multiply matrices, there is actually quite  a simple procedure 18\n",
      "Essentially, we are\n",
      "performing a bunch of dot products 12\n",
      "\n",
      "Recall our earlier sample problem, which was as follows:\n",
      "We know that our resulting matrix will have a dimension of 3 x 2 29\n",
      "So, we know it will look\n",
      "something like the following:\n",
      "Note that each element of the matrix is indexed using a double index 26\n",
      "The\n",
      "first number represents the row, and the second number represents the\n",
      "column 16\n",
      "So, the m32 element is the element in the third row of the second\n",
      "column 18\n",
      "Each element is the result of a dot product between rows and\n",
      "columns of the original matrices 18\n",
      "\n",
      "\n",
      "The mxy element is the result of the dot product of the xth row of the first matrix and the yth\n",
      "column of the second matrix 31\n",
      "Let's solve a few:\n",
      "Moving on, we will eventually get a resulting matrix that looks as follows:\n",
      "Way to go 24\n",
      "Let's come back to the movie recommendation example 9\n",
      "Recall the user's movie\n",
      "genre preferences of comedy, romance, and action, which are illustrated as follows:\n",
      "Now suppose we have 10,000 movies, all with a rating for these three categories 40\n",
      "To make a\n",
      "recommendation, we need to take the dot product of the preference vector with each of the\n",
      "10,000 movies 27\n",
      "We can use matrix multiplication to represent this 8\n",
      "\n",
      "Instead of writing them all out, let's express them using the matrix notation 16\n",
      "We already\n",
      "have U, defined here as the user's preference vector (it can also be thought of as a 3 x 1\n",
      "matrix), and we also need a movie matrix:\n",
      "M = movies = 3 x 10,000\n",
      "So, now we have two matrices; one is 3 x 1 and the other is 3 x 10,000 77\n",
      "We can't multiply\n",
      "these matrices as they are, because the dimensions do not work out 18\n",
      "We will have to change\n",
      "U a bit 9\n",
      "We can take the transpose  of the matrix (turning all rows into columns and columns\n",
      "into rows) 22\n",
      "This will switch the dimensions around:\n",
      "\n",
      "\n",
      "So, now we have two matrices  that can be multiplied together 20\n",
      "Let's visualize what this\n",
      "looks like:\n",
      "                                                                                     1  x  3                     3  x  1000\n",
      "The resulting matrix will be a 1 x 1,000  matrix (a vector) of 10,000 predictions for each\n",
      "individual movie 58\n",
      "Let's try this out in Python:\n",
      "import numpy as np\n",
      "# create user preferences\n",
      "user_pref = np 22\n",
      "array([5, 1, 3])\n",
      "# create a random movie matrix of 10,000 movies\n",
      "movies = np 26\n",
      "random 1\n",
      "randint(5,size=(3,1000))+1\n",
      "# Note that the randint will make random integers from 0-4\n",
      "# so I added a 1 at the end to increase the scale from 1-5\n",
      "We are using the numpy  array function to create our matrices 59\n",
      "We will have both a\n",
      "user_pref  and a movies  matrix to represent our data 18\n",
      "\n",
      "To check our dimensions, we can use the numpy shape  variable, as shown:\n",
      "print(user_pref 21\n",
      "shape) # (1, 3)\n",
      "print(movies 12\n",
      "shape) # (3, 1000)\n",
      "This checks out 13\n",
      "Last but not least, let's use the matrix multiplication method of numpy\n",
      "(called dot) to perform the operation, as illustrated:\n",
      "# np 29\n",
      "dot does both dot products and matrix multiplication\n",
      "np 10\n",
      "dot(user_pref, movies)\n",
      "The result is an array of integers that represents the recommendations for each movie 20\n",
      "\n",
      "\n",
      "For a quick extension of this, let's run some code  that predicts across more than 10,000\n",
      "movies, as shown:\n",
      "import numpy as np\n",
      "import time\n",
      "for num_movies in (10000, 100000, 1000000, 10000000, 100000000):\n",
      "   movies = np 68\n",
      "random 1\n",
      "randint(5,size=(3, num_movies))+1\n",
      "    now = time 16\n",
      "time()\n",
      "    np 4\n",
      "dot(user_pref, movies)\n",
      "    print((time 10\n",
      "time() - now), \"seconds to run\", num_movies, \"movies\")\n",
      "0 17\n",
      "000160932540894 seconds to run 10000 movies\n",
      "0 15\n",
      "00121188163757 seconds to run 100000 movies\n",
      "0 15\n",
      "0105860233307 seconds to run 1000000 movies\n",
      "0 16\n",
      "096577167511 seconds to run 10000000 movies\n",
      "4 15\n",
      "16197991371 seconds to run 100000000 movies\n",
      "It took only a bit over four seconds to run through 100,000,000 movies using matrix\n",
      "multiplication 37\n",
      "\n",
      "Summary\n",
      "In this chapter, we took a look at some basic mathematical principles that will become very\n",
      "important as we progress through this book 28\n",
      "Between logarithms/exponents, matrix\n",
      "algebra, and proportionality, mathematics clearly has a big role not just in the analysis of\n",
      "data but in many aspects of our lives 36\n",
      "\n",
      "The coming chapters will take a much deeper dive into two big areas of mathematics:\n",
      "probability and statistics 20\n",
      "Our goal will be to define and interpret the smallest and biggest\n",
      "theorems in these two giant fields of mathematics 23\n",
      "\n",
      "It is in the next few chapters that everything will start to come together 15\n",
      "So far in this book,\n",
      "we have looked at math examples, data exploration guidelines, and basic insights into\n",
      "types of data 25\n",
      "It is time to begin to tie all of these concepts together 12\n",
      "\n",
      "\n",
      "\n",
      "Impossible or Improbable – A\n",
      "Gentle Introduction to\n",
      "Probability\n",
      "Over the next few chapters, we will explore both probability and statistics as methods of\n",
      "examining both data-driven situations and real-world scenarios 44\n",
      "The rules of probability\n",
      "govern the basics of prediction 11\n",
      "We use probability to define the chances of the occurrence\n",
      "of an event 14\n",
      "\n",
      "In this chapter, we will look at the following topics:\n",
      "What is the probability 17\n",
      "\n",
      "The differences between the Frequentist approach and the Bayesian approach\n",
      "How to visualize probability\n",
      "How to utilize the rules of probability\n",
      "Using confusion matrices to look at the basic metrics\n",
      "Probability will help us model real-life events that include a sense of randomness and\n",
      "chance 55\n",
      "Over the next two chapters, we will look at the terminology behind probability\n",
      "theorems and how to apply them to model situations that can appear unexpectedly 30\n",
      "\n",
      "Basic definitions\n",
      "One of the most basic concepts of probability is the concept of a procedure 18\n",
      "A procedure  is\n",
      "an act that leads  to a result, for example, throwing a die or visiting a website 24\n",
      "\n",
      "\n",
      "An event  is a collection of the outcomes of a procedure, such as getting a head on a coin flip\n",
      "or leaving a website after only 4 seconds 33\n",
      "A simple event is an outcome/event of a\n",
      "procedure that cannot be broken down further 17\n",
      "For example, rolling two dice can be broken\n",
      "down into two simple events: rolling die 1 and rolling die 2 25\n",
      "\n",
      "The sample space  of a procedure is the set of all possible simple events 16\n",
      "For example, an\n",
      "experiment is performed in which a coin is flipped three times in succession 18\n",
      "What is the\n",
      "size of the sample space for this experiment 12\n",
      "\n",
      "The answer is eight because the results could be any one of the possibilities in the following\n",
      "sample space: {HHH, HHT, HTT, HTH, TTT, TTH, THH, or THT} 48\n",
      "\n",
      "Probability\n",
      "The probability  of an event represents the frequency, or chance, that the event will happen 21\n",
      "\n",
      "For notation , if A is an event, P(A)  is the probability of the occurrence of the event 23\n",
      "\n",
      "We can define the actual probability of an event, A, as follows:\n",
      "Here, A is the event in question 24\n",
      "Think of an entire universe of events where anything is\n",
      "possible, and let's represent it as a circle 21\n",
      "We can think of a single event, A, as being a\n",
      "smaller circle within that larger universe, as shown in the following diagram:\n",
      "\n",
      "\n",
      "Let's now pretend that our universe involves a research study on humans and the A event\n",
      "is people in that study who have cancer 55\n",
      "\n",
      "If our study has 100 people and A has 25 people, the probability of A or P(A)  is 25/100 29\n",
      "\n",
      "The maximum probability of any event is 1 10\n",
      "This can be understood as the red circle grows\n",
      "so large that it is the size of the universe (the larger circle) 25\n",
      "\n",
      "The most basic examples (I promise they will get more interesting) are coin flips 17\n",
      "Let's say\n",
      "we have two coins and we want the probability that we will get two heads 19\n",
      "We can very\n",
      "easily count the number of ways two coins could end up being two heads 19\n",
      "There's only one 4\n",
      "\n",
      "Both coins have to be heads 7\n",
      "But how many options are there 6\n",
      "It could either be two heads,\n",
      "two tails, or a heads/tails combination 16\n",
      "\n",
      "First, let's define A 7\n",
      "It is the event in which two heads occur 9\n",
      "The number of ways that A can\n",
      "occur is 1 13\n",
      "\n",
      "The sample space of the experiment is {HH, HT, TH, TT} , where each two-letter word\n",
      "indicates the outcome of the first and second coin simultaneously 35\n",
      "The size of the sample\n",
      "space is four 9\n",
      "So, P(getting two heads) = 1/4 13\n",
      "\n",
      "Let's refer to a quick visual table to prove it 12\n",
      "The following table denotes the options for\n",
      "coin 1 as the columns, and the options for coin 2 as the rows 25\n",
      "In each cell, there is\n",
      "either True  or False 12\n",
      "A True  value indicates that it satisfies the condition (both heads),\n",
      "and False  indicates otherwise:\n",
      "Coin 1 is heads Coin 1 is tails\n",
      "Coin 2 is heads True False\n",
      "Coin 2 is tails False False\n",
      "So, we have one out of a total of four possible outcomes 60\n",
      "\n",
      "Bayesian versus Frequentist\n",
      "The preceding example was almost  too easy 16\n",
      "In practice, we can hardly ever truly count the\n",
      "number of ways something can happen 17\n",
      "For example, let's say that we want to know the\n",
      "probability of a random  person smoking cigarettes at least once a day 26\n",
      "If we wanted  to\n",
      "approach this problem using  the classical way (the previous formula), we would need to\n",
      "figure out how many different ways a person is a smoker —someone who smokes at least\n",
      "once a day —which is not possible 52\n",
      "\n",
      "\n",
      "When faced with such a problem, two main schools of thought are considered when it\n",
      "comes to calculating probabilities in practice: the Frequentist approach  and the Bayesian\n",
      "approach 37\n",
      "This chapter will focus heavily on the Frequentist approach, while the\n",
      "subsequent chapter will dive into the Bayesian approach 24\n",
      "\n",
      "Frequentist approach\n",
      "In a Frequentist approach, the probability of an event  is calculated through\n",
      "experimentation 25\n",
      "It uses the past in order to predict the future chance of an event 14\n",
      "The basic\n",
      "formula is as follows:\n",
      "Basically, we observe several instances of the event and count the number of times A was\n",
      "satisfied 28\n",
      "The division of these numbers is an approximation of the probability 11\n",
      "\n",
      "The Bayesian approach differs by dictating that probabilities must be discerned using\n",
      "theoretical means 19\n",
      "Using the Bayes approach, we would have to think a bit more critically\n",
      "about events and why they occur 22\n",
      "Neither methodology is wholly the correct answer all of\n",
      "the time 12\n",
      "Usually, it comes down to the problem and the difficulty of using either approach 15\n",
      "\n",
      "The crux of the Frequentist approach is the relative frequency 14\n",
      "\n",
      "The relative frequency  of an event  is how often an event occurs divided by the total\n",
      "number of observations 23\n",
      "\n",
      "Example – marketing stats\n",
      "Let's say that you are interested in ascertaining how often a person who visits your website\n",
      "is likely to return on a later date 34\n",
      "This is sometimes called the rate of repeat visitors 9\n",
      "In the\n",
      "previous definition, we would define our A event as being a visitor coming back to the site 21\n",
      "\n",
      "We would then have to calculate the number of ways a person can come back, which\n",
      "doesn't really make sense at all 26\n",
      "In this case, many people would turn to a Bayesian\n",
      "approach; however, we can calculate what is known as relative frequency 26\n",
      "\n",
      "So, in this case, we can take the visitor logs and calculate the relative frequency of event A\n",
      "(repeat visitors) 26\n",
      "Let's say, of the 1,458  unique visitors in the past week, 452 were repeat\n",
      "visitors 25\n",
      "We can calculate this as follows:\n",
      "\n",
      "\n",
      "So, about 31% of your visitors are repeat visitors 19\n",
      "\n",
      "The law of large numbers\n",
      "The reason that even the Frequentist approach  can do this is that of the law of large\n",
      "numbers, which states that if we repeat a procedure over and over, the relative frequency\n",
      "probability will approach the actual probability 52\n",
      "Let's try to demonstrate this using Python 8\n",
      "\n",
      "If I were to ask you the average of the numbers 1 and 10, you would very quickly answer\n",
      "around 5 27\n",
      "This question is identical to asking you to pick the average  number between 1 and\n",
      "10 19\n",
      "\n",
      "Python will choose n random numbers between 1 and 10 and find their average 17\n",
      "\n",
      "We will repeat this experiment several times using a larger n each time, and then we will\n",
      "graph the outcome 23\n",
      "The steps are as follows:\n",
      "Pick a random number between 1 and 10 and find the average1 21\n",
      "\n",
      "Pick two random numbers between 1 and 10 and find their average2 16\n",
      "\n",
      "Pick three random numbers between 1 and 10 and find their average3 16\n",
      "\n",
      "Pick 10,000 random numbers between 1 and 10 and find their average4 19\n",
      "\n",
      "Graph the results5 5\n",
      "\n",
      "Let's take a look at the code:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "results = []\n",
      "for n in range(1,10000):\n",
      "    nums = np 48\n",
      "random 1\n",
      "randint(low=1,high=10, size=n) # choose n numbers\n",
      "between 1 and 10\n",
      "    mean = nums 28\n",
      "mean()                              # find the average of\n",
      "these numbers\n",
      "    results 14\n",
      "append(mean)                            # add the average to a\n",
      "running list\n",
      "# POP QUIZ: How large is the list results 25\n",
      "\n",
      "len(results) # 9999\n",
      "# This was tricky because I took the range from 1 to 10000 and usually we\n",
      "do from 0 to 10000\n",
      "df = pd 41\n",
      "DataFrame({ 'means' : results})\n",
      "print (df 11\n",
      "head()) # the averages in the beginning are all over the place 13\n",
      "\n",
      "# means\n",
      "\n",
      "# 9 7\n",
      "0\n",
      "# 5 6\n",
      "0\n",
      "# 6 6\n",
      "0\n",
      "# 4 6\n",
      "5\n",
      "# 4 6\n",
      "0\n",
      "print (df 6\n",
      "tail()) # as n, our size of the sample size, increases, the\n",
      "averages get closer to 5 24\n",
      "\n",
      "# means\n",
      "# 4 7\n",
      "998799\n",
      "# 5 7\n",
      "060924\n",
      "# 4 7\n",
      "990597\n",
      "# 5 7\n",
      "008802\n",
      "# 4 7\n",
      "979198\n",
      "df 5\n",
      "plot(title='Law of Large Numbers')\n",
      "plt 9\n",
      "xlabel(\"Number of throws in sample\")\n",
      "plt 9\n",
      "ylabel(\"Average Of Sample\")\n",
      "Cool, right 9\n",
      "What this is essentially showing us is that as we increase the sample size of our\n",
      "relative frequency, the frequency approaches the actual average (probability) of 5 32\n",
      "\n",
      "In our statistics chapters, we will work to define this law much more rigorously, but for\n",
      "now, just know that it is used to link the relative frequency of an event to its actual\n",
      "probability 42\n",
      "\n",
      "\n",
      "Compound events\n",
      "Sometimes, we need to deal with two or more events 15\n",
      "These are called compound events 5\n",
      "A\n",
      "compound event is an event that combines two or more simple events 14\n",
      "When this happens,\n",
      "we need a special notation 9\n",
      "\n",
      "Given events A and B:\n",
      "The probability that A and B occur is P(A ∩ B) = P(A and B)\n",
      "The probability that either A or B occurs is P(A ∪ B) = P(A or B)\n",
      "Understanding why we use the set notation for these compound events is very important 62\n",
      "\n",
      "Remember how we represented events in a universe using circles earlier 12\n",
      "Let's say that our\n",
      "universe  is 100 people who showed up for an experiment in which a new test for cancer is\n",
      "being developed:\n",
      "In the preceding diagram, the red circle, A, represents 25 people who actually have cancer 50\n",
      "\n",
      "Using the relative frequency approach, we can say that P(A) = number of people with\n",
      "cancer/number of people in study , that is, 25/100 = ¼ = 40\n",
      "25 2\n",
      "This means that there is a 25%\n",
      "chance that someone has cancer 15\n",
      "\n",
      "\n",
      "Let's introduce a second event, called B, as shown, which contains people for whom the test\n",
      "was positive (it claimed that they had cancer) 32\n",
      "Let's say that this is for 30 people 10\n",
      "So, P(B) =\n",
      "30/100 = 3/10 = 15\n",
      "3 2\n",
      "This means that there is a 30% chance that the test said positive for any\n",
      "given person:\n",
      "These are two separate events, but they interact with each other 33\n",
      "Namely, they might\n",
      "intersect  or have people in common, as shown here:\n",
      "\n",
      "\n",
      "Anyone in the space that both A and B occupies, otherwise known as A intersect B  or A ∩ B,\n",
      "are people for whom the test claimed they were positive for cancer ( A), and they actually\n",
      "do have cancer 64\n",
      "Let's say that's 20 people 8\n",
      "The test said positive for 20 people, that is, they\n",
      "have cancer, as shown here:\n",
      "This means that P(A and B) = 20/100 = 1/5 = 40\n",
      "2 = 20% 6\n",
      "\n",
      "If we want to say that someone has cancer, or the test came back positive, this would be the\n",
      "total sum (or union) of the two events, namely, the sum of 5, 20, and 10, which is 35 53\n",
      "So,\n",
      "35/100 people either have cancer or had a positive test outcome 15\n",
      "That means, P(A or B) =\n",
      "35/100 = 13\n",
      "35 = 35% 6\n",
      "\n",
      "All in all, we have people in the following four different classes:\n",
      "Pink : This refers to those people who have cancer and had a negative test\n",
      "outcome\n",
      "Purple ( A intersect B): These people have cancer and had a positive test outcome\n",
      "Blue : This refers to those people with no cancer and a positive test outcome\n",
      "White : This refers to those people with no cancer and a negative test outcome\n",
      "So, effectively, the only times the test was accurate  was in the white and purple regions 101\n",
      "In\n",
      "the blue and pink regions, the test was incorrect 12\n",
      "\n",
      "\n",
      "Conditional probability\n",
      "Let's pick an arbitrary person from  this study of 100 people 18\n",
      "Let's also assume that you are\n",
      "told that their test result was positive 16\n",
      "What is the probability of them actually having\n",
      "cancer 11\n",
      "So, we are told that event B has already taken place, and that their test came back\n",
      "positive 21\n",
      "The question now is: what is the probability that they have cancer, that is P(A) 19\n",
      "\n",
      "This is called a conditional probability of A given B or P(A|B) 17\n",
      "Effectively, it is asking you\n",
      "to calculate the probability of an event given that another event has already happened 22\n",
      "\n",
      "You can think of conditional probability as changing the relevant universe 12\n",
      "P(A|B)  (called\n",
      "the probability of A given B) is a way of saying, given that my entire universe is now B,\n",
      "what is the probability of A 37\n",
      "This is also known as transforming the sample space 9\n",
      "\n",
      "Zooming in on our previous diagram, our universe is now B, and we are concerned with\n",
      "AB (A and B) inside B 29\n",
      "\n",
      "The formula can be given as follows:\n",
      "P(A|B) = P(A and B) / P(B) = (20/100) / (30/100) = 20/30 = 42\n",
      "66 = 66%\n",
      "There is a 66%  chance that if a test result came back positive, that person had cancer 27\n",
      "In\n",
      "reality, this is the main probability that the experimenters want 15\n",
      "They want to know how\n",
      "good the test is at predicting cancer 13\n",
      "\n",
      "The rules of probability\n",
      "In probability, we have some rules  that become very useful when visualization gets too\n",
      "cumbersome 25\n",
      "These rules help us calculate compound probabilities with ease 9\n",
      "\n",
      "The addition rule\n",
      "The addition rule is used to calculate the probability of either or  events 19\n",
      "To calculate P(A ∪ B)\n",
      "= P(A or B) , we use the following formula:\n",
      "P(A ∪ B) = P(A) + P(B) − P(A ∩ B)\n",
      "\n",
      "The first part of the formula (P(A) + P(B))  makes complete sense 60\n",
      "To get the union of the\n",
      "two events, we have to add together the area of the circles in the universe 23\n",
      "But why the\n",
      "subtraction of P(A and B) 12\n",
      "This is because when we add the two circles, we are adding the\n",
      "area of intersection twice, as shown in the following diagram:\n",
      "See how both the red circles include the intersection of A and B 40\n",
      "So, when we add them, we\n",
      "need to subtract just one of them to account for this, leaving us with our formula 26\n",
      "\n",
      "You will recall that we wanted the number of people who either had cancer or had a\n",
      "positive test result 22\n",
      "If A is the event that someone has cancer, and B is that the test result\n",
      "was positive, we have the following:\n",
      "P(A or B) = P(A) + P(B) - P(A and B) = 46\n",
      "25 + 3\n",
      "30 - 3\n",
      "2 = 3\n",
      "35\n",
      "\n",
      "This is represented visually in the following diagram:\n",
      "Mutual exclusivity\n",
      "We say that two events  are mutually exclusive if they cannot occur at the same time 34\n",
      "This\n",
      "means that A∩B=∅, or just that the intersection of the events is the empty set 24\n",
      "When this\n",
      "happens, P(A ∩ B) = P(A and B) = 0 22\n",
      "\n",
      "If two events are mutually exclusive, then the following applies:\n",
      "P(A ∪ B) = P(A or B)\n",
      " ",
      "= P(A) + P(B) − P(A ∩ B) = P(A) + P(B)\n",
      "This makes the addition rule much easier 57\n",
      "Some examples of mutually exclusive events\n",
      "include the following:\n",
      "A customer seeing your site for the first time on both Twitter and Facebook\n",
      "Today is Saturday and today is Wednesday\n",
      "I failed Econ 101 and I passed Econ 101\n",
      "None of these events can occur simultaneously 53\n",
      "\n",
      "\n",
      "The multiplication rule\n",
      "The multiplication rule is used  to calculate the probability of and events 18\n",
      "To calculate P(A ∩\n",
      "B) = P(A and B) , we use the following formula:\n",
      "P(A ∩ B) = P(A and B) = P(A) P(B|A)\n",
      "Why do we use B|A instead of B 53\n",
      "This is because it is possible that B depends on A 11\n",
      "If this\n",
      "is the case, then just multiplying P(A)  and P(B)  does not give us the whole picture 26\n",
      "\n",
      "In our cancer trial example, let's find P(A and B) 15\n",
      "To do this, let's redefine A to be the event\n",
      "that the trial is positive, and B to be the person having cancer (because it doesn't matter\n",
      "what we call the events) 40\n",
      "The equation will be as follows:\n",
      "P(A ∩ B) = P(A and B) = P(A) P(B|A) = 29\n",
      "3 * 3\n",
      "6666 = 4\n",
      "2 = 20%\n",
      "It's difficult to see the true necessity of using conditional probability, so let's try another,\n",
      "more difficult problem 28\n",
      "\n",
      "For example, of a randomly selected set of 10 people, 6 have iPhones and 4 have Androids 24\n",
      "\n",
      "What is the probability that if I randomly select 2 people, they both will have iPhones 19\n",
      "This\n",
      "example can be retold using event spaces, as follows:\n",
      "I have the following two events:\n",
      "A: This event shows the probability that I choose a person with an iPhone first\n",
      "B: This event shows the probability that I choose a person with an iPhone second\n",
      "So, basically, I want the following:\n",
      "P(A and B) : P(I choose a person with an iPhone and a person with an iPhone )\n",
      "So, I can use my P(A and B) = P(A) P(B|A)  formula 107\n",
      "\n",
      "P(A)  is simple, right 9\n",
      "People with iPhones are 6 out of 10, so, I have a 6/10 = 3/5 = 0 28\n",
      "6  chance\n",
      "of A 7\n",
      "This means P(A) = 0 8\n",
      "6 2\n",
      "\n",
      "So, if I have a 0 9\n",
      "6 chance of choosing someone with an iPhone, the probability of choosing\n",
      "two should just be 0 21\n",
      "6 * 0 5\n",
      "6 , right 4\n",
      "\n",
      "But wait 3\n",
      "We only have 9 people left to choose our second person from, because one was\n",
      "taken away 20\n",
      "So, in our new transformed sample space, we have 9 people in total, 5 with\n",
      "iPhones and 4 with Androids, making P(B) = 5/9 = 40\n",
      "555 2\n",
      "\n",
      "\n",
      "So, the probability of choosing two people with iPhones is 0 14\n",
      "6 * 0 5\n",
      "555 = 0 5\n",
      "333 = 33% 6\n",
      "\n",
      "I have a 1/3 chance of choosing two people with iPhones out of 10 19\n",
      "The conditional\n",
      "probability is very important in the multiplication rule as it can drastically alter your\n",
      "answer 19\n",
      "\n",
      "Independence\n",
      "Two events are independent if one event does not affect the outcome of the other, that is\n",
      "P(B|A) = P(B)  and P(A|B) = P(A) 43\n",
      "\n",
      "If two events are independent, then the following applies:\n",
      "P(A ∩ B) = P(A) P(B|A) = P(A) P(B)\n",
      "Some examples of independent events are as follows:\n",
      "It was raining in San Francisco, and a puppy was born in India\n",
      "I flip a coin and get heads, and I flip another coin and get tails\n",
      "None of these pairs of events affect each other 84\n",
      "\n",
      "Complementary events\n",
      "The complement of A is the opposite  or negation of A 18\n",
      "If A is an event,  Ā represents the\n",
      "complement of A 16\n",
      "For example, if A is the event where someone has cancer, Ā is the event\n",
      "where someone is cancer free 24\n",
      "\n",
      "To calculate the probability of, Ā, use the following formula:\n",
      "P(Ā) = 1 − P(A)\n",
      "For example, when you throw two dice, what is the probability that you rolled higher than\n",
      "a 3 49\n",
      "\n",
      "Let A represent rolling higher than a 3 10\n",
      "\n",
      "Ā represents rolling a 3 or less 10\n",
      "\n",
      "P(A) = 1 − P(Ā)\n",
      "P(A) = l - (P(2)+P(3))\n",
      "\n",
      "= 1 - (2/36 + 2/36)\n",
      "= 1 - (4/36)\n",
      "= 32/36 = 8 / 9\n",
      "= 64\n",
      "89\n",
      "For example, a start-up team has three investor meetings coming up 16\n",
      "We will have the\n",
      "following probabilities:\n",
      "A 60% chance of getting money from the first meeting\n",
      "A 15% chance of getting money from the second\n",
      "A 45% chance of getting money from the third\n",
      "What is the probability of them getting money from at least one meeting 58\n",
      "\n",
      "Let A be the team getting money from at least one investor, and, Ā, be the team not getting\n",
      "any money 27\n",
      "P(A)  can be calculated as follows:\n",
      "P(A) = 1 − P(Ā)\n",
      "To calculate P(Ā), we need to calculate the following:\n",
      "P(Ā) = P(no money from investor 1 AND no money from investor 2 AND no money from investor\n",
      "3)\n",
      "Let's assume that these events are independent (they don't talk to each other):\n",
      "P(Ā) = P(no money from investor 1) * P(no money from investor 2) * P(no money from investor 3) =\n",
      "0 115\n",
      "4 * 0 5\n",
      "85 * 0 5\n",
      "55 = 0 5\n",
      "187\n",
      "P(A) = 1 - 0 12\n",
      "187 = 0 5\n",
      "813 = 81%\n",
      "So, the start-up has an 81% chance of getting money from at least one meeting 25\n",
      "\n",
      "\n",
      "A bit deeper\n",
      "Without getting too deep into the machine learning  terminology, this test is what is known\n",
      "as a binary classifier , which means that it is trying to predict from only two options:\n",
      "having cancer or not having cancer 47\n",
      "When we are dealing with binary classifiers, we can \n",
      "draw  what is called confusion matrices , which are 2 x 2 matrices that house all the four\n",
      "possible outcomes of our experiment 38\n",
      "\n",
      "Let's try some different numbers 7\n",
      "Let's say 165 people walked in for the study 11\n",
      "So, our n\n",
      "(sample size) is 165 people 12\n",
      "All 165 people are given the test and asked whether they  have\n",
      "cancer (provided through various other means) 24\n",
      "The following confusion matrix  shows us\n",
      "the results of this experiment:\n",
      "The matrix shows that 50 people were predicted to have no cancer and did not have it, 100\n",
      "people were predicted to have cancer and actually did have it, and so on 51\n",
      "We have the\n",
      "following four classes, again, all with different names:\n",
      "The true positives  are the tests correctly predicting positive (cancer) == 100\n",
      "The true negatives  are the tests correctly predicting negative (no cancer) == 50\n",
      "The false positives  are the tests incorrectly predicting positive (cancer) == 10\n",
      "The false negatives  are the tests incorrectly predicting negative (no cancer) == 5\n",
      "The first two classes indicate where the test was correct, or true 100\n",
      "The last two classes\n",
      "indicate where the test was incorrect, or false 15\n",
      "\n",
      "False positives are sometimes called a Type I error , whereas false negatives are called a\n",
      "Type II error :\n",
      "\n",
      "\n",
      "Type I and Type II errors\n",
      "(Source: http://marginalrevolution 38\n",
      "com/marginalrevolution/2014/05/type-i-and-type-ii-errors-simpliﬁed 23\n",
      "html)\n",
      "We will get into this in the later chapters 11\n",
      "For now, we just need to understand why we use\n",
      "the set notation to denote probabilities for compound events 21\n",
      "This is because that's what\n",
      "they are 9\n",
      "When events A and B exist in the same universe, we can use intersections and\n",
      "unions to represent them happening either at the same time,  or to represent one happening\n",
      "versus the other 40\n",
      "\n",
      "We will go into this much more in later chapters, but it is good to introduce it now 20\n",
      "\n",
      "Summary\n",
      "In this chapter, we looked at the basics of probability and will continue to dive deeper into\n",
      "this field in the following chapter 28\n",
      "We approached most of our thinking as a Frequentist,\n",
      "and expressed the basics of experimentation and using probability to predict an outcome 25\n",
      "\n",
      "The next chapter will look at the Bayesian approach to probability and will also explore the\n",
      "use of probability to solve much more complex problems 27\n",
      "We will incorporate these basic\n",
      "probability principles in much more difficult scenarios 13\n",
      "\n",
      "\n",
      "\n",
      "Advanced Probability\n",
      "In the previous chapter,  we went over the basics of probability and how we can apply\n",
      "simple theorems to complex tasks 30\n",
      "To briefly summarize, probability is the mathematics of\n",
      "modeling events that may or may not occur 19\n",
      "We use formulas in order to describe  these\n",
      "events and even look at how multiple events can behave together 21\n",
      "In this chapter, we will\n",
      "explore more complicated theorems  of probability and how we can use them in a predictive\n",
      "capacity 28\n",
      "Advanced topics, such as Bayes' theorem  and random variables , give rise to\n",
      "common machine learning algorithms, such as the Naïve Bayes algorithm (also covered in\n",
      "this book) 40\n",
      "This chapter will focus on some of the more advanced topics  in probability\n",
      "theory, including the following topics:\n",
      "Exhaustive events\n",
      "Bayes' theorem\n",
      "Basic prediction rules\n",
      "Random variables\n",
      "We have one more definition to look at before we get started (the last one before the fun\n",
      "stuff, I promise) 64\n",
      "We have to look at collectively exhaustive events 8\n",
      "\n",
      "Collectively exhaustive events\n",
      "When given a set of two or more events, if at least  one of the events must occur, then such a\n",
      "set of events  is said to be collectively exhaustive 41\n",
      "Consider the following examples:\n",
      "Given a set of events {temperature < 60, temperature > 90} , these events\n",
      "are not collectively exhaustive because there is a third option that is not given in\n",
      "this set of events; the temperature could be between 60 and 90 56\n",
      "However, they\n",
      "are mutually exhaustive  because both  cannot happen at the same time 17\n",
      "\n",
      "In a dice roll, the set of events of rolling a {1, 2, 3, 4, 5, or 6}  are\n",
      "collectively exhaustive  because these are the only possible events, and at least\n",
      "one of them must happen 56\n",
      "\n",
      "\n",
      "Bayesian ideas revisited\n",
      "In the last chapter, we talked, very briefly, about Bayesian ways of thinking 23\n",
      "In short, when\n",
      "speaking about Bayes, you are speaking about the following three things and how they  all\n",
      "interact with each other:\n",
      "A prior distribution\n",
      "A posterior distribution\n",
      "A likelihood\n",
      "Basically, we are concerned with finding the posterior 51\n",
      "That's the thing we want to know 8\n",
      "\n",
      "Another way to phrase the Bayesian way of thinking is that data shapes and updates our\n",
      "belief 19\n",
      "We have a prior probability, or what we naively think about a hypothesis, and then\n",
      "we have a posterior probability, which is what we think about a hypothesis, given some\n",
      "data 38\n",
      "\n",
      "Bayes' theorem\n",
      "Bayes' theorem is the big result of Bayesian  inference 18\n",
      "Let's see how it even comes about 8\n",
      "\n",
      "Recall that we previously defined the following:\n",
      "P(A) = The probability that event A occurs\n",
      "P(A|B) = The probability that A occurs, given that B occurred\n",
      "P(A, B) = The probability that A and B occur\n",
      "P(A, B) = P(A) * P(B|A)\n",
      "That last bullet can be read as the probability that A and B occur is the probability that A occurs\n",
      "times the probability that B occurred, given that A already occurred 100\n",
      "\n",
      "It's from that last bullet point that Bayes' theorem takes its shape 16\n",
      "\n",
      "We know the following:\n",
      "P(A, B) = P(A) * P(B|A)\n",
      "P(B, A) = P(B) * P(A|B)\n",
      "P(A, B) = P(B, A)\n",
      "\n",
      "So, we can infer this:\n",
      "P(B) * P(A|B) = P(A) * P(B|A)\n",
      "Dividing both sides by P(B)  gives us Bayes' theorem, as shown:\n",
      "You can think of Bayes' theorem as follows:\n",
      "It is a way to get from P(A|B)  to P(B|A)  (if you only have one of them)\n",
      "It is a way to get P(A|B)  if you already know P(A)  (without knowing B)\n",
      "Let's try thinking about Bayes using the terms hypothesis  and data 171\n",
      "Suppose H = your\n",
      "hypothesis about the given data  and D = the data that you are given 22\n",
      "\n",
      "Bayes can be interpreted as trying to figure out P(H|D)  (the probability that our hypothesis is\n",
      "correct, given the data at hand ) 33\n",
      "\n",
      "Let's use our terminology from before:\n",
      "Let's take a look at that formula:\n",
      "P(H)  is the probability of the hypothesis before we observe the data, called the\n",
      "prior probability , or just prior\n",
      "P(H|D)  is what we want to compute, the probability of the hypothesis after we\n",
      "observe the data, called the posterior\n",
      "P(D|H)  is the probability of the data under the given hypothesis, called the\n",
      "likelihood\n",
      "P(D)  is the probability of the data under any hypothesis, called the normalizing\n",
      "constant\n",
      "This concept is not far off from the idea of machine learning and predictive analytics 131\n",
      "In\n",
      "many cases, when considering predictive analytics, we use the given data to predict an\n",
      "outcome 20\n",
      "Using the current terminology, H (our hypothesis) can be considered our outcome\n",
      "and P(H|D)  (the probability that our hypothesis is true, given our data) is another way of\n",
      "saying what is the chance that my hypothesis is correct, given the data in front of me 60\n",
      "\n",
      "\n",
      "Let's take a look at an example of how we can use Bayes' formula in the workplace 21\n",
      "\n",
      "Consider that you have two people in charge of writing blog posts for your company, Lucy\n",
      "and Avinash 23\n",
      "From past performance, you have liked 80% of Lucy's work and only 50% of\n",
      "Avinash's work 26\n",
      "A new blog post comes to your desk in the morning, but the author isn't\n",
      "mentioned 19\n",
      "You love the article 4\n",
      "A+ 2\n",
      "What is the probability that it came from Avinash 11\n",
      "\n",
      "Each blogger blogs at a very similar rate 9\n",
      "\n",
      "Before we freak out, let's do what  any experienced mathematician (and now you) would\n",
      "do 23\n",
      "Let's write out all of our information, as shown:\n",
      "H = hypothesis = the blog came from Avinash\n",
      "D = data = you loved the blog post\n",
      "P(H|D)  = the chance that it came from Avinash, given that you loved it\n",
      "P(D|H)  = the chance that you loved it, given that it came from Avinash\n",
      "P(H)  = the chance that an article came from Avinash\n",
      "P(D)  = the chance that you love an article\n",
      "Note that some of these variables make almost no sense without context 121\n",
      "P(D) , the\n",
      "probability that you would love any given article put on your desk, is a weird concept, but\n",
      "trust me, in the context of Bayes' formula, it will be relevant very soon 44\n",
      "\n",
      "Also, note that in the last two items, they assume nothing else 15\n",
      "P(D)  does not assume the\n",
      "origin of the blog post; think of P(D)  as if an article was plopped on your desk from some\n",
      "unknown source, what is the chance that you'd like it 47\n",
      "(again, I know it sounds weird out of\n",
      "context) 13\n",
      "\n",
      "So, we want to know P(H|D) 12\n",
      "Let's try to use Bayes' theorem, as shown here:\n",
      "But, do we know the numbers on the right-hand side of this equation 29\n",
      "I claim we do 4\n",
      "Let's\n",
      "see here:\n",
      "P(H)  is the probability that any given blog post comes from Avinash 23\n",
      "As bloggers\n",
      "write at a very similar rate, we can assume this is 0 17\n",
      "5 because we have a 50/50\n",
      "chance that it came from either blogger (note how I did not assume D, the data,\n",
      "for this) 34\n",
      "\n",
      "P(D|H)  is the probability that you love a post from Avinash, which we\n",
      "previously said was 50%, so, 0 34\n",
      "5 2\n",
      "\n",
      "\n",
      "P(D)  is interesting 7\n",
      "This is the chance that you love an article in general 11\n",
      "It means\n",
      "that we must take into account the scenario whether the post came from Lucy or\n",
      "Avinash 22\n",
      "Now, if the hypothesis forms a suite, then we can use our laws of\n",
      "probability, as mentioned in the previous chapter 25\n",
      "A suite is formed when a set of\n",
      "hypotheses is both collectively exhaustive and mutually exclusive 19\n",
      "In layman's\n",
      "terms, in a suite of events, exactly one and only one hypothesis can occur 21\n",
      "In our\n",
      "case, the two hypotheses are that the article came from Lucy, or that the article\n",
      "came from Avinash 26\n",
      "This is definitely a suite because of the following reasons:\n",
      "At least one of them wrote it\n",
      "At most one of them wrote it\n",
      "Therefore, exactly one  of them wrote it\n",
      "When we have a suite, we can use our multiplication and addition rules, as follows:\n",
      "D= (From, Avinash AND loved it) OR (From Lucy AND loved it)\n",
      "P(D)= P(Loved AND from Avinash) OR P(Loved AND from Lucy)\n",
      "P(D)= P(From Avinash) P(loved|from Avinash) +P(from Lucy) P(Loved|from Lucy)\n",
      "P(D)= 129\n",
      "5( 3\n",
      "5)+ 3\n",
      "5( 3\n",
      "8)= 3\n",
      "65\n",
      "Whew 5\n",
      "Way to go 3\n",
      "Now we can finish our equation, as shown:\n",
      "This means that there is a 38% chance that this article comes from Avinash 28\n",
      "What is\n",
      "interesting is that P(H) = 0 12\n",
      "5  and P(H|D) = 0 12\n",
      "38 2\n",
      "It means that without any data, the chance\n",
      "that a blog post came from Avinash was a coin flip, or 50/50 29\n",
      "Given some data (your\n",
      "thoughts on the article), we updated our beliefs about the hypothesis and it actually\n",
      "lowered the chance 27\n",
      "This is what Bayesian thinking is all about: updating our posterior\n",
      "beliefs about something from a prior assumption, given some new data about the subject 29\n",
      "\n",
      "\n",
      "More applications of Bayes' theorem\n",
      "Bayes' theorem shows up in a lot of applications, usually when we need to make fast\n",
      "decisions based on data and probability 36\n",
      "Most recommendation engines, such as Netflix's,\n",
      "use some elements of Bayesian updating 15\n",
      "And if you think through why that might be, it\n",
      "makes sense 14\n",
      "\n",
      "Let's suppose that in our simplistic world, Netflix only has 10 categories to choose from 19\n",
      "\n",
      "Now suppose that given no data, a user's chance of liking a comedy movie out of 10\n",
      "categories is 10% (just 1/10) 34\n",
      "\n",
      "Okay, now suppose that the user has given a few comedy movies 5/5 stars 19\n",
      "Now, when\n",
      "Netflix is wondering what the chance is that the user would like another comedy, the\n",
      "probability that they might like a comedy, P(H|D) , is going to be larger than a random\n",
      "guess of 10% 49\n",
      "\n",
      "Let's try some more examples of applying Bayes' theorem using more data 16\n",
      "This time, let's\n",
      "get a bit grittier 11\n",
      "\n",
      "Example – Titanic\n",
      "A very famous dataset involves looking at the survivors of the sinking of the Titanic in\n",
      "1912 24\n",
      "We will use an application of probability in order to figure out if there were any\n",
      "demographic features  that showed a relationship to passenger survival 28\n",
      "Mainly, we are\n",
      "curious to see if we can isolate any features of our dataset that can tell us more about the\n",
      "types of people who were likely to survive this disaster 37\n",
      "First, let's read in the data, as\n",
      "shown here:\n",
      "titanic =\n",
      "pd 19\n",
      "read_csv(https://raw 6\n",
      "githubusercontent 2\n",
      "com/sinanuozdemir/SF_DAT_15/maste\n",
      "r/data/titanic 20\n",
      "csv ')#read in a csv\n",
      "titanic = titanic[['Sex', 'Survived']]  #the Sex and Survived column\n",
      "titanic 33\n",
      "head()\n",
      "\n",
      "We get the following table:\n",
      "In the preceding table, each row represents a single passenger on the ship, and, for now, we\n",
      "are looking at two specific features: the sex of the individual and whether or not they\n",
      "survived the sinking 53\n",
      "For example, the first row represents a man who did not survive,\n",
      "while the fourth row (with index 3, remember how Python indexes lists) represents a\n",
      "female who did survive 37\n",
      "\n",
      "Let's start with some basics 7\n",
      "We'll start by calculating the probability that any given person\n",
      "on the ship survived, regardless of their gender 21\n",
      "To do this, we'll count the number of yeses\n",
      "in the Survived  column and divide this figure by the total number of rows, as shown here:\n",
      "num_rows = float(titanic 41\n",
      "shape[0]) # == 891 rows\n",
      "p_survived = (titanic 19\n",
      "Survived==\"yes\") 5\n",
      "sum() / num_rows # == 7\n",
      "38\n",
      "p_notsurvived = 1 - p_survived                          # == 19\n",
      "61\n",
      "Note that I only had to calculate P(Survived) , and I used the law of conjugate probabilities to\n",
      "calculate P(Died)  because those two events are complementary 39\n",
      "Now, let's calculate the\n",
      "probability that any single passenger is male or female:\n",
      "p_male = (titanic 24\n",
      "Sex==\"male\") 4\n",
      "sum() / num_rows     # == 8\n",
      "65\n",
      "p_female = 1 - p_male # == 13\n",
      "35\n",
      "Now, let's ask ourselves a question: did having a certain gender affect the survival rate 21\n",
      "For\n",
      "this, we can estimate P(Survived|Female)  or the chance that someone survived given that\n",
      "they were a female 28\n",
      "For this, we need to divide the number of women who survived by the\n",
      "total number of women, as shown here:\n",
      "\n",
      "\n",
      "Here's the code for that calculation:\n",
      "number_of_women = titanic[titanic 43\n",
      "Sex=='female'] 4\n",
      "shape[0] # == 314\n",
      "women_who_lived = titanic[(titanic 21\n",
      "Sex=='female') &\n",
      "(titanic 8\n",
      "Survived=='yes')] 5\n",
      "shape[0]                       # == 233\n",
      "p_survived_given_woman = women_who_lived / float(number_of_women)\n",
      "p_survived_given_woman            # == 40\n",
      "74\n",
      "That's a pretty big difference 9\n",
      "It seems that gender plays a big part in this dataset 11\n",
      "\n",
      "Example  – medical studies\n",
      "A classic use of Bayes' theorem is the interpretation of medical trials 21\n",
      "Routine testing for\n",
      "illegal drug use is increasingly common in workplaces and schools 14\n",
      "The companies that\n",
      "perform these tests maintain that the tests have a high sensitivity, which means that they\n",
      "are likely to produce a positive result if there are drugs in their system 35\n",
      "They claim that these\n",
      "tests are also highly specific, which means that they are likely to yield a negative result if\n",
      "there are no drugs 28\n",
      "\n",
      "On average, let's assume that the sensitivity of common drug tests is about 60% and the\n",
      "specificity is about 99% 29\n",
      "It means that if an employee is using drugs, the test has a 60%\n",
      "chance of being positive, while if an employee is not on drugs, the test has a 99% chance of\n",
      "being negative 44\n",
      "Now, suppose these tests are applied to a workforce where the actual rate\n",
      "of drug use is 5% 22\n",
      "\n",
      "The real question here is of the people who test positive, how many actually use drugs 18\n",
      "\n",
      "In Bayesian terms, we want to compute the probability of drug use, given a positive test:\n",
      "Let D = the event that drugs are in use\n",
      "Let E = the event that the test is positive\n",
      "Let N = the event that drugs are NOT  in use\n",
      "We are looking for P(D|E) 64\n",
      "\n",
      "By using Bayes' theorem, we can extrapolate it as follows:\n",
      "\n",
      "\n",
      "The prior, P(D)  is the probability of drug use before we see the outcome of the test, which is\n",
      "5% 43\n",
      "The likelihood, P(E|D) , is the probability of a positive test assuming drug use, which is\n",
      "the same thing as the sensitivity of the test 32\n",
      "The normalizing constant, P(E) , is a little bit\n",
      "trickier 17\n",
      "\n",
      "We have to consider two things: P(E and D)  as well as P(E and N) 22\n",
      "Basically, we must\n",
      "assume that the test is capable of being incorrect when the user is not using drugs 21\n",
      "Check\n",
      "out the following equations:\n",
      "So, our original equation becomes as follows:\n",
      "This means that of the people who test positive for drug use, about a quarter are innocent 34\n",
      "\n",
      "Random variables\n",
      "A random variable  uses real numerical values  to describe a probabilistic event 19\n",
      "In our\n",
      "previous work with variables (both in math and programming), we were used to the fact\n",
      "that a variable takes on a certain value 29\n",
      "For example, we might have a right-angled triangle\n",
      "in which we are given the variable h for the hypotenuse, and we must figure out the length\n",
      "of the hypotenuse 39\n",
      "We also might have the following, in Python:\n",
      "x = 5\n",
      "Both of these variables are equal to one value at a time 27\n",
      "In a random variable, we are subject\n",
      "to randomness, which means that our variables' values are, well, just that, variable 27\n",
      "They\n",
      "might take on multiple values depending on the environment 11\n",
      "\n",
      "\n",
      "A random variable still, as shown previously, holds a value 13\n",
      "The main distinction between\n",
      "variables as we have seen them and a random variable is the fact that a random variable's\n",
      "value may change depending on the situation 31\n",
      "\n",
      "However, if a random variable can have many values, how do we keep track of them all 20\n",
      "\n",
      "Each value that a random variable might take on is associated with a percentage 15\n",
      "For every\n",
      "value that a random variable might take on, there is a single probability that the variable\n",
      "will be this value 25\n",
      "\n",
      "With a random variable, we can also obtain our probability distribution of a random\n",
      "variable, which gives the variable's possible values and their probabilities 29\n",
      "\n",
      "Written out, we generally use single capital letters (mostly the specific letter X) to denote\n",
      "random variables 22\n",
      "For example, we might have the following:\n",
      "X: The outcome of a dice roll\n",
      "Y: The revenue earned by a company this year\n",
      "Z: The score of an applicant on an interview coding quiz (0-100%)\n",
      "Effectively, a random variable is a function that maps values from the sample space of an\n",
      "event (the set of all possible outcomes) to a probability value (between 0 and 1) 87\n",
      "Think\n",
      "about the event as being expressed as the following:\n",
      "It will assign a probability to each individual option 21\n",
      "There are two main types of random\n",
      "variables: discrete and continuous 13\n",
      "\n",
      "Discrete random variables\n",
      "A discrete random variable only  takes on a countable number  of possible values, such as\n",
      "the outcome of a dice roll, as shown here:\n",
      "\n",
      "\n",
      "Note how I use a capital X to define the random variable 49\n",
      "This is a common practice 5\n",
      "Also\n",
      "note how the random variable maps a probability to each individual outcome 14\n",
      "Random\n",
      "variables have  many properties, two of which are their expected value  and the variance 19\n",
      "We\n",
      "will use a probability mass function  (PMF ) to describe a discrete random variable 19\n",
      "\n",
      "They take on the following appearance:\n",
      "P(X = x) = PMF\n",
      "So, for a dice roll, P(X = 1) = 1/6  and P(X = 5) = 1/6 48\n",
      "\n",
      "Consider the following examples of discrete variables:\n",
      "The likely result of a survey question (for example, on a scale of 1-10)\n",
      "Whether the CEO will resign within the year (either true or false)\n",
      "The expected value of a random variable defines the mean value of a long run of repeated\n",
      "samples of the random variable 66\n",
      "This is sometimes called the mean of the variable 9\n",
      "\n",
      "For example, refer to the following Python code, which defines the random variable of a\n",
      "dice roll:\n",
      "import random\n",
      "def random_variable_of_dice_roll():\n",
      "    return random 35\n",
      "randint(1, 7) # a range of (1,7) # includes 1, 2, 3, 4,\n",
      "5, 6, but NOT 7\n",
      "This function will invoke a random variable and come out with a response 53\n",
      "Let's roll 100\n",
      "dice and average the result, as follows:\n",
      "trials = []\n",
      "num_trials = 100\n",
      "for trial in range(num_trials):\n",
      "trials 34\n",
      "append( random_variable_of_dice_roll() )\n",
      "print(sum(trials)/float(num_trials))  # == 3 23\n",
      "77\n",
      "So, taking 100 dice rolls and averaging them gives us a value of 3 20\n",
      "77 2\n",
      "Let's try this with a\n",
      "wide variety of trial numbers, as illustrated here:\n",
      "num_trials = range(100,10000, 10)\n",
      "avgs = []\n",
      "for num_trial in num_trials:\n",
      "    trials = []\n",
      "    for trial in range(1,num_trial):\n",
      "        trials 57\n",
      "append( random_variable_of_dice_roll() )\n",
      "    avgs 12\n",
      "append(sum(trials)/float(num_trial))\n",
      "\n",
      "plt 10\n",
      "plot(num_trials, avgs)\n",
      "plt 8\n",
      "xlabel('Number of Trials')\n",
      "plt 7\n",
      "ylabel(\"Average\")\n",
      "We get the following graph:\n",
      "The preceding graph represents the average dice roll as we look at more and more dice\n",
      "rolls 28\n",
      "We can see that the average dice roll is rapidly approaching 3 13\n",
      "5 2\n",
      "If we look towards\n",
      "the left of the graph, we see that if we only roll a die about 100 times, then we are not\n",
      "guaranteed to get an average dice roll of 3 42\n",
      "5 2\n",
      "However, if we roll 10,000 dice one after\n",
      "another, we see that we would very likely expect the average dice roll to be about 3 32\n",
      "5 2\n",
      "\n",
      "For a discrete random variable, we can also use a simple formula, shown as follows, to\n",
      "calculate the expected value:\n",
      "Expected value = E[X]= μx=∑\n",
      "Here, xi is the ith outcome and pi is the ith probability 51\n",
      "\n",
      "So, for our dice roll, we can find the exact expected value as follows:\n",
      "\n",
      "\n",
      "The preceding result shows us that for any given dice roll, we can \"expect\" a dice roll of 3 41\n",
      "5 2\n",
      "\n",
      "Now, obviously, that doesn't make sense because we can't get a 3 18\n",
      "5 on a dice roll, but it\n",
      "does make sense when put in the context of many dice rolls 22\n",
      "If you roll 10,000 dice, your\n",
      "average dice roll should approach 3 18\n",
      "5, as shown in the graph and code previously 11\n",
      "\n",
      "The average of the expected value of a random variable is generally not enough to grasp\n",
      "the full idea behind the variable 24\n",
      "For this reason, we introduce a new concept, called\n",
      "variance 14\n",
      "\n",
      "The variance of a random variable represents the spread of the variable 13\n",
      "It quantifies the\n",
      "variability of the expected value 11\n",
      "\n",
      "The formula for the variance of a discrete random variable is expressed as follows:\n",
      "xi and pi represent the same values as before and represents the expected value of the\n",
      "variable 34\n",
      "In this formula, I also mentioned the sigma of X 11\n",
      "Sigma, in this case, is the\n",
      "standard deviation, which is defined simply as the square root of the variance 23\n",
      "Let's look at\n",
      "a more complicated example of a discrete random variable 14\n",
      "\n",
      "Variance can be thought of like a give or take  metric 14\n",
      "If I say you can expect  to win $100 from\n",
      "a poker hand, you might be very happy 22\n",
      "If I append that statement with the additional\n",
      "detail that you might win $100, give or take $80, you now have a wide  range of\n",
      "expectations to deal with, which can be frustrating and might make a risk-averse player\n",
      "more wary of joining the game 57\n",
      "We can usually say that we have  an expected value, give or\n",
      "take the standard deviation 19\n",
      "\n",
      "Consider that your team measures the success  of a new product on a Likert scale , that is, as\n",
      "being in one of five categories, where a value of 0 represents a complete failure and 4\n",
      "represents a great success 50\n",
      "They estimate that a new project has the following chances of\n",
      "success based on user testing and the preliminary results of the performance of the product 27\n",
      "\n",
      "We first have to define our random variable 9\n",
      "\n",
      "Let the X random variable represent the success of our product 12\n",
      "X is indeed a discrete\n",
      "random variable because the X variable can only take on one of five options: 0, 1, 2, 3, or 4 36\n",
      "\n",
      "\n",
      "The following is the probability distribution of our random variable, X 13\n",
      "Note how we have\n",
      "a column for each potential outcome of X and, following each outcome, we have the\n",
      "probability that that particular outcome will be achieved:\n",
      "For example, the project has a 2% chance of failing completely and a 26% chance of being a\n",
      "great success 58\n",
      "We can calculate our expected value as follows:\n",
      "E[X] = 0(0 17\n",
      "02) + 1(0 8\n",
      "07) + 2(0 8\n",
      "25) + 3(0 8\n",
      "4) + 4(0 8\n",
      "26) = 2 6\n",
      "81\n",
      "This number means that the manager can expect  a success of about 2 18\n",
      "81 out of this project 6\n",
      "\n",
      "Now, by itself, that number is not very useful 12\n",
      "Perhaps, if given several products to choose\n",
      "from, an expected value might be a way to compare the potential successes of several\n",
      "products 27\n",
      "However, in this case, when we have but the one product to evaluate, we will\n",
      "need more 21\n",
      "\n",
      "Now, let's check the variance, as shown here:\n",
      "Variance= V[X]=σX2 = (xi −μX)2pi = (0 − 2 37\n",
      "81)2(0 6\n",
      "02) + (1 − 2 9\n",
      "81)2(0 6\n",
      "07)+  ",
      "(2 − 2 11\n",
      "81)2(0 6\n",
      "25) + (3 −\n",
      "2 9\n",
      "81)2(0 6\n",
      "4) + (4 − 2 9\n",
      "81)2(0 6\n",
      "26) = 4\n",
      "93\n",
      "Now that we have both the standard deviation and the expected value of the score of the\n",
      "project, let's try to summarize our results 30\n",
      "We could say that our project will have an\n",
      "expected score of 2 15\n",
      "81 plus or minus 0 7\n",
      "96, meaning that we can expect something between\n",
      "1 12\n",
      "85 and 3 5\n",
      "77 2\n",
      "\n",
      "So, one way we can address this project is that it is probably going to have a success rating\n",
      "of 2 25\n",
      "81, give or take about a point 9\n",
      "\n",
      "You might be thinking, wow, Sinan, so at best the project will be a 3 21\n",
      "8 and at worst it will be a\n",
      "1 11\n",
      "8 2\n",
      " 1\n",
      "Not quite 2\n",
      "\n",
      "It might be better than a 4 and it might also be worse than a 1 19\n",
      "8 2\n",
      "To take this one step\n",
      "further, let's calculate the following:\n",
      "P(X >= 3)\n",
      "\n",
      "First, take a minute and convince yourself that you can read that formula to yourself 37\n",
      "What\n",
      "am I asking when I am asking for P(X >= 3) 16\n",
      "Honestly, take a minute and figure it out 9\n",
      "\n",
      "P(X >= 3)  is the probability that our random variable will take on a value at least as big as 3 27\n",
      "\n",
      "In other words, what is the chance that our product will have a success rating of 3 or\n",
      "higher 23\n",
      "To calculate this, we can calculate the following:\n",
      "P(X >= 3) = P(X = 3) + P(X = 4) = 31\n",
      "66 = 66%\n",
      "This means that we have a 66% chance that our product will rate as either a 3 or a 4 30\n",
      "\n",
      "Another way to calculate this would be the conjugate way, as shown here:\n",
      "P(X >= 3) = 1 - P(X < 3)\n",
      "Again, take a moment to convince yourself that this formula holds up 46\n",
      "I am claiming that to\n",
      "find the probability that the product will be rated at least a 3 is the same as 1 minus the\n",
      "probability that the product will receive a rating below 3 40\n",
      "If this is true, then the two events\n",
      "(X >=3  and X < 3 ) must complement one another 24\n",
      "\n",
      "This is obviously true 5\n",
      "The product can be either of the following two options:\n",
      "Be rated 3 or above\n",
      "Be rated below a 3\n",
      "Let's check our math:\n",
      "P(X < 3) = P(X = 0) + P(X = 1) + P(X = 2)\n",
      "= 0 61\n",
      "02 + 0 5\n",
      "07 + 0 5\n",
      "25\n",
      "= 4\n",
      "0341 - P(X < 3)\n",
      "= 1 - 14\n",
      "34\n",
      "= 4\n",
      "66\n",
      "= P( x >= 3)\n",
      "It checks out 14\n",
      "\n",
      "\n",
      "Types of discrete random variables\n",
      "We can get a better idea of how random  variables work in practice by looking at specific\n",
      "types of random variables 30\n",
      "These specific types of random variables model different types\n",
      "of situations and end up revealing much simpler calculations for very complex event\n",
      "modeling 26\n",
      "\n",
      "Binomial random variables\n",
      "The first type of discrete random  variable we will look at is called a binomial random\n",
      "variable 26\n",
      "With a binomial random  variable, we look at a setting in which a single event\n",
      "happens over and over and we try to count the number of times the result is positive 38\n",
      "\n",
      "Before we can understand the random variable itself, we must look at the conditions in\n",
      "which it is even appropriate 23\n",
      "\n",
      "A binomial setting has the following four conditions:\n",
      "The possible outcomes are either success or failure\n",
      "The outcomes of trials cannot affect the outcome of another trial\n",
      "The number of trials was set (a fixed sample size)\n",
      "The chance of success of each trial must always be p\n",
      "A binomial random variable is a discrete random variable, X, that counts the number of\n",
      "successes in a binomial setting 82\n",
      "The parameters are n = the number of trials  and p = the chance\n",
      "of success of each trial 21\n",
      "\n",
      "Example: Fundraising meetings\n",
      "A start-up is taking 20 VC meetings to fund and count the number of offers they receive 26\n",
      "\n",
      "The PMF  for a binomial random  variable is as follows:\n",
      "\n",
      "\n",
      "Example: Restaurant openings\n",
      "A new restaurant in a town has a 20% chance of surviving its first year 38\n",
      "If 14 restaurants\n",
      "open this year, find the probability that exactly four restaurants survive their first year of\n",
      "being open to the public 27\n",
      "\n",
      "First, we should prove that this is a binomial setting:\n",
      "The possible outcomes are either success or failure (the restaurants either survive\n",
      "or not)\n",
      "The outcomes of trials cannot affect the outcome of another trial (assume that the\n",
      "opening of one restaurant doesn't affect another restaurant's opening and\n",
      "survival)\n",
      "The number of trials was set (14 restaurants opened)\n",
      "The chance of success of each trial must always be p (we assume that it is always\n",
      "20%)\n",
      "Here, we have our two parameters of n = 14  and p = 0 115\n",
      "2 2\n",
      "So, we can now plug these numbers\n",
      "into our binomial formula, as shown here:\n",
      "So, we have a 17% chance that exactly 4 of these restaurants will be open after a year 41\n",
      "\n",
      "Example: Blood types\n",
      "A couple has a 25% chance of a having a child with type O blood 23\n",
      "What is the chance that\n",
      "three of their five kids have type O blood 15\n",
      "\n",
      "Let X = the number of children with type O blood  with n = 5  and p = 0 24\n",
      "25 , as shown here:\n",
      "We can calculate this probability for the values of 0, 1, 2, 3, 4, and 5 to get a sense of the\n",
      "probability distribution:\n",
      "\n",
      "\n",
      "From here, we can calculate  an expected value  and the variance of this variable:\n",
      "So, this family can expect to have probably one or two kids with type O blood 79\n",
      "\n",
      "What if we want to know the probability that at least three of their kids have type O blood 20\n",
      "\n",
      "To know the probability that at least three of their kids have type O blood, we can use the\n",
      "following formula for discrete random variables:\n",
      "P(x>=3) =P(X=3)+P(X=4)+P(X=3) = 51\n",
      "00098+ 4\n",
      "01465+ 4\n",
      "08789=0 5\n",
      "103\n",
      "So, there is about a 10% chance that three of their kids have type O blood 22\n",
      "\n",
      "Shortcuts to binomial expected value and variance\n",
      "Binomial random variables have special calculations for the exact values\n",
      "of the expected values and variance 29\n",
      "If X is a binomial random variable,\n",
      "then we get the following:\n",
      "E(X) = np\n",
      "V(X) = np(1 − p)\n",
      "For our preceding example, we can use the following formulas to calculate\n",
      "an exact expected value and variance:\n",
      "E(X) = 56\n",
      "25(5) = 1 8\n",
      "25\n",
      "V(X) = 1 9\n",
      "25( 3\n",
      "75) = 0 6\n",
      "9375\n",
      "A binomial random variable is a discrete random variable that counts the number of\n",
      "successes in a binomial setting 27\n",
      "It is used in a wide variety of data-driven experiments, such\n",
      "as counting the number of people who will sign up for a website given a chance of\n",
      "conversion, or even, at a simple level, predicting stock price movements given a chance of\n",
      "decline (don't worry; we will be applying much more sophisticated models to predict the\n",
      "stock market later) 74\n",
      "\n",
      "Geometric random variables\n",
      "The second discrete random variable we will take a look at is called a geometric random\n",
      "variable 24\n",
      "It is actually quite similar to the binomial random variable in that we are\n",
      "concerned with a setting in which a single event is occurring over and over 32\n",
      "However, in\n",
      "the case of a geometric setting, the major difference is that we are not fixing the sample size 23\n",
      "\n",
      "\n",
      "We are not going into exactly 20 VC meetings as a start-up, nor are we having exactly five\n",
      "kids 24\n",
      "Instead, in a geometric setting, we are modeling the number of trials we will need to\n",
      "see before we obtain even a single success 27\n",
      "Specifically, a geometric setting has the\n",
      "following four conditions:\n",
      "The possible outcomes are either success or failure\n",
      "The outcomes of trials cannot affect the outcome of another trial\n",
      "The number of trials was not set\n",
      "The chance of success of each trial must always be p\n",
      "Note that these are the exact same conditions as a binomial variable, except the third\n",
      "condition 72\n",
      "\n",
      "A geometric random variable  is a discrete  random variable, X, that counts the number of\n",
      "trials needed to obtain one success 28\n",
      "The parameters are p = the chance of success of each trial\n",
      "and (1 − p) = the chance of failure of each trial 27\n",
      "\n",
      "To transform the previous binomial examples into geometric examples, we might do the\n",
      "following:\n",
      "Count the number of VC meetings that a start-up must take in order to get their\n",
      "first yes\n",
      "Count the number of coin flips needed in order to get a head (yes, I know it's\n",
      "boring, but it's a solid example 70\n",
      "\n",
      "The formula for the PMF is as follows:\n",
      "Both the binomial and geometric settings involve outcomes that are either successes or\n",
      "failures 28\n",
      "The big difference is that binomial random variables have a fixed number of trials,\n",
      "denoted as n 20\n",
      "Geometric random variables do not have a fixed number of trials 12\n",
      "Instead,\n",
      "geometric random variables model the number of samples needed in order to obtain the\n",
      "first successful trial, whatever success might mean in those experimental conditions 30\n",
      "\n",
      "Example: Weather\n",
      "There is a 34% chance that it will rain on any day in April 21\n",
      "Find the probability that the first\n",
      "day of rain in April will occur on April 4 18\n",
      "\n",
      "Let X = the number of days until it rains  (success) with p = 0 20\n",
      "34  and (1 − p) = 0 12\n",
      "66\n",
      "\n",
      "Lets' work it out: \n",
      "= 0 13\n",
      "054\n",
      "The probability that it will rain by the April 4 is as follows:\n",
      "P(X<=4) = P(1) + P(2) + P(3) + P(4) = 44\n",
      "34 + 3\n",
      "22 + 3\n",
      "14 +>1 = 6\n",
      "8\n",
      "So, there is an 80% chance that the first rain of the month will happen within the first four\n",
      "days 27\n",
      "\n",
      "Shortcuts to geometric expected value and variance\n",
      "Geometric random variables also have special calculations for the exact\n",
      "values of the expected values and variance 29\n",
      "If X is a geometric random\n",
      "variable, then we get the following:\n",
      "E(X) = 1/p\n",
      "Poisson random variable\n",
      "The third and last specific example of a discrete  random variable is a Poisson random\n",
      "variable 48\n",
      "\n",
      "To understand why we would need  this random variable, imagine that an event that we\n",
      "wish to model has a small probability of happening and that we wish to count the number\n",
      "of times that the event occurs in a certain time frame 48\n",
      "If we have an idea of the average\n",
      "number of occurrences, µ, over a specific period of time, given from past instances, then the\n",
      "Poisson random variable, denoted by X = Poi( µ), counts the total number of occurrences of\n",
      "the event during that given time period 61\n",
      "\n",
      "In other words, the Poisson distribution is a discrete probability distribution that counts the\n",
      "number of events that occur in a given interval of time 29\n",
      "\n",
      "Consider the following examples of Poisson random variables:\n",
      "Finding the probability of having a certain number of visitors on your site within\n",
      "an hour, knowing the past performance of the site\n",
      "Estimating the number of cars crashes at an intersection based on past police\n",
      "reports\n",
      "\n",
      "If we let X = the number of events in a given interval , and the average number of events per\n",
      "interval is the λ number, then the probability of observing x events in a given interval is\n",
      "given by the following formula:\n",
      "Here, e = Euler's constant (2 110\n",
      "718 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "Example: Call center\n",
      "The number of calls arriving at your call center follows a Poisson distribution at the rate of\n",
      "five calls per hour 29\n",
      "What is the probability that exactly six calls will come in between 10 and\n",
      "11 p 18\n",
      "m 1\n",
      " 1\n",
      "\n",
      "To set up this example, let's write out our given information 14\n",
      "Let X be the number of calls\n",
      "that arrive between 10 and 11 p 17\n",
      "m 1\n",
      "This is our Poisson random variable with the mean λ = 5 14\n",
      "\n",
      "The mean is 5 because we are using 5 as our previous expected value of the number of calls\n",
      "to come in at this time 29\n",
      "This number could have come from the previous work on\n",
      "estimating the number of calls that come in every hour or specifically that come in after 10\n",
      "p 32\n",
      "m 1\n",
      "The main idea is that we do have some idea of how many calls should be coming in,\n",
      "and then we use that information to create our Poisson random variable and use it to make\n",
      "predictions 39\n",
      "\n",
      "Continuing with our example, we have the following:\n",
      "P(X = 6) = 0 21\n",
      "146\n",
      "This means that there is about a 14 12\n",
      "6% chance that exactly six calls will come between 10\n",
      "and 11 p 18\n",
      "m 1\n",
      "\n",
      "Shortcuts to Poisson expected value and variance\n",
      "Poisson random variables also have special calculations for the exact\n",
      "values of the expected values and variance 31\n",
      "If X is a Poisson random\n",
      "variable with mean, then we get the following:\n",
      "E(X) = λ\n",
      "V(X) = λ\n",
      "This is actually interesting because both the expected value and the variance are the same\n",
      "number, and that number is simply the given parameter 56\n",
      "Now that we've seen three\n",
      "examples of discrete random variables, we must take a look at the other type of random\n",
      "variable, called the continuous random variable 32\n",
      "\n",
      "\n",
      "Continuous random variables\n",
      "Switching gears entirely, unlike a discrete  random variable, a continuous random variable\n",
      "can take on an infinite  number of possible values, not just a few countable ones 40\n",
      "We call the\n",
      "functions that describe the distribution density curves instead  of PMF 16\n",
      "\n",
      "Consider the following examples of continuous variables:\n",
      "The length of a sales representative's phone call (not the number of calls)\n",
      "The actual amount of oil in a drum marked 20 gallons (not the number of oil\n",
      "drums)\n",
      "If X is a continuous random variable, then there is a function, f(x), for any constants a and b:\n",
      "The preceding f(x) function is known  as the probability density function  (PDF ) 89\n",
      "The PDF is\n",
      "the continuous random variable version of the PMF for discrete random variables 17\n",
      "\n",
      "The most important continuous  distribution is the standard normal distribution 12\n",
      "You have,\n",
      "no doubt, either heard of the normal distribution or dealt with it 16\n",
      "The idea behind it is quite\n",
      "simple 8\n",
      "The PDF of this distribution is as follows:\n",
      "Here, μ is the mean of the variable and σ is the standard deviation 24\n",
      "This might look\n",
      "confusing, but let's graph it in Python with a mean of 0 and a standard deviation of 1, as\n",
      "shown here:\n",
      "import numpy as np\n",
      "import matplotlib 40\n",
      "pyplot as plt\n",
      "def normal_pdf(x, mu = 0, sigma = 1):\n",
      " return (1 22\n",
      "/np 2\n",
      "sqrt(2*3 5\n",
      "14 * sigma**2)) * np 9\n",
      "exp((-(x-mu)**2 / (2 11\n",
      "*\n",
      "sigma**2)))\n",
      "x_values = np 9\n",
      "linspace(-5,5,100)\n",
      "y_values = [normal_pdf(x) for x in x_values]\n",
      "plt 23\n",
      "plot(x_values, y_values)\n",
      "\n",
      "We get this graph:\n",
      "This gives rise to the all-too-familiar bell curve 23\n",
      "Note that the graph is symmetrical around\n",
      "the x = 0  line 16\n",
      "Let's try changing some of the parameters 8\n",
      "First, let's try with :\n",
      "\n",
      "\n",
      "Next, let's try with the value :\n",
      "Lastly, we will try with the values :\n",
      "\n",
      "\n",
      "In all the graphs, we have the standard bell shape  that we are all familiar with, but as we\n",
      "change our parameters, we see that the bell might get skinnier, thicker, or move from left to\n",
      "right 74\n",
      "\n",
      "In the following chapters, which focus on statistics, we will make much more use of the\n",
      "normal distribution as it applies to statistical thinking 28\n",
      "\n",
      "Summary\n",
      "Probability as a field works to explain our random and chaotic world 15\n",
      "Using the basic laws\n",
      "of probability, we can model real-life events that involve randomness 17\n",
      "We can use random\n",
      "variables to represent values that may take on several values, and we can use the\n",
      "probability mass or density functions to compare product lines or look at the test results 37\n",
      "\n",
      "We have seen some of the more complicated uses of probability in prediction 14\n",
      "Using\n",
      "random variables and Bayes' theorem are excellent ways to assign probabilities to real-life\n",
      "situations 22\n",
      "In later chapters, we will revisit Bayes' theorem and use it to create a very\n",
      "powerful and fast machine learning algorithm, called the Na ïve Bayes algorithm 36\n",
      "This\n",
      "algorithm captures the power of Bayesian thinking and applies it directly to the problem of\n",
      "predictive learning 21\n",
      "\n",
      "The next two chapters are focused on statistical thinking 10\n",
      "Like probability, these chapters\n",
      "will use mathematical formulas to model real-world events 15\n",
      "The main difference, however,\n",
      "will be the terminology we use to describe the world and the way we model different types\n",
      "of events 26\n",
      "In these upcoming chapters, we will attempt to model entire populations of data\n",
      "points based solely on a sample 21\n",
      "\n",
      "We will revisit many concepts in probability to make sense of statistical theorems as they\n",
      "are closely linked, and both are important mathematical concepts in the realm of data\n",
      "science 36\n",
      "\n",
      "\n",
      "\n",
      "Basic Statistics\n",
      "This chapter will focus on the statistical knowledge required by any aspiring data scientist 18\n",
      "\n",
      "We will explore ways of sampling and obtaining data without being affected by bias and\n",
      "then use measures of statistics to quantify and visualize our data 28\n",
      "Using the z-score and the\n",
      "empirical rule, we will see how we can standardize data for the purpose of both graphing\n",
      "and interpretability 31\n",
      "\n",
      "In this chapter, we will look at the following topics:\n",
      "How to obtain and sample data\n",
      "The measures of center, variance, and relative standing\n",
      "Normalization of data using the z-score\n",
      "The empirical rule\n",
      "What are statistics 46\n",
      "\n",
      "This might seem like an odd question to ask, but I am frequently surprised by the number\n",
      "of people who cannot answer this simple and yet powerful question: what are statistics 35\n",
      "\n",
      "Statistics are the numbers you always see on the news and in the paper 15\n",
      "Statistics are useful\n",
      "when trying to prove a point or trying to scare someone, but what are they 20\n",
      "\n",
      "To answer this question, we need to back up for a minute and talk about why we even\n",
      "measure them in the first place 27\n",
      "The goal of this field is to try to explain and model the\n",
      "world around us 17\n",
      "To do that, we have to take a look at the population 13\n",
      "\n",
      "We can define a population  as the entire pool of subjects of an experiment or a model 19\n",
      "\n",
      "\n",
      "Essentially, your population is who you care about 11\n",
      "Who are you trying to talk about 7\n",
      "If\n",
      "you are trying to test whether smoking leads to heart disease, your population would be\n",
      "the smokers of the world 24\n",
      "If you are trying to study teenage drinking problems, your\n",
      "population would be all teenagers 17\n",
      "\n",
      "Now, consider that you want to ask a question about your population 14\n",
      "For example, if your\n",
      "population is all of your employees (assume that you have over 1,000 employees), perhaps\n",
      "you want  to know what percentage of them use illicit drugs 38\n",
      "The question is called a\n",
      "parameter 7\n",
      "\n",
      "We can define a parameter as a numerical measurement describing a characteristic of a\n",
      "population 17\n",
      "\n",
      "For example, if you ask all 1,000 employees and 100 of them are using drugs, the rate of\n",
      "drug use is 10% 32\n",
      "The parameter here is 10% 7\n",
      "\n",
      "However, let's get real; you probably can't ask every single employee whether they are\n",
      "using drugs 22\n",
      "What if you have over 10,000 employees 10\n",
      "It would be very difficult to track\n",
      "everyone down in order to get your answer 16\n",
      "When this happens, it's impossible to figure\n",
      "out this parameter 13\n",
      "In this case, we can estimate  the parameter 10\n",
      "\n",
      "First, we will take a sample  of the population 12\n",
      "\n",
      "We can define a sample of a population as a subset (not necessarily random) of the\n",
      "population 21\n",
      "\n",
      "So, we perhaps ask 200 of the 1,000 employees you have 17\n",
      "Of these 200, suppose 26 use\n",
      "drugs, making the drug use rate 13% 21\n",
      "Here, 13% is not a parameter because we didn't get a\n",
      "chance to ask everyone 21\n",
      "This 13% is an estimate of a parameter 10\n",
      "Do you know what that's\n",
      "called 8\n",
      "\n",
      "That's right, a statistic 7\n",
      "\n",
      "We can define a statistic as a numerical measurement describing a characteristic of a sample\n",
      "of a population 20\n",
      "\n",
      "A statistic is just an estimation of a parameter 10\n",
      "It is a number that attempts to describe an\n",
      "entire population by describing a subset of that population 20\n",
      "This is necessary because you\n",
      "can never hope to give a survey to every single teenager or to every single smoker in the\n",
      "world 26\n",
      "That's what the field of statistics is all about: taking samples of populations and\n",
      "running tests on these samples 22\n",
      "\n",
      "So, the next time you are given a statistic, just remember that number only represents a\n",
      "sample of that population, not the entire pool of subjects 31\n",
      "\n",
      "\n",
      "How do we obtain and sample data 8\n",
      "\n",
      "If statistics is about taking samples  of populations, it must be very important to know how\n",
      "we obtain these samples, and you'd be correct 30\n",
      "Let's focus on just a few of the many ways  of\n",
      "obtaining and sampling data 20\n",
      "\n",
      "Obtaining data\n",
      "There are two main ways of collecting data for our analysis: observational  and\n",
      "experimentation 24\n",
      "Both these ways  have their pros and cons, of course 12\n",
      "They each produce\n",
      "different types of behavior and, therefore, warrant different types of analysis 17\n",
      "\n",
      "Observational\n",
      "We might obtain data through observational  means, which consists of measuring specific\n",
      "characteristics but not attempting to modify the subjects being studied 30\n",
      "For example, you\n",
      "have a tracking software on your website that observes users' behavior on the website, such\n",
      "as length of time spent on certain pages and the rate of clicking  on ads, all the while not\n",
      "affecting the user's experience, then that would be an observational study 60\n",
      "\n",
      "This is one of the most common ways to get data because it's just plain easy 18\n",
      "All you have\n",
      "to do is observe and collect data 11\n",
      "Observational studies are also limited in the types of data\n",
      "you may collect 15\n",
      "This is because the observer (you) is not in control of the environment 15\n",
      "You\n",
      "may only watch and collect natural behavior 9\n",
      "If you are looking to induce a certain type of\n",
      "behavior, an observational study would not be useful 20\n",
      "\n",
      "Experimental\n",
      "An experiment  consists of a treatment and the observation of its effect on the subjects 19\n",
      "\n",
      "Subjects in an experiment are called experimental units 9\n",
      "This is usually how most scientific\n",
      "labs collect data 10\n",
      "They will put people into two or more groups (usually just two) and call\n",
      "them the control and the experimental group 24\n",
      "\n",
      "The control group is exposed to a certain environment and then observed 13\n",
      "The\n",
      "experimental group is then exposed to a different environment and then observed 14\n",
      "The\n",
      "experimenter then aggregates data from both the groups and makes a decision about which\n",
      "environment was more favorable (favorable is a quality that the experimenter gets to\n",
      "decide) 41\n",
      "\n",
      "\n",
      "In a marketing example, consider that we expose half of our users to a certain landing page\n",
      "with certain images and a certain style (website A), and we measure whether or not they\n",
      "sign up for the service 44\n",
      "Then, we expose the other half to a different landing page, different\n",
      "images, and different styles (website B) and again measure whether or not they sign up 33\n",
      "We\n",
      "can then decide which of the two sites performed better and should be used going further 18\n",
      "\n",
      "This, specifically, is called an A/B test 11\n",
      "Let's see an example in Python 7\n",
      "Let's suppose we run\n",
      "the preceding test and obtain the following results as a list of lists:\n",
      "results = [ ['A', 1], ['B', 1], ['A', 0], ['A', 0] 47\n",
      " 1\n",
      " 1\n",
      "]\n",
      "Here, each object in the list result represents a subject (person) 15\n",
      "Each person then has the\n",
      "following two attributes:\n",
      "Which website they were exposed to, represented by a single character\n",
      "Whether or not they converted ( 0 for no and 1 for yes)\n",
      "We can then aggregate and come up with the following results table:\n",
      "users_exposed_to_A = []\n",
      "users_exposed_to_B = []\n",
      "# create two lists to hold the results of each individual website\n",
      "Once we create these two lists that will eventually hold each individual conversion Boolean\n",
      "(0 or 1), we will iterate all of our results of the test and add them to the appropriate list, as\n",
      "shown:\n",
      "for website, converted in results: # iterate through the results\n",
      "  # will look something like website == 'A' and converted == 0\n",
      "  if website == 'A':\n",
      "    users_exposed_to_A 165\n",
      "append(converted)\n",
      "  elif website == 'B':\n",
      "    users_exposed_to_B 17\n",
      "append(converted)\n",
      "Now, each list contains a series of 1s and 0s 19\n",
      "\n",
      "Remember that a 1 represents a user actually converting to the site after\n",
      "seeing that web page, and a 0 represents a user seeing the page and\n",
      "leaving before signing up/converting 40\n",
      "\n",
      "To get the total number of people exposed to website A, we can use the len()  feature in\n",
      "Python, as illustrated:\n",
      "len(users_exposed_to_A) == 188 #number of people exposed to website A\n",
      "len(users_exposed_to_B) == 158 #number of people exposed to website B\n",
      "\n",
      "To count the number of people who converted, we can use the sum()  of the list, as shown:\n",
      "sum(users_exposed_to_A) == 54 # people converted from website A\n",
      "sum(users_exposed_to_B) == 48 # people converted from website B\n",
      "If we subtract the length of the lists and the sum of the list, we are left with the number of\n",
      "people who did not convert for each site, as illustrated:\n",
      "len(users_exposed_to_A) - sum(users_exposed_to_A) == 134 # did not convert\n",
      "from website A\n",
      "len(users_exposed_to_B) - sum(users_exposed_to_B) == 110 # did not convert\n",
      "from website B\n",
      "We can aggregate and summarize our results in the following table, which represents our\n",
      "experiment of website conversion testing:\n",
      "Did not sign up Signed up\n",
      "Website A 134 54\n",
      "Website B 110 48\n",
      "The results of our A/B test\n",
      "We can quickly drum up some descriptive statistics 271\n",
      "We can say that the website conversion\n",
      "rates for the two websites are as follows:\n",
      "Conversion for website A : 154 /(154+34) = 30\n",
      "288\n",
      "Conversion for website B : 48/(110+48)= 15\n",
      "3\n",
      "Not much difference, but different nonetheless 10\n",
      "Even though B has the higher conversion\n",
      "rate, can we really say that version B significantly converts better 20\n",
      "Not yet 2\n",
      "To test the\n",
      "statistical significance  of such a result, a hypothesis test should be used 19\n",
      "These tests will be\n",
      "covered in depth in the next chapter, where we will revisit this exact same example and\n",
      "finish it using a proper statistical test 30\n",
      "\n",
      "Sampling data\n",
      "Remember how statistics are the result of measuring a sample of a population 17\n",
      "Well, we\n",
      "should talk about two very common ways to decide who gets the honor of being in the\n",
      "sample that we measure 26\n",
      "We will discuss the main type of sampling, called random\n",
      "sampling, which is the most common way to decide our sample sizes and our sample\n",
      "members 30\n",
      "\n",
      "\n",
      "Probability sampling\n",
      "Probability sampling is a way  of sampling from a population, in which every person has a\n",
      "known probability of being chosen but that number might  be a different probability than\n",
      "another user 41\n",
      "The simplest (and probably the most common) probability sampling method\n",
      "is a random sampling 17\n",
      "\n",
      "Random sampling\n",
      "Suppose that we are running an A/B test and we need to figure out who will be in group A\n",
      "and who  will be in group B 35\n",
      "There are the following three suggestions from your data team:\n",
      "Separate users based on location : Users on the West coast are placed in group A,\n",
      "while users on the East coast are placed in group B\n",
      "Separate users based on the time of day they visit the site : Users who visit\n",
      "between 7 p 63\n",
      "m 1\n",
      "and 4 a 4\n",
      "m 1\n",
      "are group A, while the rest are placed in group B\n",
      "Make it completely random : Every new user has a 50/50 chance of being placed\n",
      "in either group\n",
      "The first two are valid options for choosing samples and are fairly simple to implement, but\n",
      "they both have one fundamental flaw: they are both at risk of introducing a sampling bias 71\n",
      "\n",
      "A sampling bias  occurs when the way  the sample is obtained systemically favors some\n",
      "outcome over the target outcome 24\n",
      "\n",
      "It is not difficult to see why choosing option 1 or option 2 might introduce bias 19\n",
      "If we chose\n",
      "our groups based on where they live or what time they log in, we are priming our\n",
      "experiment incorrectly and, now, we have much less control over the results 38\n",
      "\n",
      "Specifically, we are at risk of introducing a confounding factor  into our analysis, which is bad\n",
      "news 24\n",
      "\n",
      "A confounding factor  is a variable that we are not directly measuring but connects the\n",
      "variables that are being measured 24\n",
      "\n",
      "Basically, a confounding factor is like the missing element in our analysis that is invisible\n",
      "but affects our results 23\n",
      "\n",
      "In this case, option 1 is not taking into account the potential confounding factor of\n",
      "geographical taste 23\n",
      "For example, if website A is unappealing, in general, to West coast users,\n",
      "it will affect your results drastically 25\n",
      "\n",
      "\n",
      "Similarly, option 2 might introduce a temporal (time-based) confounding factor 17\n",
      "What if\n",
      "website B is better viewed in a night-time environment (which was reserved for A), and\n",
      "users are reacting negatively to the style purely because of what time it is 36\n",
      "These are both\n",
      "factors that we want to avoid, so, we should go with option 3, which is a random sample 27\n",
      "\n",
      "While sampling bias can cause confounding, it is a different concept than\n",
      "confounding 18\n",
      "Options 1 and 2 were both sampling biases because we\n",
      "chose the samples incorrectly and were also examples of confounding\n",
      "factors because there was a third variable in each case that affected our\n",
      "decision 42\n",
      "\n",
      "A random sample is chosen such that every single member of a population has an equal\n",
      "chance of being chosen as any other member 27\n",
      "\n",
      "This is probably one of the easiest and most convenient ways to decide who will be a part\n",
      "of your sample 23\n",
      "Everyone has the exact same chance of being in any particular group 12\n",
      "\n",
      "Random sampling is an effective way of reducing the impact of confounding factors 15\n",
      "\n",
      "Unequal probability sampling\n",
      "Recall that I previously said that a probability sampling might have different probabilities\n",
      "for different potential sample members 26\n",
      "But what if this actually introduced problems 7\n",
      "\n",
      "Suppose we are interested in measuring the happiness level of our employees 14\n",
      "We already\n",
      "know that we can't ask every single member of staff because that would be silly and\n",
      "exhausting 24\n",
      "So, we need to take a sample 8\n",
      "Our data team suggests random sampling, and at\n",
      "first everyone high-fives because they feel very smart and statistical 22\n",
      "But then someone asks\n",
      "a seemingly harmless question: does anyone know the percentage of men/women who\n",
      "work here 23\n",
      "\n",
      "The high fives stop and the room goes silent 11\n",
      "\n",
      "This question is extremely important because sex is likely to be a confounding factor 16\n",
      "The\n",
      "team looks into it and discovers a split of 75% men and 25% women in the company 23\n",
      "\n",
      "This means that if we introduce a random sample, our sample will likely have a similar\n",
      "split and thus favor the results for men and not women 30\n",
      "To combat this, we can favor\n",
      "including more women than men in our survey in order to make the split of our sample less\n",
      "favored for men 31\n",
      "\n",
      "\n",
      "At first glance, introducing a favoring system in our random sampling seems like a bad\n",
      "idea; however, alleviating unequal sampling and, therefore, working to remove systematic\n",
      "bias among gender, race, disability, and so on is much more pertinent 51\n",
      "A simple random\n",
      "sample, where everyone has the same chance as everyone else, is very likely to drown out\n",
      "the voices and opinions of minority population members 31\n",
      "Therefore, it can be okay to\n",
      "introduce such a favoring system in your sampling techniques 19\n",
      "\n",
      "How do we measure statistics 6\n",
      "\n",
      "Once we have our sample, it's time to quantify  our results 15\n",
      "Suppose we wish to generalize\n",
      "the happiness of our employees or we want to figure out whether salaries in the company\n",
      "are very different from person to person 30\n",
      "\n",
      "These are some common ways of measuring our results 10\n",
      "\n",
      "Measures of center\n",
      "Measures of the center are how we define  the middle, or center, of a dataset 25\n",
      "We do this\n",
      "because sometimes we wish to make generalizations about data values 15\n",
      "For example,\n",
      "perhaps we're curious about what the average rainfall in Seattle is or what the median\n",
      "height of European males is 25\n",
      "It's a way to generalize a large set of data so that it's easier to\n",
      "convey to someone 22\n",
      "\n",
      "A measure of center is a value in the middle  of a dataset 15\n",
      "\n",
      "However, this can mean different things to different people 11\n",
      "Who's to say where the middle\n",
      "of a dataset is 12\n",
      "There are so many different ways of defining the center of data 12\n",
      "Let's take a\n",
      "look at a few 9\n",
      "\n",
      "The arithmetic mean  of a dataset is found  by adding up all of the values and then dividing\n",
      "it by the number of data values 29\n",
      "\n",
      "This is likely the most common way to define the center of data, but can be flawed 19\n",
      "Suppose\n",
      "we wish to find the mean of the following numbers:\n",
      "import numpy as np\n",
      "np 19\n",
      "mean([11, 15, 17, 14]) == 14 16\n",
      "25\n",
      "\n",
      "Simple enough; our average is 14 11\n",
      "25  and all of our values are fairly close to it 13\n",
      "But what if\n",
      "we introduce a new value: 31 12\n",
      "\n",
      "np 2\n",
      "mean([11, 15, 17, 14, 31]) == 17 19\n",
      "6\n",
      "This greatly affects the mean because the arithmetic mean is sensitive to outliers 16\n",
      "The new\n",
      "value, 31, is almost twice as large as the rest of the numbers and therefore skews  the mean 26\n",
      "\n",
      "Another, and sometimes better, measure of center is the median 13\n",
      "\n",
      "The median  is the number found  in the middle of the dataset when it is sorted in order, as\n",
      "shown:\n",
      "np 27\n",
      "median([11, 15, 17, 14]) == 14 16\n",
      "5\n",
      "np 4\n",
      "median([11, 15, 17, 14, 31]) == 15\n",
      "Note how the introduction of 31 using the median did not affect the median of the dataset\n",
      "greatly 41\n",
      "This is because the median is less sensitive to outliers 10\n",
      "\n",
      "When working with datasets with many outliers, it is sometimes more useful to use the\n",
      "median of the dataset, while if your data does not have many outliers and the data points\n",
      "are mostly close to one another, then the mean is likely a better option 52\n",
      "\n",
      "But how can we tell if the data is spread out 12\n",
      "Well, we will have to introduce a new type of\n",
      "statistic 14\n",
      "\n",
      "Measures of variation\n",
      "Measures of the center are used  to quantify the middle of the data, but now we will explore\n",
      "ways of measuring how to spread out  the data we collect is 41\n",
      "This is a useful way to identify if\n",
      "our data has many outliers lurking inside 16\n",
      "Let's start with an example 6\n",
      "\n",
      "Consider that we take a random sample of 24 of our friends on Facebook and wrote down\n",
      "how many friends that they had on Facebook 28\n",
      "Here's the list:\n",
      "friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\n",
      "455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\n",
      "np 85\n",
      "mean(friends) == 789 7\n",
      "1\n",
      "\n",
      "The average of this list is just over 789 13\n",
      "So, we could say that according to this sample, the\n",
      "average Facebook friend has 789 friends 20\n",
      "But what about the person who only has 89\n",
      "friends or the person who has over 1,600 friends 23\n",
      "In fact, not a lot of these numbers are\n",
      "really that close to 789 17\n",
      "\n",
      "Well, how about we use the median 9\n",
      "The median generally is not as affected by outliers:\n",
      "np 11\n",
      "median(friends) == 769 7\n",
      "5\n",
      "The median is 769 8\n",
      "5 , which is fairly close to the mean 10\n",
      "Hmm, good thought, but still, it\n",
      "doesn't really account for how drastically different a lot of these data points are to one\n",
      "another 29\n",
      "This is what statisticians call measuring the variation of data 11\n",
      "Let's start by\n",
      "introducing the most basic measure of variation: the range 16\n",
      "The range is simply the\n",
      "maximum value minus the minimum value, as illustrated:\n",
      "np 17\n",
      "max(friends) - np 6\n",
      "min(friends) == 1684\n",
      "The range tells us how far away the two most extreme values are 22\n",
      "Now, typically the range\n",
      "isn't widely used but it does have its use in the application 20\n",
      "Sometimes, we wish to just\n",
      "know how spread apart the outliers are 14\n",
      "This is most useful in scientific measurements or\n",
      "safety measurements 12\n",
      "\n",
      "Suppose a car company wants to measure how long it takes for an airbag to deploy 19\n",
      "\n",
      "Knowing the average of that time is nice, but they also really want to know how spread\n",
      "apart the slowest time is versus the fastest time 31\n",
      "This literally could be the difference\n",
      "between life and death 11\n",
      "\n",
      "Shifting back to the Facebook example, 1,684 is our range, but I'm not quite sure it's saying\n",
      "too much about our data 32\n",
      "Now, let's take a look at the most commonly used measure  of\n",
      "variation, the standard deviation 21\n",
      "\n",
      "I'm sure many of you have heard this term thrown around a lot and it might even incite a\n",
      "degree of fear, but what does it really mean 33\n",
      "In essence, standard deviation, denoted by s\n",
      "when we are working with a sample of a population, measures how much data values\n",
      "deviate from the arithmetic mean 34\n",
      "\n",
      "It's basically a way to see how spread out the data is 14\n",
      "There is a general formula to calculate\n",
      "the standard deviation, which is as follows:\n",
      "\n",
      "\n",
      "Let's look at each of the elements in this formula in turn:\n",
      "s is our sample's standard deviation\n",
      "x is each individual data point\n",
      "is the mean of the data\n",
      "n is the number of data points\n",
      "Before you freak out, let's break it down 71\n",
      "For each value in the sample, we will take that\n",
      "value, subtract the arithmetic mean from it, square the difference, and, once we've added\n",
      "up every single point this way, we will divide the entire thing by n, the number of points in\n",
      "the sample 56\n",
      "Finally, we take a square root of everything 9\n",
      "\n",
      "Without going into an in-depth analysis of the formula, think about it this way: it's basically\n",
      "derived from the distance formula 27\n",
      "Essentially, what the standard deviation is calculating is\n",
      "a sort of average distance of how far the data values are from the arithmetic mean 26\n",
      "\n",
      "If you take a closer look at the formula, you will see that it actually makes sense:\n",
      "By taking x-, you are finding the literal difference between the value and the\n",
      "mean of the sample 40\n",
      "\n",
      "By squaring the result, , we are putting a greater penalty on outliers\n",
      "because squaring a large error only makes it much larger 28\n",
      "\n",
      "By dividing by the number of items in the sample, we are taking (literally) the\n",
      "average squared distance between each point and the mean 30\n",
      "\n",
      "By taking the square root of the answer, we are putting the number in terms that\n",
      "we can understand 22\n",
      "For example, by squaring the number of friends minus the\n",
      "mean, we changed our units to friends squared, which makes no sense 27\n",
      "Taking\n",
      "the square root puts our units back to just \"friends 13\n",
      "\n",
      "Let's go back to our Facebook example for a visualization and further explanation of this 17\n",
      "\n",
      "Let's begin to calculate the standard deviation 9\n",
      "So, we'll start calculating a few of them 10\n",
      "\n",
      "Recall that the arithmetic mean of the data was just about 789, so, we'll use 789 as the mean 26\n",
      "\n",
      "We start by taking the difference between each data value and the mean, squaring it,\n",
      "adding them all up, dividing it by one less than the number of values, and then taking its\n",
      "square root 42\n",
      "This would look as follows:\n",
      " \n",
      "\n",
      "\n",
      "On the other hand, we can take the Python approach and do all this programmatically\n",
      "(which is usually preferred):\n",
      "np 31\n",
      "std(friends) # == 425 8\n",
      "2\n",
      "What the number 425 represents is the spread of data 14\n",
      "You could say that 425 is a kind of\n",
      "average distance the data  values are from the mean 21\n",
      "What this means, in simple words, is\n",
      "that this data is pretty spread out 17\n",
      "\n",
      "So, our standard deviation is about 425 10\n",
      "This means that the number of friends that these\n",
      "people have on Facebook doesn't seem to be close to a single number and that's quite\n",
      "evident when we plot the data in a bar graph, and also graph the mean as well as the\n",
      "visualizations of the standard deviation 58\n",
      "In the following plot, every person will be\n",
      "represented by a single bar in the bar chart, and the height of the bars represent the number\n",
      "of friends that the individuals have:\n",
      "import matplotlib 39\n",
      "pyplot as plt\n",
      "friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\n",
      "455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\n",
      "y_pos = range(len(friends))\n",
      "plt 92\n",
      "bar(y_pos, friends)\n",
      "plt 7\n",
      "plot((0, 25), (789, 789), 'b-')\n",
      "plt 18\n",
      "plot((0, 25), (789+425, 789+425), 'g-')\n",
      "plt 22\n",
      "plot((0, 25), (789-425, 789-425), 'r-')\n",
      "Here's the chart that we get:\n",
      "\n",
      "\n",
      "The blue line in the center is drawn at the mean (789), the red line near the bottom is drawn\n",
      "at the mean minus the standard deviation (789-425 = 364), and finally the green line\n",
      "towards the top is drawn at the mean plus the standard deviation (789+425 = 1,214) 96\n",
      "\n",
      "Note how most  of the data lives between the green and the red lines while the outliers live\n",
      "outside the lines 24\n",
      "There are three people who have friend counts below the red line and\n",
      "three people who have a friend count above the green line 25\n",
      "\n",
      "It's important to mention that the units for standard deviation are, in fact, the same units as\n",
      "the data's units 26\n",
      "So, in this example, we would say that the standard deviation is 425 friends\n",
      "on Facebook 20\n",
      "\n",
      "Another measure of variation is the variance, as described in the previous\n",
      "chapter 16\n",
      "The variance is simply the standard deviation squared 8\n",
      "\n",
      "So, now we know that the standard deviation and variance is good for checking how\n",
      "spread out our data is, and that we can use it along with the mean to create a kind of range\n",
      "that a lot of our data lies in 49\n",
      "But what if we want to compare the spread of two different\n",
      "datasets, maybe even with completely different units 21\n",
      "That's where the coefficient of\n",
      "variation comes into play 11\n",
      "\n",
      "Definition\n",
      "The coefficient of variation  is defined as the ratio of the data's standard deviation to its\n",
      "mean 23\n",
      "\n",
      "This ratio (which, by the way, is only helpful if we're working in the ratio level of\n",
      "measurement, where the division is allowed  and is meaningful) is a way to standardize the \n",
      "standard  deviation, which makes it easier to compare across datasets 55\n",
      "We use this measure\n",
      "frequently when attempting to compare means, and it spreads across populations that exist\n",
      "at different scales 24\n",
      "\n",
      "\n",
      "Example – employee salaries\n",
      "If we look at the mean and standard  deviation of employees' salaries in the same company\n",
      "but among different departments, we see that, at first glance, it may be tough to compare\n",
      "variations:\n",
      "This is especially true when the mean salary of one department is $25,000 , while another\n",
      "department has a mean salary in the six-figure area 78\n",
      "\n",
      "However, if we look at the last column, which is our coefficient of variation, it becomes\n",
      "clearer that the people in the executive department may be getting paid more but they are\n",
      "also getting wildly different salaries 44\n",
      "This is probably because the CEO is earning way more\n",
      "than an office manager, who is still in the executive department, which makes the data very\n",
      "spread out 32\n",
      "\n",
      "On the other hand, everyone in the mailroom, while not making as much money, is making\n",
      "just about the same as everyone else in the mailroom, which is why their coefficient of\n",
      "variation is only 8% 47\n",
      "\n",
      "With measures of variation, we can begin to answer big questions, such as how to spread\n",
      "out this data is or how we can come up with a good range that most of the data falls into 41\n",
      "\n",
      "Measures of relative standing\n",
      "We can combine both  the measures of centers and variations to create measures of relative\n",
      "standing 25\n",
      "Measures of variation  measure where particular data values are positioned,\n",
      "relative to the entire dataset 17\n",
      "\n",
      "\n",
      "Let's begin by learning a very  important value  in statistics, the z-score 18\n",
      "The z-score  is a way\n",
      "of telling us how far away a single data value is from the mean 22\n",
      "The z-score of an x data\n",
      "value is as follows:\n",
      "                                                                         z = \n",
      "Let's break down this formula:\n",
      "X is the data point\n",
      "the mean\n",
      "s is the standard deviation\n",
      "Remember that the standard deviation was (sort of) an average distance that the data is\n",
      "from the mean and now the z-score is an individualized value for each particular data\n",
      "point 76\n",
      "We can find the z-score of a data value by subtracting it from the mean and dividing\n",
      "it by the standard deviation 25\n",
      "The output will be the standardized distance a value is from a\n",
      "mean 14\n",
      "We use the z-score all over statistics 8\n",
      "It is a very effective way of normalizing data\n",
      "that exists on very different scales, and also to put data in the context of its mean 29\n",
      "\n",
      "Let's take our previous data on the number of friends on Facebook and standardize the data\n",
      "to the z-score 24\n",
      "For each data point, we will find its z-score by applying the preceding\n",
      "formula 17\n",
      "We will take each individual, subtract the average friends from the value, and\n",
      "divide that by the standard deviation, as shown:\n",
      "z_scores = []\n",
      "m = np 33\n",
      "mean(friends)  # average friends on Facebook\n",
      "s = np 14\n",
      "std(friends)   # standard deviation friends on Facebook\n",
      "for friend in friends:\n",
      "    z = (friend - m)/s  # z-score\n",
      "    z_scores 34\n",
      "append(z)  # make a list of the scores for plotting\n",
      "Now, let's plot these z-scores on a bar chart 27\n",
      "The following chart shows the same\n",
      "individuals from our previous example using friends on Facebook, but instead of the bar\n",
      "height revealing the raw number of friends, now each bar is the z-score of the number of\n",
      "friends they have on Facebook 49\n",
      "If we graph the z-scores, we'll notice a few things:\n",
      "plt 16\n",
      "bar(y_pos, z_scores)\n",
      "\n",
      "We get this chart:\n",
      "We can see that we have negative values (meaning that the data point is below the mean) 31\n",
      "\n",
      "The bars' lengths no longer represent the raw number of friends, but the degree to which\n",
      "that friend count differs from the mean 27\n",
      "\n",
      "This chart makes it very easy to pick out the individuals with much lower and higher\n",
      "friends on an average 22\n",
      "For example, the individual at index 0 has fewer friends on average\n",
      "(they had 109 friends where the average was 789) 28\n",
      "\n",
      "What if we want to graph the standard deviations 10\n",
      "Recall that we earlier graphed three\n",
      "horizontal lines: one at the mean, one at the mean plus the standard deviation (x+s) , and\n",
      "one at the mean minus the standard deviation (x-s) 43\n",
      "\n",
      "If we plug these values into the formula for the z-score, we will get the following:\n",
      "            \n",
      "\n",
      "\n",
      "This is no coincidence 26\n",
      "When we standardize the data using the z-score, our standard\n",
      "deviations become the metric of choice 21\n",
      "Let's plot a new graph with the standard deviations\n",
      "added:\n",
      "plt 14\n",
      "bar(y_pos, z_scores)\n",
      "plt 8\n",
      "plot((0, 25), (1, 1), 'g-')\n",
      "plt 18\n",
      "plot((0, 25), (0, 0), 'b-')\n",
      "plt 18\n",
      "plot((0, 25), (-1, -1), 'r-')\n",
      "The preceding code is adding the following three lines:\n",
      "A blue line at y = 0  that represents zero standard deviations away from the mean\n",
      "(which is on the x-axis)\n",
      "A green line that represents one standard deviation above the mean\n",
      "A red line that represents one standard deviation below the mean\n",
      "Let's look at the graph we get:\n",
      "The colors of the lines match  up with the lines drawn in the earlier graph of the raw friend\n",
      "count 108\n",
      "If you look carefully, the same people still fall outside of the green and the red lines 18\n",
      "\n",
      "Namely, the same three people still fall below the red (lower) line, and the same three\n",
      "people fall above the green (upper) line 32\n",
      "\n",
      "\n",
      "Z-scores are an effective way to standardize  data 13\n",
      "This means that we can put the entire set\n",
      "on the same scale 14\n",
      "For example, if we also measure each person's general happiness scale\n",
      "(which is between 0 and 1), we might have a dataset similar to the following dataset:\n",
      "friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\n",
      "455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\n",
      "happiness = [ 117\n",
      "8, 3\n",
      "6, 3\n",
      "3, 3\n",
      "6, 3\n",
      "6, 3\n",
      "4, 3\n",
      "8, 3\n",
      "5, 3\n",
      "4, 3\n",
      "3, 3\n",
      "3, 3\n",
      "6, 3\n",
      "2, 3\n",
      "8, 1, 6\n",
      "6,\n",
      " 3\n",
      "2, 3\n",
      "7, 3\n",
      "5, 3\n",
      "3, 3\n",
      "1, 0, 6\n",
      "3, 1]\n",
      "import pandas as pd\n",
      "df = pd 14\n",
      "DataFrame({'friends':friends, 'happiness':happiness})\n",
      "df 14\n",
      "head()\n",
      "We get this table:\n",
      "These data points are on two different dimensions, each with a very different scale 22\n",
      "The\n",
      "friend count can be in the thousands while our happiness score is stuck between 0 and 1 21\n",
      "\n",
      "To remedy this (and for some statistical/machine learning modeling, this concept will\n",
      "become essential), we can simply standardize the dataset using a prebuilt standardization\n",
      "package in scikit-learn, as follows:\n",
      "from sklearn import preprocessing\n",
      "df_scaled = pd 55\n",
      "DataFrame(preprocessing 3\n",
      "scale(df), columns =\n",
      "['friends_scaled', 'happiness_scaled'])\n",
      "df_scaled 16\n",
      "head()\n",
      "\n",
      "This code will scale both the friends and happiness columns simultaneously, thus revealing\n",
      "the z-score for each column 23\n",
      "It is important to note that by doing this, the preprocessing\n",
      "module in sklearn  is doing the following things separately for each column:\n",
      "Finding the mean of the column\n",
      "Finding the standard deviation of the column\n",
      "Applying the z-score function to each element in the column\n",
      "The result is two columns, as shown, that exist on the same scale as each other even if they\n",
      "were not previously:\n",
      "Now, we can plot friends and happiness on the same scale and the graph will at least be\n",
      "readable:\n",
      "df_scaled 106\n",
      "plot(kind='scatter', x = 'friends_scaled', y =\n",
      "'happiness_scaled')\n",
      "\n",
      "The preceding code gives us this graph:\n",
      "Now, our data is standardized to the z-score and this scatter plot is fairly easily\n",
      "interpretable 45\n",
      "In later chapters, this idea  of standardization will not only make our data\n",
      "more interpretable, but it will also be essential in our model optimization 31\n",
      "Many machine\n",
      "learning algorithms will require us to have standardized columns as they are reliant on the\n",
      "notion of scale 23\n",
      "\n",
      "The insightful part – correlations in data\n",
      "Throughout this book, we will discuss the difference between having data and having\n",
      "actionable insights about  your data 31\n",
      "Having data is only one step to a successful data\n",
      "science operation 13\n",
      "Being able to obtain, clean, and plot data helps to tell the story that the\n",
      "data has to offer but cannot reveal the moral 27\n",
      "In order to take this entire example one step\n",
      "further, we will look at the relationship between having friends on Facebook and\n",
      "happiness 28\n",
      "\n",
      "\n",
      "In subsequent chapters, we will look at a specific machine learning algorithm that attempts\n",
      "to find relationships between quantitative features, called linear regression, but we do not\n",
      "have to wait until then to begin to form hypotheses 43\n",
      "We have a sample of people, a measure\n",
      "of their online social presence, and their reported happiness 20\n",
      "The question of the day here is\n",
      "this: can we find a relationship between the number of friends on Facebook and overall\n",
      "happiness 27\n",
      "\n",
      "Now, obviously, this is a big question and should be treated respectfully 15\n",
      "Experiments to\n",
      "answer this question should be conducted in a laboratory setting, but we can begin to form\n",
      "a hypothesis about this question 27\n",
      "Given the nature of our data, we really only have the\n",
      "following three options for a hypothesis:\n",
      "There is a positive association between the number of online friends and\n",
      "happiness (as one goes up, so does the other)\n",
      "There is a negative association between them (as the number of friends goes up,\n",
      "your happiness goes down)\n",
      "There is no association between the variables (as one changes, the other doesn't\n",
      "really change that much)\n",
      "Can we use basic statistics to form a hypothesis about this question 101\n",
      "I say we can 4\n",
      "But first,\n",
      "we must introduce a concept called correlation 10\n",
      "\n",
      "Correlation coefficients  are a quantitative measure  that describes the strength of\n",
      "association/relationship between two variables 22\n",
      "\n",
      "The correlation between the two sets of data tells us about how they move together 16\n",
      "Would\n",
      "changing one help us predict the other 9\n",
      "This concept is not only interesting in this case, but\n",
      "it is one of the core assumptions that many machine learning models make on data 27\n",
      "For\n",
      "many prediction algorithms to work, they rely on the fact that there is some sort of\n",
      "relationship between the variables that we are looking at 29\n",
      "The learning algorithms then\n",
      "exploit this relationship in order to make accurate predictions 15\n",
      "\n",
      "A few things to note about a correlation coefficient are as follows:\n",
      "It will lie between -1 and 1\n",
      "The greater the absolute value (closer to -1 or 1), the stronger the relationship\n",
      "between the variables:\n",
      "The strongest correlation is a -1 or a 1\n",
      "The weakest correlation is a 0\n",
      "\n",
      "A positive correlation means that as one variable increases, the other one tends to\n",
      "increase as well\n",
      "A negative correlation means that as one variable increases, the other one tends\n",
      "to decrease\n",
      "We can use pandas  to quickly show us correlation coefficients between every feature and\n",
      "every other feature in the DataFrame, as illustrated:\n",
      "# correlation between variables\n",
      "df 138\n",
      "corr()\n",
      "We get this table:\n",
      "This table shows the correlation between friends  and happiness 17\n",
      "Note the first two things:\n",
      "The diagonal of the matrix is filled with positive is 16\n",
      "This is because they represent\n",
      "the correlation between the variable and itself, which, of course, forms a perfect\n",
      "line, making the correlation perfectly positive 30\n",
      "\n",
      "The matrix is symmetrical across the diagonal 9\n",
      "This is true for any correlation\n",
      "matrix made in pandas 11\n",
      "\n",
      "There are a few caveats to trusting the correlation coefficient 12\n",
      "One is that, in general, a\n",
      "correlation will attempt to measure a linear  relationship between variables 21\n",
      "This means that\n",
      "if there is no visible correlation revealed by this measure, it does not mean that there is no\n",
      "relationship between the variables, only that there is no line of best fit that goes through the\n",
      "lines easily 45\n",
      "There might be a non-linear  relationship that defines the two variables 13\n",
      "\n",
      "It is important to realize that causation is not implied by correlation 14\n",
      "Just because there is a\n",
      "weak negative correlation between these two variables does not necessarily mean that your\n",
      "overall happiness decreases as the number of friends you keep on Facebook go up 34\n",
      "This\n",
      "causation must be tested further and, in later chapters, we will attempt to do just that 22\n",
      "\n",
      "To sum up, we can use correlation to make hypotheses about the relationship between\n",
      "variables, but we will need to use more sophisticated statistical methods and machine\n",
      "learning algorithms to solidify these assumptions and hypotheses 41\n",
      "\n",
      "\n",
      "The empirical rule\n",
      "Recall that a normal distribution is defined  as having a specific probability distribution that\n",
      "resembles a bell curve 27\n",
      "In statistics, we love it when our data behaves normally 11\n",
      "For\n",
      "example, we may have data that resembles a normal distribution, like so:\n",
      "The empirical rule  states that we can expect a certain amount of data to live between sets of\n",
      "standard deviations 39\n",
      "Specifically, the empirical rule states the following for data that is\n",
      "distributed normally:\n",
      "About 68% of the data falls within 1 standard deviation\n",
      "About 95% of the data falls within 2 standard deviations\n",
      "About 99 47\n",
      "7% of the data falls within 3 standard deviations\n",
      "For example, let's see if our Facebook friends' data holds up to this 29\n",
      "Let's use our DataFrame\n",
      "to find the percentage of people that fall within 1, 2, and 3 standard deviations of the mean,\n",
      "as shown:\n",
      "# finding the percentage of people within one standard deviation of the\n",
      "mean\n",
      "within_1_std = df_scaled[(df_scaled['friends_scaled'] <= 1) &\n",
      "(df_scaled['friends_scaled'] >= -1)] 77\n",
      "shape[0]\n",
      "within_1_std / float(df_scaled 12\n",
      "shape[0])\n",
      "# 0 7\n",
      "75\n",
      "# finding the percentage of people within two standard deviations of the\n",
      "mean\n",
      "within_2_std = df_scaled[(df_scaled['friends_scaled'] <= 2) &\n",
      "(df_scaled['friends_scaled'] >= -2)] 47\n",
      "shape[0]\n",
      "within_2_std / float(df_scaled 12\n",
      "shape[0])\n",
      "\n",
      "# 0 7\n",
      "916\n",
      "# finding the percentage of people within three standard deviations of the\n",
      "mean\n",
      "within_3_std = df_scaled[(df_scaled['friends_scaled'] <= 3) &\n",
      "(df_scaled['friends_scaled'] >= -3)] 47\n",
      "shape[0]\n",
      "within_3_std / float(df_scaled 12\n",
      "shape[0])\n",
      "# 1 7\n",
      "0\n",
      "We can see that our data does seem to follow the empirical rule 16\n",
      "About 75% of the people\n",
      "are within a single standard deviation of the mean 17\n",
      "About 92% of the people are within two\n",
      "standard deviations, and all of them are within three standard deviations 23\n",
      "\n",
      "Example: Exam scores\n",
      "Let's say that we're measuring the scores of an exam and the scores generally have a bell-\n",
      "shaped normal distribution 30\n",
      "The average of the exam was 84% and the standard deviation\n",
      "was 6% 18\n",
      "We can say the following, with approximate certainty:\n",
      "About 68% of the class scored between 78% and 90% because 78 is 6 units below\n",
      "84, and 90 is 6 units above 84\n",
      "If we were asked what percentage of the class scored between 72 % and 96%, we\n",
      "would notice that 72 is 2 standard deviations below the mean, and 96 is 2\n",
      "standard deviations above the mean, so the empirical rule tells us that about 95%\n",
      "of the class scored in that range\n",
      "However, not all data is normally  distributed, so we can't always use the empirical rule 133\n",
      "We\n",
      "have another theorem that helps us analyze any kind of distribution 13\n",
      "In the next chapter,\n",
      "we will go into depth about when we can assume the normal distribution 18\n",
      "This is because\n",
      "many statistical tests and hypotheses require the underlying data to come from a normally\n",
      "distributed population 21\n",
      "\n",
      "Previously, when we standardized our data to the z-score, we did not\n",
      "require the normal distribution assumption 22\n",
      "\n",
      "\n",
      "Summary\n",
      "In this chapter, we covered much of the basic statistics required by most data scientists 19\n",
      "\n",
      "Everything from how we obtain/sample data to how to standardize data according to the z-\n",
      "score and applications of the empirical rule was covered 28\n",
      "We also reviewed how to take\n",
      "samples for data analysis 11\n",
      "In addition, we reviewed various statistical measures, such as the\n",
      "mean and standard deviation, that help describe data 22\n",
      "\n",
      "In the next chapter, we will look at much more advanced applications of statistics 16\n",
      "One\n",
      "thing that we will consider is how to use hypothesis tests on data that we can assume to be\n",
      "normal 23\n",
      "As we use these tests, we will also quantify our errors and identify the best\n",
      "practices to solve these errors 23\n",
      "\n",
      "\n",
      "\n",
      "Advanced Statistics\n",
      "We are concerned with making inferences about entire populations based on certain\n",
      "samples of data 21\n",
      "We will be using hypothesis tests along with different estimation tests in\n",
      "order to gain a better understanding of populations, given samples of data 26\n",
      "\n",
      "The key topics that we will cover in this chapter are as follows:\n",
      "Point estimates\n",
      "Confidence intervals\n",
      "The central limit theorem\n",
      "Hypothesis testing\n",
      "Point estimates\n",
      "Recall that, in the previous chapter, we mentioned how difficult it is to obtain a population\n",
      "parameter; so, we had to use sample data to calculate a statistic that was an estimate of a\n",
      "parameter 77\n",
      "When we make these estimates, we call them point estimates 11\n",
      "\n",
      "A point estimate  is an estimate  of a population parameter based on sample data 17\n",
      "\n",
      "We use point estimates to estimate population means, variances, and other statistics 16\n",
      "To\n",
      "obtain these estimates, we simply apply the function that we wish to measure for our\n",
      "population to a sample of the data 27\n",
      "For example, suppose there is a company of 9,000\n",
      "employees and we are interested in ascertaining the average length of breaks taken by\n",
      "employees in a single day 36\n",
      "As we probably cannot ask every single person, we will take a\n",
      "sample of the 9,000 people and take a mean of the sample 29\n",
      "This sample mean will be our\n",
      "point estimate 9\n",
      "\n",
      "\n",
      "The following code is broken into three parts:\n",
      "We will use the probability distribution, known as the Poisson distribution , to\n",
      "randomly generate 9,000 answers to the question: for how many minutes in a day\n",
      "do you usually take breaks 51\n",
      "This will represent our population 5\n",
      "Remember, from\n",
      "Chapter 6 , Advanced Probability , that the Poisson random variable  is used when\n",
      "we know the average value of an event and wish to model a distribution around\n",
      "it 39\n",
      "\n",
      "Note that this average value is not usually known 10\n",
      "I am calculating it to\n",
      "show the difference between our parameter and our statistic 15\n",
      "I also set a\n",
      "random seed in order to encourage reproducibility (this allows us to get\n",
      "the same random numbers each time) 28\n",
      "\n",
      "We will take a sample of 100 employees (using the Python random sample method) and\n",
      "find a point estimate of a mean (called a sample mean) 33\n",
      "\n",
      "Note that this is just over 1% of our population 13\n",
      "\n",
      "Compare our sample mean (the mean of the sample of 100 employees) to our population\n",
      "mean 21\n",
      "\n",
      "Let's take a look at the following code:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "from scipy 28\n",
      "stats import poisson\n",
      "np 6\n",
      "random 1\n",
      "seed(1234)\n",
      "long_breaks = stats 10\n",
      "poisson 2\n",
      "rvs(loc=10, mu=60, size=3000)\n",
      "# represents 3000 people who take about a 60 minute break\n",
      "The long_breaks  variable represents 3,000  answers to the question, \" how many minutes\n",
      "on an average do you take breaks for 60\n",
      ",\"  and these answers will be on the longer side 11\n",
      "Let's\n",
      "see a visualization of this distribution, shown as follows:\n",
      "pd 15\n",
      "Series(long_breaks) 5\n",
      "hist()\n",
      "\n",
      "\n",
      "We can see that our average of 60 minutes is to the left of the distribution 19\n",
      "Also, because we\n",
      "only sampled 3,000  people, our bars are at their highest at around 700–800 people 27\n",
      "\n",
      "Now, let's model 6,000  people who take, on an average, about 15 minutes' worth of breaks 27\n",
      "\n",
      "Let's again use the Poisson distribution to simulate 6,000  people, as shown:\n",
      "short_breaks = stats 26\n",
      "poisson 2\n",
      "rvs(loc=10, mu=15, size=6000)\n",
      "# represents 6000 people who take about a 15 minute break\n",
      "pd 31\n",
      "Series(short_breaks) 5\n",
      "hist()\n",
      "\n",
      "\n",
      "Okay, so, we have a distribution for the people who take longer breaks and a distribution\n",
      "for the people who take shorter breaks 28\n",
      "Again, note how our average break length of 15\n",
      "minutes falls to the left-hand side of the distribution, and note that the tallest bar is for\n",
      "about 1,600  people:\n",
      "breaks = np 44\n",
      "concatenate((long_breaks, short_breaks))\n",
      "# put the two arrays together to get our \"population\" of 9000 people\n",
      "The breaks  variable is the amalgamation of all the 9,000  employees, both long and short\n",
      "break takers 54\n",
      "Let's see the entire distribution of people in a single visualization:\n",
      "pd 14\n",
      "Series(breaks) 5\n",
      "hist()\n",
      "We can see we have two humps 10\n",
      "On the left, we have our larger hump of people who take\n",
      "about a 15-minute break, and on the right, we have a smaller hump of people who take\n",
      "longer breaks 41\n",
      "Later on, we will investigate this graph further 9\n",
      "\n",
      "We can find the total average break length by running the following code:\n",
      "breaks 17\n",
      "mean()\n",
      "# 39 5\n",
      "99 minutes is our parameter 6\n",
      "\n",
      "\n",
      "Our average company break length is about 40 minutes 11\n",
      "Remember that our population is\n",
      "the entire company's employee size of 9,000 people, and our parameter is 40 minutes 26\n",
      "In the\n",
      "real world, our goal would be to estimate the population parameter because we would not\n",
      "have the resources to ask every single employee in a survey their average break length for\n",
      "many reasons 39\n",
      "Instead, we will use a point estimate 8\n",
      "\n",
      "So, to make our point, we want to simulate a world where we ask 100 random people about\n",
      "the length of their breaks 28\n",
      "To do this, let's take a random sample of 100 employees out of\n",
      "the 9,000 employees we simulated, as shown:\n",
      "sample_breaks = np 34\n",
      "random 1\n",
      "choice(a = breaks, size=100)\n",
      "# taking a sample of 100 employees\n",
      "Now, let's take the mean of the sample and subtract it from the population mean and see\n",
      "how far off we were:\n",
      "breaks 46\n",
      "mean() - sample_breaks 6\n",
      "mean()\n",
      "# difference between means is 4 9\n",
      "09 minutes, not bad 6\n",
      "\n",
      "This is extremely interesting because, with only about 1% of our population (100 out of\n",
      "9,000), we were able to get within 4 minutes of our population parameter and get a very\n",
      "accurate estimate of our population mean 50\n",
      "Not bad 2\n",
      "\n",
      "Here, we calculated a point estimate for the mean, but we can also do this for proportion\n",
      "parameters 22\n",
      "By proportion, I am referring to a ratio of two quantitative values 13\n",
      "\n",
      "Let's suppose that in a company of 10,000 people, our employees are 20% white, 10% black,\n",
      "10% Hispanic, 30% Asian, and 30% identify as other 44\n",
      "We will take a sample of 1,000\n",
      "employees and see whether their race proportions are similar:\n",
      "employee_races = ([\"white\"]*2000) + ([\"black\"]*1000) +\\\n",
      "                   ([\"hispanic\"]*1000) + ([\"asian\"]*3000) +\\\n",
      "                   ([\"other\"]*3000)\n",
      "The employee_races  represents our employee population 84\n",
      "For example, in our company\n",
      "of 10,000 people, 2,000 people are white (20%) and 3,000 people are Asian (30%) 35\n",
      "\n",
      "Let's take a random sample of 1,000 people, as shown:\n",
      "import random\n",
      "demo_sample = random 24\n",
      "sample(employee_races, 1000)   # Sample 1000 values\n",
      "\n",
      "for race in set(demo_sample):\n",
      "    print( race + \" proportion estimate:\" )\n",
      "    print( demo_sample 40\n",
      "count(race)/1000 6\n",
      ")\n",
      "The output obtained would be as follows:\n",
      "hispanic proportion estimate:\n",
      "0 15\n",
      "103\n",
      "white proportion estimate:\n",
      "0 8\n",
      "192\n",
      "other proportion estimate:\n",
      "0 8\n",
      "288\n",
      "black proportion estimate:\n",
      "0 8\n",
      "1\n",
      "asian proportion estimate:\n",
      "0 9\n",
      "317\n",
      "We can see that the race proportion estimates are very close to the underlying population's\n",
      "proportions 23\n",
      "For example, we got 10 7\n",
      "3% for Hispanic in our sample and the population\n",
      "proportion for Hispanic was 10% 20\n",
      "\n",
      "Sampling distributions\n",
      "In Chapter 7 , Basic Statistics , we mentioned how much we love it when data follows the\n",
      "normal distribution 26\n",
      "One of the reasons for this is that many statistical tests (including the\n",
      "ones we will use in this chapter) rely on data that follows a normal pattern, and for the\n",
      "most part, a lot of real-world data is not normal (surprised 52\n",
      " 1\n",
      "Take our employee  break\n",
      "data, for example —you might think I was just being fancy creating data using the Poisson\n",
      "distribution, but I had a reason for this 35\n",
      "I specifically wanted non-normal data, as shown:\n",
      "pd 11\n",
      "DataFrame(breaks) 5\n",
      "hist(bins=50,range=(5,100))\n",
      "\n",
      "\n",
      "As you can see, our data is definitely not following a normal distribution; it appears to be\n",
      "bi-modal , which means that there are two peaks of break times, at around 25 and 70\n",
      "minutes 55\n",
      "As our data is not normal, many of the most popular statistics tests may not apply;\n",
      "however, if we follow the given procedure, we can create normal data 32\n",
      "Think I'm crazy 4\n",
      "\n",
      "Well, see for yourself 6\n",
      "\n",
      "First off, we will need to utilize what is known as a sampling distribution , which is a\n",
      "distribution of point estimates of several samples of the same size 32\n",
      "Our procedure for\n",
      "creating a sampling distribution will be the following:\n",
      "Take 500 different samples of the break times of size 100 each1 28\n",
      "\n",
      "Take a histogram of these 500 different point estimates (revealing their2 16\n",
      "\n",
      "distribution)\n",
      "The number of elements in the sample (100) was arbitrary but large enough to be a\n",
      "representative sample of the population 28\n",
      "The number of samples I took (500) was also\n",
      "arbitrary, but large enough to ensure that our data would converge to a normal\n",
      "distribution:\n",
      "point_estimates = []\n",
      "for x in range(500):         # Generate 500 samples\n",
      "   sample = np 54\n",
      "random 1\n",
      "choice(a= breaks, size=100)\n",
      "\n",
      "#take a sample of 100 points\n",
      "point_estimates 21\n",
      "append( sample 3\n",
      "mean() )\n",
      "# add the sample mean to our list of point estimates\n",
      "pd 16\n",
      "DataFrame(point_estimates) 5\n",
      "hist()\n",
      "# look at the distribution of our sample means\n",
      "Behold 14\n",
      "The sampling distribution of the sample mean appears to be normal even though\n",
      "we took data from an underlying bimodal population distribution 25\n",
      "It is important to note\n",
      "that the bars in this histogram represent the average break length of 500 samples of\n",
      "employees, where each sample has 100 people in it 34\n",
      "In other words, a sampling distribution\n",
      "is a distribution of several point estimates 15\n",
      "\n",
      "Our data converged to a normal distribution because of something called the central limit\n",
      "theorem , which states that the sampling distribution (the distribution of point estimates)\n",
      "will approach a normal distribution as we increase the number of samples taken 44\n",
      "\n",
      "\n",
      "What's more, as we take more and more samples, the mean of the sampling distribution\n",
      "will approach the true population mean, as shown:\n",
      "breaks 32\n",
      "mean() - np 4\n",
      "array(point_estimates) 5\n",
      "mean()\n",
      "# 3\n",
      "047 minutes difference\n",
      "This is actually a very exciting result because it means that we can get even closer than a\n",
      "single point estimate by taking multiple point estimates and utilizing the central limit\n",
      "theorem 39\n",
      "\n",
      "In general, as we increase the number of samples taken, our estimate will\n",
      "get closer to the parameter (actual value) 26\n",
      "\n",
      "Confidence intervals\n",
      "While point estimates are okay, estimates of a population parameter and sampling\n",
      "distributions are even better 24\n",
      "There are the following two main issues with these\n",
      "approaches:\n",
      "Single point estimates are very prone to error (due to sampling bias among other\n",
      "things)\n",
      "Taking multiple samples of a certain size for sampling distributions might not be\n",
      "feasible, and may sometimes be even more infeasible than actually finding the\n",
      "population parameter\n",
      "For these reasons and more, we may turn to a concept known as the confidence interval to\n",
      "find statistics 86\n",
      "\n",
      "A confidence interval  is a range of values based on a point estimate that contains the true\n",
      "population parameter at some confidence level 26\n",
      "\n",
      "Confidence  is an important concept in advanced statistics 11\n",
      "Its meaning is sometimes\n",
      "misconstrued 9\n",
      "Informally, a confidence level does not represent a probability of being correct ;\n",
      "instead, it represents the frequency that the obtained answer will be accurate 28\n",
      "For example,\n",
      "if you want to have a 95% chance of capturing the true population parameter using only a\n",
      "single point estimate, you have to set your confidence level to 95% 38\n",
      "\n",
      "\n",
      "Higher confidence levels result in wider (larger) confidence intervals in\n",
      "order to be more sure 20\n",
      "\n",
      "Calculating a confidence interval involves finding a point estimate and then incorporating a\n",
      "margin of error to create a range 23\n",
      "The margin of error  is a value that represents our\n",
      "certainty that our point estimate is accurate and is based on our desired confidence level,\n",
      "the variance of the data, and how big your sample is 41\n",
      "There are many ways to calculate\n",
      "confidence intervals; for the purpose of brevity and simplicity, we will look at a single way\n",
      "of taking the confidence interval of a population mean 37\n",
      "For this confidence interval, we\n",
      "need the following:\n",
      "A point estimate 14\n",
      "For this, we will take our sample mean of break lengths from\n",
      "our previous example 17\n",
      "\n",
      "An estimate of the population standard deviation, which represents the variance\n",
      "in the data 17\n",
      "This is calculated by taking the sample standard deviation (the\n",
      "standard deviation of the sample data) and dividing that number by the square\n",
      "root of the population size 32\n",
      "\n",
      "The degrees of freedom (which is the -1 sample size) 14\n",
      "\n",
      "Obtaining these numbers might seem arbitrary but, trust me, there is a reason for all of\n",
      "them 23\n",
      "However, again for simplicity, I will use prebuilt Python modules, as shown, to\n",
      "calculate our confidence interval and then demonstrate its value:\n",
      "import math\n",
      "sample_size = 100\n",
      "# the size of the sample we wish to take\n",
      "sample = np 52\n",
      "random 1\n",
      "choice(a= breaks, size = sample_size)\n",
      "# a sample of sample_size taken from the 9,000 breaks population from\n",
      "before\n",
      "sample_mean = sample 33\n",
      "mean()\n",
      "# the sample mean of the break lengths sample\n",
      "\n",
      "sample_stdev = sample 17\n",
      "std()\n",
      "# sample standard deviation\n",
      "sigma = sample_stdev/math 13\n",
      "sqrt(sample_size)\n",
      "# population standard deviation estimate\n",
      "stats 11\n",
      "t 1\n",
      "interval(alpha = 0 5\n",
      "95,              # Confidence level 95%\n",
      "                 df= sample_size - 1,       # Degrees of freedom\n",
      "                 loc = sample_mean,         # Sample mean\n",
      "                 scale = sigma)             # Standard deviation\n",
      "# (36 49\n",
      "36, 45 5\n",
      "44)\n",
      "To reiterate, this range of values (from 36 15\n",
      "36  to 45 6\n",
      "44 ) represents a confidence interval for\n",
      "the average break time with 95% confidence 18\n",
      "\n",
      "We already know that our population parameter is 39 11\n",
      "99 , and note that the interval\n",
      "includes the population mean of 39 16\n",
      "99 2\n",
      "\n",
      "I mentioned earlier that the confidence  level is not a percentage of accuracy of our interval\n",
      "but the percent chance that the interval will even contain the population parameter at all 34\n",
      "\n",
      "To better understand the confidence level, let's take 10,000 confidence intervals and see\n",
      "how often our population means falls in the interval 29\n",
      "First, let's make a function, as\n",
      "illustrated, that makes a single confidence interval from our breaks data:\n",
      "# function to make confidence interval\n",
      "def makeConfidenceInterval():\n",
      "    sample_size = 100\n",
      "    sample = np 48\n",
      "random 1\n",
      "choice(a= breaks, size = sample_size)\n",
      "    sample_mean = sample 15\n",
      "mean()\n",
      "\n",
      "    # sample mean\n",
      "    sample_stdev = sample 13\n",
      "std()\n",
      "    # sample standard deviation\n",
      "    sigma = sample_stdev/math 15\n",
      "sqrt(sample_size)\n",
      "    # population Standard deviation estimate\n",
      "    return stats 14\n",
      "t 1\n",
      "interval(alpha = 0 5\n",
      "95, df= sample_size - 1, loc =\n",
      "sample_mean, scale = sigma)\n",
      "Now that we have a function that will create a single confidence interval, let's create a\n",
      "procedure that will test the probability that a single confidence interval will contain the true\n",
      "population parameter, 39 60\n",
      "99 :\n",
      "Take 10,000 confidence intervals of the sample mean 14\n",
      "1 2\n",
      "\n",
      "Count the number of times that the population parameter falls into our2 14\n",
      "\n",
      "confidence intervals 3\n",
      "\n",
      "Output the ratio of the number of times the parameter fell into the interval by3 17\n",
      "\n",
      "10,000:\n",
      "times_in_interval = 0 11\n",
      "\n",
      "for i in range(10000):\n",
      "    interval = makeConfidenceInterval()\n",
      "    if 39 21\n",
      "99 >= interval[0] and 39 10\n",
      "99 <= interval[1]:\n",
      "    # if 39 12\n",
      "99 falls in the interval\n",
      "        times_in_interval += 1\n",
      "print(times_in_interval / 10000)\n",
      "# 0 27\n",
      "9455\n",
      "\n",
      "Success 5\n",
      "We see that about 95% of our confidence intervals contained our actual population\n",
      "mean 17\n",
      "Estimating population parameters through point estimates and confidence intervals\n",
      "is a relatively simple and powerful form of statistical inference 21\n",
      "\n",
      "Let's also take a quick look at how  the size of confidence intervals changes as we change\n",
      "our confidence level 24\n",
      "Let's calculate confidence intervals for multiple confidence levels and\n",
      "look at how large the intervals are by looking at the difference between the two numbers 27\n",
      "\n",
      "Our hypothesis will be that as we make our confidence level larger, we will likely see larger\n",
      "confidence intervals to be surer that we catch the true population parameter:\n",
      "for confidence in ( 38\n",
      "5, 3\n",
      "8, 3\n",
      "85, 3\n",
      "9, 3\n",
      "95, 3\n",
      "99):\n",
      "    confidence_interval = stats 8\n",
      "t 1\n",
      "interval(alpha = confidence, df=\n",
      "sample_size - 1, loc = sample_mean, scale = sigma)\n",
      "    length_of_interval = round(confidence_interval[1] -\n",
      "confidence_interval[0], 2)\n",
      "    # the length of the confidence interval\n",
      "    print( \"confidence {0} has a interval of size {1}\" 68\n",
      "format(confidence,\n",
      "length_of_interval))\n",
      "confidence 0 11\n",
      "5 has an interval of size 2 9\n",
      "56\n",
      "confidence 0 6\n",
      "8 has an interval of size 4 9\n",
      "88\n",
      "confidence 0 6\n",
      "85 has an interval of size 5 9\n",
      "49\n",
      "confidence 0 6\n",
      "9 has an interval of size 6 9\n",
      "29\n",
      "confidence 0 6\n",
      "95 has an interval of size 7 9\n",
      "51\n",
      "confidence 0 6\n",
      "99 has an interval of size 9 9\n",
      "94\n",
      "We can see that as we wish to be more confident  in our interval, our interval expands in\n",
      "order to compensate 27\n",
      "\n",
      "Next, we will take our concept of confidence levels and look at statistical hypothesis testing\n",
      "in order to both expand on these topics and also create (usu ally)  even more powerful\n",
      "statistical inferences 42\n",
      "\n",
      "\n",
      "Hypothesis tests\n",
      "Hypothesis tests are one of the most widely used tests in statistics 20\n",
      "They come in many\n",
      "forms; however, all of them have the same basic purpose 17\n",
      "\n",
      "A hypothesis test  is a statistical test that is used to ascertain whether we are allowed to\n",
      "assume that a certain condition is true for the entire population, given a data sample 36\n",
      "\n",
      "Basically, a hypothesis test is a test for a certain hypothesis that we have about an entire\n",
      "population 21\n",
      "The result of the test then tells us whether we should believe the hypothesis or\n",
      "reject it for an alternative one 22\n",
      "\n",
      "You can think of the hypothesis test's framework to determine whether the observed\n",
      "sample data deviates from what was to be expected from the population itself 30\n",
      "Now, this\n",
      "sounds like a difficult task but, luckily, Python comes to the rescue and includes built-in\n",
      "libraries to conduct these tests easily 29\n",
      "\n",
      "A hypothesis test generally looks at two opposing hypotheses about a population 13\n",
      "We call\n",
      "them the null hypothesis  and the alternative hypothesis 12\n",
      "The null hypothesis is the\n",
      "statement being tested and is the default correct  answer; it is our starting point and our\n",
      "original hypothesis 27\n",
      "The alternative hypothesis is the statement that opposes the null\n",
      "hypothesis 14\n",
      "Our test will tell us which hypothesis we should trust and which we should\n",
      "reject 16\n",
      "\n",
      "Based on sample data from a population, a hypothesis test determines whether or not to\n",
      "reject  the null hypothesis 23\n",
      "We usually use a p-value  (which is based on our significance\n",
      "level) to make this conclusion 21\n",
      "\n",
      "A very common misconception is that statistical hypothesis tests are\n",
      "designed to select the more likely  of the two hypotheses 24\n",
      "This is incorrect 3\n",
      "\n",
      "A hypothesis test will default to the null hypothesis until  there is enough\n",
      "data to support the alternative hypothesis 22\n",
      "\n",
      "The following are some examples of questions you can answer with a hypothesis test:\n",
      "Does the mean break time of employees differ from 40 minutes 28\n",
      "\n",
      "Is there a difference between people who interacted with website A and people\n",
      "who interacted with website B (A/B testing) 27\n",
      "\n",
      "Does a sample of coffee beans vary significantly in taste from the entire\n",
      "population of beans 18\n",
      "\n",
      "\n",
      "Conducting a hypothesis test\n",
      "There are multiple types of hypothesis  tests out there, and among them are dozens of\n",
      "different procedures and metrics 30\n",
      "Nonetheless, there are five basic steps that most\n",
      "hypothesis tests follow, which are as follows:\n",
      "Specify the hypotheses:1 26\n",
      "\n",
      "Here, we formulate our two hypotheses: the null and the alternative 14\n",
      "\n",
      "We usually use the notation of H0 to represent the null hypothesis and\n",
      "Ha to represent our alternative hypothesis 22\n",
      "\n",
      "Determine the sample size for the test sample:2 12\n",
      "\n",
      "This calculation depends on the chosen test 8\n",
      "Usually, we have to\n",
      "determine a proper sample size in order to utilize theorems, such as\n",
      "the central limit theorem and assume the normality of data 34\n",
      "\n",
      "Choose a significance level (usually called alpha or α): 3 14\n",
      "\n",
      "A significance level of 0 7\n",
      "05 is common 4\n",
      "\n",
      "Collect the data:4 6\n",
      "\n",
      "Collect a sample of data to conduct the test 10\n",
      "\n",
      "Decide whether to reject or fail to reject the null hypothesis:5 15\n",
      "\n",
      "This step changes slightly based on the type of test being used 13\n",
      "\n",
      "The final result will either yield a rejection of  the null hypothesis in\n",
      "favor of the alternative or fail to reject the null hypothesis 27\n",
      "\n",
      "In this chapter, we will look at the following three types of hypothesis tests:\n",
      "One-sample t-tests\n",
      "Chi-square goodness of fit\n",
      "Chi-square test for association/independence\n",
      "There are many more tests 41\n",
      "However, these three are a great combination of distinct,\n",
      "simple, and powerful tests 16\n",
      "One of the biggest things to consider when choosing which test\n",
      "we should implement is the type of data we are working with, specifically, are we dealing\n",
      "with continuous or categorical data 36\n",
      "In order to truly see the effects of a hypothesis, I\n",
      "suggest we dive right into an example 20\n",
      "First, let's look at the use of t-tests to deal with\n",
      "continuous data 17\n",
      "\n",
      "\n",
      "One sample t-tests\n",
      "The one sample t-test  is a statistical  test used to determine  whether a quantitative\n",
      "(numerical) data sample differs significantly  from another dataset (the population or\n",
      "another sample) 45\n",
      "Suppose, in our previous employee break time example, we look,\n",
      "specifically, at the engineering department's break times, as shown:\n",
      "long_breaks_in_engineering = stats 35\n",
      "poisson 2\n",
      "rvs(loc=10, mu=55, size=100)\n",
      "short_breaks_in_engineering = stats 22\n",
      "poisson 2\n",
      "rvs(loc=10, mu=15, size=300)\n",
      "engineering_breaks = np 19\n",
      "concatenate((long_breaks_in_engineering,\n",
      "short_breaks_in_engineering))\n",
      "print(breaks 20\n",
      "mean())\n",
      "# 39 5\n",
      "99\n",
      "print(engineering_breaks 8\n",
      "mean())\n",
      "# 34 5\n",
      "825\n",
      "Note that I took the same approach as making the original break times, but with the\n",
      "following two differences:\n",
      "I took a smaller sample from the Poisson distribution (to simulate that we took a\n",
      "sample of 400 people from the engineering department) 53\n",
      "\n",
      "Instead of using μ of 60 as before, I used 55 to simulate the fact that the\n",
      "engineering department's break behavior isn't exactly the same as the company's\n",
      "behavior as a whole 41\n",
      "\n",
      "It is easy to see that there seems to be a difference (of over 5 minutes) between the\n",
      "engineering department and the company as a whole 31\n",
      "We usually don't have the entire\n",
      "population and the population parameters at our disposal, but I have them simulated in\n",
      "order for the example to work 30\n",
      "So, even though we (the omniscient readers) can see a\n",
      "difference, we will assume that we know nothing of these population parameters and,\n",
      "instead, rely on a statistical test in order to ascertain these differences 44\n",
      "\n",
      "\n",
      "Example of a one-sample t-test\n",
      "Our objective here is to ascertain whether  there is a difference between the overall\n",
      "population's (company employees) break times and the break times of employees in the\n",
      "engineering department 44\n",
      "\n",
      "Let's now conduct a t-test at a 95% confidence level in order to find a difference (or not 24\n",
      " 1\n",
      "\n",
      "Technically speaking, this test will tell us if the sample comes from the same distribution as\n",
      "the population 22\n",
      "\n",
      "Assumptions of the one-sample t-test\n",
      "Before diving into the five steps, we must first acknowledge that t-tests must satisfy the\n",
      "following two conditions to work properly:\n",
      "The population distribution should be normal, or the sample should be large  (n ≥\n",
      "30)\n",
      "In order to make the assumption that the sample is independently, randomly\n",
      "sampled, it is sufficient to enforce that the population size should be at least 10\n",
      "times larger than the sample size (10n < N)\n",
      "Note that our test requires that either the underlying data be normal (which we know is not\n",
      "true for us), or that the sample size is at least 30 points large 136\n",
      "For the t-test, this condition is\n",
      "sufficient to assume normality 15\n",
      "This test also requires independence, which is satisfied by\n",
      "taking a sufficiently small  sample 17\n",
      "Sounds weird, right 4\n",
      "The basic idea is that our sample\n",
      "must be large enough to assume normality (through conclusions similar to the central limit\n",
      "theorem) but small enough as to be independent  of the population 38\n",
      "\n",
      "Now, let's follow our five steps:\n",
      "Specify the hypotheses 13\n",
      "1 2\n",
      "\n",
      "We will let H0 = the engineering department take breaks of the same duration as\n",
      "the company as a whole\n",
      "If we let this be the company average, we may write the following:\n",
      "H0:\n",
      "Note how this is our null, or default , hypothesis 53\n",
      "It is what we would assume,\n",
      "given no data 10\n",
      "What we would like to show is the alternative hypothesis 10\n",
      "\n",
      "\n",
      "Now that we actually have some options for our alternative, we could either say\n",
      "that the engineering mean (let's call it that) is lower than the company average,\n",
      "higher than the company average, or just flat out different (higher or lower) than\n",
      "the company average:\n",
      "If we wish to answer the question, \"i s the sample mean different\n",
      "from the company average 77\n",
      ",\" then this is called a two-tailed test\n",
      "and our alternative hypothesis would be as follows:\n",
      "Ha:\n",
      "If we want to find out whether  the sample mean is lower than the\n",
      "company average  or if the sample mean is higher than the\n",
      "company average , then we are dealing with a one-tailed test  and\n",
      "our alternative hypothesis would be one of the following\n",
      "hypotheses:\n",
      "Ha:(engineering takes longer breaks)\n",
      "Ha:(engineering takes shorter breaks)\n",
      "The difference between one and two tails is the difference of dividing a number\n",
      "later on by two or not 116\n",
      "The process remains completely unchanged for both 7\n",
      "For\n",
      "this example, let's choose the two-tailed test 13\n",
      "So, we are testing for whether or not\n",
      "this sample of the engineering department's average break times is different from\n",
      "the company average 27\n",
      "\n",
      "Our test will end in one of two possible conclusions: we will either reject\n",
      "the null hypothesis, which means that the engineering department's break\n",
      "times are different from the company average, or we will fail to reject the\n",
      "null hypothesis, which means that there wasn't enough evidence in the\n",
      "sample to support rejecting the null 66\n",
      "\n",
      "Determine the sample size for the test sample 10\n",
      "2 2\n",
      "\n",
      "As mentioned earlier, most tests (including this one) make the assumption that\n",
      "either the underlying data is normal or that our sample is in the right range:\n",
      "The sample is at least 30 points (it is 400)\n",
      "The sample is less than 10% of the population (which would be 900\n",
      "people)\n",
      "\n",
      "Choose a significance level (usually called alpha or α) 78\n",
      "We will choose a 95% 3 9\n",
      "\n",
      "significance level, which means that our alpha would actually be 1 - 16\n",
      "95 = 3\n",
      "05\n",
      "Collect the data 6\n",
      "This is generated through the two Poisson distributions 9\n",
      "4 2\n",
      "\n",
      "Decide whether to reject or fail to reject the null hypothesis 13\n",
      "As mentioned before,5 5\n",
      "\n",
      "this step varies based on the test used 9\n",
      "For a one-sample  t-test, we must calculate\n",
      "two numbers: the test statistic and our p-value 22\n",
      "Luckily, we can do this in one line\n",
      "in Python 12\n",
      "\n",
      "A test statistic is a value that is derived  from sample data during a type of hypothesis test 20\n",
      "\n",
      "They are used to determine whether or not to reject the null hypothesis 14\n",
      "\n",
      "The test statistic is used to compare the observed data with what is expected under the null\n",
      "hypothesis 22\n",
      "The test statistic is used in conjunction with the p-value 11\n",
      "\n",
      "The p-value is the probability that the observed data occurred this way by chance 16\n",
      "\n",
      "When the data is showing very strong evidence against the null hypothesis, the test statistic\n",
      "becomes large (either positive or negative) and the p-value usually becomes very small,\n",
      "which means that our test is showing powerful results and what is happening is, probably,\n",
      "not happening by chance 57\n",
      "\n",
      "In the case of a t-test, a t value  is our test statistic, as shown:\n",
      "t_statistic, p_value = stats 29\n",
      "ttest_1samp(a= engineering_breaks,\n",
      " popmean= breaks 15\n",
      "mean())\n",
      "We input the engineering_breaks  variable (which holds 400 break times) and the\n",
      "population mean ( popmean ), and we obtain the following numbers:\n",
      "t_statistic == -5 40\n",
      "742\n",
      "p_value == 6\n",
      "00000018\n",
      "The test result shows that the t value  is -5 17\n",
      "742 2\n",
      "This is a standardized metric that reveals\n",
      "the deviation of the sample mean from the null hypothesis 18\n",
      "The p-value is what gives us our\n",
      "final answer 11\n",
      "Our p-value is telling us how often our result would appear by chance 14\n",
      "So, for\n",
      "example, if our p-value was 11\n",
      "06, then that would mean we should expect to observe this\n",
      "data by chance about 6% of the time 24\n",
      "This means that about 6% of samples would yield\n",
      "results like this 15\n",
      "\n",
      "\n",
      "We are interested in how our p-value compares to our significance level:\n",
      "If the p-value is less than  the significance level, then we can reject  the null\n",
      "hypothesis\n",
      "If the p-value is greater than  the significance level, then we failed to reject  the null\n",
      "hypothesis\n",
      "Our p-value is way lower than 71\n",
      "05 (our chosen significance level), which means that we may\n",
      "reject our null hypothesis in favor of the alternative 23\n",
      "This means that the engineering\n",
      "department seems to take different break lengths than the company as a whole 19\n",
      "\n",
      "The use of p-values is controversial 8\n",
      "Many journals have actually banned\n",
      "the use of p-values in tests for significance 15\n",
      "This is because of the nature of\n",
      "the value 10\n",
      "Suppose our p-value came out to 7\n",
      "04 2\n",
      "It means that 4% of the\n",
      "time, our data just randomly happened to appear this way and is not\n",
      "significant in any way 28\n",
      "4% is not that small of a percentage 10\n",
      "For this\n",
      "reason, many people are switching to different statistical tests 13\n",
      "However,\n",
      "that does not mean that p-values are useless 11\n",
      "It merely means that we\n",
      "must be careful and aware of what the number is telling us 18\n",
      "\n",
      "There are many other types of t-tests, including one-tailed tests (mentioned before) and\n",
      "paired tests as well as two sample t-tests (both not mentioned yet) 36\n",
      "These procedures can be\n",
      "readily found in statistics literature; however, we should look at something very\n",
      "important —what happens when we get it wrong 30\n",
      "\n",
      "Type I and type II errors\n",
      "We've mentioned both the type I and type II errors in Chapter 5 , Impossible or Improbable –\n",
      "A Gentle Introduction to Probability , about probability in the examples of a binary classifier,\n",
      "but they also apply to hypothesis tests 54\n",
      "\n",
      "A type I error occurs  if we reject the null hypothesis  when it is actually true 19\n",
      "This is also\n",
      "known as a false positive 9\n",
      "The type I error rate is equal to the significance level α, which\n",
      "means that if we set a higher confidence level, for example, a significance level of 99%, our\n",
      "α is 39\n",
      "01, and therefore our false positive rate is 1% 13\n",
      "\n",
      "A type II error occurs if we fail to reject  the null hypothesis when it is actually false 20\n",
      "This is\n",
      "also known as a false negative 9\n",
      "The higher we set our confidence level, the more likely we\n",
      "are to actually see a type II error 21\n",
      "\n",
      "\n",
      "Hypothesis testing for categorical variables\n",
      "T-tests (among other tests) are hypothesis tests  that work to compare and contrast\n",
      "quantitative variables and underlying population distributions 34\n",
      "In this section, we will\n",
      "explore two new tests, both of which serve to explore qualitative data 21\n",
      "They are both a form\n",
      "of test called chi-square tests 12\n",
      "These two tests will perform the following two tasks for us:\n",
      "Determine whether a sample of categorical variables is taken from a specific\n",
      "population (similar to the t-test)\n",
      "Determine whether two variables affect each other and are associated with each\n",
      "other\n",
      "Chi-square goodness of fit test\n",
      "The one-sample t-test was used to check whether a sample means differed from the\n",
      "population mean 76\n",
      "The chi-square goodness of fit test is very similar to the one sample t-test\n",
      "in that it tests  whether the distribution of the sample data matches an expected distribution,\n",
      "while the big difference is that it is testing for categorical variables 46\n",
      "\n",
      "For example, a chi-square goodness  of fit test would be used to see whether the race\n",
      "demographics of your company match that of the entire city of the U 35\n",
      "S 1\n",
      "population 1\n",
      "It can\n",
      "also be used to see if users of your website show similar characteristics to average internet\n",
      "users 21\n",
      "\n",
      "As we are working with categorical data, we have to be careful because categories such as\n",
      "\"male,\" \"female,\" or \"other\" don't have any mathematical meaning 35\n",
      "Therefore, we must\n",
      "consider counts of the variables rather than the actual variables themselves 16\n",
      "\n",
      "In general, we use the chi-square goodness of fit test in the following cases:\n",
      "We want to analyze one categorical variable from one population\n",
      "We want to determine whether a variable fits a specified or expected distribution\n",
      "In a chi-square test, we compare what is observed to what we expect 58\n",
      "\n",
      "Assumptions of the chi-square goodness of ﬁt test\n",
      "There are two usual  assumptions of this test, as follows:\n",
      "All the expected counts are at least five\n",
      "Individual observations are independent and the population should be at least 10\n",
      "times as large as the sample ( 10n < N)\n",
      "\n",
      "The second assumption should look familiar to the t-test; however, the first assumption\n",
      "should look foreign 84\n",
      "Expected counts are something we haven't talked about yet but are\n",
      "about to 15\n",
      "\n",
      "When formulating our null and alternative hypotheses for this test, we consider a default\n",
      "distribution of categorical variables 22\n",
      "For example, if we have a die and we are testing\n",
      "whether or not the outcomes are coming from a fair die, our hypothesis might look as\n",
      "follows:\n",
      "H0: The specified distribution of the categorical variable is correct 46\n",
      "\n",
      "p1 = 1/6, p2 = 1/6, p3 = 1/6, p4 = 1/6, p5 = 1/6, p6 = 1/6\n",
      "Our alternative hypothesis is quite simple, as shown:\n",
      "Ha : The specified distribution of the categorical variable is not correct 71\n",
      "At least one of the pi\n",
      "values is not correct 11\n",
      "\n",
      "In the t-test, we used our test statistic (the t-value) to find our p-value 21\n",
      "In a chi-square test,\n",
      "our test statistic is, well, a chi-square:\n",
      "Test Statistic: χ2 = ",
      " over k categories\n",
      "Degrees of Freedom = k − 1\n",
      "A critical value is when we use χ2 as well as our degrees of freedom and our significance\n",
      "level, and then reject the null hypothesis if the p-value is below our significance level (the\n",
      "same as before) 85\n",
      "\n",
      "Let's look at an example to understand this further 11\n",
      "\n",
      "Example of a chi-square test for goodness of ﬁt\n",
      "The CDC categorizes adult BMIs into four classes: Under/Normal , Over , Obesity , and\n",
      "Extreme Obesity 37\n",
      "A 2009 survey showed the distribution for adults in the US to be 31 17\n",
      "2%,\n",
      "33 5\n",
      "1%, 29 5\n",
      "4%, and 6 6\n",
      "3% respectively 4\n",
      "A total of 500 adults were randomly sampled and their\n",
      "BMI categories were recorded 16\n",
      " ",
      " Is there evidence to suggest that BMI trends have changed\n",
      "since 2009 17\n",
      "Let's test at the 0 7\n",
      "05 significance level:\n",
      "\n",
      "\n",
      "First, let's calculate our expected values 13\n",
      "In a sample of 500, we expect  156 to be\n",
      "Under/Normal  (that's 31 24\n",
      "2% of 500), and we fill in the remaining boxes in the same way:\n",
      "First, check the conditions that are as follows:\n",
      "All of the expected counts are greater than five\n",
      "Each observation is independent and our population is very large ( much more\n",
      "than 10 times of 500 people )\n",
      "Next, carry out a goodness of fit test 71\n",
      "We will set our null and alternative hypotheses:\n",
      "H0: The 2009 BMI distribution is still correct 21\n",
      "\n",
      "Ha: The 2009 BMI distribution is no longer correct (at least one of the proportions\n",
      "is different now) 25\n",
      "We can calculate our test statistic by hand:\n",
      "Alternatively, we can use our handy-dandy Python skills, as shown:\n",
      "observed = [102, 178, 186, 34]\n",
      "expected = [156, 165 46\n",
      "5, 147, 31 8\n",
      "5]\n",
      "chi_squared, p_value = stats 10\n",
      "chisquare(f_obs= observed, f_exp= expected)\n",
      "chi_squared, p_value\n",
      "#(30 22\n",
      "1817679275599, 1 9\n",
      "26374310311106e-06)\n",
      "\n",
      "Our p-value is lower than 16\n",
      "05; therefore, we may reject the null hypothesis in favor of the\n",
      "fact that the BMI trends today are different from what they were in 2009 32\n",
      "\n",
      "Chi-square test for association/independence\n",
      "Independence as a concept in probability is when knowing the value of one variable tells\n",
      "you nothing about  the value of another 34\n",
      "For example, we might expect that the country and\n",
      "the month you were born in are independent 19\n",
      "However, knowing which type of phone you\n",
      "use might indicate your creativity levels 15\n",
      "Those variables might not be independent 6\n",
      "\n",
      "The chi-square test for association/independence helps us ascertain whether two categorical\n",
      "variables are independent of one another 22\n",
      "The test for independence is commonly used to\n",
      "determine whether variables like education levels or tax brackets vary based on\n",
      "demographic factors, such as gender, race, and religion 35\n",
      "Let's look back at an example\n",
      "posed in the preceding chapter, the A/B split test 19\n",
      "\n",
      "Recall that we ran a test and exposed half of our users to a certain landing page ( Website\n",
      "A), exposed the other half to a different landing page ( Website B ), and then measured the\n",
      "sign-up rates for both sites 48\n",
      "We obtained the following results:\n",
      "Did not sign up Signed up\n",
      "Website A 134 54\n",
      "Website B 110 48\n",
      "Results of our A/B test\n",
      "We calculated website conversions but what we really want to know is whether there is a\n",
      "difference between the two variables: which website was the user exposed to 64\n",
      "and did the user\n",
      "sign up 7\n",
      "For this, we will use our chi-square test 10\n",
      "\n",
      "Assumptions of the chi-square independence test\n",
      "There are the following two assumptions of this test:\n",
      "All expected counts are at least five\n",
      "Individual observations are independent and the population should be at least 10\n",
      "times as large as the sample ( 10n < N)\n",
      "Note that they are exactly the same as the last chi-square test 69\n",
      "\n",
      "\n",
      "Let's set up our hypotheses:\n",
      "H0: There is no association between two categorical variables in the population of\n",
      "interest\n",
      "H0: Two categorical variables are independent in the population of interest\n",
      "Ha: There is an association between two categorical variables in the population of\n",
      "interest\n",
      "Ha: Two categorical variables are not independent in the population of interest\n",
      "You might notice that we are missing something important here 81\n",
      "Where are the expected\n",
      "counts 6\n",
      "Earlier, we had a prior distribution to compare our observed results to but now we\n",
      "do not 19\n",
      "For this reason, we will have to create some 10\n",
      "We can use the following formula to\n",
      "calculate the expected values for each value 15\n",
      "In each cell of the table, we can use the\n",
      "following:\n",
      "Expected Count = to calculate our chi-square test statistic and our degrees of freedom\n",
      "Here, r is the number of rows and c is the number of columns 45\n",
      "Of course, as before, when\n",
      "we calculate our p-value, we will reject the null if that p-value is less than the significance\n",
      "level 30\n",
      "Let's use some built-in Python methods, as shown, in order to quickly get our results:\n",
      "observed = np 24\n",
      "array([[134, 54],[110, 48]])\n",
      "# built a 2x2 matrix as seen in the table above\n",
      "chi_squared, p_value, degrees_of_freedom, matrix =\n",
      "stats 41\n",
      "chi2_contingency(observed= observed)\n",
      "chi_squared, p_value\n",
      "# (0 19\n",
      "04762692369491045, 0 10\n",
      "82724528704422262)\n",
      "\n",
      "We can see that our p-value is quite large; therefore, we fail to reject our null hypothesis\n",
      "and we cannot say for sure that seeing a particular website has any effect on a whether or\n",
      "not a user signs up 53\n",
      "There is no association between these variables 7\n",
      "\n",
      "Summary\n",
      "In this chapter, we looked at different statistical tests, including chi-square and t-tests as\n",
      "well as point estimates and confidence intervals, in order to ascertain population\n",
      "parameters based on sample data 41\n",
      "We were able to find that even with small samples of\n",
      "data, we can make powerful assumptions about the underlying population as a whole 26\n",
      "\n",
      "Using concepts reviewed in this chapter, data scientists will be able to make inferences\n",
      "about entire datasets based on certain samples of data 27\n",
      "In addition, they will be able to use\n",
      "hypothesis tests to gain a better understanding of full datasets, given samples of data 27\n",
      "\n",
      "Statistics is a very wide and expansive subject that cannot truly be covered in a single\n",
      "chapter; however, our understanding of the subject will allow us to carry on and talk more\n",
      "about how we can use statistics and probability in order to communicate our ideas through\n",
      "data science in the next chapter 59\n",
      "\n",
      "In the next chapter, we will discuss different ways of communicating results from data\n",
      "analysis including various presentation styles as well as visualization techniques 27\n",
      "\n",
      "\n",
      "\n",
      "Communicating Data\n",
      "This chapter deals with the different ways of communicating results from our analysis 18\n",
      "\n",
      "Here, we will look at different presentation styles as well as visualization techniques 15\n",
      "The\n",
      "point of this chapter is to take our results and be able to explain them in a coherent,\n",
      "intelligible way so that anyone, whether they are data savvy or not, may understand and\n",
      "use our results 44\n",
      "\n",
      "Much of what we will discuss will be how to create effective graphs through labels, keys,\n",
      "colors, and more 23\n",
      "We will also look at more advanced visualization techniques, such as\n",
      "parallel coordinate plots 16\n",
      "\n",
      "In this chapter, we will look into the following topics:\n",
      "Identifying effective and ineffective visualizations\n",
      "Recognizing when charts are attempting to \"trick\" the audience\n",
      "Being able to identify causation versus correlation\n",
      "Constructing appealing visuals that offer valuable insight\n",
      "\n",
      "Why does communication matter 57\n",
      "\n",
      "Being able to conduct experiments and manipulate data in a coding language is not enough\n",
      "to conduct practical and applied data science 24\n",
      "This is because data science is, generally, only\n",
      "as good as how it is used in practice 20\n",
      "For instance, a medical data scientist might be able to\n",
      "predict the chance of a tourist contracting malaria in developing countries with >98%\n",
      "accuracy; however, if these results are published in a poorly marketed journal and online\n",
      "mentions of the study are minimal, their groundbreaking results that could potentially\n",
      "prevent deaths would never truly see the light of day 68\n",
      "\n",
      "For this reason, communication of results is arguably as important as the results\n",
      "themselves 19\n",
      "A famous example of poor management of the distribution of results is the case\n",
      "of Gregor Mendel 20\n",
      "Mendel is widely recognized as one of the founders of modern genetics 13\n",
      "\n",
      "However, his results (including data and charts) were not well adopted until after his\n",
      "death 20\n",
      "Mendel even sent them to Charles Darwin, who largely ignored Mendel's papers,\n",
      "which were written in unknown Moravian journals 25\n",
      "\n",
      "Generally, there are two ways of presenting results: verbal and visual 14\n",
      "Of course, both the\n",
      "verbal and visual forms of communication can be broken down into dozens of\n",
      "subcategories, including slide decks, charts, journal papers, and even university lectures 37\n",
      "\n",
      "However, we can find common elements of data presentation that can make anyone in the\n",
      "field more aware and effective in their communication skills 27\n",
      "\n",
      "Let's dive right into effective (and ineffective) forms of communication, starting with\n",
      "visuals 20\n",
      "\n",
      "Identifying effective and ineffective\n",
      "visualizations\n",
      "The main goal of data visualization is to have  the reader quickly digest the data, including\n",
      "possible trends, relationships, and more 36\n",
      "Ideally, a reader will not have to spend more than\n",
      "5-6 seconds digesting a single visualization 21\n",
      "For this reason, we must make  visuals very\n",
      "seriously and ensure that we are making a visual as effective as possible 25\n",
      "Let's look at five\n",
      "basic types of graphs: scatter plots, line graphs, bar charts, histograms, and box plots 25\n",
      "\n",
      "\n",
      "Scatter plots\n",
      "A scatter plot is probably one of the simplest graphs to create 17\n",
      "It is made by creating two\n",
      "quantitative  axes and using data points to represent observations 18\n",
      "The main goal of a scatter\n",
      "plot is to highlight relationships between two variables and, if possible, reveal a correlation 23\n",
      "\n",
      "For example, we can look at two variables: the average hours of TV watched in a day and a\n",
      "0-100 scale of work performance (0 being very poor performance and 100 being excellent\n",
      "performance) 44\n",
      "The goal here is to find a relationship (if it exists) between watching TV and\n",
      "average work performance 21\n",
      "\n",
      "The following code simulates a survey of a few people, in which they revealed the amount\n",
      "of television they watched, on average, in a day against a company-standard work\n",
      "performance metric 39\n",
      "This line of code is creating 14 sample survey results of people\n",
      "answering the question of how many hours of TV they watch in a day:\n",
      "import pandas as pd\n",
      "hours_tv_watched = [0, 0, 0, 1, 1 55\n",
      "3, 1 5\n",
      "4, 2, 2 8\n",
      "1, 2 5\n",
      "6, 3 5\n",
      "2, 4 5\n",
      "1, 4 5\n",
      "4, 4 5\n",
      "4,\n",
      "5]\n",
      "This next line of code is creating 14 new sample survey results of the same people being\n",
      "rated on their work performance on a scale from 0 to 100 38\n",
      "For example, the first person\n",
      "watched 0 hours of TV a day and was rated 87/100 on their work, while the last person\n",
      "watched, on an average, 5 hours of TV a day and was rated 72/100:\n",
      "work_performance = [87, 89, 92, 90, 82, 80, 77, 80, 76, 85, 80, 75, 73, 72]\n",
      "Here, we are creating a DataFrame in order to simplify our exploratory data analysis and\n",
      "make it easier to make a scatter plot:\n",
      "df = pd 129\n",
      "DataFrame({'hours_tv_watched':hours_tv_watched,\n",
      "'work_performance':work_performance})\n",
      "Now, we are actually making our scatter plot:\n",
      "df 30\n",
      "plot(x='hours_tv_watched', y='work_performance', kind='scatter')\n",
      "\n",
      "In the following plot, we can see that our axes represent the number of hours of TV\n",
      "watched in a day and the person's work performance metric:\n",
      "The scatter plot: hours of TV watched versus work performance\n",
      "Each point on a scatter plot represents a single observation (in this case a person) and its\n",
      "location is a result of where the observation stands on each variable 93\n",
      "This scatter plot does\n",
      "seem to show a relationship, which implies that as we watch more TV in the day, it seems\n",
      "to affect our work performance 32\n",
      "\n",
      "Of course, as we are now experts in statistics from the last two chapters, so we know that\n",
      "this might not be causational 28\n",
      "A scatter plot may only work to reveal a correlation or an\n",
      "association, but not a causation 20\n",
      "Advanced statistical tests, such as the ones we saw in\n",
      "Chapter 8 , Advanced Statistics , might work to reveal causation 25\n",
      "Later on in this chapter, we\n",
      "will see the damaging effects that trusting correlation might have 18\n",
      "\n",
      "Line graphs\n",
      "Line graphs are, perhaps, one of the most  widely used graphs in data communication 21\n",
      "A\n",
      "line graph simply uses lines to connect data points and usually represents time on the x-\n",
      "axis 20\n",
      "Line graphs are a popular way to show changes in variables over time 13\n",
      "The line graph,\n",
      "like the scatter plot, is used to plot quantitative  variables 16\n",
      "\n",
      "\n",
      "As a great example, many of us wonder about the possible links between what we see on\n",
      "TV and our behavior in the world 27\n",
      "A friend of mine once took this thought to an extreme:\n",
      "he wondered if he could find a relationship between the TV show The X-Files  and the\n",
      "amount of UFO sightings in the U 39\n",
      "S 1\n",
      "He found the number of sightings of UFOs per year\n",
      "and plotted them over time 17\n",
      "He then added a quick graphic to ensure that readers would be\n",
      "able to identify the point in time when The X-Files  were released:\n",
      "Total reported UFO sightings since 1963 (Source: http://www 43\n",
      "questionable-economics 3\n",
      "com/what-do-we-know-about-aliens/)\n",
      "It appears to be clear that right after 1993, the year of The X-Files'  premiere, the number of\n",
      "UFO sightings started to climb drastically 45\n",
      "\n",
      "This graphic, albeit light-hearted, is an excellent example of a simple line graph 17\n",
      "We are\n",
      "told what each axis measures, we can quickly see a general trend in the data, and we can\n",
      "identify with the author's intent, which is to show a relationship between the number of\n",
      "UFO sightings and The X-Files'  premiere 54\n",
      "\n",
      "\n",
      "On the other hand, the following is a less impressive line chart:\n",
      "The line graph: gas price changes\n",
      "This line graph attempts to highlight the change in the price of gas by plotting three points\n",
      "in time 43\n",
      "At first glance, it is not much different than the previous graph; we have time on\n",
      "the bottom x-axis and a quantitative value on the vertical y-axis 32\n",
      "The (not so) subtle\n",
      "difference here is that the three points are equally spaced out on the x-axis; however, if we\n",
      "read their actual time indications, they are not equally spaced out in time 42\n",
      "A year separates\n",
      "the first two points whereas a mere 7 days separate the last two points 19\n",
      "\n",
      "Bar charts\n",
      "We generally turn to bar charts when  trying to compare variables across different groups 19\n",
      "\n",
      "For example, we can plot the number of countries per continent using a bar chart 17\n",
      "Note how\n",
      "the x-axis does not represent a quantitative variable; in fact, when using a bar chart, the x-\n",
      "axis is generally a categorical variable, while the y-axis is quantitative 38\n",
      "\n",
      "\n",
      "Note that, for this code, I am using the World Health Organization's report on alcohol\n",
      "consumption around the world by country:\n",
      "from matplotlib import pyplot as plt\n",
      "drinks =\n",
      "pd 39\n",
      "read_csv('https://raw 6\n",
      "githubusercontent 2\n",
      "com/sinanuozdemir/principles_of_\n",
      "data_science/master/data/chapter_2/drinks 23\n",
      "csv')\n",
      "drinks 4\n",
      "continent 1\n",
      "value_counts() 3\n",
      "plot(kind='bar', title='Countries per\n",
      "Continent')\n",
      "plt 14\n",
      "xlabel('Continent')\n",
      "plt 6\n",
      "ylabel('Count')\n",
      "The following graph shows us a count of the number of countries in each continent 19\n",
      "We can\n",
      "see the continent code at the bottom of the bars and the bar height represents the number of\n",
      "countries we have in each continent 28\n",
      "For example, we see that Africa has the most countries\n",
      "represented in our survey, while South America has the fewest:\n",
      "Bar chart: countries in continent\n",
      "In addition to the count of countries, we can also plot the average beer servings per\n",
      "continent using a bar chart, as shown:\n",
      "drinks 61\n",
      "groupby('continent') 5\n",
      "beer_servings 3\n",
      "mean() 2\n",
      "plot(kind='bar')\n",
      "\n",
      "The preceding code gives us this chart:\n",
      "Bar chart: average beer served per country\n",
      "Note how a scatter plot or a line graph would not be able to support this data because they\n",
      "can only handle quantitative variables; bar graphs have the ability to demonstrate\n",
      "categorical values 59\n",
      "\n",
      "We can also use bar charts to graph variables that change over time, like a line graph 19\n",
      "\n",
      "Histograms\n",
      "Histograms show the frequency distribution  of a single quantitative variable by splitting\n",
      "the data, by range, into equidistant bins and plotting the raw count of observations in each\n",
      "bin 41\n",
      "A histogram is effectively a bar chart where the x-axis is a bin (subrange) of values and\n",
      "the y-axis is a count 28\n",
      "As an example, I will import a store's daily number of unique\n",
      "customers, as shown:\n",
      "rossmann_sales = pd 25\n",
      "read_csv('data/ rossmann 8\n",
      "csv ')\n",
      "rossmann_sales 5\n",
      "head()\n",
      "\n",
      "We get the following table:\n",
      "Note how we have multiple store data (in the first Store  column) 23\n",
      "Let's subset this data for\n",
      "only the first store, as shown:\n",
      "first_rossmann_sales = rossmann_sales[rossmann_sales['Store']==1]\n",
      "Now, let's plot a histogram of the first store's customer count:\n",
      "first_rossmann_sales['Customers'] 57\n",
      "hist(bins=20)\n",
      "plt 7\n",
      "xlabel('Customer Bins')\n",
      "plt 7\n",
      "ylabel('Count')\n",
      "This is what we get:\n",
      "Histogram: customer counts\n",
      "\n",
      "The x-axis is now categorical in that each category is a selected range of values; for example,\n",
      "600-620 customers would potentially be a category 44\n",
      "The y-axis, like a bar chart, is plotting\n",
      "the number of observations in each category 19\n",
      "In this graph, for example, one might take\n",
      "away the fact that most of the time, the number of customers on any given day will fall\n",
      "between 500 and 700 37\n",
      "\n",
      "Altogether, histograms are used to visualize the distribution of values that a quantitative\n",
      "variable can take on 22\n",
      "\n",
      "In a histogram, we do not put spaces between bars 12\n",
      "\n",
      "Box plots\n",
      "Box plots are also used to show  a distribution of values 16\n",
      "They are created by plotting the\n",
      "five-number summary, as follows:\n",
      "The minimum value\n",
      "The first quartile (the number that separates the 25% lowest values from the rest)\n",
      "The median\n",
      "The third quartile (the number that separates the 25% highest values from the\n",
      "rest)\n",
      "The maximum value\n",
      "In pandas, when we create box plots, the red line denotes the median, the top of the box (or\n",
      "the right if it is horizontal) is the third quartile, and the bottom (left) part of the box is the\n",
      "first quartile 117\n",
      "\n",
      "The following is a series of box plots showing the distribution of beer consumption\n",
      "according to continents:\n",
      "drinks 22\n",
      "boxplot(column='beer_servings', by='continent')\n",
      "\n",
      "We get this graph:\n",
      "Box plot: beer consumption by continent\n",
      "Now, we can clearly see the distribution of beer consumption across the seven continents\n",
      "and how they differ 45\n",
      "Africa and Asia have a much lower median of beer consumption than\n",
      "Europe or North America 17\n",
      "\n",
      "Box plots also have the added bonus of being able to show outliers much better than a\n",
      "histogram 21\n",
      "This is because the minimum and maximum are parts of the box plot 13\n",
      "\n",
      "Getting back to the customer data, let's look at the same store customer numbers, but using\n",
      "a box plot:\n",
      "first_rossmann_sales 30\n",
      "boxplot(column='Customers', vert=False)\n",
      "\n",
      "This is the graph we get:\n",
      "Box plot: customer sales\n",
      "This is the exact same data as plotted earlier  in the histogram; however, now it is shown as\n",
      "a box plot 47\n",
      "For the purpose of comparison, I will show you both graphs, one after the other:\n",
      "Histogram: customer counts\n",
      "\n",
      "\n",
      "Box plot: customer sales\n",
      "Note how the x-axis for each graph is the same, ranging from 0 to 1,200 50\n",
      "The box plot is\n",
      "much quicker at giving us a center for the data, the red line is the median, while the\n",
      "histogram works much better in showing us how spread out the data is and where people's\n",
      "biggest bins are 49\n",
      "For example, the histogram reveals that there is a very large bin of zero\n",
      "people 17\n",
      "This means that for a little over 150 days of data, there were zero customers 17\n",
      "\n",
      "Note that we can get the exact numbers to construct a box plot using the describe  feature\n",
      "in pandas, as shown:\n",
      "first_rossmann_sales['Customers'] 34\n",
      "describe()\n",
      "min         0 6\n",
      "000000\n",
      "25%       463 9\n",
      "000000\n",
      "50%       529 9\n",
      "000000\n",
      "75%       598 9\n",
      "750000\n",
      "max      1130 9\n",
      "000000\n",
      "\n",
      "When graphs and statistics lie\n",
      "I should be clear, statistics don't lie, people lie 22\n",
      "One of the easiest ways to trick your\n",
      "audience is to confuse  correlation with  causation 20\n",
      "\n",
      "Correlation versus causation\n",
      "I don't think I would be allowed  to publish this book without taking a deeper dive into the\n",
      "differences between correlation and causation 35\n",
      "For this example, I will continue to use my\n",
      "data for TV consumption and work performance 18\n",
      "\n",
      "Correlation  is a quantitative metric  between -1 and 1 that measures how two variables move\n",
      "with each other 25\n",
      "If two variables have a correlation close to -1, it means that as one variable\n",
      "increases, the other decreases, and if two variables have a correlation close to +1, it means\n",
      "that those variables move together in the same direction; as one increases, so does the other,\n",
      "and the same when decreasing 64\n",
      "\n",
      "Causation  is the idea that one variable  affects another 14\n",
      "For example, we can look at two\n",
      "variables: the average hours of TV watched in a day and a 0-100 scale of work performance\n",
      "(0 being very poor performance and 100 being excellent performance) 44\n",
      "One might expect\n",
      "that these two factors are negatively correlated, which means that as the number of hours of\n",
      "TV watched increases in a 24-hour day, your overall work performance goes down 38\n",
      "Recall\n",
      "the code from earlier, which is as follows 11\n",
      "Here, I am looking at the same sample of 14\n",
      "people as before and their answers to the question  how many hours of TV do you watch on\n",
      "average per night :\n",
      "import pandas as pd\n",
      "hours_tv_watched = [0, 0, 0, 1, 1 61\n",
      "3, 1 5\n",
      "4, 2, 2 8\n",
      "1, 2 5\n",
      "6, 3 5\n",
      "2, 4 5\n",
      "1, 4 5\n",
      "4, 4 5\n",
      "4,\n",
      "5]\n",
      "These are the same 14 people as mentioned earlier, in the same order, but now, instead of\n",
      "the number of hours of TV they watched, we have their work performance as graded by the\n",
      "company or a third-party system:\n",
      "work_performance = [87, 89, 92, 90, 82, 80, 77, 80, 76, 85, 80, 75, 73, 72]\n",
      "Then, we produce a DataFrame:\n",
      "df = pd 108\n",
      "DataFrame({'hours_tv_watched':hours_tv_watched,\n",
      "'work_performance':work_performance})\n",
      "\n",
      "Earlier, we looked at a scatter plot of these two variables and it seemed to clearly show a\n",
      "downward trend between the variables: as TV consumption went up, work performance\n",
      "seemed to go down 60\n",
      "However, a correlation coefficient, a number between -1 and 1, is a\n",
      "great way to identify relationships between variables and, at the same time, quantify them\n",
      "and categorize their strength 40\n",
      "\n",
      "Now, we can introduce a new line of code that shows us the correlation between these two\n",
      "variables:\n",
      "df 23\n",
      "corr() # -0 5\n",
      "824\n",
      "Recall that a correlation, if close to -1, implies a strong negative correlation, while a\n",
      "correlation close to +1 implies a strong positive correlation 35\n",
      "\n",
      "This number helps support the hypothesis because a correlation coefficient close to -1\n",
      "implies not only a negative correlation but a strong one at that 29\n",
      "Again, we can see this via a\n",
      "scatter plot between the two variables 15\n",
      "So, both our visual and our numbers are aligned\n",
      "with each other 14\n",
      "This is an important concept that should be true when communicating\n",
      "results 13\n",
      "If your visuals and your numbers are off, people are less likely to take your analysis\n",
      "seriously:\n",
      "Correlation: hours of TV watched and work performance\n",
      "\n",
      "I cannot stress enough that correlation and causation are different  from each other 47\n",
      "\n",
      "Correlation simply quantifies the degree to which variables change together, whereas\n",
      "causation is the idea that one variable actually determines the value of another 31\n",
      "If you wish\n",
      "to share the results of the findings of your correlational work, you might be met with\n",
      "challengers in the audience asking for more work to be done 36\n",
      "What is more terrifying is that\n",
      "no one might know that the analysis is incomplete and you may make actionable decisions\n",
      "based on simple correlational work 29\n",
      "\n",
      "It is very often the case that two variables might be correlated with each other but they do\n",
      "not have any causation between them 27\n",
      "This can be for a variety of reasons, some of which\n",
      "are as follows:\n",
      "There might be a confounding factor  between them 27\n",
      "This means that there is a third\n",
      "lurking variable that is not being factored and in, that acts as a bridge between\n",
      "the two variables 31\n",
      "For example, previously we showed that you might find that\n",
      "the amount of TV you watch is negatively correlated with work performance,\n",
      "that is, as the number of hours of TV you watch increases, your overall work\n",
      "performance may decrease 46\n",
      "That is a correlation 4\n",
      "It doesn't seem quite right to\n",
      "suggest that watching TV is the actual cause of a decrease in the quality of work\n",
      "performed 27\n",
      "It might seem more plausible to suggest that there is a third factor,\n",
      "perhaps hours of sleep every night, that might answer this question 26\n",
      "Perhaps,\n",
      "watching more TV decreases the amount of time you have for sleep, which in\n",
      "turn limits your work performance 24\n",
      "The number of hours of sleep per night is the\n",
      "confounding factor 14\n",
      "\n",
      "They might not have anything to do with each other 11\n",
      "It might simply be a\n",
      "coincidence 9\n",
      "There are many variables that are correlated but simply do not cause\n",
      "each other 15\n",
      "Consider the following example:\n",
      "\n",
      "\n",
      "Correlation analysis: cheese consumption and civil engineering doctorates\n",
      "It is much more likely that these two variables only happen to correlate (more strongly than\n",
      "our previous example, I may add) than cheese consumption determines the number of civil\n",
      "engineering doctorates in the world 58\n",
      "\n",
      "You have likely heard the statement correlation does not imply causation  and the last graph is\n",
      "exactly the reason why data scientists must believe that 30\n",
      "Just because there exists a\n",
      "mathematical correlation between variables does not mean they have causation between\n",
      "them 22\n",
      "There might be confounding factors between them or they just might not have\n",
      "anything to do with each other 21\n",
      "\n",
      "Let's see what happens when we ignore confounding variables and correlations become\n",
      "extremely misleading 19\n",
      "\n",
      "Simpson's paradox\n",
      "Simpson's paradox is a formal reason for why we need to take confounding variables\n",
      "seriously 28\n",
      "The paradox states that a correlation between two variables can be completely\n",
      "reversed when we take different factors into account 22\n",
      "This means that even if a graph might\n",
      "show a positive correlation, these variables can become anti-correlated  when another factor\n",
      "(most likely a confounding one) is taken into consideration 38\n",
      "This can be very troublesome to\n",
      "statisticians 10\n",
      "\n",
      "\n",
      "Suppose we wish to explore the relationship between two different splash pages (recall our\n",
      "previous A/B testing in Chapter 7 , Basic Statistics ) 30\n",
      "We will call these pages Page A  and\n",
      "Page B  once again 15\n",
      "We have two splash pages that we wish to compare and contrast, and\n",
      "our main metric for choosing will be in our conversion rates, just as earlier 30\n",
      "\n",
      "Suppose we run a preliminary test and find the following conversion results:\n",
      "Page A Page B\n",
      "75% (263/350) 83% (248/300)\n",
      "This means that Page  B has almost a 10% higher conversion rate than Page A 53\n",
      "So, right off\n",
      "the bat, it seems like Page B  is the better choice because it has a higher rate of conversion 26\n",
      "If\n",
      "we were going to communicate this data to our colleagues, it would seem that we are in the\n",
      "clear 23\n",
      "\n",
      "However, let's see what happens when we also take into account the coast that the user was\n",
      "closer to, as shown:\n",
      "Page A Page B\n",
      "West Coast 95% (76 / 80) 93% (231/250)\n",
      "East Coast 72% (193/270) 34% (17/50)\n",
      "Both 75% (263/350) 83% (248/300)\n",
      "Thus the paradox 90\n",
      "When we break the sample down by location, it seems that Page A  was\n",
      "better in both categories but was worse overall 25\n",
      "That's the beauty and, also, the horrifying\n",
      "nature of the paradox 15\n",
      "This happens because of the unbalanced classes between the four\n",
      "groups 13\n",
      "\n",
      "The Page A /East Coast  group and the Page B /West Coast  group are providing most of the\n",
      "people in the sample, therefore skewing the results to not be as expected 39\n",
      "The confounding\n",
      "variable here might be the fact that the pages were given at different hours of the day and\n",
      "the West coast people were more likely to see Page B , while the East coast people were\n",
      "more likely to see Page A 48\n",
      "\n",
      "There is a resolution to Simpson's paradox (and therefore an answer); however, the proof\n",
      "lies in a complex system of Bayesian networks and is a bit out of the scope of this book 39\n",
      "\n",
      "\n",
      "The main takeaway from Simpson's paradox is that we should not unduly give causational\n",
      "power to correlated variables 23\n",
      "There might be confounding variables that have to be\n",
      "examined 13\n",
      "Therefore, if you are able to reveal a correlation between two variables (such as\n",
      "website category and conversion rate or TV consumption and work performance), then you\n",
      "should absolutely try to isolate as many variables as possible that might be the reason for\n",
      "the correlation, or can at least help explain your case further 61\n",
      "\n",
      "If correlation doesn't imply causation, then what\n",
      "does 13\n",
      "\n",
      "As a data scientist, it is often quite  frustrating to work with correlations and not be able to\n",
      "draw conclusive causality 26\n",
      "The best way to confidently obtain causality is, usually, through\n",
      "randomized experiments, such as the ones we saw in Chapter 8 , Advanced Statistics 31\n",
      "One\n",
      "would have to split up the population groups into randomly sampled groups and run\n",
      "hypothesis tests to conclude, with a degree of certainty, that there is a true causation\n",
      "between variables 40\n",
      "\n",
      "Verbal communication\n",
      "Apart from visual demonstrations of data, verbal communication is just as important when\n",
      "presenting results 23\n",
      "If you are not merely uploading results or publishing, you are usually\n",
      "presenting data  to a room of data scientists, executives, or to a conference hall 32\n",
      "\n",
      "In any case, there are key areas to focus on when giving a verbal presentation, especially\n",
      "when the presentation is regarding findings of data 28\n",
      "\n",
      "There are generally two styles of oral presentation: one meant for more professional\n",
      "settings, including corporate  offices where the problem at hand is usually tied directly to\n",
      "company performance or some other key performance indicator  (KPI), and another meant\n",
      "more for a room of your peers where the key idea is to motivate the audience to care about\n",
      "your work 72\n",
      "\n",
      "It's about telling a story\n",
      "Whether it is a formal or casual presentation, people like to hear stories 22\n",
      "When you are\n",
      "presenting results, you are not just spitting out facts and metrics, you are attempting to \n",
      "frame  the minds of your audience to believe in and care about what you have to say 42\n",
      "\n",
      "\n",
      "When giving a presentation, always be aware of your audience and try to gauge their\n",
      "reactions/interest in what you are saying 27\n",
      "If they seem unengaged, try to relate the problem\n",
      "to them:\n",
      "\"Just think, when popular TV shows like Game of Thrones come back, your employees will all spend\n",
      "more time watching TV and therefore will have lower work performance 47\n",
      "\n",
      "Now you have their attention 6\n",
      "It's about relating to your audience; whether it's your boss or\n",
      "your mom's friend, you have to find a way to make it relevant 30\n",
      "\n",
      "On the more formal side of things\n",
      "When presenting data findings to a more  formal audience, I like to stick to the following six\n",
      "steps:\n",
      "Outline the state of the problem : In this step, we go over the current state of the 1 52\n",
      "\n",
      "problem, including what the problem is and how the problem came to the\n",
      "attention of the team of data scientists 23\n",
      "\n",
      "Define the nature of the data : Here, we go into more depth about who this 2 20\n",
      "\n",
      "problem affects, how the solution would change the situation, and previous work\n",
      "done on the problem, if any 23\n",
      "\n",
      "Divulge an initial hypothesis : Here, we state what we believed to be the 3 20\n",
      "\n",
      "solution before doing any work 6\n",
      "This might seem like a more novice approach to\n",
      "presentations; however, this can be a good time to outline not just your initial\n",
      "hypothesis but, perhaps, the hypothesis of the entire company 41\n",
      "For example, \"we\n",
      "took a poll and 61% of the company believes there is no correlation between\n",
      "hours of TV watched and work performance 30\n",
      "\n",
      "Describe the solution and, possibly, the tools that led to the solution : Get into 4 20\n",
      "\n",
      "how you solved the problem, any statistical tests used, and any assumptions that\n",
      "were made during the course of the problem 25\n",
      "\n",
      "Share the impact that your solution will have on the problem : Talk about 5 17\n",
      "\n",
      "whether your solution was different from the initial hypothesis 10\n",
      "What will this\n",
      "mean for the future 8\n",
      "How can we take action on this solution to improve\n",
      "ourselves and our company 16\n",
      "\n",
      "Future steps : Share what future steps can be taken with the problem, such as 6 19\n",
      "\n",
      "how to implement the solution and what further work this research sparked 13\n",
      "\n",
      "\n",
      "By following these steps, we can hit on all of the major areas of the data science method 20\n",
      "\n",
      "The first thing you want to hit on during a formal presentation is action 15\n",
      "You want your\n",
      "words and solutions to be actionable 10\n",
      "There must be a clear path to take upon the\n",
      "completion of the project and the future steps should be defined 22\n",
      "\n",
      "The why/how/what strategy of presenting\n",
      "When speaking on a less formal level, the why/how/what strategy is a quick and easy way\n",
      "to create a presentation worthy of praise 38\n",
      "It is quite simple, as shown:\n",
      "This model is borrowed from famous  advertisements 16\n",
      "The kind where they would not even\n",
      "tell you what the product was until there were three seconds left 20\n",
      "They want to catch your\n",
      "attention and then, finally, reveal what it was that was so exciting 20\n",
      "Consider the following\n",
      "example:\n",
      "\"Hello everyone 9\n",
      "I am here to tell you about why we seem to have a hard time focusing on our job\n",
      "when the Olympics are being aired 26\n",
      "After mining survey results and merging this data with\n",
      "company-standard work performance data, I was able to find a correlation between the number of\n",
      "hours of TV watched per day and average work performance 38\n",
      "Knowing this, we can all be a bit more\n",
      "aware of our TV watching habits and make sure we don't let it affect our work 28\n",
      "Thank you 2\n",
      "\n",
      "This chapter was actually formatted in this way 9\n",
      "We started with why we should care about\n",
      "data communication, then we talked about how to accomplish it (through correlation,\n",
      "visuals, and so on), and finally I am telling you the what , which is the why/how/what\n",
      "strategy (insert mind-blowing sound effect here) 59\n",
      "\n",
      "Summary\n",
      "Data communication is not an easy task 10\n",
      "It is one thing to understand the mathematics of\n",
      "how data science works, but it is a completely different thing to try to convince a room of\n",
      "data scientists and non-data scientists alike of your results and their value to them 45\n",
      "In this\n",
      "chapter, we went over basic chart making, how to identify faulty causation, and how to\n",
      "hone our oral presentation skills 28\n",
      "\n",
      "Our next few chapters will really begin to hit at one of the biggest talking points of data\n",
      "science 21\n",
      "In the last nine chapters, we spoke about everything related to how to obtain data,\n",
      "clean data, and visualize data in order to gain a better understanding of the environment\n",
      "that the data represents 38\n",
      "\n",
      "\n",
      "We then turned to look at the basic and advanced probability/statistics laws in order to use\n",
      "quantifiable theorems and tests on our data to get actionable results and answers 36\n",
      "\n",
      "In subsequent chapters, we will take a look into machine learning and the situations in\n",
      "which machine learning performs well and doesn't perform well 28\n",
      "As we take a journey into\n",
      "this material, I urge you, the reader, to keep an open mind and truly understand not just\n",
      "how machine learning works, but also why we need to use it 41\n",
      "\n",
      "\n",
      "0\n",
      "How to Tell If Your Toaster Is\n",
      "Learning – Machine Learning\n",
      "Essentials\n",
      "Machine learning has become quite the phrase of the decade 30\n",
      "It seems as though every time\n",
      "we hear about the next great start-up or turn on the news, we hear something about a\n",
      "revolutionary piece of machine learning technology and how it will change the way we\n",
      "live 44\n",
      "\n",
      "This chapter focuses on machine learning as a practical part of data science 14\n",
      "We will cover\n",
      "the following topics in this chapter:\n",
      "Defining the different types of machine learning, along with examples of each\n",
      "kind\n",
      "Regression and classification\n",
      "What is machine learning and how is it used in data science 44\n",
      "\n",
      "The differences between machine learning and statistical modeling and how\n",
      "machine learning is a broad category of the latter\n",
      "Our aim will be to utilize statistics, probability, and algorithmic thinking in order to\n",
      "understand and apply essential machine learning skills to practical industries, such as\n",
      "marketing 56\n",
      "Examples will include predicting star ratings of restaurant reviews, predicting\n",
      "the presence of a disease, spam email detection, and much more 25\n",
      "This chapter focuses on\n",
      "machine learning as a whole and as a single statistical model 16\n",
      "The subsequent chapters will\n",
      "deal with many more models, some of which are much more complex 18\n",
      "\n",
      "We will also turn our focus on metrics, which tell us how effective our models are 18\n",
      "We will\n",
      "use metrics in order to conclude results and make predictions using machine learning 16\n",
      "\n",
      "\n",
      "What is machine learning 5\n",
      "\n",
      "It wouldn't make sense to continue without a concrete definition of what machine learning\n",
      "is 18\n",
      "Well, let's back up for a minute 9\n",
      "In Chapter 1 , How to Sound Like a Data Scientist , we\n",
      "defined machine learning as giving computers the ability to learn  from data without being\n",
      "given explicit rules by a programmer 37\n",
      "This definition still holds true 5\n",
      "Machine learning is\n",
      "concerned with the ability to ascertain certain patterns (signals) out of data, even if the data\n",
      "has inherent errors in it (noise) 34\n",
      "\n",
      "Machine learning models are able to learn from data without the explicit help of a human 17\n",
      "\n",
      "That is the main difference between machine learning models and classical algorithms 13\n",
      "\n",
      "Classical algorithms are told how to find the best answer in a complex system and the\n",
      "algorithm then searches for these best solutions and often works faster and more efficiently\n",
      "than a human 37\n",
      "However, the bottleneck here is that the human has to first come up with the\n",
      "best solution 19\n",
      "In machine learning, the model is not told the best solution and, instead, is\n",
      "given several examples of the problem and is told to figure out the best solution 33\n",
      "\n",
      "Machine learning is just another tool in the belt of a data scientist 14\n",
      "It is on the same level as\n",
      "statistical tests (chi-square or t-tests) or uses base probability/statistics to estimate\n",
      "population parameters 29\n",
      "Machine learning is often regarded as the only thing data scientists\n",
      "know how to do, and this is simply untrue 22\n",
      "A true data scientist is able to recognize when\n",
      "machine learning is applicable and, more importantly, when it is not 23\n",
      "\n",
      "Machine learning is a game of correlations and relationships 10\n",
      "Most machine learning\n",
      "algorithms in existence are concerned with finding and/or exploiting relationships between\n",
      "datasets (often represented as columns in a pandas DataFrame) 29\n",
      "Once machine learning\n",
      "algorithms can pinpoint certain correlations, the model can either use these relationships to\n",
      "predict future observations or generalize the data to reveal interesting patterns 31\n",
      "\n",
      "Perhaps a great way to explain machine learning is to offer an example of a problem\n",
      "coupled with two possible solutions: one using a machine learning algorithm and the other\n",
      "utilizing a non-machine learning algorithm 42\n",
      "\n",
      "Example – facial recognition\n",
      "This problem is very well documented 12\n",
      "Given a picture of a face, who does it belong to 12\n",
      "\n",
      "However, I argue that there  is a more important question that must be asked even before\n",
      "this 21\n",
      "Suppose you wish to implement a home security system that recognizes who is\n",
      "entering your house 18\n",
      "Most likely, during the day, your house will be empty most of the\n",
      "time and the facial recognition will kick in only if there is a person in the shot 33\n",
      "This is\n",
      "exactly the question I propose we try and solve —given a photo, is there a face in it 24\n",
      "\n",
      "\n",
      "Given this problem, I propose the following two solutions:\n",
      "A non-machine learning algorithm that will define a face as having a roundish\n",
      "structure, two eyes, hair, nose, and so on 40\n",
      "The algorithm then looks for these\n",
      "hard-coded features in the photo and returns whether or not it was able to find\n",
      "any of these features 28\n",
      "\n",
      "A machine learning algorithm that will work a bit differently 11\n",
      "The model will\n",
      "only be given several pictures of faces and non-faces that are labeled as such 20\n",
      "\n",
      "From the examples (called training sets), it would figure out its own definition of\n",
      "a face 20\n",
      "\n",
      "The machine learning version of the solution is never told what a face is, it is merely given\n",
      "several examples, some with faces, and some without 32\n",
      "It is then up to the machine learning\n",
      "model to figure out the difference between the two 18\n",
      "Once it figures out the difference\n",
      "between the two, it uses this information to take in a picture and predict whether or not\n",
      "there is a face in the new picture 34\n",
      "For example, to train the model, we will give it the\n",
      "following three photographs:\n",
      "Images for training machine learning model\n",
      "\n",
      "The model will then figure out the difference between the pictures labeled as Face  and the\n",
      "pictures labeled as No Face  and be able to use that difference to find faces in future photos 62\n",
      "\n",
      "Machine learning isn't perfect\n",
      "There are many caveats of machine  learning 16\n",
      "Many are specific to different models being\n",
      "implemented, but there are some assumptions that are universal for any machine learning\n",
      "model:\n",
      "The data used, for the most part, is preprocessed and cleaned using the methods\n",
      "outlined in the earlier chapters 48\n",
      "Almost no machine learning model will tolerate\n",
      "dirty data with missing values or categorical values 16\n",
      "Use dummy variables and\n",
      "filling/dropping techniques to handle these discrepancies 14\n",
      "\n",
      "Each row of a cleaned dataset represents a single observation of the environment\n",
      "we are trying to model 20\n",
      "\n",
      "If our goal is to find relationships between variables, then there is an assumption\n",
      "that there is some kind of relationship between these variables 27\n",
      "\n",
      "This assumption is particularly important 6\n",
      "Many machine learning models take\n",
      "this assumption very seriously 10\n",
      "These models are not able to communicate that\n",
      "there might not be a relationship 15\n",
      "\n",
      "Machine learning models are generally considered semi-automatic, which means\n",
      "that intelligent decisions by humans are still needed 21\n",
      "\n",
      "The machine is very smart but has a hard time putting things into context 15\n",
      "The\n",
      "output of most models is a series of numbers and metrics attempting to quantify\n",
      "how well the model did 22\n",
      "It is up to a human to put these metrics into perspective\n",
      "and communicate the results to an audience 20\n",
      "\n",
      "Most machine learning models are sensitive to noisy data 10\n",
      "This means that the\n",
      "models get confused when you include data that doesn't make sense 17\n",
      "For\n",
      "example, if you are attempting to find relationships between economic data\n",
      "around the world and one of your columns is puppy adoption rates in the capital\n",
      "city, that information is likely not to be relevant and will confuse the model 46\n",
      "\n",
      "These assumptions will come up again and again when dealing with machine learning 14\n",
      "\n",
      "They are all too important and are often ignored by novice data scientists 14\n",
      "\n",
      "\n",
      "How does machine learning work 6\n",
      "\n",
      "Each flavor of machine  learning and each individual model works in very different ways,\n",
      "exploiting different parts of mathematics and data science 26\n",
      "However, in general, machine\n",
      "learning works by taking in data, finding relationships within the data, and giving as\n",
      "output what the model learned, as illustrated in the following diagram:\n",
      "An overview of machine learning models\n",
      "As we explore the different types of machine learning models, we will see how they\n",
      "manipulate data differently and come up with different outputs for different applications 75\n",
      "\n",
      "Types of machine learning\n",
      "There are many ways to segment machine  learning and dive deeper 18\n",
      "In Chapter 1 , How to\n",
      "Sound Like a Data Scientist , I mentioned statistical and probabilistic models 21\n",
      "These models\n",
      "utilize statistics and probability, which we've seen in the previous chapters, in order to find\n",
      "relationships between data and make predictions 29\n",
      "In this chapter, we will implement both\n",
      "types of models 12\n",
      "In the following chapter, we will see machine learning outside the rigid\n",
      "mathematical world of statistics/probability 23\n",
      "You can segment machine learning models by\n",
      "different characteristics, including the following:\n",
      "The types of data/organic structures they utilize (tree/graph/neural network)\n",
      "The field of mathematics they are most related to (statistical/probabilistic)\n",
      "The level of computation required to train (deep learning)\n",
      "\n",
      "For the purpose of education, I will offer my own breakdown of machine learning models 75\n",
      "\n",
      "Branching off from  the top level of machine learning, there are the following three subsets:\n",
      "Supervised learning\n",
      "Unsupervised learning\n",
      "Reinforcement learning\n",
      "Supervised learning\n",
      "Simply put, supervised learning finds  associations between features of a dataset and a\n",
      "target variable 56\n",
      "For example, supervised learning models might try to find the association\n",
      "between a person's health features (heart rate, obesity level, and so on) and that person's\n",
      "risk of having a heart attack (the target variable) 46\n",
      "\n",
      "These associations allow supervised models to make predictions based on past examples 13\n",
      "\n",
      "This is often the first thing that comes to people's minds when they hear the phrase,\n",
      "machine learning, but it in no way does it encompass the realm of machine learning 35\n",
      "\n",
      "Supervised machine learning models are often called predictive analytics models , named\n",
      "for their ability to predict the future based on the past 26\n",
      "\n",
      "Supervised machine learning requires  a certain type of data called labeled data 15\n",
      "This means\n",
      "that we must teach our model by giving it historical examples that are labeled with the\n",
      "correct answer 22\n",
      "Recall the facial recognition example 5\n",
      "That is a supervised learning model\n",
      "because we are training our model with the previous pictures labeled as either face or not\n",
      "face, and then asking the model to predict whether or not a new picture has a face in it 44\n",
      "\n",
      "Specifically, supervised learning works using parts of the data to predict another part 16\n",
      "First,\n",
      "we must separate data into two parts, as follows:\n",
      "The predictors, which are the columns that will be used to make our prediction 28\n",
      "\n",
      "These are sometimes called features, input values, variables, and independent\n",
      "variables 16\n",
      "\n",
      "The response, which is the column that we wish to predict 13\n",
      "This is sometimes\n",
      "called outcome, label, target, and dependent variable 14\n",
      "\n",
      "Supervised learning attempts to find a relationship between the predictors and the\n",
      "response in order to make a prediction 22\n",
      "The idea is that, in the future, a data observation\n",
      "will present itself and we will only know the predictors 23\n",
      "The model will then have to use\n",
      "the predictors to make an accurate prediction of the response value 19\n",
      "\n",
      "\n",
      "Example – heart attack prediction\n",
      "Suppose we wish to predict whether  someone will have a heart attack within a year 24\n",
      "To\n",
      "predict this, we are given that person's cholesterol level, blood pressure, height, their\n",
      "smoking habits, and perhaps more 28\n",
      "From this data, we must ascertain the likelihood of a\n",
      "heart attack 14\n",
      "Suppose, to make this prediction, we look at previous patients and their\n",
      "medical history 17\n",
      "As these are previous patients, we know not only their predictors\n",
      "(cholesterol, blood pressure, and so on), but we also know if they actually had a heart\n",
      "attack (because it already happened 42\n",
      " 1\n",
      "\n",
      "This is a supervised machine learning problem because we are doing the following:\n",
      "We are making a prediction about someone\n",
      "We are using historical training data to find relationships between medical\n",
      "variables and heart attacks:\n",
      "An overview of supervised models\n",
      "The hope here is that a patient will walk in tomorrow and our model will be able to identify\n",
      "whether or not the patient is at risk for a heart attack based on her/his conditions (just like a\n",
      "doctor would 91\n",
      " 1\n",
      "\n",
      "\n",
      "As the model sees more and more labeled data, it adjusts itself in order to match the correct\n",
      "labels given to us 25\n",
      "We can use different metrics (explained later in this chapter) to pinpoint\n",
      "exactly how well our supervised machine learning model is doing and how it can better\n",
      "adjust itself 34\n",
      "\n",
      "One of the biggest drawbacks of supervised  machine learning is that we need this labeled\n",
      "data, which can be very difficult to get a hold of 30\n",
      "Suppose we wish to predict heart attacks;\n",
      "we might need thousands of patients along with all of their medical information and years'\n",
      "worth of follow-up records for each person, which could be a nightmare to obtain 40\n",
      "\n",
      "In short, supervised models use historical labeled data in order to make predictions about\n",
      "the future 19\n",
      "Some possible applications for supervised learning include the following:\n",
      "Stock price predictions\n",
      "Weather predictions\n",
      "Crime predictions\n",
      "Note how each of the preceding  examples uses the word prediction , which makes sense\n",
      "seeing how I emphasized supervised learning's ability to make predictions about the\n",
      "future 52\n",
      "Predictions, however, are not where the story ends 11\n",
      "\n",
      "Here is a visualization of how supervised models use labeled data to fit themselves and\n",
      "prepare themselves to make predictions:\n",
      "Note how the supervised model learns from a bunch of training data and then, when it is\n",
      "ready, it looks at unseen cases and outputs a prediction 53\n",
      "\n",
      "\n",
      "It's not only about predictions\n",
      "Supervised learning exploits  the relationship between the predictors and the response to\n",
      "make predictions, but sometimes, it is enough just knowing that there even is a\n",
      "relationship 41\n",
      "Suppose we are using a supervised machine learning model to predict\n",
      "whether or not a customer will purchase a given item 22\n",
      "A possible dataset might look as\n",
      "follows:\n",
      "Person ID Age Gender Employed 16\n",
      "Bought the product 3\n",
      "\n",
      "1 63 F N Y\n",
      "2 24 M Y N\n",
      "Note that, in this case, our predictors are Age, Gender, and Employed 32\n",
      ", while our response\n",
      "is Bought the product 9\n",
      "This is because we want to see if, given someone's age, gender, and\n",
      "employment status, they will buy the product 26\n",
      "\n",
      "Assume that a model is trained on this data and can make accurate predictions about\n",
      "whether or not someone will buy something 25\n",
      "That, in and of itself, is exciting, but there's\n",
      "something else that is arguably even more exciting 22\n",
      "The fact that we could make accurate\n",
      "predictions implies that there is a relationship between these variables, which means that to\n",
      "know if someone will buy your product, you only need to know their age, gender, and\n",
      "employment status 46\n",
      "This might contradict the previous market research, indicating that\n",
      "much more must be known about a potential customer to make such a prediction 25\n",
      "\n",
      "This speaks to supervised machine learning's ability to understand which predictors affect\n",
      "the response and how 19\n",
      "For example, are women more likely to buy the product, w hich age\n",
      "groups are prone to decline the product, is there a combination of age and gender that is a\n",
      "better predictor than any one column on its own 46\n",
      "As someone's age increases, do their\n",
      "chances of buying the product go up, down, or stay the same 24\n",
      "\n",
      "It is also possible that all of the columns are not necessary 13\n",
      "A possible output of a machine\n",
      "learning might suggest that only certain columns are necessary to make the prediction and\n",
      "that the other columns are only noise (they do not correlate to the response and therefore\n",
      "confuse the model) 45\n",
      "\n",
      "\n",
      "Types of supervised learning\n",
      "There are, in general, two types  of supervised learning models: regression  and\n",
      "classification 25\n",
      "The difference between the two is quite  simple and lies in the response\n",
      "variable 16\n",
      "\n",
      "Regression\n",
      "Regression models attempt to predict  a continuous response 12\n",
      "This means that the response\n",
      "can take on a range of infinite values 14\n",
      "Consider the following examples:\n",
      "Dollar amounts\n",
      "Salary\n",
      "Budget\n",
      "Temperature\n",
      "Time\n",
      "Generally recorded in seconds or minutes\n",
      "Classiﬁcation\n",
      "Classification  attempts to predict  a categorical response, which means that the response\n",
      "only has a finite amount of choices 53\n",
      "Here are some  examples:\n",
      "Cancer grade (1, 2, 3, 4, 5)\n",
      "True/false questions, such as the following examples:\n",
      "\"Will this person have a heart attack within a year 45\n",
      "\n",
      "\"Will you get this job 7\n",
      "\n",
      "Given a photo of a face, who does this face belong to 14\n",
      "(facial recognition)\n",
      "Predict the year someone was born:\n",
      "Note that there are many possible answers (over 100) but still\n",
      "finitely many more\n",
      "Example – regression\n",
      "The following graphs show a relationship between three categorical variables (age, year\n",
      "they were born, and education level) and a person's wage:\n",
      "\n",
      "\n",
      "Regression examples (source: https://lagunita 75\n",
      "stanford 2\n",
      "edu/c4x/HumanitiesScience/StatLearning/asset/introduction 15\n",
      "pdf)\n",
      "Note that, even though each predictor is categorical, this example is regressive because the\n",
      "y axis, our dependent variable, our response, is continuous 32\n",
      "\n",
      "Our earlier heart attack example is classification because the response was: will this person\n",
      "have a heart attack within a year 24\n",
      "This has only two possible answers: Yes or No 10\n",
      "\n",
      "Data is in the eyes of the beholder\n",
      "Sometimes, it can be tricky to decide  whether or not you should use classification or\n",
      "regression 31\n",
      "Consider that we are interested in the weather outside 9\n",
      "We could ask the\n",
      "question, how hot is it outside 12\n",
      "In this case, your answer is on a continuous scale, and some\n",
      "possible answers are 60 20\n",
      "7 degrees or 98 degrees 7\n",
      "However, as an exercise, go and ask 10\n",
      "people what the temperature is outside 18\n",
      "I guarantee you that someone (if not most people)\n",
      "will not answer in some exact degrees but will bucket their answer and say something like\n",
      "it's in the 60s 35\n",
      "\n",
      "We might wish to consider this problem as a classification problem, where the response\n",
      "variable is no longer in exact degrees but is in a bucket 29\n",
      "There would only be a finite\n",
      "number of buckets in theory, making the model perhaps learn the differences between 60s\n",
      "and 70s a bit better 32\n",
      "\n",
      "\n",
      "Unsupervised learning\n",
      "The second type of machine learning  does not deal  with predictions but has a much more\n",
      "open objective 27\n",
      "Unsupervised learning takes in a set of predictors and utilizes relationships\n",
      "between the predictors in order to accomplish tasks such as the following:\n",
      "It reduces the dimension of the data by condensing variables together 39\n",
      "An\n",
      "example of this would be file compression 9\n",
      "Compression works by utilizing\n",
      "patterns in the data and representing the data in a smaller format 17\n",
      "\n",
      "It finds groups of observations that behave similarly and groups them together 13\n",
      "\n",
      "The first element on this list is called dimension reduction  and the second is called\n",
      "clustering 20\n",
      "Both of these are examples of unsupervised learning because they do not\n",
      "attempt to find a relationship between predictors and a specific response and therefore are\n",
      "not used to make predictions of any kind 38\n",
      "Unsupervised models, instead, are utilized to\n",
      "find organizations and representations of the data that were previously unknown 22\n",
      "\n",
      "The following screenshot is a representation of a cluster analysis:\n",
      "Example of cluster analysis\n",
      "\n",
      "The model will recognize that each uniquely colored cluster of observations is similar to\n",
      "another but different from the other clusters 39\n",
      "\n",
      "A big advantage for unsupervised learning is that it does not require labeled data, which\n",
      "means that it is much easier to get data that complies with unsupervised learning models 38\n",
      "\n",
      "Of course, a drawback to this is that we lose all predictive power because the response\n",
      "variable holds the information to make predictions and, without it, our model will be\n",
      "hopeless in making any sort of predictions 44\n",
      "\n",
      "A big drawback is that it is difficult  to see how well  we are doing 18\n",
      "In a regression or\n",
      "classification problem, we can easily tell how well our models are predicting by comparing\n",
      "our models' answers to the actual answers 29\n",
      "For example, if our supervised model predicts\n",
      "rain and it is sunny outside, the model was incorrect 20\n",
      "If our supervised model predicts the\n",
      "price will go up by 1 dollar and it goes up by 99 cents, our model was very close 29\n",
      "In\n",
      "supervised modeling, this concept is foreign because we have no answer to compare our\n",
      "models to 21\n",
      "Unsupervised models are merely suggesting differences and similarities, which\n",
      "then require a human's interpretation:\n",
      "An overview of unsupervised models\n",
      "In short, the main goal of unsupervised models is to find similarities and differences\n",
      "between data observations 49\n",
      "We will discuss unsupervised models in depth in later chapters 12\n",
      "\n",
      "\n",
      "Reinforcement learning\n",
      "In reinforcement learning, algorithms get to choose  an action in an environment and then\n",
      "are rewarded (positively or negatively) for choosing this action 35\n",
      "The algorithm then  adjusts\n",
      "itself and modifies its strategy in order to accomplish some goal, which is usually to get\n",
      "more rewards 27\n",
      "\n",
      "This type of machine  learning is very popular in AI-assisted gameplay as agents  (the AI) are\n",
      "allowed to explore a virtual world and collect rewards and learn the best navigation\n",
      "techniques 41\n",
      "This model is also popular in robotics, especially in the field of self-automated\n",
      "machinery, including cars:\n",
      "Self-driving cars ( image source: https://www 34\n",
      "quora 2\n",
      "com/How-do-Googles-self-driving-cars-work)\n",
      "\n",
      "Self-driving cars read in sensor input, act accordingly, and are then rewarded for taking a\n",
      "certain action 36\n",
      "The car then adjusts its behavior to collect more rewards 10\n",
      "It can be thought\n",
      "that reinforcement is similar to supervised learning in that the agent is learning  from its \n",
      "past actions to make better moves in the future; however, the main difference lies in the\n",
      "reward 42\n",
      "The reward does not have to be tied in any way to a correct  or incorrect  decision 19\n",
      "\n",
      "The reward simply encourages (or discourages) different actions 12\n",
      "\n",
      "Reinforcement learning is the least explored of the three types of machine learning and\n",
      "therefore is not explored in great length in this text 29\n",
      "The remainder of this chapter will focus\n",
      "on supervised and unsupervised learning 15\n",
      "\n",
      "Overview of the types of machine learning\n",
      "Of the three types of machine  learning —supervised, unsupervised, and reinforcement\n",
      "learning —we can imagine the world of machine learning as something like this:\n",
      "Each of the three types of machine learning has its benefits and its drawbacks, as listed:\n",
      "Supervised machine learning : This exploits  relationships between predictors \n",
      "and response variables to make predictions of future data observations 83\n",
      "The pros\n",
      "are as follows:\n",
      "It can make future predictions\n",
      "It can quantify relationships between predictors and response\n",
      "variables\n",
      "It can show us how variables affect each other and how much\n",
      "\n",
      "The cons are as follows:\n",
      "It requires labeled data (which can be difficult to get)\n",
      "Unsupervised machine learning : This finds  similarities and differences between\n",
      "data points 71\n",
      "The pros are as follows:\n",
      "It can find groups of data  points that behave similarly that a\n",
      "human would never have noted 25\n",
      "\n",
      "It can be a preprocessing step for supervised learning 10\n",
      "\n",
      "Think of clustering a bunch of data points and then using these\n",
      "clusters as the response 18\n",
      "\n",
      "It can use unlabeled data, which is much easier to find 14\n",
      "\n",
      "The cons are as follows:\n",
      "It has zero predictive power\n",
      "It can be hard to determine if we are on the right track\n",
      "It relies much more on human interpretation\n",
      "Reinforcement learning : This is reward-based learning that encourages agents to\n",
      "take particular actions  in their environments 57\n",
      "The pros are as follows:\n",
      "Very complicated rewards systems create very complicated AI\n",
      "systems\n",
      "It can learn in almost any environment, including our own Earth\n",
      "The cons are as follows:\n",
      "The agent is erratic at first and makes many terrible choices before\n",
      "realizing that these choices have negative rewards\n",
      "For example, a car might crash into a wall and not know that that is\n",
      "not okay until the environment negatively rewards it\n",
      "It can take a while before the agent avoids decisions altogether\n",
      "The agent might play it safe and only choose one action and be\n",
      "\"too afraid\" to try anything else for fear of being punished\n",
      "\n",
      "How does statistical modeling fit into all of\n",
      "this 134\n",
      "\n",
      "Up until now, I have been using the term  machine learning, but you may ask how statistical\n",
      "modeling plays a role in all of this 31\n",
      "\n",
      "This is still a debated topic in the field of data science 13\n",
      "I believe that statistical modeling is\n",
      "another term for machine learning models that heavily relies on using mathematical rules\n",
      "borrowed from probability and statistics to create relationships between data variables\n",
      "(often in a predictive sense) 41\n",
      "\n",
      "The remainder of this chapter will focus mostly on one statistical/probabilistic\n",
      "model —linear regression 21\n",
      "\n",
      "Linear regression\n",
      "Finally 5\n",
      "We will explore our first true machine learning model 9\n",
      "Linear regression is a form\n",
      "of regression, which means that it is a machine learning model that attempts to find a\n",
      "relationship between predictors and a response variable and that response variable is, you\n",
      "guessed it, continuous 44\n",
      "This notion is synonymous with making a line of best fit 11\n",
      "\n",
      "In the case of linear regression, we will attempt to find a linear relationship between our\n",
      "predictors and our response variable 25\n",
      "Formally, we wish to solve a formula of the following\n",
      "format:\n",
      "Let's look at the constituents of this formula:\n",
      "y is our response variable\n",
      "xi is our ith variable ( ith column or ith predictor)\n",
      "B0 is the intercept\n",
      "Bi is the coefficient for the xi term\n",
      "\n",
      "Let's take a look at some data before we go in depth 71\n",
      "This dataset is publically available and\n",
      "attempts to predict  the number of bikes needed on a particular day for a bike sharing\n",
      "program:\n",
      "# read the data and set the datetime as the index\n",
      "# taken from Kaggle: https://www 50\n",
      "kaggle 3\n",
      "com/c/bike-sharing-demand/data\n",
      "import pandas as pd\n",
      "import matplotlib 15\n",
      "pyplot as plt\n",
      "%matplotlib inline\n",
      "url =\n",
      "'https://raw 13\n",
      "githubusercontent 2\n",
      "com/justmarkham/DAT8/master/data/bikeshare 14\n",
      "c\n",
      "sv'\n",
      "bikes = pd 8\n",
      "read_csv(url)\n",
      "bikes 6\n",
      "head()\n",
      "Following is the output:\n",
      "We can see that every row represents a single hour of bike usage 20\n",
      "In this case, we are\n",
      "interested in predicting count , which represents the total number of bikes rented in the\n",
      "period of that hour 27\n",
      "\n",
      "Let's, for example, look at a scatter plot between temperature (the temp  column) and\n",
      "count , as shown:\n",
      "bikes 29\n",
      "plot(kind='scatter', x='temp', y='count', alpha=0 16\n",
      "2)\n",
      "\n",
      "We get the following graph as output:\n",
      "And now, let's use a module, called seaborn , to draw ourselves a line of best fit, as follows:\n",
      "import seaborn as sns #using seaborn to get a line of best fit\n",
      "sns 51\n",
      "lmplot(x='temp', y='count', data=bikes, aspect=1 17\n",
      "5,\n",
      "scatter_kws={'alpha':0 10\n",
      "2})\n",
      "Following is the output:\n",
      "\n",
      "\n",
      "This line in the graph attempts to visualize and quantify the relationship between temp  and\n",
      "count 26\n",
      "To make a prediction, we simply find a given temperature and then see where the\n",
      "line would predict the count 22\n",
      "For example, if the temperature is 20 degrees (Celsius, mind\n",
      "you), then our line would predict that about 200 bikes will be rented 31\n",
      "If the temperature is\n",
      "above 40 degrees, then more than 400 bikes will be needed 19\n",
      "\n",
      "It appears that as temp  goes up, our count  also goes up 16\n",
      "Let's see if our correlation value,\n",
      "which quantifies a linear relationship between variables, also matches this notion:\n",
      "bikes[['count', 'temp']] 30\n",
      "corr()\n",
      "# 0 5\n",
      "3944\n",
      "There is a (weak) positive correlation between the two variables 16\n",
      "Now, let's go back to the\n",
      "form of the linear regression:\n",
      "Our model will attempt to draw a perfect  line between all of the dots in the preceding\n",
      "graph but, of course, we can clearly see that there is no perfect line between these dots 53\n",
      "The\n",
      "model will then find the best fit  line possible 12\n",
      "How 1\n",
      "We can draw infinite lines between the\n",
      "data points, but what makes a line the best 18\n",
      "\n",
      "Consider the following diagram:\n",
      "In our model, we are given the x and the y and the model learns  the beta coefficients, also\n",
      "known as model coefficients :\n",
      "The black dots are the observed values of x and y 45\n",
      "\n",
      "The blue line is our line of best fit 10\n",
      "\n",
      "The red lines between the dots and the line are called the residuals; they are the\n",
      "distances between the observed values and the line 28\n",
      "They are how wrong the line\n",
      "is 8\n",
      "\n",
      "\n",
      "Each data point has a residual or a distance to the line of best fit 16\n",
      "The sum of squared\n",
      "residuals  is the summation of each residual squared 17\n",
      "The best fit line has the smallest sum\n",
      "of squared residual value 13\n",
      "Let's build this line in Python:\n",
      "# create X and y\n",
      "feature_cols = ['temp'] # a list of the predictors\n",
      "X = bikes[feature_cols] # subsetting our data to only the predictors\n",
      "y = bikes['count'] # our response variable\n",
      "Note how we made an X and a y variable 65\n",
      "These represent our predictors and our response\n",
      "variable 9\n",
      "Then, we will import our machine learning module, scikit-learn , as shown:\n",
      "# import scikit-learn, our machine learning module\n",
      "from sklearn 32\n",
      "linear_model import LinearRegression\n",
      "Finally, we will fit our model to the predictors and the response variable, as follows:\n",
      "linreg = LinearRegression() #instantiate a new model\n",
      "linreg 39\n",
      "fit(X, y) #fit the model to our data\n",
      "# print the coefficients\n",
      "print(linreg 22\n",
      "intercept_)\n",
      "print(linreg 6\n",
      "coef_)\n",
      "6 3\n",
      "04621295962  # our Beta_0\n",
      "[ 9 15\n",
      "17054048]     # our beta parameters\n",
      "Let's interpret this:\n",
      "B0 (6 20\n",
      "04)  is the value of y when X = 0\n",
      "It is the estimation of bikes that will be rented when the temperature is 0 degrees\n",
      "Celsius\n",
      "So, at 0 degrees, six bikes are predicted to be in use (it's cold 55\n",
      "\n",
      "Sometimes, it might not make sense  to interpret the intercept at all because there might not\n",
      "be a concept of zero of something 27\n",
      "Recall the levels of data 5\n",
      "Not all levels have this notion 6\n",
      "\n",
      "Temperature exists at a level that has the inherent notion of no bikes ; so, we are safe 20\n",
      "Be\n",
      "careful in the future though and verify results: \n",
      "B1 (9 17\n",
      "17)  is our temperature coefficient 8\n",
      "\n",
      "It is the change in y divided by the change in x1 14\n",
      "\n",
      "It represents how x and y move together 9\n",
      "\n",
      "\n",
      "A change in 1 degree Celsius is associated with an increase of about nine bikes\n",
      "rented 20\n",
      "\n",
      "The sign of this coefficient is important 8\n",
      "If it were negative, that would imply that\n",
      "a rise in temperature is associated with a drop in rentals 21\n",
      "\n",
      "Consider the following representation of the beta coefficients in a linear regression:\n",
      "It is important to reiterate that these  are all statements of correlation and not a statement of\n",
      "causation 37\n",
      "We have no means of stating whether or not the rental increase is caused by the\n",
      "change in temperature, it is just that there appears to be movement together 31\n",
      "\n",
      "Using scikit-learn  to make predictions is easy:\n",
      "linreg 15\n",
      "predict(20)\n",
      "# 189 7\n",
      "4570\n",
      "This means that 190 bikes will likely be rented if the temperature is 20 degrees 21\n",
      "\n",
      "Adding more predictors\n",
      "Adding more predictors to the model  is as simple as telling the linear regression model in\n",
      "scikit-learn  about them 30\n",
      "\n",
      "Before we do, we should look at the data dictionary provided to us to make more sense out\n",
      "of these predictors:\n",
      "season : 1 = spring, 2 = summer, 3 = fall, and 4 = winter\n",
      "holiday : Whether the day is considered a holiday\n",
      "workingday : Whether the day is a weekend or holiday\n",
      "\n",
      "weather :\n",
      "Clear , Few clouds , Partly cloudy\n",
      "Mist + Cloudy , Mist + Broken clouds , Mist + Few\n",
      "clouds , Mist\n",
      "Light Snow , Light Rain + Thunderstorm + Scattered\n",
      "clouds , Light Rain + Scattered clouds\n",
      "Heavy Rain  + Ice Pallets  + Thunderstorm  + Mist , Snow +\n",
      "Fog\n",
      "temp : Temperature in Celsius\n",
      "atemp : Feels like temperature in Celsius\n",
      "humidity : Relative humidity\n",
      "windspeed : Wind speed\n",
      "casual : Number of non-registered user rentals initiated\n",
      "registered : Number of registered user rentals initiated\n",
      "count : Number of total rentals\n",
      "Now let's actually create our linear  regression model 210\n",
      "As before, we will first create a list\n",
      "holding the features we wish to look at, create our features and our response datasets ( X\n",
      "and y), and then fit our linear regression 38\n",
      "Once we fit our regression model, we will take a\n",
      "look at the model's coefficients in order to see how our features are interacting with our\n",
      "response:\n",
      "# create a list of features\n",
      "feature_cols = ['temp', 'season', 'weather', 'humidity']\n",
      "# create X and y\n",
      "X = bikes[feature_cols]\n",
      "y = bikes['count']\n",
      "# instantiate and fit\n",
      "linreg = LinearRegression()\n",
      "linreg 86\n",
      "fit(X, y)\n",
      "# pair the feature names with the coefficients\n",
      "result = zip(feature_cols, linreg 22\n",
      "coef_)\n",
      "resultSet = set(result)\n",
      "print(resultSet)\n",
      "\n",
      "This gives us the following output:\n",
      "[('temp', 7 23\n",
      "8648249924774403),\n",
      " ('season', 22 13\n",
      "538757532466754),\n",
      " ('weather', 6 12\n",
      "6703020359238048),\n",
      " ('humidity', -3 13\n",
      "1188733823964974)]\n",
      "And this is what that means:\n",
      "Holding all other predictors constant, a 1 unit increase in temperature is\n",
      "associated with a rental increase of 7 39\n",
      "86 bikes\n",
      "Holding all other predictors constant, a 1 unit increase in season is associated\n",
      "with a rental increase of 22 28\n",
      "5 bikes\n",
      "Holding all other predictors constant, a 1 unit increase in weather is associated\n",
      "with a rental increase of 6 28\n",
      "67 bikes\n",
      "Holding all other predictors constant, a 1 unit increase in humidity is associated\n",
      "with a rental decrease of 3 28\n",
      "12 bikes\n",
      "This is interesting 7\n",
      "Note that, as weather  goes up (meaning that the weather is getting\n",
      "closer to overcast), the bike demand goes up, as is the case when the season variables\n",
      "increase (meaning that we are approaching winter) 46\n",
      "This is not what I was expecting at all 9\n",
      "\n",
      "Let's take a look at the individual scatter  plots between each predictor and the response, as\n",
      "illustrated:\n",
      "feature_cols = ['temp', 'season', 'weather', 'humidity']\n",
      "# multiple scatter plots\n",
      "sns 45\n",
      "pairplot(bikes, x_vars=feature_cols, y_vars='count', kind='reg')\n",
      "We get the following output:\n",
      "\n",
      "\n",
      "Note how the weather line is trending downwards, which is the opposite of what the last\n",
      "linear model was suggesting 48\n",
      "Now, we have to worry about which of these predictors are\n",
      "actually helping us make the prediction, and which ones are just noise 26\n",
      "To do so, we're\n",
      "going to need some more advanced metrics 14\n",
      "\n",
      "Regression metrics\n",
      "There are three main metrics  when using regression  machine learning models 17\n",
      "They are as\n",
      "follows:\n",
      "The mean absolute error\n",
      "The mean squared error\n",
      "The root mean squared error\n",
      "Each metric attempts to describe and quantify the effectiveness of a regression model by\n",
      "comparing a list of predictions to a list of correct answers 50\n",
      "Each of the following mentioned\n",
      "metrics is slightly different from the rest and tells a different story:\n",
      "Let's look at the coefficients:\n",
      "n is the number of observations\n",
      "yi is the actual value\n",
      "ŷ is the predicted value\n",
      "\n",
      "Let's take a look in Python:\n",
      "# example true and predicted response values\n",
      "true = [9, 6, 7, 6]\n",
      "pred = [8, 7, 7, 12]\n",
      "# note that each value in the last represents a single prediction for a\n",
      "model\n",
      "# So we are comparing four predictions to four actual answers\n",
      "# calculate these metrics by hand 125\n",
      "\n",
      "from sklearn import metrics\n",
      "import numpy as np\n",
      "print('MAE:', metrics 17\n",
      "mean_absolute_error(true, pred))\n",
      "print('MSE:', metrics 13\n",
      "mean_squared_error(true, pred))\n",
      "print('RMSE:', np 13\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(true, pred)))\n",
      "Following is the output:\n",
      "MAE: 2 17\n",
      "0\n",
      "MSE: 9 8\n",
      "5\n",
      "RMSE: 3 8\n",
      "08220700148\n",
      "The breakdown  of these  numbers is as follows:\n",
      "MAE is probably the easiest to understand, because it's just the average error 33\n",
      "It\n",
      "denotes, on an average, how wrong the model is 14\n",
      "\n",
      "MSE is more effective than MAE, because MSE punishes larger errors, which tends\n",
      "to be much more useful in the real world 29\n",
      "\n",
      "RMSE  is even more popular than MSE, because it is much more interpretable 18\n",
      "\n",
      "RMSE  is usually the preferred metric for regression, but no matter which one you choose,\n",
      "they are all loss functions and therefore are something to be minimized 32\n",
      "Let's use RMSE  to\n",
      "ascertain which columns are helping and which are hurting 18\n",
      "\n",
      "Let's start with only using temperature 8\n",
      "Note that our procedure will be as follows:\n",
      "Create our X and our y variables 1 18\n",
      "\n",
      "Fit a linear regression model2 7\n",
      "\n",
      "Use the model to make a list of predictions based on X 3 15\n",
      "\n",
      "Calculate RMSE  between the predictions and the actual values 4 14\n",
      "\n",
      "\n",
      "Let's take a look at the code:\n",
      "from sklearn import metrics\n",
      "# import metrics from scikit-learn\n",
      "feature_cols = ['temp']\n",
      "# create X and y\n",
      "X = bikes[feature_cols]\n",
      "linreg = LinearRegression()\n",
      "linreg 51\n",
      "fit(X, y)\n",
      "y_pred = linreg 10\n",
      "predict(X)\n",
      "np 4\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y, y_pred)) # RMSE\n",
      "# Can be interpreted loosely as an average error\n",
      "#166 24\n",
      "45\n",
      "Now, let's try it using temperature and humidity, as shown:\n",
      "feature_cols = ['temp', 'humidity']\n",
      "# create X and y\n",
      "X = bikes[feature_cols]\n",
      "linreg = LinearRegression()\n",
      "linreg 47\n",
      "fit(X, y)\n",
      "y_pred = linreg 10\n",
      "predict(X)\n",
      "np 4\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y, y_pred)) # RMSE\n",
      "# 157 15\n",
      "79\n",
      "It got better 6\n",
      "Let's try using even more predictors, as illustrated:\n",
      "feature_cols = ['temp', 'humidity', 'season', 'holiday', 'workingday',\n",
      "'windspeed', 'atemp']\n",
      "# create X and y\n",
      "X = bikes[feature_cols]\n",
      "linreg = LinearRegression()\n",
      "linreg 60\n",
      "fit(X, y)\n",
      "y_pred = linreg 10\n",
      "predict(X)\n",
      "np 4\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y, y_pred)) # RMSE\n",
      "# 155 15\n",
      "75\n",
      "Even better 5\n",
      "At first, this seems like a major triumph, but there is actually a hidden danger\n",
      "here 19\n",
      "Note that we are training the line to fit to X and y and then asking it to predict X again 21\n",
      "\n",
      "This is actually a huge mistake in machine learning because it can lead to overfitting , which\n",
      "means that our model is merely memorizing  the data and regurgitating it back to us 40\n",
      "\n",
      "\n",
      "Imagine that you are a student, and you walk into the first day of class and the teacher says\n",
      "that the final exam is very difficult in this class 32\n",
      "In order to prepare you, she gives you\n",
      "practice test after practice test after practice test 18\n",
      "The day of the final exam arrives and you \n",
      "are shocked to find out that every question on the exam is exactly  the same as in the\n",
      "practice test 32\n",
      "Luckily, you did them so many times that you remember the answer and get a\n",
      "100% in the exam 22\n",
      "\n",
      "The same thing applies here, more or less 10\n",
      "By fitting and predicting on the same data, the\n",
      "model is memorizing the data and getting better at it 22\n",
      "A great way to combat this\n",
      "overfitting problem is to use the train/test approach to fit machine learning models, which\n",
      "works as illustrated:\n",
      "Essentially, we will take the following steps:\n",
      "Split up the dataset into two parts: a training and a test set 1 56\n",
      "\n",
      "Fit our model on the training set and then test it on the test set, just like in school,2 23\n",
      "\n",
      "where the teacher would teach from one set of notes and then test us on different\n",
      "(but similar) questions\n",
      "Once our model is good enough (based on our metrics), we turn our model's3 42\n",
      "\n",
      "attention toward the entire dataset\n",
      "Our model awaits new data previously unseen by anyone4 17\n",
      "\n",
      "The goal here is to minimize the out-of-sample errors of our model, which are the errors\n",
      "our model has on data that it has never seen before 32\n",
      "This is important because the main idea\n",
      "(usually) of a supervised model is to predict outcomes for new data 22\n",
      "If our model is unable\n",
      "to generalize from our training data and use that to predict unseen cases, then our model\n",
      "isn't very good 29\n",
      "\n",
      "\n",
      "The preceding diagram outlines a simple way of ensuring that our model can effectively\n",
      "ingest the training data and use it to predict data points that the model itself has never seen 35\n",
      "\n",
      "Of course, as data scientists, we know that the test set also has answers attached to them,\n",
      "but the model doesn't know that 28\n",
      "\n",
      "All of this might sound complicated, but luckily, the scikit-learn  package has a built-in\n",
      "method to do this, as shown:\n",
      "from sklearn 33\n",
      "cross_validation import train_test_split\n",
      "# function that splits data into training and testing sets\n",
      "feature_cols = ['temp']\n",
      "X = bikes[feature_cols]\n",
      "y = bikes['count']\n",
      "# setting our overall data X, and y\n",
      "# Note that in this example, we are attempting to find an association\n",
      "between the temperature of the day and the number of bike rentals 74\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y) # split the data\n",
      "into training and testing sets\n",
      "# X_train and y_train will be used to train the model\n",
      "# X_test and y_test will be used to test the model\n",
      "# Remember that all four of these variables are just subsets of the overall\n",
      "X and y 77\n",
      "\n",
      "linreg = LinearRegression()\n",
      "# instantiate the model\n",
      "linreg 14\n",
      "fit(X_train, y_train)\n",
      "# fit the model to our training set\n",
      "y_pred = linreg 21\n",
      "predict(X_test)\n",
      "# predict our testing set\n",
      "np 11\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y_test, y_pred)) # RMSE\n",
      "# Calculate our metric: 166 20\n",
      "91\n",
      "We will spend more time on the reasoning behind this train/test split in Chapter 12 , Beyond\n",
      "the Essentials , and look into an even more helpful method, but the main reason we must go\n",
      "through this extra work is because we do not want to fall into a trap where our model is\n",
      "simply regurgitating our dataset back to us and will not be able to handle unseen data\n",
      "points 84\n",
      "\n",
      "In other words, our train/test split is ensuring that the metrics we are looking at are more\n",
      "honest estimates of our sample performance 28\n",
      "\n",
      "\n",
      "Now, let's try again with more predictors, as follows:\n",
      "feature_cols = ['temp', 'workingday']\n",
      "X = bikes[feature_cols]\n",
      "y = bikes['count']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "# Pick a new random training and test set\n",
      "linreg = LinearRegression()\n",
      "linreg 74\n",
      "fit(X_train, y_train)\n",
      "y_pred = linreg 12\n",
      "predict(X_test)\n",
      "# fit and predict\n",
      "np 10\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y_test, y_pred))\n",
      "# 166 12\n",
      "95\n",
      "Now our model actually got worse with that addition 12\n",
      "This implies that workingday  might\n",
      "not be very predictive of our response, the bike rental count 20\n",
      "\n",
      "Now, all of this is good and well, but how well is our model really doing at predicting 21\n",
      "We\n",
      "have our root mean squared error of around 167 bikes, but is that good 18\n",
      "One way to\n",
      "discover this is to evaluate the null model 12\n",
      "\n",
      "The null model in supervised  machine learning  represents effectively guessing the expected\n",
      "outcome over and over, and seeing how you did 26\n",
      "For example, in regression, if we only\n",
      "ever guess the average number of hourly bike rentals, then how well would that model do 27\n",
      "\n",
      "First, let's get the average hourly bike rental, as shown:\n",
      "average_bike_rental = bikes['count'] 25\n",
      "mean()\n",
      "average_bike_rental\n",
      "# 191 11\n",
      "57\n",
      "This means that, overall, in this dataset, regardless of weather, time, day of the week,\n",
      "humidity, and everything else, the average number of bikes that go out every hour is about\n",
      "192 44\n",
      "\n",
      "Let's make a fake prediction list, wherein every single guess is 191 16\n",
      "57 2\n",
      "Let's make this guess\n",
      "for every single hour, as follows:\n",
      "num_rows = bikes 18\n",
      "shape[0]\n",
      "num_rows\n",
      "# 10886\n",
      "null_model_predictions = [average_bike_rental]*num_rows\n",
      "null_model_predictions\n",
      "\n",
      "The output is as follows:\n",
      "[191 38\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 7\n",
      " 1\n",
      " 1\n",
      "\n",
      " 191 3\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824]\n",
      "So, now we have 10886  values, all of them are the average hourly bike rental number 28\n",
      "\n",
      "Now, let's see what RMSE  would be if our model only ever guessed the expected value of\n",
      "the average hourly bike rental count:\n",
      "np 31\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y, null_model_predictions))\n",
      "The output is as follows:\n",
      "181 16\n",
      "13613\n",
      "Simply guessing, it looks like our RMSE  would be 181 bikes 19\n",
      "So, even with one or two\n",
      "features, we can beat it 14\n",
      "Beating the null model is a kind of baseline in machine learning 13\n",
      "If\n",
      "you think about it, why go through any effort at all if your machine learning  is not even \n",
      "better  than just guessing 28\n",
      "\n",
      "We've spent a great deal of time on linear regression, but I'd like to now take some time to\n",
      "look at our next machine learning model, which is actually, somewhat, a cousin of linear\n",
      "regression 45\n",
      "They are based on very similar ideas but have one major difference —while\n",
      "linear regression is a regression model and can only be used to make predictions of\n",
      "continuous numbers, our next machine learning model will be a classification model, which\n",
      "means that it will attempt to make associations between features and a categorical response 61\n",
      "\n",
      "Logistic regression\n",
      "Our first classification model is called logistic  regression 14\n",
      "I can already hear the questions\n",
      "you have in your head: what makes is logistic 17\n",
      "Why is it called regression if you claim that\n",
      "this is a classification algorithm 15\n",
      "All in good time, my friend 7\n",
      "\n",
      "\n",
      "Logistic regression is a generalization of the linear regression model adapted to fit\n",
      "classification problems 19\n",
      "In linear regression, we use a set of quantitative feature variables to\n",
      "predict a continuous response variable 19\n",
      "In logistic regression, we use a set of quantitative\n",
      "feature variables to predict the probabilities  of class membership 21\n",
      "These probabilities can\n",
      "then be mapped to class labels, hence predicting a class for each observation 18\n",
      "\n",
      "When performing linear regression, we use the following function to make our line of best\n",
      "fit:\n",
      "Here, y is our response variable (the thing we wish to predict), our beta represents our\n",
      "model parameters and x represents our input variable (a single one in this case, but it can\n",
      "take in more, as we have seen) 69\n",
      "\n",
      "Briefly, let's assume that one of the response options in our classification problem is class 1 21\n",
      "\n",
      "When performing logistic regression, we use the following form:\n",
      "Probability of y=1, given x\n",
      "Here, y represents the conditional probability that our response variable belongs to class 1,\n",
      "given our data, x 43\n",
      "Now, you might be wondering what on earth is that monstrosity of a\n",
      "function on the right-hand side, and where did the e variable come from 33\n",
      "Well, that\n",
      "monstrosity is called the logistic function and it is actually wonderful 18\n",
      "And that variable, e,\n",
      "is no variable at all 11\n",
      "Let's back up a tick 6\n",
      "\n",
      "The variable e is a special number, like π 11\n",
      "It is, approximately, 2 7\n",
      "718, and is called Euler's\n",
      "number 10\n",
      "It is used frequently in modeling environments with natural  growth and decay 13\n",
      "\n",
      "For example, scientists use e to model the population growth of bacteria and buffalo alike 17\n",
      "\n",
      "Euler's number is used to model the radioactive decay of chemicals and to calculate\n",
      "continuous compound interest 21\n",
      "Today, we will use e for a very special purpose, for machine\n",
      "learning 16\n",
      "\n",
      "Why can't we just make a linear regression directly to the probability of the data point\n",
      "belonging to a certain class, like this:\n",
      "\n",
      "\n",
      "We can't do that for a few reasons, but I will point out a big one 48\n",
      "Linear regression, because\n",
      "it attempts to relate to a continuous response variable, assumes that our y is continuous 21\n",
      "In\n",
      "our case, y would represent the probability of an event occurring 14\n",
      "Even though our\n",
      "probability is, in fact, a continuous range, it is just that —a range between 0 and 1 27\n",
      "A line\n",
      "would extrapolate beyond 0 and 1 and  be able  to predict a probability of -4 or 1,542 29\n",
      "We can't\n",
      "have that 6\n",
      "Our graph must  be bound neatly between 0 and 1 on the y axis, just like a real\n",
      "probability 24\n",
      "\n",
      "Another reason is a bit more philosophical 8\n",
      "Using a linear regression, we are making a\n",
      "serious assumption 12\n",
      "Our big assumption here is that there is a linear relationship between\n",
      "probability and our features 17\n",
      "In general, if we think about the probability of an event, we\n",
      "tend to think of smooth curves representing them, not a single boring line 30\n",
      "So, we need\n",
      "something a bit more appropriate 10\n",
      "For this, let's go back and revisit basic probability for a\n",
      "minute 15\n",
      "\n",
      "Probability, odds, and log odds\n",
      "We are familiar with the basic concept of probability  in that the probability of an event\n",
      "occurring can be simply modeled as the number of ways the event can occur divided by all\n",
      "of the possible  outcomes 51\n",
      "For example, if, out of 3,000  people who walked into a store, 1,000\n",
      "actually bought something, then we could say that the probability of a single person buying\n",
      "an item is as shown here:\n",
      "Pr(buy) = 1000/3000 = 1/3 = 33 67\n",
      "3%\n",
      "However, we also have  a related concept, called odds 15\n",
      "The odds of an outcome occurring is\n",
      "the ratio of the number of ways that the outcome occurs divided by every other possible\n",
      "outcome instead of all possible outcomes 31\n",
      "In the same example, the odds of a person buying\n",
      "something would be as follows:\n",
      "Odds(buy) = 1000/3000 = 1/3 = 33 38\n",
      "3%\n",
      "This means that, for every customer you convert, you will not convert two customers 19\n",
      "These\n",
      "concepts are so related, there is even a formula to get from one to the other 20\n",
      "We have that:\n",
      "\n",
      "\n",
      "Let's check this with our example, as illustrated:\n",
      "It checks out 18\n",
      "\n",
      "Let's use Python to make a table of probabilities and associated odds, as shown:\n",
      "# create a table of probability versus odds\n",
      "table = pd 30\n",
      "DataFrame({'probability':[0 5\n",
      "1, 0 5\n",
      "2, 0 5\n",
      "25, 0 5\n",
      "5, 0 5\n",
      "6, 0 5\n",
      "8, 0 5\n",
      "9]})\n",
      "table['odds'] = table 10\n",
      "probability/(1 - table 5\n",
      "probability)\n",
      "table\n",
      "Following is the output:\n",
      "So, we see that, as our probabilities increase, so do our odds, but at a much faster rate 31\n",
      "In\n",
      "fact, as the probability  of an event occurring nears 1, our odds will shoot off into infinity 24\n",
      "\n",
      "Earlier, we said that we couldn't simply regress to probability  because our line would\n",
      "shoot off into positive and negative infinities, predicting improper probabilities, but what if\n",
      "we regress to odds 41\n",
      "Well, odds  go off to positive infinity, but alas, they will merely\n",
      "approach 0 on the bottom, but never go below 0 31\n",
      "Therefore, we cannot simply regress to\n",
      "probability  or odds 12\n",
      "It looks like we've hit rock bottom folks 9\n",
      "\n",
      "\n",
      "However, wait, natural numbers and logarithms come to the rescue 14\n",
      "Think of logarithms as\n",
      "follows:\n",
      "Basically, logarithms and exponents are one and the same 21\n",
      "We are just so used to writing \n",
      "exponents  in the first way that we forget there is another way to write them 25\n",
      "How about\n",
      "another example 5\n",
      "If we take the logarithm of a number, we are asking the question, what\n",
      "exponent would we need to put on this number to make it the given number 34\n",
      "\n",
      "Note that np 4\n",
      "log  automatically does  all logarithms in base e, which is what we want:\n",
      "np 19\n",
      "log(10) # == 2 8\n",
      "3025\n",
      "# meaning that e ^ 2 11\n",
      "302 == 10\n",
      "# to prove that\n",
      "2 12\n",
      "71828**2 5\n",
      "3025850929940459 # == 9 11\n",
      "9999\n",
      "# e ^ log(10) == 10\n",
      "Let's go ahead and add the logarithm of odds , or logodds , to our table , as follows:\n",
      "# add log-odds to the table\n",
      "table['logodds'] = np 57\n",
      "log(table 2\n",
      "odds)\n",
      "table\n",
      "Following is the output:\n",
      "\n",
      "\n",
      "So, now every row has the probability  of a single event occurring, the odds  of that event\n",
      "occurring, and now the logodds  of that event occurring 45\n",
      "Let's go ahead and ensure that our\n",
      "numbers are on the up and up 16\n",
      "Let's choose a probability of 6\n",
      "25, as illustrated:\n",
      "prob = 8\n",
      "25\n",
      "odds = prob / (1 - prob)\n",
      "odds\n",
      "# 0 19\n",
      "33333333\n",
      "logodds = np 10\n",
      "log(odds)\n",
      "logodds\n",
      "# -1 11\n",
      "09861228\n",
      "It checks out 8\n",
      "Wait, look 3\n",
      "Our logodds  variable seems to go down below zero and, in fact,\n",
      "logodds  is not bounded above, nor is it bounded below, which means that it is a great\n",
      "candidate for a response variable for linear regression 48\n",
      "In fact, this is where our story of\n",
      "logistic regression really begins 15\n",
      "\n",
      "The math of logistic regression\n",
      "The long and short of it is that logistic  regression is a linear regression between our feature,\n",
      "x, and the log-odds of our data belonging to a certain class that we will call true for the sake\n",
      "of generalization 54\n",
      "\n",
      "If p represents the probability of a data point belonging to a particular class, then logistic\n",
      "regression can be written as follows:\n",
      "If we rearrange our variables and solve this for p, we would get the logistic function, which\n",
      "takes on an S shape, where y is bounded by [0,1] :\n",
      "\n",
      "\n",
      "\n",
      "The preceding graph represents the logistic function's ability to map our continuous input,\n",
      "x, to a smooth probability curve that begins at the left, near probability 0, and as we\n",
      "increase x, our probability of belonging to a certain class rises naturally  and smoothly up to\n",
      "probability 1 124\n",
      "Let's explain that in other words:\n",
      "Logistic regression gives an output of the probabilities of a specific class being\n",
      "true\n",
      "Those probabilities can be converted into class predictions\n",
      "The logistic function has some nice properties, as follows:\n",
      "It takes on an S shape\n",
      "Output is bounded by 0 and 1, as a probability should be\n",
      "\n",
      "In order to interpret the output value of a logistic function, we must understand the\n",
      "difference between probability and odds 90\n",
      "The odds of an event are given by the ratio of the\n",
      "probability of the event by its complement, as shown:\n",
      "In linear regression, the β1 parameter represents the change in the response variable for a\n",
      "unit change in x 46\n",
      "In logistic  regression, β1 represents the change in the log-odds for a unit\n",
      "change in x 23\n",
      "This means gives us the change in the odds for a unit change in x 15\n",
      "\n",
      "Consider that we are interested in mobile purchase behavior 10\n",
      "Let y be a class label denoting\n",
      "purchase/no purchase, and let x denote whether the phone was an iPhone 23\n",
      "Also, suppose\n",
      "that we perform a logistic regression, and we get β= 0 18\n",
      "693 2\n",
      "In this case, the odds ratio is\n",
      "np 10\n",
      "exp(0 3\n",
      "693) = 2 , which means that the likelihood of purchase is twice as high if the phone\n",
      "is an iPhone 25\n",
      "\n",
      "Our examples have mostly been binary classification, meaning that we are\n",
      "only predicting one of two outcomes, but logistic regression can handle\n",
      "predicting multiple options in our categorical response using a one-\n",
      "versus-all approach, meaning that it will fit a probability curve for each\n",
      "categorical response 58\n",
      "\n",
      "Back to our bikes briefly to see scikit-learn's logistic regression in action 17\n",
      "I will begin by\n",
      "making a new response variable that is categorical 13\n",
      "To make things simple, I made a\n",
      "column called above_average , which is true if the hourly bike rental count is above\n",
      "average and false otherwise:\n",
      "# Make a categorical response\n",
      "bikes['above_average'] = bikes['count'] >= average_bike_rental\n",
      "As mentioned before, we should look at our null model 66\n",
      "In regression, our null model\n",
      "always predicts the average response, but in classification, our null model always predicts\n",
      "the most common outcome 27\n",
      "In this case, we can use a pandas value count to see that 14\n",
      "About\n",
      "60% of the time, the bike rental count is not above average:\n",
      "bikes['above_average'] 23\n",
      "value_counts(normalize=True)\n",
      "\n",
      "Now, let's actually use logistic regression to try and predict whether or not the hourly bike\n",
      "rental count will be above average, as shown:\n",
      "from sklearn 38\n",
      "linear_model import LogisticRegression\n",
      "feature_cols = ['temp']\n",
      "# using only temperature\n",
      "X = bikes[feature_cols]\n",
      "y = bikes['above_average']\n",
      "# make our overall X and y variables, this time our y is\n",
      "# out binary response variable, above_average\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "# make our train test split\n",
      "logreg = LogisticRegression()\n",
      "# instantiate our model\n",
      "logreg 94\n",
      "fit(X_train, y_train)\n",
      "# fit our model to our training set\n",
      "logreg 18\n",
      "score(X_test, y_test)\n",
      "# score it on our test set to get a better sense of out of sample\n",
      "performance\n",
      "# 0 29\n",
      "65650257\n",
      "It seems that, by only using temperature, we can beat the null model of guessing false all of\n",
      "the time 28\n",
      "This is our first step in making our model the best it can be 14\n",
      "\n",
      "Between linear and logistic  regression, I'd say we already have a great tool belt of machine\n",
      "learning forming, but I have a question —it seems that both of these algorithms are only\n",
      "able to take in quantitative columns as features, but what if I have a categorical feature that\n",
      "I think has an association to my response 67\n",
      "\n",
      "Dummy variables\n",
      "Dummy variables are used when we are hoping to convert a categorical feature into a\n",
      "quantitative one 23\n",
      "Remember that we have two types of categorical features: nominal and\n",
      "ordinal 14\n",
      "Ordinal features have natural order among them, while nominal data does not 14\n",
      "\n",
      "\n",
      "Encoding qualitative (nominal) data using separate columns is called making dummy\n",
      "variables and it works by turning each unique category of a nominal column into its own\n",
      "column that is either true or false 40\n",
      "\n",
      "For example, if we had a column  for someone's college major and we wished to plug that\n",
      "information into a linear or logistic regression, we couldn't because they only take in\n",
      "numbers 40\n",
      "So, for each row, we had new columns that represent the single nominal column 16\n",
      "\n",
      "In this case, we have four unique majors: computer science, engineering, business, and\n",
      "literature 22\n",
      "We end up with three new columns (we omit computer science as it is not\n",
      "necessary):\n",
      "Note that the first row has a 0 in all of the columns, which means that this person did not\n",
      "major in engineering, did not major in business, and did not major in literature 58\n",
      "The second\n",
      "person has a single 1 in the Engineering  column as that is the major they studied 21\n",
      "\n",
      "In our bikes example, let's define a new column, called when_is_it , which is going to be\n",
      "one of the following four options:\n",
      "Morning\n",
      "Afternoon\n",
      "Rush_hour\n",
      "Off_hours\n",
      "To do this, our approach will be to make a new column that is simply the hour of the day,\n",
      "use that column to determine when in the day it is, and explore whether or not we think\n",
      "that column might help us predict the above_daily  column:\n",
      "bikes['hour'] = bikes['datetime'] 107\n",
      "apply(lambda x:int(x[11]+x[12]))\n",
      "# make a column that is just the hour of the day\n",
      "bikes['hour'] 30\n",
      "head()\n",
      "0 1 2 3\n",
      "\n",
      "Great, now let's define a function that turns these hours into strings 24\n",
      "For this example, let's\n",
      "define the hours between 5 and 11 as morning, between 11 a 23\n",
      "m 1\n",
      "and 4 p 4\n",
      "m 1\n",
      "as being\n",
      "afternoon, 4 and 6 as being rush hour, and everything else as being off hours:\n",
      "# this function takes in an integer hour\n",
      "# and outputs one of our four options\n",
      "def when_is_it(hour):\n",
      "    if hour >= 5 and hour < 11:\n",
      "        return \"morning\"\n",
      "    elif hour >= 11 and hour < 16:\n",
      "        return \"afternoon\"\n",
      "    elif hour >= 16 and hour < 18:\n",
      "        return \"rush_hour\"\n",
      "    else:\n",
      "        return \"off_hours\"\n",
      "Let's apply this function  to our new hour  column and make our brand new column,\n",
      "when_is_it :\n",
      "bikes['when_is_it'] = bikes['hour'] 146\n",
      "apply(when_is_it)\n",
      "bikes[['when_is_it', 'above_average']] 17\n",
      "head()\n",
      "Following is the table:\n",
      "Let's try to use only this new column to determine whether or not the hourly bike rental\n",
      "count will be above average 31\n",
      "Before we do, let's do the basics of exploratory data analysis\n",
      "and make a graph to see if we can visualize a difference between the four times of the day 34\n",
      "\n",
      "Our graph will be a bar chart with one bar per time of the day 16\n",
      "Each bar will represent the\n",
      "percentage of times that this time of the day had a greater than normal bike rental:\n",
      "bikes 25\n",
      "groupby('when_is_it') 7\n",
      "above_average 2\n",
      "mean() 2\n",
      "plot(kind='bar')\n",
      "\n",
      "Following is the output:\n",
      "We can see that there is a pretty big difference 20\n",
      "For example, when it is off hours, the chance\n",
      "of having more than average bike rentals is about 25%, whereas during rush hour, the\n",
      "chance of being above average is over 80% 42\n",
      "Okay, this is exciting, but let's use some built-in\n",
      "pandas tools to extract dummy columns, as follows:\n",
      "when_dummies = pd 30\n",
      "get_dummies(bikes['when_is_it'], prefix='when__')\n",
      "when_dummies 18\n",
      "head()\n",
      "\n",
      "Following is the output:\n",
      "when_dummies = when_dummies 14\n",
      "iloc[:, 1:]\n",
      "# remove the first column\n",
      "when_dummies 15\n",
      "head()\n",
      "Following is the output:\n",
      "Great 8\n",
      "Now we have a DataFrame full of numbers that we can plug in to our logistic\n",
      "regression:\n",
      "X = when_dummies\n",
      "# our new X is our dummy variables\n",
      "y = bikes 38\n",
      "above_average\n",
      "logreg = LogisticRegression()\n",
      "# instantiate our model\n",
      "logreg 16\n",
      "fit(X_train, y_train)\n",
      "# fit our model to our training set\n",
      "logreg 18\n",
      "score(X_test, y_test)\n",
      "# score it on our test set to get a better sense of out of sample\n",
      "\n",
      "performance\n",
      "# 0 29\n",
      "685157\n",
      "This is even better than just using the temperature 13\n",
      "What if we tacked temperature and\n",
      "humidity onto that 11\n",
      "So, now we are using the temperature, humidity, and our time of day\n",
      "dummy variables to predict whether or not we will have higher than average bike rentals:\n",
      "new_bike = pd 38\n",
      "concat([bikes[['temp', 'humidity']], when_dummies], axis=1) #\n",
      "combine temperature, humidity, and the dummy variables\n",
      "X = new_bike # our new X is our dummy variables\n",
      "y = bikes 46\n",
      "above_average\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "logreg = LogisticRegression() # instantiate our model\n",
      "logreg 35\n",
      "fit(X_train, y_train) # fit our model to our training set\n",
      "logreg 18\n",
      "score(X_test, y_test) # score it on our test set to get a better\n",
      "sense of out of sample performance\n",
      "# 0 29\n",
      "75165\n",
      "Wow 5\n",
      "Okay, let's quit while we're ahead 9\n",
      "\n",
      "Summary\n",
      "In this chapter, we looked at machine learning and its different subcategories 17\n",
      "We explored\n",
      "supervised, unsupervised, and reinforcement learning strategies and looked at situations\n",
      "where each one would come in handy 26\n",
      "\n",
      "Looking into linear regression, we were able to find relationships between predictors and a\n",
      "continuous response variable 20\n",
      "Through the train/test split, we were able to help avoid\n",
      "overfitting our machine learning models and get a more generalized prediction 26\n",
      "We were\n",
      "able to use metrics, such as the root mean squared error, to evaluate our models as well 22\n",
      "\n",
      "\n",
      "By extending our notion of linear regression into logistic regression, we were able to then\n",
      "find association between the same predictors, but now to categorical responses 30\n",
      "By\n",
      "introducing dummy variables into the mix, we were able to add categorical features to our\n",
      "models and improve our performance even further 27\n",
      "\n",
      "In the next few chapters, we will be taking a much deeper dive into many more machine\n",
      "learning models and, along the way, we will learn new metrics, new validation techniques,\n",
      "and more importantly, new ways of applying our data science to the world 52\n",
      "\n",
      "\n",
      "1\n",
      "Predictions Don't Grow on\n",
      "Trees – or Do They 15\n",
      "\n",
      "Our goal in this chapter is to see and apply concepts learned from previous chapters in\n",
      "order to construct and use modern learning algorithms in order to glean insights and make\n",
      "predictions on real datasets 38\n",
      "While we explore the following algorithms, we should always\n",
      "remember that we are constantly keeping our metrics in mind 21\n",
      "\n",
      "In this chapter , we will be looking at the following machine learning algorithms:\n",
      "Decision trees\n",
      "Naive Bayes classification\n",
      "k-means clustering  \n",
      "The first two are examples of supervised learning, while the final algorithm is an example\n",
      "of unsupervised learning 52\n",
      "\n",
      "Let's get to it 6\n",
      "\n",
      "Naive  Bayes classification\n",
      "Let's get right into it 14\n",
      "Let's begin with Naive  Bayes classification 10\n",
      "This machine learning\n",
      "model relies heavily on results from previous chapters, specifically with Bayes' theorem:\n",
      "\n",
      "\n",
      "Let's look a little closer at the specific features of this formula:\n",
      "P(H)  is the probability of the hypothesis before we observe  the data, called the\n",
      "prior probability , or just prior\n",
      "P(H|D)  is what we want to compute, the probability of the hypothesis after we\n",
      "observe the data, called the posterior\n",
      "P(D|H)  is the probability of the data under the given  hypothesis, called the\n",
      "likelihood\n",
      "P(D)  is the probability of the data under  any hypothesis, called the normalizing\n",
      "constant\n",
      "Naive Bayes classification is a classification model, and therefore a supervised model 150\n",
      "\n",
      "Given this, what kind of data do we need 11\n",
      "\n",
      "Labeled data\n",
      "Unlabeled data\n",
      "(Insert Jeopardy music here )\n",
      "If you answered labeled data , then you're well on your way to becoming a data scientist 36\n",
      "\n",
      "Suppose we have a dataset with n features, ( x1, x2, …, xn) and a class label, C 28\n",
      "For\n",
      "example, let's take some data involving spam text classification 13\n",
      "Our data would consist of\n",
      "rows of individual text samples and columns of both our features and our class labels 21\n",
      "Our\n",
      "features would be words and phrases that are contained within the text samples and our\n",
      "class labels are simply spam  or not spam 27\n",
      "In this scenario, I will replace the not spam\n",
      "class with an easier-to-say word, ham:\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "df =\n",
      "pd 33\n",
      "read_table('https://raw 6\n",
      "githubusercontent 2\n",
      "com/sinanuozdemir/sfdat22/mast\n",
      "er/data/sms 18\n",
      "tsv',\n",
      "                   sep='\\t', header=None, names=['label', 'msg'])\n",
      "df\n",
      "\n",
      "Here is a sample of text data in a row column format:\n",
      "Let's do some preliminary statistics to see what we are dealing with 46\n",
      "Let's see the difference\n",
      "in the number of ham and spam messages at our disposal:\n",
      "df 19\n",
      "label 1\n",
      "value_counts() 3\n",
      "plot(kind=\"bar\")\n",
      "This gives us a bar chart, as follows:\n",
      "\n",
      "\n",
      "So, we have way more ham messages than we do spam 27\n",
      "Because this is a classification\n",
      "problem, it will be very useful to know our null accuracy rate , which is the percentage\n",
      "chance of predicting a single row correctly if we keep guessing the most common class,\n",
      "ham:\n",
      "df 45\n",
      "label 1\n",
      "value_counts() / df 5\n",
      "shape[0]\n",
      "ham     0 8\n",
      "865937\n",
      "spam    0 8\n",
      "134063\n",
      "So if we blindly guessed ham, we would be correct about 87% of the time, but we can do\n",
      "better than that 31\n",
      "If we have a set of classes, C, and features, xi, then we can use Bayes'\n",
      "theorem to predict the probability that a single row belongs to class C, using the following\n",
      "formula:\n",
      "Let's look at this formula in a little more detail:\n",
      "P(class C | {xi}) : The posterior probability is the probability that the row belongs\n",
      "to class C  given the features {xi} 83\n",
      "\n",
      "P({xi} | class C) : This is the likelihood that we would observe these features given\n",
      "that the row was in class C 29\n",
      "\n",
      "P(class C) : This is the prior probability 11\n",
      "It is the probability that the data point\n",
      "belongs to class C  before we see any data 19\n",
      "\n",
      "P({xi}) : This is our normalization constant 11\n",
      "\n",
      "For example, imagine we have an email with three words: send cash now 16\n",
      "We'll use\n",
      "Naive Bayes to classify the email as either being spam or ham:\n",
      "We are concerned with the difference of these two numbers 29\n",
      "We can use the following\n",
      "criteria to classify any single text sample:\n",
      "If P(spam | send cash now)  is larger than P(ham | send cash now) ,\n",
      "then we will classify the text as spam\n",
      "\n",
      "If P(ham | send cash now)  is larger than P(spam | send cash now) , then\n",
      "we will label the text ham\n",
      "Because both equations have P (send money now)  in the denominator, we can ignore them 95\n",
      "\n",
      "So, now we are concerned with the following:\n",
      "Let's figure out the numbers  in this equation:\n",
      "P(spam) = 0 29\n",
      "134063\n",
      "P(ham) = 0 11\n",
      "865937\n",
      "P(send cash now | spam)\n",
      "P(send cash now | ham)\n",
      "The final two likelihoods might seem like they would not be so difficult to calculate 34\n",
      "All we\n",
      "have to do is count the numbers of spam messages that include the send money\n",
      "now phrase and divide that by the total number of spam messages:\n",
      "df 33\n",
      "msg = df 3\n",
      "msg 1\n",
      "apply(lambda x:x 4\n",
      "lower())\n",
      "# make all strings lower case so we can search easier\n",
      "df[df 16\n",
      "msg 1\n",
      "str 1\n",
      "contains('send cash now')] 6\n",
      "shape\n",
      "(0, 2)\n",
      "Oh no 10\n",
      "There are none 3\n",
      "There are literally zero texts with the exact phrase send cash now 12\n",
      "\n",
      "The hidden problem here is that this phrase is very specific and we can't assume that we\n",
      "will have enough data in the world to have seen this exact phrase many times before 36\n",
      "\n",
      "Instead, we can make a naïve assumption  in our Bayes' theorem 17\n",
      "If we assume that the\n",
      "features (words) are conditionally independent (meaning that no word affects the existence\n",
      "of another word), then we can rewrite the formula:\n",
      "spams = df[df 39\n",
      "label == 'spam']\n",
      "for word in ['send', 'cash', 'now']:\n",
      "    print( word, spams[spams 27\n",
      "msg 1\n",
      "str 1\n",
      "contains(word)] 3\n",
      "shape[0] /\n",
      "float(spams 8\n",
      "shape[0]))\n",
      "\n",
      "P(send|spam) = 0 12\n",
      "096\n",
      "P(cash |spam) = 0 12\n",
      "091\n",
      "P(now|spam) = 0 11\n",
      "280\n",
      "With this, we can calculate the following:\n",
      "P(send cash now| spam) ∗ P(spam)= ( 26\n",
      "096 ∗ 4\n",
      "091∗ 4\n",
      "280) ∗ 5\n",
      "134 = 0 5\n",
      "00032\n",
      "Repeating the same procedure for ham gives us the following:\n",
      "P(send|ham) = 0 24\n",
      "03\n",
      "P(cash|ham) = 0 12\n",
      "003\n",
      "P(now|ham) = 0 11\n",
      "109\n",
      "The fact that these numbers are both very low is not as important as the fact that the spam\n",
      "probability is much larger than the ham calculation 31\n",
      "If we calculate 0 5\n",
      "00032 / 0 6\n",
      "0000084 =\n",
      "38 6\n",
      "1 we see that the send cash now  probability for spam is 38 times higher than for spam 21\n",
      "\n",
      "Doing this means that we can classify send cash now  as spam 14\n",
      "Simple, right 3\n",
      "\n",
      "Let's use Python to implement a Naive Bayes classifier without having to do all of these\n",
      "calculations ourselves 24\n",
      "\n",
      "First, let's revisit the count vectorizer in scikit-learn, which turns text into numerical data\n",
      "for us 25\n",
      "Let's assume that we will train on three documents (sentences):\n",
      "# simple count vectorizer example\n",
      "from sklearn 23\n",
      "feature_extraction 2\n",
      "text import CountVectorizer\n",
      "# start with a simple example\n",
      "train_simple = ['call you tonight',\n",
      "                'Call me a cab',\n",
      "                'please call me 32\n",
      " 1\n",
      " 1\n",
      "PLEASE 44 3\n",
      "\n",
      "# learn the 'vocabulary' of the training data\n",
      "vect = CountVectorizer()\n",
      "train_simple_dtm = vect 24\n",
      "fit_transform(train_simple)\n",
      "pd 6\n",
      "DataFrame(train_simple_dtm 5\n",
      "toarray(), columns=vect 6\n",
      "get_feature_names())\n",
      "\n",
      "\n",
      "Note that each row represents one of the three documents (sentences), each column\n",
      "represents one of the words present in the documents, and each cell contains the number of\n",
      "times each word appears in each document 47\n",
      "\n",
      "We can then use the count vectorizer to transform new incoming test documents to\n",
      "conform with our training set (the three sentences):\n",
      "# transform testing data into a document-term matrix (using existing\n",
      "vocabulary, notice don't is missing)\n",
      "test_simple = [\"please don't call me\"]\n",
      "test_simple_dtm = vect 66\n",
      "transform(test_simple)\n",
      "test_simple_dtm 8\n",
      "toarray()\n",
      "pd 4\n",
      "DataFrame(test_simple_dtm 5\n",
      "toarray(), columns=vect 6\n",
      "get_feature_names())\n",
      "Note how, in our test sentence, we had a new word, namely don't 21\n",
      "When we vectorized it,\n",
      "because we hadn't seen that word previously in our training data, the vectorizer simply\n",
      "ignored it 26\n",
      "This is important and incentivizes data scientists to obtain as much data as\n",
      "possible for their training sets 20\n",
      "\n",
      "Now, let's do this for our actual data:\n",
      "# split into training and testing sets\n",
      "from sklearn 22\n",
      "cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(df 23\n",
      "msg, df 3\n",
      "label,\n",
      "random_state=1)\n",
      "# instantiate the vectorizer\n",
      "vect = CountVectorizer()\n",
      "# learn vocabulary and create document-term matrix in a single step\n",
      "train_dtm = vect 36\n",
      "fit_transform(X_train)\n",
      "train_dtm\n",
      "The following is the output: \n",
      "<4179x7456 sparse matrix of type '<class 'numpy 30\n",
      "int64'>'\n",
      " with 55209 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "Note that the format is in a sparse matrix, meaning the matrix is so large and full of zeroes 37\n",
      "\n",
      "There is a special format to deal with objects such as this 13\n",
      "Take a look  at the number of\n",
      "columns 10\n",
      "\n",
      "There are 7,456 words 8\n",
      "\n",
      "This means that in our training set, there are 7,456 unique words to look at 20\n",
      "We can now\n",
      "transform our test data to conform to our vocabulary:\n",
      "# transform testing data into a document-term matrix\n",
      "test_dtm = vect 29\n",
      "transform(X_test)\n",
      "test_dtm\n",
      "The output is as follows:\n",
      "<1393x7456 sparse matrix of type '<class 'numpy 28\n",
      "int64'>'\n",
      "        with 17604 stored elements in Compressed Sparse Row format>\n",
      "Note that we have the same exact number of columns because it is conforming to our test\n",
      "set to be exactly the same vocabulary as before 46\n",
      "No more, no less 5\n",
      "\n",
      "Now let's build a Naive Bayes model (similar to the linear regression process):\n",
      "## MODEL BUILDING WITH NAIVE BAYES\n",
      "# train a Naive Bayes model using train_dtm\n",
      "from sklearn 45\n",
      "naive_bayes import MultinomialNB\n",
      "# import our model\n",
      "nb = MultinomialNB()\n",
      "# instantiate our model\n",
      "nb 28\n",
      "fit(train_dtm, y_train)\n",
      "# fit it to our training set\n",
      "Now the nb variable holds our fitted model 24\n",
      "The training phase of the model involves\n",
      "computing the likelihood function, which is the conditional probability of each feature\n",
      "given each class:\n",
      "# make predictions on test data using test_dtm\n",
      "preds = nb 42\n",
      "predict(test_dtm)\n",
      "preds\n",
      "The output is as follows:\n",
      "array(['ham', 'ham', 'ham', 24\n",
      " 1\n",
      " 1\n",
      ", 'ham', 'spam', 'ham'],\n",
      "      dtype='|S4')\n",
      "\n",
      "The prediction phase of the model involves computing the posterior probability of each\n",
      "class given the observed features, and choosing the class with the highest probability 45\n",
      "\n",
      "We will use sklearn's built-in accuracy and confusion matrix to look at how well our Naive\n",
      "Bayes models are performing:\n",
      "# compare predictions to true labels\n",
      "from sklearn import metrics\n",
      "print metrics 41\n",
      "accuracy_score(y_test, preds)\n",
      "print metrics 9\n",
      "confusion_matrix(y_test, preds)\n",
      "The output is as follows:\n",
      "accuracy == 0 17\n",
      "988513998564\n",
      "confusion matrix ==\n",
      "[[1203    5]\n",
      " [  11  174]]\n",
      "First off, our accuracy is great 32\n",
      "Compared to our null accuracy, which was 87%, 99% is a\n",
      "fantastic improvement 20\n",
      "\n",
      "Now to our confusion matrix 6\n",
      "From before, we know that each row represents actual values\n",
      "while columns represent predicted values, so the top left value, 1,203, represents our true\n",
      "negatives 35\n",
      "But what is negative and positive 6\n",
      "We gave the model the spam  and ham strings\n",
      "as our classes, not positive and negative 19\n",
      "\n",
      "We can use the following:\n",
      "nb 8\n",
      "classes_\n",
      "The output is as follows:\n",
      "array(['ham', 'spam'])\n",
      "We can then line up the indices so that 1,203 refers to true ham predictions and 174 refers to\n",
      "true spam  predictions 43\n",
      "\n",
      "There were also five false spam classifications , meaning that five messages were predicted as\n",
      "spam , but were actually ham, as well as 11 false ham classifications 32\n",
      "\n",
      "In summary, Naive Bayes classification uses  Bayes' theorem in order to fit posterior\n",
      "probabilities of classes so that data points are correctly labeled as belonging to the proper\n",
      "class 39\n",
      "\n",
      "\n",
      "Decision trees\n",
      "Decision trees are supervised models  that can either perform regression or classification 17\n",
      "\n",
      "Let's take a look at some major league baseball player data from 1986-1987 20\n",
      "Each dot\n",
      "represents a single player in the league:\n",
      "Years ( x-axis) : Number of years played in the major leagues\n",
      "Hits ( y-axis) : Number of hits the player had in the previous year\n",
      "Salary (color) : Low salary is blue/green, high salary is red/yellow\n",
      "The preceding data is our training data 70\n",
      "The idea is to build a model that predicts the salary\n",
      "of future players based on Years  and Hits 21\n",
      "A decision tree aims to make splits  on our data in\n",
      "order to segment the data points that act similarly to each other, but differently to the\n",
      "others 32\n",
      "The tree makes multiples of these splits in order to make the most accurate\n",
      "prediction possible 17\n",
      "Let's see a tree built for the preceding data:\n",
      "\n",
      "\n",
      "Let's read this from top to bottom:\n",
      "The first split is Years < 4 28\n",
      "5 : when a splitting rule is true , you follow the left\n",
      "branch 16\n",
      "When a splitting rule is false , you follow the right branch 12\n",
      "So for a new\n",
      "player, if they have been playing for less than 4 17\n",
      "5 years, we will go down the left\n",
      "branch 12\n",
      "\n",
      "For players in the left branch, the mean salary is $166,000, hence you label it with\n",
      "that value (salary has been divided by 1,000 and log-transformed to 5 42\n",
      "11 for ease\n",
      "of computation) 8\n",
      "\n",
      "For players in the right branch, there is a further split on Hits < 117 18\n",
      "5 , dividing\n",
      "players into two more salary regions: $403,000 (transformed to 6 22\n",
      "00) and $846,000\n",
      "(transformed to 6 14\n",
      "74) 3\n",
      "\n",
      "This tree doesn't just give us predictions; it also provides some more information about our\n",
      "data:\n",
      "It seems that the number of years in the league is the most important factor in\n",
      "determining salary, with a smaller number of years correlating to a lower salary 55\n",
      "\n",
      "If a player has not been playing for long (< 4 13\n",
      "5 years), the number of hits they\n",
      "have is not an important factor when it comes to their salary 22\n",
      "\n",
      "\n",
      "For players with 5+ years under their  belt, hits are an important factor for their\n",
      "salary determination 23\n",
      "\n",
      "Our tree only made up to two decisions before spitting out an answer (two is\n",
      "called our depth of the tree) 26\n",
      "\n",
      "How does a computer build a regression tree 9\n",
      "\n",
      "Modern decision  tree algorithms tend to use a recursive  binary splitting approach:\n",
      "The process begins at the top of the tree 25\n",
      "1 2\n",
      "\n",
      "For every feature, it will examine every possible split and choose the feature and2 17\n",
      "\n",
      "split such that the resulting tree has the lowest possible Mean Squared Error\n",
      "(MSE ) 19\n",
      "The algorithm  makes that split 6\n",
      "\n",
      "It will then examine the two resulting regions and again make a single split (in3 18\n",
      "\n",
      "one of the regions) to minimize the MSE 10\n",
      "\n",
      "It will keep repeating Step 3  until a stopping criterion is met: 4 18\n",
      "\n",
      "Maximum tree depth (maximum number of splits required to arrive at\n",
      "a leaf)\n",
      "Minimum number of observations in a leaf (final) node\n",
      "For classification trees, the algorithm is very similar with the biggest difference being the\n",
      "metric we optimize over 49\n",
      "Because MSE only exists for regression problems, we cannot use\n",
      "it 13\n",
      "However, instead of accuracy, classification trees optimize over either the Gini index  or\n",
      "entropy 19\n",
      "\n",
      "How does a computer fit a classification tree 9\n",
      "\n",
      "Similarly to a regression tree, a classification  tree is built by optimizing over  a metric (in\n",
      "this case, the Gini index) and choosing the best split to make this optimization 39\n",
      "More\n",
      "formally, at each node, the tree will take the following steps:\n",
      "Calculate the purity of the data1 24\n",
      "\n",
      "Select a candidate split2 6\n",
      "\n",
      "Calculate the purity of the data after the split3 11\n",
      "\n",
      "Repeat for all variables4 6\n",
      "\n",
      "Choose the variable with the greatest increase in purity5 11\n",
      "\n",
      "Repeat for each split until some stop criteria is met6 12\n",
      "\n",
      "\n",
      "Let's say that we are predicting the likelihood of death aboard a luxury cruise ship given\n",
      "demographic features 22\n",
      "Suppose we start with 25 people, 10 of whom survived, and 15 of\n",
      "whom died:\n",
      "Before split All\n",
      "Survived 10\n",
      "Died 15\n",
      "We first calculate the Gini index before doing anything:\n",
      "In this example, overall classes are survived  and died, illustrated in the following formula:\n",
      "This means that the purity of the dataset is 0 77\n",
      "48 2\n",
      "\n",
      "Now let's consider a potential split on gender 10\n",
      "We first calculate the Gini index for each\n",
      "gender:\n",
      "\n",
      "\n",
      "The following formula calculates Gini index for male and female as follows:\n",
      "Once we have the Gini index for each gender, we then calculate the overall Gini index for\n",
      "the split on gender, as follows:\n",
      "So, the gini coefficient for splitting  on gender is 0 69\n",
      "27 2\n",
      "We then follow this procedure  for\n",
      "three potential splits:\n",
      "Gender (male or female)\n",
      "Number of siblings on board (0 or 1+)\n",
      "Class (first and second versus third)\n",
      "In this example, we would choose the gender to split on as it is the lowest Gini index 58\n",
      "\n",
      "\n",
      "The following table briefly summarizes the differences between classification and\n",
      "regression decision trees:\n",
      "Regression trees Classification trees\n",
      "Predict a quantitative response Predict a qualitative response\n",
      "Prediction is the average value in each\n",
      "leafPrediction is the most common label in each leaf\n",
      "Splits are chosen to minimize MSESplits are chosen to minimize Gini index\n",
      "(usually)\n",
      "Let's use scikit-learn's built-in decision tree function in order to build a decision tree:\n",
      "# read in the data\n",
      "titanic = pd 102\n",
      "read_csv('short_titanic 7\n",
      "csv')\n",
      "# encode female as 0 and male as 1\n",
      "titanic['Sex'] = titanic 23\n",
      "Sex 1\n",
      "map({'female':0, 'male':1})\n",
      "# fill in the missing values for age with the median age\n",
      "titanic 27\n",
      "Age 1\n",
      "fillna(titanic 5\n",
      "Age 1\n",
      "median(), inplace=True)\n",
      "# create a DataFrame of dummy variables for Embarked\n",
      "embarked_dummies = pd 22\n",
      "get_dummies(titanic 6\n",
      "Embarked, prefix='Embarked')\n",
      "embarked_dummies 12\n",
      "drop(embarked_dummies 6\n",
      "columns[0], axis=1, inplace=True)\n",
      "# concatenate the original DataFrame and the dummy DataFrame\n",
      "titanic = pd 26\n",
      "concat([titanic, embarked_dummies], axis=1)\n",
      "# define X and y\n",
      "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
      "X = titanic[feature_cols]\n",
      "y = titanic 55\n",
      "Survived\n",
      "X 4\n",
      "head()\n",
      "\n",
      "\n",
      "Note that we are going to use class, sex, age, and dummy variables for city embarked as our\n",
      "features:\n",
      "# fit a classification tree with max_depth=3 on all data from sklearn 41\n",
      "tree\n",
      "import DecisionTreeClassifier treeclf = DecisionTreeClassifier(max_depth=3,\n",
      "random_state=1) treeclf 24\n",
      "fit(X, y)\n",
      "max_depth  is a limit to the depth of our tree 17\n",
      "It means that, for any data point, our tree is\n",
      "only able to ask up to three questions and make up to three  splits 28\n",
      "We can output our tree\n",
      "into a visual  format and we will obtain the following:\n",
      "We can notice a few things:\n",
      "Sex is the first split, meaning that sex is the most important determining factor of\n",
      "whether or not a person survived the crash\n",
      "Embarked_Q  was never used in any split\n",
      "For either classification or regression trees, we can also do something very interesting with\n",
      "decision trees, which is that we can output a number that represents each feature's\n",
      "importance in the prediction of our data points:\n",
      "# compute the feature importances\n",
      "pd 113\n",
      "DataFrame({'feature':feature_cols,\n",
      "'importance':treeclf 13\n",
      "feature_importances_})\n",
      "\n",
      "\n",
      "The importance scores are an average  Gini index difference for each variable, with higher \n",
      "values  corresponding to higher importance  to the prediction 33\n",
      "We can use this information to\n",
      "select fewer features in the future 13\n",
      "For example, both of the embarked variables are very\n",
      "low in comparison to the rest of the features, so we may be able to say that they are not\n",
      "important in our prediction of life or death 41\n",
      "\n",
      "Unsupervised learning\n",
      "It's time to see some examples of unsupervised  learning, given that we spend a majority of\n",
      "this book on supervised learning models 34\n",
      "\n",
      "When to use unsupervised learning\n",
      "There are many times when unsupervised learning  can be appropriate 22\n",
      "Some very common\n",
      "examples include the following:\n",
      "There is no clear response  variable 16\n",
      "There is nothing that we are explicitly trying\n",
      "to predict or correlate to other variables 16\n",
      "\n",
      "To extract structure from data where no apparent structure/patterns exist (can be\n",
      "a supervised learning problem) 23\n",
      "\n",
      "When an unsupervised concept called feature extraction is used 12\n",
      "Feature\n",
      "extraction is the process of creating new features from existing ones 14\n",
      "These new\n",
      "features can be even stronger than the original features 12\n",
      "\n",
      "\n",
      "The first tends to be the most common reason that data scientists choose to use\n",
      "unsupervised learning 21\n",
      "This case arises frequently when we are working with data and we\n",
      "are not explicitly trying to predict any of the columns and we merely wish to find patterns\n",
      "of similar (and dissimilar) groups of points 41\n",
      "The second option comes into play even if we\n",
      "are explicitly attempting to use a supervised model to predict a response variable 23\n",
      "\n",
      "Sometimes, simple EDA might not produce any clear patterns in the data in the few\n",
      "dimensions that humans can imagine, whereas a machine might pick up on data points\n",
      "behaving similarly to each other in greater dimensions 44\n",
      "\n",
      "The third common reason to use unsupervised learning is to extract new features from\n",
      "features that already exist 22\n",
      "This process (lovingly called feature extraction ) might produce\n",
      "features that can be used in a future supervised model or that can be used for presentation\n",
      "purposes (marketing or otherwise) 38\n",
      "\n",
      "k-means clustering\n",
      "k-means clustering is our first example  of an unsupervised machine learning model 23\n",
      "\n",
      "Remember this means that we are not making predictions; we are trying instead to extract\n",
      "structure from seemingly unstructured data 24\n",
      "\n",
      "Clustering is a family of unsupervised machine learning models that attempt to group data\n",
      "points into clusters  with centroids 25\n",
      "\n",
      "Definition\n",
      "Cluster : This is a group of data points that behave similarly 15\n",
      "\n",
      "Centroid : This is the center  of a cluster 12\n",
      "It can be thought of as an average\n",
      "point in the cluster 13\n",
      "\n",
      "The preceding definition can be quite vague, but it becomes specific when narrowed down\n",
      "to specific domains 20\n",
      "For example, online shoppers who behave similarly might shop for\n",
      "similar things or at similar shops, whereas similar software companies might make\n",
      "comparable software at comparable prices 32\n",
      "\n",
      "\n",
      "Here is a visualization of clusters of points:\n",
      "In the preceding diagram, our human brains can very easily see the difference between the\n",
      "four clusters 29\n",
      "We can see that the red cluster is at the bottom-left of the graph while the\n",
      "green cluster lives in the bottom-right portion of the graph 29\n",
      "This means that the red data\n",
      "points are similar to each other, but not similar to data points in the other clusters 24\n",
      "\n",
      "We can also see the centroids of each cluster as the square in each color 16\n",
      "Note that the\n",
      "centroid is not an actual data point, but is merely an abstraction of a cluster that represents\n",
      "the center of the cluster 28\n",
      "\n",
      "The concept of similarity  is central to the definition of a cluster, and therefore to cluster\n",
      "analysis 21\n",
      " ",
      "In general, greater similarity between points leads to better clustering 14\n",
      "In most\n",
      "cases, we turn data into points in n-dimensional space and use the distance between these\n",
      "points as a form of similarity 27\n",
      "The centroid of the cluster then is usually the average of each\n",
      "dimension (column) for each data point in each cluster 24\n",
      "So, for example, the centroid of the\n",
      "red cluster is the result of taking the average value of each column of each red data point 28\n",
      "\n",
      "\n",
      "The purpose of cluster analysis is to enhance our understanding of a dataset by dividing\n",
      "the data into groups 21\n",
      "Clustering provides a layer of abstraction from individual data points 11\n",
      "\n",
      "The goal is to extract and enhance the natural structure of the data 14\n",
      "There are many kinds of\n",
      "classification procedures 8\n",
      "For our class, we will be focusing on k-means clustering, which is\n",
      "one of the most popular clustering algorithms 24\n",
      "\n",
      "k-means is an iterative method that partitions  a dataset into k clusters 16\n",
      "It works in four\n",
      "steps:\n",
      "Choose k initial centroids (note that k is an input) 1 21\n",
      "\n",
      "For each point, assign the point to the nearest centroid2 13\n",
      "\n",
      "Recalculate the centroid positions3 7\n",
      "\n",
      "Repeat Step 2  and Step 3  until stopping criteria is met 4 18\n",
      "\n",
      "Illustrative example – data points\n",
      "Imagine that we have the following  data points in a two-dimensional space:\n",
      "Each dot is colored gray to assume no prior grouping before applying the k-means\n",
      "algorithm 41\n",
      "The goal here is to eventually color in each dot and create groupings (clusters),\n",
      "as illustrated in the following plot:\n",
      "\n",
      "\n",
      "Here, Step 1  has been applied 33\n",
      "We have (randomly) chosen three centroids (red, blue, and\n",
      "yellow) 18\n",
      "\n",
      "Most k-means algorithms place random initial centroids, but there exist\n",
      "other pre-compute methods to place initial centroids 24\n",
      "For now, random is\n",
      "fine 7\n",
      "\n",
      "\n",
      "\n",
      "The first part of Step 2  has been applied 12\n",
      "For each data point, we found the most similar\n",
      "centroid (closest):\n",
      "The second part of Step 2  has been applied here 27\n",
      "We have colored in each data point in\n",
      "accordance with its most similar centroid:\n",
      "This is Step 3  and the crux of k-means 32\n",
      "Note that we have physically moved the centroids to\n",
      "be the actual center of each cluster 17\n",
      "We have, for each color, computed the average  point\n",
      "and made that point the new centroid 20\n",
      "For example, suppose the three red data points had\n",
      "the coordinates: ( 1, 3 ), (2, 5 ), and (3, 4 ) 34\n",
      "The center ( red cross ) would be calculated as\n",
      "follows:\n",
      "# centroid calculation\n",
      "import numpy as np\n",
      "red_point1 = np 28\n",
      "array([1, 3])\n",
      "red_point2 = np 12\n",
      "array([2, 5])\n",
      "red_point3 = np 12\n",
      "array([3, 4])\n",
      "\n",
      "red_center = (red_point1 + red_point2 + red_point3) / 3 26\n",
      "\n",
      "red_center\n",
      "# array([ 2 9\n",
      ",  4 4\n",
      "\n",
      "That is, the ( 2, 4 ) point would be the coordinates of the preceding red cross 22\n",
      "\n",
      "None of the actual data points will ever move 10\n",
      "They cannot 2\n",
      "The only\n",
      "entities that move are the centroids, which are not actual data points 16\n",
      "\n",
      "We continue with our algorithm by repeating Step 2 11\n",
      "Here is the first part where we find the\n",
      "closest center for each point 15\n",
      "Note a big change: the point that is circled in the following\n",
      "figure used to be a yellow point, but has changed to be a red cluster point because the\n",
      "yellow cluster moved closer to its yellow constituents:\n",
      "\n",
      "\n",
      "It might help to think of points as being planets in space with\n",
      "gravitational pull 61\n",
      "Each centroid is pulled by the planets' gravity 9\n",
      "\n",
      "Here is the second part of Step 2  again 12\n",
      "We have assigned each point to the color of the\n",
      "closest cluster:\n",
      "Here, we recalculate once more the centroids for each cluster ( Step 3 ) 31\n",
      "Note that the blue\n",
      "center did not move at all, while the yellow and red centers both moved 20\n",
      "\n",
      "Because we have reached a stopping  criterion (clusters do not move if we repeat Step 2  and\n",
      "Step 3 ), we finalize our algorithm and we have our three clusters, which is the final result of\n",
      "the k-means algorithm:\n",
      "\n",
      "\n",
      "Illustrative example – beer 56\n",
      "\n",
      "Enough data  science, beer 7\n",
      "\n",
      "OK, OK, settle down 7\n",
      "It's a long book; let's grab a beer 11\n",
      "On that note, did you know there\n",
      "are many types of beer 14\n",
      "I wonder if we could possibly group beers into different categories\n",
      "based on different quantitative features … Let's try 21\n",
      "See the following:\n",
      "# import the beer dataset\n",
      "url = ' 13\n",
      " 1\n",
      "/data/beer 4\n",
      "txt'\n",
      "beer = pd 5\n",
      "read_csv(url, sep=' ')\n",
      "print beer 9\n",
      "shape\n",
      "The output is as follows:\n",
      "(20, 5)\n",
      "beer 15\n",
      "head()\n",
      "\n",
      "\n",
      "Here we have 20 beers with five columns: name , calories , sodium , alcohol , and cost 22\n",
      "In\n",
      "clustering (like almost all machine learning models), we like quantitative features, so we\n",
      "will ignore the name of the beer in our clustering:\n",
      "# define X\n",
      "X = beer 38\n",
      "drop('name', axis=1)\n",
      "Now we will perform k-means using scikit-learn:\n",
      "# K-means with 3 clusters\n",
      "from sklearn 32\n",
      "cluster import KMeans\n",
      "km = KMeans(n_clusters=3, random_state=1)\n",
      "km 20\n",
      "fit(X)\n",
      "n_clusters  is our k 9\n",
      "It is our input number of clusters 7\n",
      "random_state  as\n",
      "always produces reproducible results for educational purposes 13\n",
      "Using\n",
      "three clusters for now is random 8\n",
      "\n",
      "Our k-means algorithm has run the algorithm on our data points and come up with three\n",
      "clusters:\n",
      "# save the cluster labels and sort by cluster\n",
      "beer['cluster'] = km 38\n",
      "labels_\n",
      "We can take a look at the center of each cluster by using a groupby  and mean  statement:\n",
      "# calculate the mean of each feature for each cluster\n",
      "beer 36\n",
      "groupby('cluster') 5\n",
      "mean()\n",
      "On human inspection, we can see that cluster 0 has, on average, a higher calorie, sodium,\n",
      "and alcohol content and costs more 30\n",
      "These might be considered heavier beers 6\n",
      "Cluster 2 has\n",
      "on average a very low alcohol content and very few calories 16\n",
      "These are probably light beers 5\n",
      "\n",
      "Cluster 1 is somewhere in the middle 9\n",
      "\n",
      "\n",
      "Let's use Python to make  a graph to see this in more detail:\n",
      "import matplotlib 19\n",
      "pyplot as plt\n",
      "%matplotlib inline\n",
      "# save the DataFrame of cluster centers\n",
      "centers = beer 19\n",
      "groupby('cluster') 5\n",
      "mean()\n",
      "# create a \"colors\" array for plotting\n",
      "colors = np 15\n",
      "array(['red', 'green', 'blue', 'yellow'])\n",
      "# scatter plot of calories versus alcohol, colored by cluster (0=red,\n",
      "1=green, 2=blue)\n",
      "plt 39\n",
      "scatter(beer 3\n",
      "calories, beer 3\n",
      "alcohol, c=colors[list(beer 8\n",
      "cluster)],\n",
      "s=50)\n",
      "# cluster centers, marked by \"+\"\n",
      "plt 15\n",
      "scatter(centers 3\n",
      "calories, centers 3\n",
      "alcohol, linewidths=3, marker='+',\n",
      "s=300, c='black')\n",
      "# add labels\n",
      "plt 23\n",
      "xlabel('calories')\n",
      "plt 6\n",
      "ylabel('alcohol')\n",
      "A big part of unsupervised learning is human inspection 16\n",
      "Clustering has\n",
      "no context of the problem domain and can only tell us the clusters it found\n",
      "it cannot tell us what the clusters mean 28\n",
      "\n",
      "\n",
      "\n",
      "Choosing an optimal number for K and\n",
      "cluster validation\n",
      "A big part of k-means clustering is knowing the optimal number of clusters 27\n",
      "If we knew this\n",
      "number ahead of time, then that might defeat the purpose of even using  unsupervised\n",
      "learning 25\n",
      "So we need a way to evaluate the output of our cluster analysis 13\n",
      "\n",
      "The problem here is that, because we are not performing any kind of prediction, we cannot\n",
      "gauge how right  the algorithm is at predictions 30\n",
      "Metrics such as accuracy and RMSE go\n",
      "right out of the window 14\n",
      "\n",
      "The Silhouette Coefficient\n",
      "The Silhouette Coefficient  is a common metric  for evaluating clustering performance in\n",
      "situations when the true cluster assignments are not known 35\n",
      "\n",
      "A Silhouette Coefficient is calculated for each observation as follows:\n",
      " \n",
      "Let's look a little closer at the specific features of this formula:\n",
      "a: Mean distance to all other points in its cluster\n",
      "b: Mean distance to all other points in the next nearest cluster\n",
      "It ranges from -1 (worst) to 1 (best) 70\n",
      "A global score  is calculated by taking the mean score\n",
      "for all observations 15\n",
      "In general, a Silhouette Coefficient of 1 is preferred, while a score of -1\n",
      "is not preferable:\n",
      "# calculate Silhouette Coefficient for K=3\n",
      "from sklearn import metrics\n",
      "metrics 42\n",
      "silhouette_score(X, km 5\n",
      "labels_)\n",
      "The output is as follows:\n",
      "0 9\n",
      "67317750464557957\n",
      "\n",
      "Let's try calculating the coefficient for multiple values of K to find the best value:\n",
      "# center and scale the data\n",
      "from sklearn 34\n",
      "preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "X_scaled = scaler 15\n",
      "fit_transform(X)\n",
      "# calculate SC for K=2 through K=19\n",
      "k_range = range(2, 20)\n",
      "scores = []\n",
      "for k in k_range:\n",
      "    km = KMeans(n_clusters=k, random_state=1)\n",
      "    km 51\n",
      "fit(X_scaled)\n",
      "    scores 6\n",
      "append(metrics 2\n",
      "silhouette_score(X, km 5\n",
      "labels_))\n",
      "# plot the results\n",
      "plt 9\n",
      "plot(k_range, scores)\n",
      "plt 7\n",
      "xlabel('Number of clusters')\n",
      "plt 7\n",
      "ylabel('Silhouette Coefficient')\n",
      "plt 8\n",
      "grid(True)\n",
      "So, it looks like our optimal number of beer clusters is 4 17\n",
      "This means that our k-means\n",
      "algorithm has determined that there seem to be four distinct types of beer 21\n",
      "\n",
      "\n",
      "k-means is a popular algorithm because of its computational  efficiency and simple and\n",
      "intuitive nature 21\n",
      "k-means, however, highly scale dependent and is not suitable for data with\n",
      "widely varying shapes and densities 23\n",
      "There are ways to combat this issue by scaling data\n",
      "using scikit-learn's standard scalar:\n",
      "# center and scale the data\n",
      "from sklearn 29\n",
      "preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "X_scaled = scaler 15\n",
      "fit_transform(X)\n",
      "# K-means with 3 clusters on scaled data\n",
      "km = KMeans(n_clusters=3, random_state=1)\n",
      "km 31\n",
      "fit(X_scaled)\n",
      "Easy 5\n",
      "\n",
      "Now let's take a look at the third reason to use unsupervised methods that falls under the\n",
      "third option in our reasons to use unsupervised methods: feature extraction 36\n",
      "\n",
      "Feature extraction and principal component\n",
      "analysis\n",
      "Sometimes we have an overwhelming  number of columns and likely  not enough rows to\n",
      "handle the great quantity of columns 32\n",
      "\n",
      "A great example of this is when we were looking at the send cash now  example in our\n",
      "Nave Bayes example 26\n",
      "We had literally 0 instances of texts with that exact phrase, so instead,\n",
      "we turned to a naïve assumption that allowed us to extrapolate a probability for both of our\n",
      "categories 37\n",
      "\n",
      "The reason we had this problem in the first place is because of something called the curse of\n",
      "dimensionality 22\n",
      "The curse of dimensionality basically says that as we introduce and\n",
      "consider new feature columns, we need almost exponentially more rows (data points) in\n",
      "order to fill in the empty spaces that we create 40\n",
      "\n",
      "\n",
      "Consider an example where we attempt to use a learning model that utilizes the distance\n",
      "between points on a corpus of text that has 4,086 pieces of text and that the whole thing has\n",
      "been Countvectorized 44\n",
      "Let's assume that these texts between them have 18,884 words:\n",
      "X 16\n",
      "shape\n",
      "The output is as follows:\n",
      "(20, 4)\n",
      "Now, let's do an experiment 21\n",
      "I will first consider a single word as the only dimension of our\n",
      "text 15\n",
      "Then, I will count how many of pieces of text are within 1 unit of each other 19\n",
      "For\n",
      "example, if two sentences both contain that word, they would be 0 units away\n",
      "and, similarly, if neither of them contain the word, they would be 0 units away from one\n",
      "another:\n",
      "d = 1\n",
      "# Let's look for points within 1 unit of one another\n",
      "X_first_word = X 68\n",
      "iloc[:,:1]\n",
      "# Only looking at the first column, but ALL of the rows\n",
      "from sklearn 21\n",
      "neighbors import NearestNeighbors\n",
      "# this module will calculate for us distances between each point\n",
      "neigh = NearestNeighbors(n_neighbors=4086)\n",
      "neigh 32\n",
      "fit(X_first_word)\n",
      "# tell the module to calculate each distance between each point\n",
      "Note that we have 16,695,396 ( 4086*4086 ) distances to scan over 39\n",
      "\n",
      "A = neigh 4\n",
      "kneighbors_graph(X_first_word, mode='distance') 11\n",
      "todense()\n",
      "# This matrix holds all distances (over 16 million of them)\n",
      "num_points_within_d = (A < d) 27\n",
      "sum()\n",
      "# Count the number of pairs of points within 1 unit of distance\n",
      "num_points_within_d\n",
      "16258504\n",
      "\n",
      "So, 16 30\n",
      "2 million pairs of texts are within a single unit of distance 13\n",
      "Now, let's try again with\n",
      "the first two words:\n",
      "X_first_two_words = X 19\n",
      "iloc[:,:2]\n",
      "neigh = NearestNeighbors(n_neighbors=4086)\n",
      "neigh 19\n",
      "fit(X_first_two_words)\n",
      "A = neigh 9\n",
      "kneighbors_graph(X_first_two_words, mode='distance') 12\n",
      "todense()\n",
      "num_points_within_d = (A < d) 13\n",
      "sum()\n",
      "num_points_within_d\n",
      "16161970\n",
      "Great 12\n",
      "By adding this new column, we lost about 100,000 pairs of points that were within a\n",
      "single unit of distance 25\n",
      "This is because we are adding space in between  them for every \n",
      "dimension  that we add 19\n",
      "Let's take this test a step further and calculate this number for the\n",
      "first 100 words and then plot the results:\n",
      "d = 1\n",
      "# Scan for points within one unit\n",
      "num_columns = range(1, 100)\n",
      "# Looking at the first 100 columns\n",
      "points = []\n",
      "# We will be collecting the number of points within 1 unit for a graph\n",
      "neigh = NearestNeighbors(n_neighbors=X 86\n",
      "shape[0])\n",
      "for subset in num_columns:\n",
      "    X_subset = X 15\n",
      "iloc[:,:subset]\n",
      "  # look at the first column, then first two columns, then first three\n",
      "columns, etc\n",
      "    neigh 28\n",
      "fit(X_subset)\n",
      "    A = neigh 8\n",
      "kneighbors_graph(X_subset, mode='distance') 10\n",
      "todense()\n",
      "    num_points_within_d = (A < d) 14\n",
      "sum()\n",
      "# calculate the number of points within 1 unit\n",
      "    points 15\n",
      "append(num_points_within_d)\n",
      "Now, let's plot the number of points within 1 unit versus the number of dimensions we\n",
      "looked at:\n",
      "\n",
      "\n",
      "We can see clearly that the number of points within a single unit of one another goes down\n",
      "dramatically as we introduce more and more columns 59\n",
      "And this is only the first 100\n",
      "columns 10\n",
      "Let's see how many points are within a single unit by the time we consider all\n",
      "18,000+ words:\n",
      "neigh = NearestNeighbors(n_neighbors=4086)\n",
      "neigh 38\n",
      "fit(X)\n",
      "A = neigh 6\n",
      "kneighbors_graph(X, mode='distance') 9\n",
      "todense()\n",
      "num_points_within_d = (A < d) 13\n",
      "sum()\n",
      "num_points_within_d\n",
      "4090\n",
      "By the end, only 4,000 sentences are within a unit of one another 27\n",
      "All of this space that we\n",
      "add in by considering new columns makes it harder for the finite amount of points we have\n",
      "to stay happily within range of each other 33\n",
      "We would have to add in more points in order\n",
      "to fill in this gap 16\n",
      "And that, my friends, is why we should consider using dimension\n",
      "reduction 16\n",
      "\n",
      "\n",
      "The curse of dimensionality is solved by either adding more data points (which is not\n",
      "always possible) or implementing dimension reduction 26\n",
      "Dimension reduction is simply the\n",
      "act of reducing the number of columns in our dataset and not the number of rows 22\n",
      "There\n",
      "are two ways of implementing dimension reduction:\n",
      "Feature selection : This is the act of subsetting our column features and only\n",
      "using the best features\n",
      "Feature extraction : This is the act of mathematically transforming our feature set\n",
      "into a new extracted coordinate system\n",
      "We are familiar with feature selection as the process of saying the \" Embarked_Q \" is not\n",
      "helping my decision tree; let's get rid of it and see how it performs 91\n",
      "It is literally when we (or the\n",
      "machine) make the decision to ignore certain columns 18\n",
      "\n",
      "Feature extraction is a bit trickier …\n",
      "In feature extraction, we are using usually fairly complicated mathematical formulas in\n",
      "order to obtain new super columns  that are usually better than any single original column 40\n",
      "\n",
      "Our primary model for doing so is called Principal Component Analysis  (PCA ) 16\n",
      "PCA will\n",
      "extract a set number of super columns in order  to represent our original  data with much\n",
      "fewer columns 25\n",
      "Let's take a concrete example 6\n",
      "Previously, I mentioned some text with 4,086\n",
      "rows and over 18,000 columns 20\n",
      "That dataset is actually a set of Yelp online reviews:\n",
      "url = ' 14\n",
      " 1\n",
      "/data/yelp 4\n",
      "csv'\n",
      "yelp = pd 6\n",
      "read_csv(url, encoding='unicode-escape')\n",
      "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
      "yelp_best_worst = yelp[(yelp 39\n",
      "stars==5) | (yelp 8\n",
      "stars==1)]\n",
      "# define X and y\n",
      "X = yelp_best_worst 17\n",
      "text\n",
      "y = yelp_best_worst 9\n",
      "stars == 5\n",
      "Our goal is to predict whether or not a person gave a 5- or 1-star review  based on the words\n",
      "they used in the review 36\n",
      "Let's set a base line with logistic regression and see how well we\n",
      "can predict this binary category:\n",
      "from sklearn 23\n",
      "linear_model import LogisticRegression\n",
      "lr = LogisticRegression()\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
      "# Make our training and testing sets\n",
      "vect = CountVectorizer(stop_words='english')\n",
      "# Count the number of words but remove stop words like a, an, the, you, etc\n",
      "\n",
      "X_train_dtm = vect 79\n",
      "fit_transform(X_train)\n",
      "X_test_dtm = vect 11\n",
      "transform(X_test)\n",
      "# transform our text into document term matrices\n",
      "lr 14\n",
      "fit(X_train_dtm, y_train)\n",
      "# fit to our training set\n",
      "lr 17\n",
      "score(X_test_dtm, y_test)\n",
      "# score on our testing set\n",
      "The output is as follows:\n",
      "0 23\n",
      "91193737\n",
      "So, by utilizing all of the words in our corpus, our model seems to have over a 91%\n",
      "accuracy 28\n",
      "Not bad 2\n",
      "\n",
      "Let's try only using the top 100 used words:\n",
      "vect = CountVectorizer(stop_words='english', max_features=100)\n",
      "# Only use the 100 most used words\n",
      "X_train_dtm = vect 43\n",
      "fit_transform(X_train)\n",
      "X_test_dtm = vect 11\n",
      "transform(X_test)\n",
      "print( X_test_dtm 10\n",
      "shape) # (1022, 100)\n",
      "lr 11\n",
      "fit(X_train_dtm, y_train)\n",
      "lr 10\n",
      "score(X_test_dtm, y_test)\n",
      "The output is as follows:\n",
      "0 16\n",
      "8816\n",
      "Note how our training and testing matrices have 100 columns 15\n",
      "This is because I told our\n",
      "vectorizer to only look at the top 100 words 18\n",
      "See also that our performance took a hit and is\n",
      "now down to 88% accuracy 18\n",
      "This makes sense because we are ignoring over 4,700 words in\n",
      "our corpus 17\n",
      "\n",
      "Now, let's take a different approach 9\n",
      "Let's import a PCA module and tell it to make us 100\n",
      "new super columns and see how that performs:\n",
      "from sklearn import decomposition\n",
      "# We will be creating 100 super columns\n",
      "vect = CountVectorizer(stop_words='english')\n",
      "# Don't ignore any words\n",
      "pca = decomposition 59\n",
      "PCA(n_components=100)\n",
      "# instantate a pca object\n",
      "\n",
      "X_train_dtm = vect 19\n",
      "fit_transform(X_train) 5\n",
      "todense()\n",
      "# A dense matrix is required to pass into PCA, does not affect the overall\n",
      "message\n",
      "X_train_dtm = pca 28\n",
      "fit_transform(X_train_dtm)\n",
      "X_test_dtm = vect 13\n",
      "transform(X_test) 4\n",
      "todense()\n",
      "X_test_dtm = pca 9\n",
      "transform(X_test_dtm)\n",
      "print( X_test_dtm 12\n",
      "shape) # (1022, 100)\n",
      "lr 11\n",
      "fit(X_train_dtm, y_train)\n",
      "lr 10\n",
      "score(X_test_dtm, y_test)\n",
      "The output is as follows:\n",
      " 15\n",
      "89628\n",
      "Not only do our matrices still have 100 columns, but these columns are no longer words in\n",
      "our corpus 26\n",
      "They are complex transformations of columns and are 100 new columns 12\n",
      "Also\n",
      "note that using 100 of these new columns gives us a better predictive performance than\n",
      "using the 100 top words 25\n",
      "\n",
      "Feature extraction is a great way to use mathematical formulas to extract brand new\n",
      "columns that generally perform better than just selecting the best ones beforehand 28\n",
      "\n",
      "But how do we visualize these new super columns 10\n",
      "Well, I can think of no better way than\n",
      "to look at an example using image analysis 19\n",
      "Specifically, let's make facial recognition\n",
      "software 9\n",
      "OK 1\n",
      "OK 1\n",
      "Let's begin by importing some faces given to us by scikit-learn:\n",
      "from sklearn 18\n",
      "datasets import fetch_lfw_people\n",
      "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0 24\n",
      "4)\n",
      "# introspect the images arrays to find the shapes (for plotting)\n",
      "n_samples, h, w = lfw_people 27\n",
      "images 1\n",
      "shape\n",
      "# for machine learning we use the 2 data directly (as relative pixel #\n",
      "positions info is ignored by this model)\n",
      "X = lfw_people 31\n",
      "data\n",
      "y = lfw_people 7\n",
      "target\n",
      "n_features = X 6\n",
      "shape[1]\n",
      "X 5\n",
      "shape (1288, 1850)\n",
      "\n",
      "We have gathered 1,288 images of people's faces, and each one has 1,850 features (pixels)\n",
      "that identify  that person 39\n",
      "Here's an example:\n",
      "plt 6\n",
      "imshow(X[0] 5\n",
      "reshape((h, w)), cmap=plt 8\n",
      "cm 1\n",
      "gray)\n",
      "lfw_people 4\n",
      "target_names[y[0]] 'Hugo Chavez'\n",
      "plt 12\n",
      "imshow(X[100] 5\n",
      "reshape((h, w)), cmap=plt 8\n",
      "cm 1\n",
      "gray)\n",
      "lfw_people 4\n",
      "target_names[y[100]] 'George W Bush'\n",
      "\n",
      "\n",
      "Great 12\n",
      "To get a glimpse at the type of dataset we are looking at, let's look at a few overall\n",
      "metrics:\n",
      "# the label to predict is the id of the person\n",
      "target_names = lfw_people 42\n",
      "target_names\n",
      "n_classes = target_names 8\n",
      "shape[0]\n",
      "print(\"Total dataset size:\")\n",
      "print(\"n_samples: %d\" % n_samples)\n",
      "print(\"n_features: %d\" % n_features)\n",
      "print(\"n_classes: %d\" % n_classes)\n",
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n",
      "So, we have 1,288 images, 1,850 features, and 7 classes (people) to choose from 96\n",
      "Our goal is\n",
      "to make a classifier that will assign the person's face a name based on the 1,850 pixels given\n",
      "to us 29\n",
      "\n",
      "Let's take a base line and see how logistic regression performs on our data without doing\n",
      "anything:\n",
      "from sklearn 23\n",
      "linear_model import LogisticRegression\n",
      "from sklearn 8\n",
      "metrics import accuracy_score\n",
      "from time import time # for timing our work\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "X, y, test_size=0 39\n",
      "25, random_state=1)\n",
      "# get our training and test set\n",
      "t0 = time() # get the time now\n",
      "logreg = LogisticRegression()\n",
      "logreg 35\n",
      "fit(X_train, y_train)\n",
      "# Predicting people's names on the test set\n",
      "y_pred = logreg 23\n",
      "predict(X_test)\n",
      "print( accuracy_score(y_pred, y_test), \"Accuracy\")\n",
      "print( (time() - t0), \"seconds\" )\n",
      "The output is as follows:\n",
      "0 37\n",
      "810559006211 Accuracy\n",
      "6 8\n",
      "31762504578 seconds\n",
      "So, within 6 12\n",
      "3 seconds, we were able to get an 81% on our test set 17\n",
      "Not too bad 3\n",
      "\n",
      "\n",
      "Now let's try this with our super faces :\n",
      "# split into a training and testing set\n",
      "from sklearn 22\n",
      "cross_validation import train_test_split\n",
      "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled #\n",
      "dataset): unsupervised feature extraction / dimensionality reduction\n",
      "n_components = 75\n",
      "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
      "% (n_components, X_train 67\n",
      "shape[0]))\n",
      "pca = decomposition 8\n",
      "PCA(n_components=n_components,\n",
      "whiten=True) 10\n",
      "fit(X_train)\n",
      "# This whiten parameter speeds up the computation of our extracted columns\n",
      "# Projecting the input data on the eigenfaces orthonormal basis\n",
      "X_train_pca = pca 39\n",
      "transform(X_train)\n",
      "X_test_pca = pca 10\n",
      "transform(X_test)\n",
      "The preceding code is collecting 75 extracted columns from our 1,850 unprocessed columns 22\n",
      "\n",
      "These are our super faces 6\n",
      "Now, let's plug in our newly extracted columns into our logistic\n",
      "regression and compare:\n",
      "t0 = time()\n",
      "# Predicting people's names on the test set WITH PCA\n",
      "logreg 39\n",
      "fit(X_train_pca, y_train)\n",
      "y_pred = logreg 14\n",
      "predict(X_test_pca)\n",
      "print accuracy_score(y_pred, y_test), \"Accuracy\"\n",
      "print (time() - t0), \"seconds\"\n",
      "0 30\n",
      "82298136646 Accuracy\n",
      "0 8\n",
      "194181919098 seconds\n",
      "Wow 8\n",
      "Not only was this entire calculation about 30 times faster than the unprocessed\n",
      "images, but the predictive performance also got better 25\n",
      "This shows us that PCA and feature\n",
      "extraction, in general, can help us all around when performing machine learning on\n",
      "complex datasets with many columns 30\n",
      "By searching for these patterns in the dataset and\n",
      "extracting new feature  columns, we can speed up and enhance  our learning algorithms 27\n",
      "\n",
      "Let's look at one more interesting thing 9\n",
      "I mentioned before that one of the purposes of this\n",
      "example was to examine and visualize our eigenfaces , as they are called: our super columns 29\n",
      "\n",
      "I will not disappoint 5\n",
      "Let's write some code that will show us our super columns as they\n",
      "would look to us humans:\n",
      "def plot_gallery(images, titles, n_row=3, n_col=4):\n",
      "\"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
      "plt 49\n",
      "figure(figsize=(1 4\n",
      "8 * n_col, 2 8\n",
      "4 * n_row))\n",
      "plt 7\n",
      "subplots_adjust(bottom=0, left= 9\n",
      "01, right= 5\n",
      "99, top= 5\n",
      "90, hspace= 6\n",
      "35)\n",
      "for i in range(n_row * n_col):\n",
      "\n",
      "plt 14\n",
      "subplot(n_row, n_col, i + 1)\n",
      "plt 13\n",
      "imshow(images[i], cmap=plt 6\n",
      "cm 1\n",
      "gray)\n",
      "plt 3\n",
      "title(titles[i], size=12)\n",
      "# plot the gallery of the most significative eigenfaces\n",
      "eigenfaces = pca 26\n",
      "components_ 2\n",
      "reshape((n_components, h, w))\n",
      "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces 30\n",
      "shape[0])]\n",
      "plot_gallery(eigenfaces, eigenface_titles)\n",
      "plt 15\n",
      "show()\n",
      "\n",
      "\n",
      "Wow 3\n",
      "A haunting and yet beautiful representation of what the data believes to be the most\n",
      "importance features of a face 22\n",
      "As we move from the top-left (first super column) to the\n",
      "bottom, it is actually somewhat easy to see what the image is trying to tell us 32\n",
      "The first\n",
      "super column looks like a very general face structure with eyes and nose and a mouth 19\n",
      "It is\n",
      "almost saying \"I represent the basic qualities of a face that all faces must have 19\n",
      "Our second\n",
      "super column directly to its right seems to be telling us about shadows in the image 19\n",
      "The\n",
      "next one might be telling us that skin tone plays a role in detecting who this is, which might\n",
      "be why the third face is much darker than the first two 35\n",
      "\n",
      "Using feature extraction unsupervised learning methods such as PCA can give us a very\n",
      "deep look into our data and reveal to us what the data believes to be the most important\n",
      "features, not just what we believe them to be 47\n",
      "Feature extraction is a great preprocessing\n",
      "tool that can speed up our future learning methods, make them more powerful, and give us\n",
      "more insight into how the data believes it should be viewed 37\n",
      "To sum up this section, we\n",
      "will list the pros and cons 14\n",
      "\n",
      "Here are the pros of using feature extraction:\n",
      "Our models become much faster\n",
      "Our predictive performance can become better\n",
      "It can give us insight into the extracted features (eigenfaces)\n",
      "And here are the cons of using feature extraction:\n",
      "We lose some of the interpretability  of our features as they  are new\n",
      "mathematically-derived columns, not our old ones\n",
      "We can lose predictive performance because we are losing information as we\n",
      "extract fewer columns\n",
      "\n",
      "Summary\n",
      "Between decision trees, Na ïve Bayes classification, feature extraction, and k-means\n",
      "clustering, we have seen that machine learning goes way beyond the simplicity of linear\n",
      "and logistic regression and can solve many types of complicated problems 142\n",
      "\n",
      "We also saw examples of both supervised and unsupervised learning and, in doing so,\n",
      "became familiar with many types of data science related problems 30\n",
      "\n",
      "In the next chapter, we will be looking at even more complicated learning algorithms,\n",
      "including artificial neural networks and ensembling techniques 25\n",
      "We will also see and\n",
      "understand more complicated concepts in data science, including the bias-variance\n",
      "tradeoff, as well as the concept of overfitting 33\n",
      "\n",
      "\n",
      "2\n",
      "Beyond the Essentials\n",
      "In this chapter,  we will be discussing some of the more complicated parts of data science\n",
      "that can put some people off 32\n",
      "The reason for this is that data science is not all fun and\n",
      "machine learning 16\n",
      "Sometimes, we have to discuss and consider theoretical and\n",
      "mathematical paradigms and evaluate our procedures 21\n",
      "\n",
      "This chapter will explore many of these procedures step by step so that we completely and\n",
      "totally understand the topics 23\n",
      "We will be discussing topics such as the following:\n",
      "Cross-validation\n",
      "The bias/variance tradeoff\n",
      "Overfitting and underfitting\n",
      "Ensembling techniques\n",
      "Random forests\n",
      "Neural networks\n",
      "These are only some of the topics to be covered 49\n",
      "At no point do I want you to be confused 10\n",
      "I\n",
      "will attempt to explain each procedure/algorithm with the utmost care and with many\n",
      "examples and visuals 21\n",
      "\n",
      "\n",
      "The bias/variance tradeoff\n",
      "We have discussed the concept of bias and variance  briefly in the previous chapters 23\n",
      "When\n",
      "we are discussing these two concepts, we are generally speaking of supervised learning\n",
      "algorithms 19\n",
      "We are specifically talking about deriving errors from our predictive models\n",
      "due to bias and variance 17\n",
      "\n",
      "Errors due to bias\n",
      "When speaking of errors due to bias, we are speaking  of the difference between the\n",
      "expected prediction of our model and the actual (correct) value, which we are trying to\n",
      "predict 44\n",
      "Bias, in effect, measures how far, in general, our model's predictions are from the\n",
      "correct value 22\n",
      "\n",
      "Think about bias as simply being the difference between a predicted value and the actual\n",
      "value 18\n",
      "For example, consider that our model, represented as F(x), predicts the value of 29 as\n",
      "follows:\n",
      "Here, the value of 29 should have been predicted as 79:\n",
      "If a machine learning model tends to be very accurate in its prediction (regression or\n",
      "classification), then it is considered a low bias model, whereas if the model is more often\n",
      "than not wrong, it is considered to be a high bias model 89\n",
      "\n",
      "Bias is a measure you can use to judge models on the basis of their accuracy  or just how\n",
      "correct the model is on average 28\n",
      "\n",
      "Error due to variance\n",
      "An error due to variance is dependent  on the variability of a model's prediction for a given\n",
      "data point 28\n",
      "Imagine that you repeat the machine learning model building process over and\n",
      "over 14\n",
      "The variance is measured by looking at how much the predictions for a fixed point\n",
      "vary between different end results 21\n",
      "\n",
      "\n",
      "To imagine variance in your head, think about a population of data points 15\n",
      "If you were to\n",
      "take randomized samples over and over, how drastically would your machine learning\n",
      "model change or fit differently each time 26\n",
      "If the model does not change much between\n",
      "samples, the model would be considered a low variance model 20\n",
      "If your model changes\n",
      "drastically between samples, then that model would be considered a high variance model 20\n",
      "\n",
      "Variance is a great measure with which to judge our model on the basis of generalizability 20\n",
      "\n",
      "If our model has a low variance, we can expect it to behave in a certain way when using it\n",
      "in the wild and predict values without human supervision 32\n",
      "\n",
      "Our goal is to optimize both bias and variance 10\n",
      "Ideally, we are looking for the lowest\n",
      "possible variance and bias 13\n",
      "\n",
      "I find that this can be best explained using an example 12\n",
      "\n",
      "Example – comparing body and brain weight of\n",
      "mammals\n",
      "Imagine that we are considering a relationship between  the brain weight of mammals and\n",
      "their corresponding body weights 34\n",
      "A hypothesis might read that there is a positive\n",
      "correlation between the two (as one goes up, so does the other) 26\n",
      "But how strong is this\n",
      "relationship 7\n",
      "Is it even linear 4\n",
      "Perhaps, as the brain weight increases, there is a logarithmic\n",
      "or quadratic increase in body weight 20\n",
      "\n",
      "Let's use Python to explore this:\n",
      "# # Exploring the Bias-Variance Tradeoff\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "%matplotlib inline\n",
      "\n",
      "I will be using a module, called seaborn , to visualize data points as a scatter plot and also\n",
      "to graph linear (and higher polynomial) regression models:\n",
      "# ## Brain and body weight\n",
      "'''\n",
      "This is a [dataset]) of the average\n",
      "weight of the body and the brain for\n",
      "62 mammal species 102\n",
      "Let's read it into pandas and\n",
      "take a quick look:\n",
      "'''\n",
      "df =\n",
      "pd 17\n",
      "read_table('http://people 6\n",
      "sc 1\n",
      "fsu 2\n",
      "edu/~jburkardt/datasets/regression/x01 13\n",
      "\n",
      "txt', sep='\\s+', skiprows=33, names=['id','brain','body'], index_col='id')\n",
      "df 26\n",
      "head()\n",
      "We are going to take a small subset of the samples  to exaggerate the visual representations\n",
      "of bias and variance, as follows:\n",
      "# We're going to focus on a smaller subset in which the body weight is less\n",
      "than 200:\n",
      "df = df[df 55\n",
      "body < 200]\n",
      "df 6\n",
      "shape\n",
      "The output is as follows:\n",
      "(51, 2)\n",
      "\n",
      "We're actually going to pretend that there are only 51 mammal species in existence 31\n",
      "In other\n",
      "words, we are pretending that this is the entire dataset of brain and body weights for every \n",
      "known  mammal species:\n",
      "# Let's create a scatterplot\n",
      "sns 37\n",
      "lmplot(x='body', y='brain', data=df, ci=None, fit_reg=False)\n",
      "Scatter plot of mammalian brain and body weights\n",
      "There appears to be a relationship between brain and body weight for mammals 44\n",
      "So far, we\n",
      "might assume that it is a positive correlation 13\n",
      "\n",
      "\n",
      "Now, let's throw a linear regression  into the mix 13\n",
      "Let's use seaborn  to make and plot a\n",
      "first-degree polynomial (linear) regression:\n",
      "sns 20\n",
      "lmplot(x='body', y='brain', data=df, ci=None)\n",
      "The same scatter plot as before with a linear regression visualization put in\n",
      "Now, let's pretend that a new mammal species is discovered 43\n",
      "We measure the body weight\n",
      "of every member of this species that we can find and calculate an average body weight of\n",
      "100 25\n",
      "We want to predict the average brain weight of this species (rather than measuring it\n",
      "directly) 20\n",
      "Using this line, we might predict a brain weight of about 45 14\n",
      "\n",
      "\n",
      "Something you might note is that this line isn't that close to the data points in the graph, so\n",
      "maybe it isn't the best model to use 32\n",
      "You might argue that the bias is too high 9\n",
      "And I would\n",
      "agree 5\n",
      "Linear regression models tend to have a high bias, but linear regression also has\n",
      "something up its sleeve: it has a very low variance 27\n",
      "However, what does that really mean 7\n",
      "\n",
      "Let's say that we take our entire population of mammals  and randomly split them into two\n",
      "samples, as follows:\n",
      "# set a random seed for reproducibility np 35\n",
      "random 1\n",
      "seed(12345) # randomly\n",
      "assign every row to either sample 1 or sample 2 df['sample'] =\n",
      "np 26\n",
      "random 1\n",
      "randint(1, 3, len(df)) df 11\n",
      "head()\n",
      "We include a new sample column:\n",
      "# Compare the two samples, they are fairly different 19\n",
      "\n",
      "df 2\n",
      "groupby('sample')[['brain', 'body']] 11\n",
      "mean()\n",
      "\n",
      "\n",
      "We can now tell seaborn  to create two plots, in which  the left plot only uses the data from\n",
      "sample 1 and the right plot only uses the data from sample 2:\n",
      "# col='sample' subsets the data by sample and creates two\n",
      "# separate plots\n",
      "sns 60\n",
      "lmplot(x='body', y='brain', data=df, ci=None, col='sample')\n",
      "Side-by-side scatter plots of linear regressions for samples 1 and 2\n",
      "They barely look different, right 43\n",
      "If you look closely, you will note that not a single data\n",
      "point is shared between the samples and yet the line looks almost identical 27\n",
      "To further\n",
      "show this point, let's put both the lines of best fit in the same graph and use colors to\n",
      "separate the samples, as illustrated:\n",
      "# hue='sample' subsets the data by sample and creates a # single plot\n",
      "sns 51\n",
      "lmplot(x='body', y='brain', data=df, ci=None, hue='sample')\n",
      "\n",
      "\n",
      "Presentation of two lines of best ﬁt in the same graph\n",
      "The line looks pretty similar between the two plots, despite the fact that they used separate\n",
      "samples of data 55\n",
      "In both cases, we would predict a brain weight of about 45 14\n",
      "\n",
      "The fact that even though the linear regression was given to completely distinct datasets\n",
      "pulled from the same population, it produced a very similar line, suggests that the model is\n",
      "of low variance 39\n",
      "\n",
      "\n",
      "What if we increased our model's complexity and allowed it to learn more 15\n",
      "Instead of\n",
      "fitting a line, let's let seaborn  fit a fourth-degree polynomial (a quartic polynomial) 24\n",
      "By\n",
      "adding to the degree  of the polynomial, the graph will be able to make twists and turns in\n",
      "order to fit our data better, as shown:\n",
      "# What would a low bias, high variance model look like 45\n",
      "Let's try\n",
      "polynomial regression, with an fourth order polynomial:\n",
      "sns 15\n",
      "lmplot(x='body', y='brain', data=df, ci=None, col='sample', order=4)\n",
      "Using a quartic polynomial for regression purposes\n",
      "Note how, for two distinct samples from the same population, the quartic polynomial looks\n",
      "vastly different 55\n",
      "This is a sign of high variance 7\n",
      "\n",
      "This model is low bias because it matches our data well 12\n",
      "However, it has high variance\n",
      "because the models are wildly different depending upon which points happen to be in the\n",
      "sample 24\n",
      "(For a body weight of 100, the brain weight prediction would either be 40 or 0,\n",
      "depending upon which data happened to be in the sample 32\n",
      "\n",
      "\n",
      "Our polynomial is also unaware of the general relationship of the data 13\n",
      "It seems obvious\n",
      "that there is a positive correlation between the brain and body weight of mammals 18\n",
      "\n",
      "However, in our quartic  polynomials, this relationship is nowhere to be found and is\n",
      "unreliable 24\n",
      "In our first sample (the graph on the left), the polynomial ends up shooting\n",
      "downwards, while in the second graph, the graph is going upwards towards the end 34\n",
      "Our\n",
      "model is unpredictable and can behave wildly differently, depending on the given training\n",
      "set 18\n",
      "\n",
      "It is our job, as data scientists, to find the middle ground 15\n",
      "\n",
      "Perhaps we can create a model that has less bias than the linear model, and less variance\n",
      "than the fourth-order polynomial:\n",
      "# Let's try a second order polynomial instead:\n",
      "sns 37\n",
      "lmplot(x='body', y='brain', data=df, ci=None, col='sample', order=2)\n",
      "Scatter plot using a quadratic polynomial as our estimator\n",
      "This plot seems to have a good balance of bias and variance 47\n",
      "\n",
      "\n",
      "Two extreme cases of bias/variance tradeoff\n",
      "What we just saw were two extreme cases  of model fitting: one was underfitting and the\n",
      "other was overfitting 37\n",
      "\n",
      "Underfitting\n",
      "Underfitting occurs when our models  make little to no attempt  to fit our data 23\n",
      "Models that\n",
      "are high bias and low variance are prone to underfitting 15\n",
      "In the case of the mammal\n",
      "brain/body weight example, the linear regression is underfitting our data 22\n",
      "While we have a\n",
      "general shape of the relationship, we are left with a high bias 18\n",
      "\n",
      "If your learning algorithm shows high bias and/or is underfitting, the following suggestions\n",
      "may help:\n",
      "Use more features : Try including new features into the model if it helps with our\n",
      "predictive power 42\n",
      "\n",
      "Try a more complicated model : Adding complexity to your model can help\n",
      "improve bias 18\n",
      "An overly complicated model will hurt too 7\n",
      "\n",
      "Overfitting\n",
      "Overfitting is the result of the model trying too hard to fit into the training set, resulting in a\n",
      "lower bias but a much higher variance 35\n",
      "Models that are low bias and high variance are \n",
      "prone  to overfitting 17\n",
      "In the case of the mammal brain/body weight example, the fourth-\n",
      "degree polynomial (quartic) regression is overfitting our data 28\n",
      "\n",
      "If your learning algorithm  shows high variance and/or is overfitting, the following\n",
      "suggestions may help:\n",
      "Use fewer features : Using fewer features can decrease our variance and prevent\n",
      "overfitting\n",
      "Fit on more training samples : Using more training data points in our cross-\n",
      "validation can reduce the effect of overfitting, and improve our high variance\n",
      "estimator\n",
      "\n",
      "How bias/variance play into error functions\n",
      "Error functions (which measure how incorrect  our models are) can be thought of as\n",
      "functions of bias, variance, and irreducible error 113\n",
      "Mathematically put, the error of\n",
      "predicting a dataset using  our supervised learning model might look as follows:\n",
      "Here, Bias2 is our bias term squared (which arises when simplifying the mentioned\n",
      "statement from more complicated equations), and Variance  is a measurement of how much\n",
      "our model fitting varies between randomized samples 66\n",
      "\n",
      "Simply put, both bias and variance contribute to errors 11\n",
      "As we increase our model\n",
      "complexity (for example, go from a linear regression to an eighth-degree polynomial\n",
      "regression or grow our decision trees deeper), we find that Bias2 decreases, Variance\n",
      "increases, and the total error of the model forms a parabolic shape, as illustrated:\n",
      "Relationship of bias and variance\n",
      "Our goal, as data scientists, is to find the sweet spot  that optimizes our model complexity 87\n",
      "It\n",
      "is easy to overfit our data 9\n",
      "To combat overfitting in practice, we should always use cross-\n",
      "validation (splitting up datasets iteratively and retraining models and averaging metrics) to\n",
      "get the best predictor of an error 39\n",
      "\n",
      "\n",
      "To illustrate this point, I will introduce (quickly) a new supervised algorithm and\n",
      "demonstrate the bias/variance tradeoff visually 29\n",
      "\n",
      "We will be using the K-Nearest Neighbors  (KNN ) algorithm, which is a supervised\n",
      "learning algorithm that uses a lookalike paradigm, which means  that it makes predictions\n",
      "based on similar data points seen in the past 51\n",
      "\n",
      "KNN has a complexity input, K, which represents  how many similar  data points to\n",
      "compare to 23\n",
      "If K = 3 , then, for a given input, we look to the nearest three data points and use\n",
      "them for our prediction 28\n",
      "In this case, K represents our model's complexity:\n",
      "from sklearn 13\n",
      "neighbors import KNeighborsClassifier\n",
      "# read in the iris data\n",
      "from sklearn 15\n",
      "datasets import load_iris\n",
      "iris = load_iris()\n",
      "X, y = iris 17\n",
      "data, iris 3\n",
      "target\n",
      "So, we have our X and our y 11\n",
      "A great way to overfit a model is to train and predict the exact\n",
      "same data:\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn 32\n",
      "fit(X, y)\n",
      "knn 7\n",
      "score(X, y)\n",
      "The output is as follows:\n",
      "1 12\n",
      "0\n",
      "Wow, 100% accuracy 9\n",
      " 1\n",
      "This is too good to be true 7\n",
      "\n",
      "By training and predicting the same data, we are essentially telling our data to purely\n",
      "memorize the training set and spit it back to us (this is called our training error) 38\n",
      "This is the\n",
      "reason we introduced our training and test sets in Chapter 10 , How to Tell If Your Toaster Is\n",
      "Learning – Machine Learning Essentials 31\n",
      "\n",
      "K folds cross-validation\n",
      "K folds cross-validation is a much better estimator  of our model's performance, even more\n",
      "so than our train-test split 31\n",
      "Here's how it works:\n",
      "We will take a finite number of equal slices of our data (usually 3, 5, or 10) 30\n",
      "1 2\n",
      "\n",
      "Assume that this number is called k 9\n",
      "\n",
      "For each \"fold\" of the cross-validation, we will treat k-1 of the sections as the 2 24\n",
      "\n",
      "training set, and the remaining section as our test set 12\n",
      "\n",
      "\n",
      "For the remaining folds, a different arrangement of k-1 sections is considered for 3 19\n",
      "\n",
      "our training set and a different section is our training set 12\n",
      "\n",
      "We compute a set metric for each fold of the cross-validation 13\n",
      "4 2\n",
      "\n",
      "We average our scores at the end 8\n",
      "5 2\n",
      "\n",
      "Cross-validation is effectively using multiple train-test splits being done on the same\n",
      "dataset 17\n",
      "This is done for a few reasons, but mainly because cross-validation is the most \n",
      "honest  estimate of our model's out-of-sample  (OOS ) error 34\n",
      "\n",
      "To explain this visually, let's look at our mammal brain and body weight example for a\n",
      "second 22\n",
      "The following code manually creates a five-fold cross-validation, wherein five\n",
      "different training and test sets are made from the same population:\n",
      "import matplotlib 28\n",
      "pyplot as plt\n",
      "from sklearn 6\n",
      "cross_validation import KFold\n",
      "df =\n",
      "pd 9\n",
      "read_table('http://people 6\n",
      "sc 1\n",
      "fsu 2\n",
      "edu/~jburkardt/datasets/regression/x01 13\n",
      "\n",
      "txt', sep='\\s+', skiprows=33, names=['id','brain','body'])\n",
      "df = df[df 24\n",
      "brain < 300][df 6\n",
      "body < 500]\n",
      "# limit points for visibility\n",
      "nfolds = 5\n",
      "fig, axes = plt 22\n",
      "subplots(1, nfolds, figsize=(14,4))\n",
      "for i, fold in enumerate(KFold(len(df), n_folds=nfolds,\n",
      "                              shuffle=True)):\n",
      "    training, validation = fold\n",
      "    x, y = df 49\n",
      "iloc[training]['body'], df 8\n",
      "iloc[training]['brain']\n",
      "    axes[i] 11\n",
      "plot(x, y, 'ro')\n",
      "    x, y = df 14\n",
      "iloc[validation]['body'], df 8\n",
      "iloc[validation]['brain']\n",
      "    axes[i] 11\n",
      "plot(x, y, 'bo')\n",
      "plt 9\n",
      "tight_layout()\n",
      "Five-fold cross-validation: red = training sets, blue = test sets\n",
      "\n",
      "Here, each graph shows the exact same population of mammals, but the dots are colored\n",
      "red if they belong to the training set of that fold and blue if they belong to the testing set 56\n",
      "By\n",
      "doing this, we are obtaining five different instances of the same machine learning model in\n",
      "order to see if the performance remains consistent across the folds 30\n",
      "\n",
      "If you stare at the dots long enough, you will note that each dot appears in a training set\n",
      "exactly four times ( k - 1), while the same dot appears in a test set exactly once and only\n",
      "once 47\n",
      "\n",
      "Some features of k-fold cross-validation include  the following:\n",
      "It is a more accurate estimate of the OOS prediction error than a single train-test\n",
      "split because it is taking several independent train-test splits and averaging the\n",
      "results together 47\n",
      "\n",
      "It is a more efficient use of data than single train-test splits because the entire\n",
      "dataset is being used for multiple train-test splits instead of just one 31\n",
      "\n",
      "Each record in our dataset is used for both training and testing 13\n",
      "\n",
      "This method presents a clear tradeoff between efficiency and computational\n",
      "expense 14\n",
      "A 10-fold CV is 10x more expensive computationally than a single\n",
      "train/test split 20\n",
      "\n",
      "This method can be used for parameter tuning and model selection 12\n",
      "\n",
      "Basically, whenever we wish to test a model on a set of data, whether we just completed\n",
      "tuning some parameters or feature engineering, a k-fold cross-validation is an excellent\n",
      "way to estimate the performance on our model 46\n",
      "\n",
      "Of course, sklearn  comes with an easy-to-use cross-validation module, called\n",
      "cross_val_score , which automatically splits up our dataset for us, runs the model on\n",
      "each fold, and gives us a neat and tidy output of results:\n",
      "# Using a training set and test set is so important\n",
      "# Just as important is cross validation 69\n",
      "Remember cross validation\n",
      "# is using several different train test splits and\n",
      "# averaging your results 18\n",
      "\n",
      "## CROSS-VALIDATION\n",
      "# check CV score for K=1\n",
      "from sklearn 18\n",
      "cross_validation import cross_val_score, train_test_split\n",
      "tree = KNeighborsClassifier(n_neighbors=1)\n",
      "scores = cross_val_score(tree, X, y, cv=5, scoring='accuracy')\n",
      "scores 41\n",
      "mean()\n",
      "0 3\n",
      "95999999999\n",
      "\n",
      "This is a much more reasonable accuracy than our previous score of 1 20\n",
      "Remember that we\n",
      "are not getting 100% accuracy anymore because we have a distinct training and test set 21\n",
      "The\n",
      "data points that KNN has never seen are the test points and it, therefore, cannot match\n",
      "them exactly to themselves 26\n",
      "\n",
      "Let's try cross-validating KNN with K=5 (increasing our model's complexity), as shown:\n",
      "# check CV score for K=5\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
      "scores\n",
      "np 67\n",
      "mean(scores)\n",
      "0 4\n",
      "97333333\n",
      "Even better 7\n",
      "So, now we have to find the best K 10\n",
      "The best K is the one that maximizes  our\n",
      "accuracy 13\n",
      "Let's try a few:\n",
      "# search for an optimal value of K\n",
      "k_range = range(1, 30, 2) # [1, 3, 5, 7, 41\n",
      " 1\n",
      " 1\n",
      ", 27, 29]\n",
      "errors = []\n",
      "for k in k_range:\n",
      "    knn = KNeighborsClassifier(n_neighbors=k)\n",
      "   # instantiate a KNN with k neighbors\n",
      "   scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
      " # get our five accuracy scores\n",
      "    accuracy = np 69\n",
      "mean(scores)\n",
      "   # average them together\n",
      "    error = 1 - accuracy\n",
      "   # get our error, which is 1 minus the accuracy\n",
      "    errors 33\n",
      "append(error)\n",
      "   # keep track of a list of errors\n",
      "\n",
      "We now have an error value (1 - accuracy) for each value of K (1, 3, 5, 7, 9 43\n",
      " 1\n",
      ", 1\n",
      " 1\n",
      ", 29):\n",
      "# plot the K values (x-axis) versus the 5-fold CV score (y-axis)\n",
      "plt 25\n",
      "figure()\n",
      "plt 3\n",
      "plot(k_range, errors)\n",
      "plt 7\n",
      "xlabel('K')\n",
      "plt 5\n",
      "ylabel('Error')\n",
      "Graph of errors of the KNN model against KNN's complexity, represented by the value of K\n",
      "Compare this graph to the previous graph of model complexity  and bias/variance 40\n",
      "Toward\n",
      "the left, our graph has a higher bias and is underfitting 17\n",
      "As we increased our model's\n",
      "complexity, the error term began to go down, but after a while, our model became overly\n",
      "complex, and the high variance kicked in, making our error term go back up 44\n",
      "\n",
      "It seems that the optimal value of K is between 6 and 10 16\n",
      "\n",
      "\n",
      "Grid searching\n",
      "sklearn  also has, up its sleeve, another useful  tool called grid searching 21\n",
      "A grid search will\n",
      "by brute force try many different model parameters and give us the best one based on a\n",
      "metric of our choosing 27\n",
      "For example, we can choose to optimize KNN for accuracy in the\n",
      "following manner:\n",
      "from sklearn 20\n",
      "grid_search import GridSearchCV\n",
      "from sklearn 9\n",
      "neighbors import KNeighborsClassifier\n",
      "# import our grid search module\n",
      "knn = KNeighborsClassifier(n_jobs=-1)\n",
      "# instantiate a blank slate KNN, no neighbors\n",
      "k_range = list(range(1, 31, 2))\n",
      "print(k_range)\n",
      "#k_range = range(1, 30)\n",
      "param_grid = dict(n_neighbors=k_range)\n",
      "# param_grid = {\"n_ neighbors\": [1, 3, 5, 91\n",
      " 1\n",
      " 1\n",
      "}\n",
      "print(param_grid)\n",
      "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
      "grid 25\n",
      "fit(X, y)\n",
      "\n",
      "In the grid 8\n",
      "fit()  line of code, what is happening is that, for each combination of features\n",
      "in this case, we have 15 different possibilities for K, so we are cross-validating each one five\n",
      "times 43\n",
      "This means that by the end of this code, we will have 15 * 5 = 75  different KNN\n",
      "models 27\n",
      "You can see how, when applying  this technique to more complex models, we could\n",
      "run into difficulties with time:\n",
      "# check the results of the grid search\n",
      "grid 34\n",
      "grid_scores_\n",
      "grid_mean_scores = [result[1] for result in grid 16\n",
      "grid_scores_]\n",
      "# this is a list of the average accuracies for each parameter\n",
      "# combination\n",
      "plt 22\n",
      "figure()\n",
      "plt 3\n",
      "ylim([0 3\n",
      "9, 1])\n",
      "plt 7\n",
      "xlabel('Tuning Parameter: N nearest neighbors')\n",
      "plt 11\n",
      "ylabel('Classification Accuracy')\n",
      "plt 6\n",
      "plot(k_range, grid_mean_scores)\n",
      "plt 9\n",
      "plot(grid 2\n",
      "best_params_['n_neighbors'], grid 8\n",
      "best_score_, 'ro',\n",
      "markersize=12, markeredgewidth=1 17\n",
      "5,\n",
      "         markerfacecolor='None', markeredgecolor='r')\n",
      "Classiﬁcation Accuracy versus Tuning Parameters in N nearest neighbors\n",
      "Note that the preceding graph is basically the same as the one we achieved previously with\n",
      "our for loop, but much easier 55\n",
      "\n",
      "\n",
      "We see that seven neighbors (circled in the preceding graph) seem to have the best\n",
      "accuracy 21\n",
      "However, we can also, very easily, get our best parameters and our best model, as\n",
      "shown:\n",
      "grid 23\n",
      "best_params_\n",
      "# {'n_neighbors': 7}\n",
      "grid 12\n",
      "best_score_\n",
      "# 0 6\n",
      "9799999999\n",
      "grid 7\n",
      "best_estimator_\n",
      "# actually returns the unfit model with the best parameters\n",
      "# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
      "           weights='uniform')\n",
      "I'll take this one step further 65\n",
      "Maybe you've noted that KNN has other parameters as well,\n",
      "such as algorithm , p, and weights 21\n",
      "A quick look at the scikit-learn documentation reveals\n",
      "that we have some options for each of these, which are as follows:\n",
      "p is an integer and represents the type of distance we wish to use 41\n",
      "By default, we\n",
      "use p=2, which is our standard distance formula 16\n",
      "\n",
      "weights  is, by default, uniform , but can also be distance , which weighs points\n",
      "by their distance, meaning that close neighbors have a greater impact on the\n",
      "prediction 36\n",
      "\n",
      "algorithm  is how the model finds the nearest neighbors 11\n",
      "We can try\n",
      "ball_tree , kd_tree , or brute 12\n",
      "The default is auto , which tries to use the best\n",
      "one automatically:\n",
      "knn = KNeighborsClassifier()\n",
      "k_range = range(1, 30)\n",
      "algorithm_options = ['kd_tree', 'ball_tree', 'auto', 'brute']\n",
      "p_range = range(1, 8)\n",
      "weight_range = ['uniform', 'distance']\n",
      "param_grid = dict(n_neighbors=k_range, weights=weight_range,\n",
      "algorithm=algorithm_options, p=p_range)\n",
      "# trying many more options\n",
      "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
      "grid 118\n",
      "fit(X, y)\n",
      "\n",
      "The preceding code takes about a minute to run on my laptop  because it is trying many, 1,\n",
      "648, different combinations of parameters and cross-validating each one five times 41\n",
      "All in\n",
      "all, to get the best answer, it is fitting 8,400 different KNN models:\n",
      "grid 24\n",
      "best_score_\n",
      "0 4\n",
      "98666666\n",
      "grid 6\n",
      "best_params_\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 6, 'p': 3, 'weights': 'uniform'}\n",
      "Grid searching is a simple (but inefficient) way of parameter tuning our models to get the\n",
      "best possible outcome 51\n",
      "It should be noted that to get the best possible outcome, data\n",
      "scientists should use feature manipulation (both reduction and engineering) to obtain better\n",
      "results in practice as well 35\n",
      "It should not merely be up to the model to achieve the best\n",
      "performance 15\n",
      "\n",
      "Visualizing training error versus cross-validation\n",
      "error\n",
      "I think it is important once again to go over and compare the cross-validation error and the\n",
      "training error 32\n",
      "This time, let's put them both on the same graph to compare  how they both \n",
      "change  as we vary the model  complexity 28\n",
      "\n",
      "I will use the mammal dataset once more to show the cross-validation error and the\n",
      "training error (the error on predicting the training set) 30\n",
      "Recall that we are attempting to\n",
      "regress the body weight of a mammal to the brain weight of a mammal:\n",
      "# This function uses a numpy polynomial fit function to\n",
      "# calculate the RMSE of given X and y\n",
      "def rmse(x, y, coefs):\n",
      "    yfit = np 62\n",
      "polyval(coefs, x)\n",
      "    rmse = np 12\n",
      "sqrt(np 2\n",
      "mean((y - yfit) ** 2))\n",
      "    return rmse\n",
      "xtrain, xtest, ytrain, ytest = train_test_split(df['body'], df['brain'])\n",
      "train_err = []\n",
      "validation_err = []\n",
      "degrees = range(1, 8)\n",
      "for i, d in enumerate(degrees):\n",
      "    p = np 69\n",
      "polyfit(xtrain, ytrain, d)\n",
      "  # built in numpy polynomial fit function\n",
      "\n",
      "    train_err 22\n",
      "append(rmse(xtrain, ytrain, p))\n",
      "    validation_err 14\n",
      "append(rmse(xtest, ytest, p))\n",
      "fig, ax = plt 16\n",
      "subplots()\n",
      "# begin to make our graph\n",
      "ax 11\n",
      "plot(degrees, validation_err, lw=2, label = 'cross-validation error')\n",
      "ax 19\n",
      "plot(degrees, train_err, lw=2, label = 'training error')\n",
      "# Our two curves, one for training error, the other for cross validation\n",
      "ax 34\n",
      "legend(loc=0)\n",
      "ax 6\n",
      "set_xlabel('degree of polynomial')\n",
      "ax 8\n",
      "set_ylabel('RMSE')\n",
      "So, we see that as we increase our degree of fit, our training error goes down without a\n",
      "hitch, but we are now smart enough to know that as we increase the model complexity, our\n",
      "model is overfitting to our data and is merely regurgitating our data back to us, whereas\n",
      "our cross-validation error line is much more honest and begins to perform poorly after\n",
      "about degree 2 or 3 93\n",
      "\n",
      "\n",
      "Let's recap:\n",
      "Underfitting occurs when the cross-validation error and the training error are\n",
      "both high\n",
      "Overrfitting occurs when the cross-validation error is high, while the training\n",
      "error is low\n",
      "We have a good fit when the cross-validation error is low, and only slightly\n",
      "higher than the training error\n",
      "Both underfitting (high bias) and overfitting (high variance) will result in poor\n",
      "generalization of the data 93\n",
      "\n",
      "Here are some tips if you face high  bias or variance 13\n",
      "\n",
      "Try the following if your model  tends to have a high bias :\n",
      "Try adding more features to the training and test sets\n",
      "Either add to the complexity of your model  or try a more modern sophisticated\n",
      "model\n",
      "Try the following if your model tends to have a high variance :\n",
      "Try to include more training samples, which reduces the effect of overfitting\n",
      "In general, the bias/variance tradeoff is the struggle to minimize the bias and variance in\n",
      "our learning algorithms 96\n",
      "Many newer learning algorithms, invented in the past few\n",
      "decades, were made with the intention of having the best of both worlds 26\n",
      "\n",
      "Ensembling techniques\n",
      "Ensemble learning , or ensembling, is the process of combining  multiple predictive models\n",
      "to produce a supermodel that is more accurate than any individual model on its own:\n",
      "Regression : We will take the average of the predictions for each model\n",
      "Classification : Take a vote and use the most common prediction, or take the\n",
      "average of the predicted probabilities\n",
      "\n",
      "Imagine that we are working on a binary classification problem (predicting either 0 or 1):\n",
      "# ENSEMBLING\n",
      "import numpy as np\n",
      "# set a seed for reproducibility\n",
      "np 116\n",
      "random 1\n",
      "seed(12345)\n",
      "# generate 2000 random numbers (between 0 and 1) for each model,\n",
      "representing 2000 observations\n",
      "mod1 = np 35\n",
      "random 1\n",
      "rand(2000)\n",
      "mod2 = np 9\n",
      "random 1\n",
      "rand(2000)\n",
      "mod3 = np 9\n",
      "random 1\n",
      "rand(2000)\n",
      "mod4 = np 9\n",
      "random 1\n",
      "rand(2000)\n",
      "mod5 = np 9\n",
      "random 1\n",
      "rand(2000)\n",
      "Now, we simulate five different learning models, and each  has about a 70% accuracy, as\n",
      "follows:\n",
      "# each model independently predicts 1 (the \"correct response\") if random\n",
      "number was at least 0 52\n",
      "4\n",
      "preds1 = np 8\n",
      "where(mod1 > 0 6\n",
      "4, 1, 0)\n",
      "preds2 = np 14\n",
      "where(mod2 > 0 6\n",
      "4, 1, 0)\n",
      "preds3 = np 14\n",
      "where(mod3 > 0 6\n",
      "4, 1, 0)\n",
      "preds4 = np 14\n",
      "where(mod4 > 0 6\n",
      "4, 1, 0)\n",
      "preds5 = np 14\n",
      "where(mod5 > 0 6\n",
      "4, 1, 0)\n",
      "print(preds1 13\n",
      "mean())\n",
      "0 3\n",
      "596\n",
      "print (preds2 8\n",
      "mean())\n",
      "0 3\n",
      "6065\n",
      "print (preds3 9\n",
      "mean())\n",
      "0 3\n",
      "591\n",
      "print (preds4 8\n",
      "mean())\n",
      "0 3\n",
      "5965\n",
      "print( preds5 8\n",
      "mean())\n",
      "# 0 5\n",
      "611\n",
      "# Each model has an \"accuracy of around 60% on its own\n",
      "Now, let's apply my degrees in magic 28\n",
      "Er 1\n",
      " 1\n",
      " 1\n",
      "sorry, math:\n",
      "# average the predictions and then round to 0 or 1\n",
      "ensemble_preds = np 22\n",
      "round((preds1 + preds2 + preds3 + preds4 +\n",
      "preds5)/5 20\n",
      "0) 3\n",
      "astype(int)\n",
      "ensemble_preds 6\n",
      "mean()\n",
      "\n",
      "The output is as follows:\n",
      "0 9\n",
      "674\n",
      "As you add more models to a voting process, the probability of errors will decrease; this is\n",
      "known as Condorcet's jury theorem 31\n",
      "\n",
      "Crazy, right 5\n",
      "\n",
      "For ensembling to work well in practice, the models must  have the following\n",
      "characteristics:\n",
      "Accuracy : Each model must at least outperform the null model\n",
      "Independence : A model's prediction process is not affected by another model's\n",
      "prediction process\n",
      "If you have a bunch of individually OK models, the edge case mistakes made by one model\n",
      "are probably not going to be made by the other models, so the mistakes will be ignored\n",
      "when combining the models 95\n",
      "\n",
      "There are the following two basic methods for ensembling:\n",
      "Manually ensemble your individual models by writing a good deal of code\n",
      "Use a model that ensembles for you\n",
      "We're going to look at a model that ensembles for us 48\n",
      "To do this, let's take a look at decision\n",
      "trees again 14\n",
      "\n",
      "Decision trees tend to have low bias and high variance 11\n",
      "Given any dataset, the tree can keep\n",
      "asking questions (making decisions) until it is able to nitpick and distinguish between every\n",
      "single  example in the dataset 33\n",
      "It could keep asking question after question until there is only\n",
      "a single example in each leaf (terminal) node 22\n",
      "The tree is trying too hard, growing too deep,\n",
      "and just memorizing every single detail of our training set 22\n",
      "However, if we started over, the\n",
      "tree could potentially ask different questions and still grow very deep 20\n",
      "This means that\n",
      "there are many possible trees that could distinguish between all elements, which means\n",
      "the higher variance 22\n",
      "It is unable to generalize well 6\n",
      "\n",
      "In order to reduce the variance of a single tree, we can place a restriction on the number of\n",
      "questions asked in a tree (the max_depth  parameter) or we can create an ensemble version\n",
      "of decision trees, called random forests 49\n",
      "\n",
      "\n",
      "Random forests\n",
      "The primary weakness of decision trees is that different  splits in the training  data can lead\n",
      "to very different trees 27\n",
      "Bagging is a general purpose procedure to reduce the variance of a\n",
      "machine learning method but is particularly useful for decision trees 24\n",
      "\n",
      "Bagging is short for Bootstrap aggregation, which means  the aggregation of Bootstrap\n",
      "samples 18\n",
      "What is a Bootstrap sample 5\n",
      "\n",
      "A Bootstrap sample is a smaller  sample that is \" bootstrapped\" from a larger  sample 21\n",
      "\n",
      "Bootstrapping is a type of resampling where large numbers of smaller  samples of the same\n",
      "size are repeatedly drawn, with replacement, from a single original  sample:\n",
      "# set a seed for reproducibility\n",
      "np 46\n",
      "random 1\n",
      "seed(1)\n",
      "# create an array of 1 through 20\n",
      "nums = np 18\n",
      "arange(1, 21)\n",
      "print (nums)\n",
      "The output is as follows:\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "# sample that array 20 times with replacement\n",
      "np 79\n",
      "random 1\n",
      "choice(a=nums, size=20, replace=True)\n",
      "The preceding command will create a bootstrapped sample as follows:\n",
      " [ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]\n",
      "# This is our bootstrapped sample notice it has repeat variables 87\n",
      "\n",
      "So, how does bagging work for decision trees 11\n",
      "\n",
      "Grow B trees using Bootstrap samples from the training data 1 13\n",
      "\n",
      "Train each tree on its Bootstrap sample and make predictions2 12\n",
      "\n",
      "Combine the predictions:3 6\n",
      "\n",
      "Average the predictions for regression trees\n",
      "Take a vote for classification trees\n",
      "\n",
      "The following are a few things to note:\n",
      "Each Bootstrap sample should be the same size as the original training set\n",
      "B should be a large enough value that the error seems to have stabilized\n",
      "The trees are grown intentionally deep so that they have low bias/high variance\n",
      "The reason we grow the trees intentionally deep is that the bagging  inherently increases\n",
      "predictive accuracy by reducing the variance, similar to how cross-validation reduces the\n",
      "variance associated with estimating our out-of-sample error 111\n",
      "\n",
      "Random forests are a variation  of bagged trees 11\n",
      "\n",
      "However, when building each tree, each time we consider a split between the features, a\n",
      "random sample of m features is chosen as split candidates from the full set of p features 37\n",
      "\n",
      "The split is only allowed to be one of those m features:\n",
      "A new random sample of features is chosen for every single  tree at every single\n",
      "split\n",
      "For classification, m is typically chosen to be the square root of p\n",
      "For regression, m is typically chosen to be somewhere between p/3 and p\n",
      "What's the point 69\n",
      "\n",
      "Suppose there is one very strong feature in the dataset 12\n",
      "When using decision (or bagged)\n",
      "trees, most of the trees will use that feature as the top split, resulting in an ensemble of\n",
      "similar trees that are highly correlated with each other 38\n",
      "\n",
      "If our trees are highly correlated with each other, then averaging these quantities will not\n",
      "significantly reduce variance (which is the entire goal of ensembling) 32\n",
      "Also, by randomly\n",
      "leaving out candidate features from each split, random forests reduce the variance of the\n",
      "resulting model 25\n",
      "\n",
      "\n",
      "Random forests can be used in both classification and regression problems and can be\n",
      "easily used in scikit-learn 24\n",
      "Let's try to predict MLB salaries based on statistics about the\n",
      "player, as shown:\n",
      "# read in the data\n",
      "url = ' 27\n",
      " 1\n",
      "/data/hitters 4\n",
      "csv'\n",
      "hitters = pd 6\n",
      "read_csv(url)\n",
      "# remove rows with missing values\n",
      "hitters 13\n",
      "dropna(inplace=True)\n",
      "# encode categorical variables as integers\n",
      "hitters['League'] = pd 19\n",
      "factorize(hitters 4\n",
      "League)[0]\n",
      "hitters['Division'] = pd 11\n",
      "factorize(hitters 4\n",
      "Division)[0]\n",
      "hitters['NewLeague'] = pd 12\n",
      "factorize(hitters 4\n",
      "NewLeague)[0]\n",
      "# define features: exclude career statistics (which start with \"C\") and the\n",
      "response (Salary)\n",
      "feature_cols = [h for h in hitters 35\n",
      "columns if h[0] 6\n",
      "= 'C' and h 6\n",
      "=\n",
      "'Salary']\n",
      "# define X and y\n",
      "X = hitters[feature_cols]\n",
      "y = hitters 20\n",
      "Salary\n",
      "Let's try and predict the salary  first using a single  decision tree, as illustrated:\n",
      "from sklearn 23\n",
      "tree import DecisionTreeRegressor\n",
      "# list of values to try for max_depth\n",
      "max_depth_range = range(1, 21)\n",
      "# list to store the average RMSE for each value of max_depth\n",
      "RMSE_scores = []\n",
      "# use 10-fold cross-validation with each value of max_depth\n",
      "from sklearn 63\n",
      "cross_validation import cross_val_score\n",
      "for depth in max_depth_range:\n",
      "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
      "    MSE_scores = cross_val_score(treereg, X, y, cv=10,\n",
      "scoring='mean_squared_error')\n",
      "    RMSE_scores 61\n",
      "append(np 2\n",
      "mean(np 2\n",
      "sqrt(-MSE_scores)))\n",
      "# plot max_depth (x-axis) versus RMSE (y-axis)\n",
      "plt 22\n",
      "plot(max_depth_range, RMSE_scores)\n",
      "\n",
      "plt 10\n",
      "xlabel('max_depth')\n",
      "plt 6\n",
      "ylabel('RMSE (lower is better)')\n",
      "RMSE for decision tree models against the max depth of the tree (complexity)\n",
      "Let's do the same thing, but this time with a random forest:\n",
      "from sklearn 43\n",
      "ensemble import RandomForestRegressor\n",
      "# list of values to try for n_estimators\n",
      "estimator_range = range(10, 310, 10)\n",
      "# list to store the average RMSE for each value of n_estimators\n",
      "RMSE_scores = []\n",
      "# use 5-fold cross-validation with each value of n_estimators (WARNING:\n",
      "SLOW 67\n",
      "\n",
      "for estimator in estimator_range:\n",
      "    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",
      "    MSE_scores = cross_val_score(rfreg, X, y, cv=5,\n",
      "scoring='mean_squared_error')\n",
      "    RMSE_scores 54\n",
      "append(np 2\n",
      "mean(np 2\n",
      "sqrt(-MSE_scores)))\n",
      "\n",
      "# plot n_estimators (x-axis) versus RMSE (y-axis)\n",
      "plt 22\n",
      "plot(estimator_range, RMSE_scores)\n",
      "plt 10\n",
      "xlabel('n_estimators')\n",
      "plt 6\n",
      "ylabel('RMSE (lower is better)')\n",
      "RMSE for random forest models against the max depth of the tree (complexity)\n",
      "Note already the y-axis; our RMSE is much lower on an average 41\n",
      "See how we can obtain a\n",
      "major increase in predictive power using random forests 15\n",
      "\n",
      "\n",
      "In random forests, we still have the concept of important features, like we had in decision\n",
      "trees:\n",
      "# n_estimators=150 is sufficiently good\n",
      "rfreg = RandomForestRegressor(n_estimators=150, random_state=1)\n",
      "rfreg 48\n",
      "fit(X, y)\n",
      "# compute feature importances\n",
      "pd 12\n",
      "DataFrame({'feature':feature_cols,\n",
      "'importance':rfreg 13\n",
      "feature_importances_}) 5\n",
      "sort('importance', ascending =\n",
      "False)\n",
      "So, it looks like the number of years the player  has been in the league  is still the most\n",
      "important feature when deciding that player's salary 40\n",
      "\n",
      "\n",
      "Comparing random forests with decision trees\n",
      "It is important to realize that just using  random forests is not the solution  to your data\n",
      "science problems 31\n",
      "While random forests provide many advantages, many disadvantages\n",
      "also come with them 14\n",
      "\n",
      "The advantages of random forests are as follows:\n",
      "Their performance is competitive with the best-supervised learning methods\n",
      "They provide a more reliable estimate of feature importance\n",
      "They allow you to estimate out-of-sample errors without using train/test splits or\n",
      "cross-validation\n",
      "The disadvantages of random forests are as follows:\n",
      "They are less interpretable (cannot visualize an entire forest of decision trees)\n",
      "They are slower to train and predict (not great for production or real-time\n",
      "purposes)\n",
      "Neural networks\n",
      "Probably one of the most talked about machine  learning models, neural networks are\n",
      "computational networks built to model animals' nervous systems 124\n",
      "Before getting too deep\n",
      "into the structure, let's take a look at the big advantages of neural networks 21\n",
      "\n",
      "The key component of a neural network is that it is not only a complex structure, but it is\n",
      "also a complex and flexible structure 28\n",
      "This means the following two things:\n",
      "Neural networks are able to estimate any function shape (this is called being non-\n",
      "parametric)\n",
      "Neural networks can adapt and literally change their own internal structure\n",
      "based on their environment\n",
      "\n",
      "Basic structure\n",
      "Neural networks are made up of interconnected nodes ( perceptrons ) that each  take in input\n",
      "(quantitative value), and output other quantitative values 79\n",
      "Signals travel through  the\n",
      "network and eventually end up at a prediction node:\n",
      "Visualization of neural network interconnected nodes\n",
      "Another huge advantage of neural networks is that they can be used for supervised\n",
      "learning, unsupervised learning, and reinforcement learning problems 49\n",
      "The ability to be so\n",
      "flexible, predict many functional shapes, and adapt to their surroundings make neural\n",
      "networks highly preferable in select fields, as follows:\n",
      "Pattern recognition : This is probably the most  common application of neural\n",
      "networks 49\n",
      "Some examples are handwriting recognition and image processing\n",
      "(facial recognition) 13\n",
      "\n",
      "Entity movement : Examples for this include  self-driving cars, robotic animals,\n",
      "and drone movement 19\n",
      "\n",
      "Anomaly detection : As neural networks are good at recognizing  patterns, they\n",
      "can also be used to recognize when a data point does not fit a pattern 32\n",
      "Think of a\n",
      "neural network monitoring a stock price movement; after a while of learning the\n",
      "general pattern of a stock price, the network can alert you when something is\n",
      "unusual in the movement 41\n",
      "\n",
      "\n",
      "The simplest form of a neural network is a single perceptron 13\n",
      "A perceptron, visualized as\n",
      "follows, takes in some input and outputs a signal:\n",
      "This signal is obtained by combining  the input with several weights and then is put\n",
      "through some activation function 41\n",
      "In cases of simple binary outputs, we generally use the\n",
      "logistic function, as shown:\n",
      "To create a neural network, we need to connect multiple perceptrons to each other in a\n",
      "network fashion, as illustrated in the following graph 48\n",
      "\n",
      "\n",
      "A multilayer perceptron  (MLP ) is a finite acyclic  graph 19\n",
      "The nodes are neurons with\n",
      "logistic activation:\n",
      "As we train the model, we update the weights (which are random at first) of the model in\n",
      "order to get the best predictions possible 39\n",
      "If an observation goes through the model and is\n",
      "outputted as false when it should have been true, the logistic functions in the single\n",
      "perceptrons are changed slightly 35\n",
      "This is called back-propagation 6\n",
      "Neural networks are \n",
      "usually  trained in batches, which means that the network is given several training  data\n",
      "points at once several times, and each time, the back-propagation algorithm will trigger an\n",
      "internal weight change in the network 47\n",
      "\n",
      "It isn't hard to see that we can grow the network very deep and have many hidden layers,\n",
      "which are associated with the complexity of the neural network 31\n",
      "When we grow our neural\n",
      "networks very deep, we are dipping our toes into the idea of deep learning 22\n",
      "The main\n",
      "advantage of deep neural networks (networks with many layers) is that they can\n",
      "approximate almost any shape function and they can (theoretically) learn optimal\n",
      "combinations of features for us and use these combinations to obtain the best predictive\n",
      "power 55\n",
      "\n",
      "\n",
      "Let's see this in action 7\n",
      "I will be using a module called PyBrain to make my neural\n",
      "networks 16\n",
      "However, first let's take a look at a new dataset, which is a dataset of \n",
      "handwritten  digits 23\n",
      "We will first try to recognize digits using a random forest, as shown:\n",
      "from sklearn 17\n",
      "cross_validation import cross_val_score\n",
      "from sklearn import datasets\n",
      "import matplotlib 14\n",
      "pyplot as plt\n",
      "from sklearn 6\n",
      "ensemble import RandomForestClassifier\n",
      "%matplotlib inline\n",
      "digits = datasets 12\n",
      "load_digits()\n",
      "plt 4\n",
      "imshow(digits 2\n",
      "images[100], cmap=plt 6\n",
      "cm 1\n",
      "gray_r, interpolation='nearest')\n",
      "# a 4 digit\n",
      "X, y = digits 18\n",
      "data, digits 3\n",
      "target\n",
      "# 64 pixels per image\n",
      "X[0] 13\n",
      "shape\n",
      "# Try Random Forest\n",
      "rfclf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
      "cross_val_score(rfclf, X, y, cv=5, scoring='accuracy') 41\n",
      "mean()\n",
      "0 3\n",
      "9382782\n",
      "\n",
      "Pretty good 7\n",
      "An accuracy of 94% is nothing to laugh at, but can we do even better 18\n",
      "\n",
      "Warning 2\n",
      "The PyBrain syntax can be a bit tricky 9\n",
      "\n",
      "from pybrain 4\n",
      "datasets            import ClassificationDataSet\n",
      "from pybrain 9\n",
      "utilities           import percentError\n",
      "from pybrain 9\n",
      "tools 1\n",
      "shortcuts     import buildNetwork\n",
      "from pybrain 9\n",
      "supervised 1\n",
      "trainers import BackpropTrainer\n",
      "from pybrain 10\n",
      "structure 1\n",
      "modules   import SoftmaxLayer\n",
      "from numpy import ravel\n",
      "# pybrain has its own data sample class that we must add\n",
      "# our training and test set to\n",
      "ds = ClassificationDataSet(64, 1 , nb_classes=10)\n",
      "for k in xrange(len(X)):\n",
      "    ds 59\n",
      "addSample(ravel(X[k]),y[k])\n",
      "# their equivalent of train test split\n",
      "test_data, training_data = ds 25\n",
      "splitWithProportion( 0 7\n",
      "25 )\n",
      "# pybrain's version of dummy variables\n",
      "test_data 14\n",
      "_convertToOneOfMany( )\n",
      "training_data 9\n",
      "_convertToOneOfMany( )\n",
      "print test_data 10\n",
      "indim # number of pixels going in\n",
      "# 64\n",
      "print test_data 16\n",
      "outdim # number of possible options (10 digits)\n",
      "# 10\n",
      "# instantiate the model with 64 hidden layers (standard params)\n",
      "fnn = buildNetwork( training_data 36\n",
      "indim, 64, training_data 8\n",
      "outdim,\n",
      "outclass=SoftmaxLayer )\n",
      "trainer = BackpropTrainer( fnn, dataset=training_data, momentum=0 28\n",
      "1,\n",
      "learningrate=0 7\n",
      "01 , verbose=True, weightdecay=0 10\n",
      "01)\n",
      "# change the number of epochs to try to get better results 15\n",
      "\n",
      "trainer 2\n",
      "trainEpochs (10) # 10 batches\n",
      "\n",
      "print 'Percent Error on Test dataset: ' , \\\n",
      "        percentError( trainer 27\n",
      "testOnClassData (\n",
      "           dataset=test_data )\n",
      "           , test_data['class'] )\n",
      "The model will output a final error on a test set:\n",
      "Percent Error on Test dataset: 4 38\n",
      "67706013363\n",
      "accuracy = 1 - 11\n",
      "0467706013363\n",
      "accuracy\n",
      "0 10\n",
      "95322\n",
      "Already better 6\n",
      "Both the random forests and neural networks  do very well with this\n",
      "problem because both of them are non-parametric, which means that they do not rely on\n",
      "the underlying shape of the data to make predictions 42\n",
      "They are able to estimate any shape\n",
      "of function 10\n",
      "\n",
      "To predict the shape, we can use the following code:\n",
      "plt 14\n",
      "imshow(digits 2\n",
      "images[0], cmap=plt 6\n",
      "cm 1\n",
      "gray_r, interpolation='nearest')\n",
      "fnn 9\n",
      "activate(X[0])\n",
      "array([ 0 9\n",
      "92183643,  0 8\n",
      "00126609,  0 8\n",
      "00303146,  0 8\n",
      "00387049,  0 8\n",
      "01067609,\n",
      "        0 8\n",
      "00718017,  0 8\n",
      "00825521,  0 8\n",
      "00917995,  0 8\n",
      "00696929,  0 8\n",
      "02773482])\n",
      "\n",
      "\n",
      "The array represents a probability for every single digit, which means that there is a 92%\n",
      "chance that the digit in the preceding screenshot is a 0 (which it is) 42\n",
      "Note how the next\n",
      "highest probability is for a 9, which makes sense because 9 and 0 have similar shapes\n",
      "(ovular) 30\n",
      "\n",
      "Neural networks do have a major flaw 9\n",
      "If left alone, they have a very high variance 10\n",
      "To see\n",
      "this, let's run the exact same code as the preceding one and train the exact same type of\n",
      "neural network  on the exact same data, as illustrated:\n",
      "# Do it again and see the difference in error\n",
      "fnn = buildNetwork( training_data 56\n",
      "indim, 64, training_data 8\n",
      "outdim,\n",
      "outclass=SoftmaxLayer )\n",
      "trainer = BackpropTrainer( fnn, dataset=training_data, momentum=0 28\n",
      "1,\n",
      "learningrate=0 7\n",
      "01 , verbose=True, weightdecay=0 10\n",
      "01)\n",
      "# change the number of eopchs to try to get better results 17\n",
      "\n",
      "trainer 2\n",
      "trainEpochs (10)\n",
      "print ('Percent Error on Test dataset: ' , \\\n",
      "        percentError( trainer 22\n",
      "testOnClassData (\n",
      "           dataset=test_data )\n",
      "           , test_data['class'] ) )\n",
      "accuracy = 1 - 24\n",
      "0645879732739\n",
      "accuracy\n",
      "0 10\n",
      "93541\n",
      "See how just rerunning the model and instantiating different weights made the network\n",
      "turn out to be different than before 27\n",
      "This is a symptom of being a high variance model 10\n",
      "In\n",
      "addition, neural networks generally require many training samples in order to combat the\n",
      "high variances  of the model and also require a large amount of computation power to work\n",
      "well in production environments 41\n",
      "\n",
      "\n",
      "Summary\n",
      "This concludes our long journey into the principles of data science 14\n",
      "In the last 300-odd\n",
      "pages, we looked at different techniques in probability, statistics, and machine learning to\n",
      "answer the most difficult questions out there 32\n",
      "I would like to personally congratulate you\n",
      "on making it through this book 14\n",
      "I hope that it proved useful and inspired you to learn even\n",
      "more 14\n",
      "\n",
      "This isn't everything I need to know 9\n",
      "\n",
      "Nope 3\n",
      "There is only so much I can fit into a principles  level book 14\n",
      "There is still so much to\n",
      "learn 8\n",
      "\n",
      "Where can I learn more 6\n",
      "\n",
      "I recommend going to find open source data challenges ( https://www 14\n",
      "kaggle 3\n",
      "com/  is a\n",
      "good source) for this 11\n",
      "I'd also recommend seeking out, trying, and solving your own\n",
      "problems at home 17\n",
      "\n",
      "When do I get to call myself a data scientist 11\n",
      "\n",
      "When you begin cultivating actionable insights from datasets, both large and small, that\n",
      "companies and people can use, then you have the honor of calling yourself a true data\n",
      "scientist 37\n",
      "\n",
      "In next chapter, we will apply concepts learned in this book to real-life case studies,\n",
      "including building predictive models to predict the stock market 28\n",
      "We will also cover the\n",
      "emerging machine learning topic of TensorFlow 13\n",
      "\n",
      "\n",
      "3\n",
      "Case Studies\n",
      "In this chapter, we will take a look at a few case studies to help you develop a better\n",
      "understanding of the topics we've seen so far 37\n",
      "\n",
      "Case study 1 – Predicting stock prices\n",
      "based on social media\n",
      "Our first case study will be quite  exciting 25\n",
      "We will attempt to predict the price of the stock\n",
      "of a publicly traded company using only social media sentiment 21\n",
      "While this example  will\n",
      "not use any explicit statistical/machine learning algorithms, we will utilize exploratory data\n",
      "analysis  (EDA ) and use visuals in order to achieve our goal 37\n",
      "\n",
      "Text sentiment analysis\n",
      "When talking about sentiment, it should be clear  what is meant 18\n",
      "By sentiment, I am\n",
      "referring to a quantitative value (at the interval level) between -1 and 1 24\n",
      "If the sentiment\n",
      "score of a text piece is close to -1, it is said to have negative sentiment 22\n",
      "If the sentiment score\n",
      "is close to 1, then the text is said to have positive sentiment 20\n",
      "If the sentiment score is close to\n",
      "0, we say it has neutral sentiment 16\n",
      "We will use a Python module called TextBlob  to\n",
      "measure our text sentiment:\n",
      "from textblob import TextBlob\n",
      "# use the textblob module to make a function called stringToSentiment that\n",
      "returns a sentences sentiment\n",
      "def stringToSentiment(text):\n",
      "    return TextBlob(text) 59\n",
      "sentiment 1\n",
      "polarity\n",
      "\n",
      "Now, we can use this function, which calls the Textblob  module to score text out of the\n",
      "box:\n",
      "stringToSentiment('i hate you')\n",
      "# -0 38\n",
      "8\n",
      "stringToSentiment('i love you')\n",
      "# 0 15\n",
      "5\n",
      "stringToSentiment('i see you')\n",
      "# 0 15\n",
      "0\n",
      "Now, let's read  in our tweets  for our study :\n",
      "# read in tweets data into a dataframe\n",
      "from textblob import TextBlob\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "# these tweets are from last May and are about Apple (AAPL)\n",
      "tweets = pd 60\n",
      "read_csv(' 3\n",
      " 1\n",
      "/data/ so_many_tweets 6\n",
      "csv ')\n",
      "tweets 3\n",
      "head()\n",
      "Exploratory data analysis\n",
      "So we have four  columns, as follows:\n",
      "Text : Unstructured text at the nominal level\n",
      "Date : Datetime (we will think of datetime in a continuous way)\n",
      "Status : Status unique ID at the nominal level\n",
      "Retweet : Status ID of tweet showing that this tweet was a retweet at the nominal\n",
      "level\n",
      "\n",
      "So we have four columns, but how many rows 84\n",
      "Also, what does each row represent 7\n",
      "It\n",
      "seems that each  row represents a single tweet about the company:\n",
      "tweets 17\n",
      "shape\n",
      "The output is as follows:\n",
      "(52512, 4)\n",
      "So, we have four columns and 52512  tweets/rows at our disposal 32\n",
      "Oh boy … Our goal here\n",
      "is to eventually use the tweets' sentiments, so we will likely need a sentiment column in the\n",
      "DataFrame 28\n",
      "Using our fairly straightforward function from the previous example, let's add\n",
      "this column 16\n",
      "\n",
      "# create a new column in tweets called sentiment that maps\n",
      "stringToSentiment to the text column\n",
      "tweets['sentiment'] = tweets['Text'] 32\n",
      "apply(stringToSentiment)\n",
      "tweets 7\n",
      "head()\n",
      "The preceding code will apply the stringToSentiment  function to each and every\n",
      "element in the Text  column of the tweets  DataFrame :\n",
      "tweets 32\n",
      "head()\n",
      "So, now we have a sense for the sentiment score for each tweet in this dataset 19\n",
      "Let's simplify\n",
      "our problem and try to use an entire days' worth of tweets to predict whether or not the\n",
      "price of AAPL will increase within 24 hours 34\n",
      "If this is the case, we have another issue here 11\n",
      "\n",
      "The Date  column reveals that we have multiple tweets for each day 14\n",
      "Just look at the first\n",
      "five tweets; they are all on the same day 16\n",
      "We will resample this dataset in order to get a\n",
      "sense of the average  sentiment of the stock on Twitter every day 25\n",
      "\n",
      "\n",
      "We will do this in three steps:\n",
      "We will ensure that the Date  column is of the Python datetime  type 24\n",
      "1 2\n",
      "\n",
      "We will replace our DataFrame's index with the datetime  column (which allows 2 18\n",
      "\n",
      "us to use complex datetime  functions) 9\n",
      "\n",
      "We will resample the data so that each row, instead of representing a tweet, will3 20\n",
      "\n",
      "represent a single day with an aggregated sentiment score for each day:\n",
      "The index of the DataFrame is a special series used to identify rows in our\n",
      "structure 31\n",
      "By default, a DataFrame will use incremental integers to\n",
      "represent rows ( 0 for the first row, 1 for the second row, and so on) 32\n",
      "\n",
      "import pandas as pd\n",
      "tweets 7\n",
      "index = pd 3\n",
      "RangeIndex(start=0, stop=52512, step=1)\n",
      "# As a list, we can splice it\n",
      "list(tweets 28\n",
      "index)[:5]\n",
      "[0, 1, 2, 3, 4]\n",
      "Let's tackle this date  issue now 27\n",
      "We will ensure  that the Date  column is of the 4 14\n",
      "\n",
      "Python datetime  type:\n",
      "# cast the date column as a datetime\n",
      "tweets['Date'] = pd 21\n",
      "to_datetime(tweets 4\n",
      "Date)\n",
      "tweets['Date'] 6\n",
      "head()\n",
      "Date\n",
      "2015-05-24 03:46:08   2015-05-24 03:46:08\n",
      "2015-05-24 04:17:42   2015-05-24 04:17:42\n",
      "2015-05-24 04:13:22   2015-05-24 04:13:22\n",
      "2015-05-24 04:08:34   2015-05-24 04:08:34\n",
      "2015-05-24 04:04:42   2015-05-24 04:04:42\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "We will replace our DataFrame's index with the datetime  column (which allows 5 167\n",
      "\n",
      "us to use complex datetime  functions ):\n",
      "tweets 10\n",
      "index = tweets 3\n",
      "Date\n",
      "tweets 3\n",
      "index\n",
      "Index([u'2015-05-24 03:46:08', u'2015-05-24 04:17:42',\n",
      "u'2015-05-24 04:13:22',\n",
      "       u'2015-05-24 04:08:34', u'2015-05-24 04:04:42',\n",
      "u'2015-05-24 04:00:01',\n",
      "       u'2015-05-24 03:54:07', u'2015-05-24 04:25:29',\n",
      "u'2015-05-24 04:24:47',\n",
      "       u'2015-05-24 04:06:42',\n",
      " 157\n",
      " 1\n",
      " 1\n",
      "\n",
      "       u'2015-05-02 16:30:02', u'2015-05-02 16:29:35',\n",
      "u'2015-05-02 16:28:26',\n",
      "       u'2015-05-02 16:27:53', u'2015-05-02 16:27:02',\n",
      "u'2015-05-02 16:26:39',\n",
      "       u'2015-05-02 16:25:00', u'2015-05-02 16:23:39',\n",
      "u'2015-05-02 16:23:38',\n",
      "       u'2015-05-02 16:23:21'],\n",
      "      dtype='object', name=u'Date', length=52512)\n",
      "tweets 171\n",
      "head()\n",
      "\n",
      "\n",
      "Note that the black index on the left used to be numbers, but now is the\n",
      "exact datetime  that the tweet was sent 28\n",
      "\n",
      "Resample the data so that each row, instead of representing a tweet, will6 18\n",
      "\n",
      "represent a single day with an aggregated sentiment score for each day:\n",
      "# create a dataframe called daily_tweets which resamples tweets\n",
      "by D, averaging the columns\n",
      "daily_tweets = tweets[['sentiment']] 41\n",
      "resample('D', how='mean')\n",
      "# I only want the sentiment column in my new Dataframe 21\n",
      "\n",
      "daily_tweets 3\n",
      "head()\n",
      "That's looking better 6\n",
      "Now, each row represents a single day and the sentiment score \n",
      "column  is showing us an average  sentiment for the day 25\n",
      "Let's see how many days' worth of\n",
      "tweets we have:\n",
      "daily_tweets 16\n",
      "shape\n",
      "(23, 1)\n",
      "OK, so we went from over 50,000 tweets to only 23 days 25\n",
      "Now, let's take a look at the\n",
      "progression of sentiment over several days :\n",
      "# plot the sentiment as a line graph\n",
      "daily_tweets 29\n",
      "sentiment 1\n",
      "plot(kind='line')\n",
      "\n",
      "\n",
      "Average daily sentiment in regard to a speciﬁc company for 23 days in May 2015\n",
      "import pandas as pd\n",
      "import pandas_datareader as pdr\n",
      "import datetime\n",
      "historical_prices = pdr 50\n",
      "get_data_yahoo('AAPL',\n",
      "                          start=datetime 11\n",
      "datetime(2015, 5, 2),\n",
      "                          end=datetime 14\n",
      "datetime(2015, 5, 25))\n",
      "prices = pd 14\n",
      "DataFrame(historical_prices)\n",
      "prices 6\n",
      "head()\n",
      "\n",
      "\n",
      "Now, two things are primarily of interest to us here:\n",
      "We are really only interested in the Close  column, which is the final price set for\n",
      "the trading day 36\n",
      "\n",
      "We also need to set the index  of this DataFrame to be datetimes,  so that we can\n",
      "merge the sentiment and the price DataFrames together:\n",
      "prices 34\n",
      "info() #the columns aren't numbers 8\n",
      "\n",
      "<class 'pandas 5\n",
      "core 1\n",
      "frame 1\n",
      "DataFrame'>\n",
      "DatetimeIndex: 15 entries, 2015-05-22 to 2015-05-04\n",
      "Data columns (total 8 columns):\n",
      "Adj_Close    15 non-null object\n",
      "Close        15 non-null object        # NOT A NUMBER\n",
      "Date         15 non-null object\n",
      "High         15 non-null object\n",
      "Low          15 non-null object\n",
      "Open         15 non-null object\n",
      "Symbol       15 non-null object\n",
      "Volume       15 non-null object\n",
      "dtypes: object(8)\n",
      "Let's fix that 114\n",
      "While we're at it, let's also fix Volume , which represents t he number of stocks\n",
      "traded on that day:\n",
      "# cast the column as numbers\n",
      "prices 34\n",
      "Close= not_null_close 5\n",
      "Close 1\n",
      "astype('float')\n",
      "prices 6\n",
      "Volume = not_null_close 5\n",
      "Volume 1\n",
      "astype('float')\n",
      "Now, let's try to plot both the volume and price of AAPL in the same graph:\n",
      "# plot both volume and close as line graphs in the same graph, what do you\n",
      "notice is the problem 47\n",
      "\n",
      "prices[[\"Volume\", 'Close']] 9\n",
      "plot()\n",
      "\n",
      "\n",
      "Trade volume versus date\n",
      "Woah, what's wrong here 14\n",
      "Well, if we look carefully, Volume  and Close  are on very\n",
      "different scales 18\n",
      "\n",
      "prices[[\"Volume\", 'Close']] 9\n",
      "describe()\n",
      "\n",
      "\n",
      "And by a lot 6\n",
      "The Volume  column has a mean in the tens of millions, while the average\n",
      "closing price is merely 125 23\n",
      "\n",
      "from sklearn 3\n",
      "preprocessing import StandardScaler\n",
      "# scale the columns by z scores using StandardScaler\n",
      "# Then plot the scaled data\n",
      "s = StandardScaler()\n",
      "only_prices_and_volumes = prices[[\"Volume\", 'Close']]\n",
      "price_volume_scaled = s 47\n",
      "fit_transform(only_prices_and_volumes)\n",
      "pd 10\n",
      "DataFrame(price_volume_scaled, columns=[\"Volume\", 'Close']) 12\n",
      "plot()\n",
      "Correlation of volume and closing prices \n",
      "That looks much better 14\n",
      "You can see how, as the price of AAPL went down somewhere in\n",
      "the middle, the volume of trading also went up 26\n",
      "This is actually fairly common:\n",
      "# concatinate prices 10\n",
      "Close, and daily_tweets 5\n",
      "sentiment\n",
      "merged = pd 5\n",
      "concat([prices 3\n",
      "Close, daily_tweets 4\n",
      "sentiment], axis=1)\n",
      "merged 7\n",
      "head()\n",
      "\n",
      "\n",
      "Hmm, why are there some null Close  values 12\n",
      "Well, if you look up May 2, 2015 on a\n",
      "calendar, you will see that it is a Saturday and the markets are closed on Saturdays,\n",
      "meaning there cannot be a closing price 41\n",
      "So, we need to make a decision on whether or not\n",
      "to remove these rows because we still have sentiment for that day 25\n",
      "Eventually, we will be\n",
      "attempting to predict the next day's closing price and whether the price increased or not, so\n",
      "let's go ahead and remove any null values in our dataset:\n",
      "# Delete any rows with missing values in any column\n",
      "merged 51\n",
      "dropna(inplace=True)\n",
      "Now, let's attempt to graph our plot:\n",
      "merged 16\n",
      "plot()\n",
      "# wow that looks awful\n",
      "Closing price/sentiment score versus date\n",
      "\n",
      "Wow, that's terrible 22\n",
      "Once again, we must scale our features in order to gain any valuable\n",
      "insight:\n",
      "# scale the columns by z scores using StandardScaler\n",
      "from sklearn 31\n",
      "preprocessing import StandardScaler\n",
      "s = StandardScaler()\n",
      "merged_scaled = s 14\n",
      "fit_transform(merged)\n",
      "pd 6\n",
      "DataFrame(merged_scaled, columns=merged 8\n",
      "columns) 2\n",
      "plot()\n",
      "# notice how sentiment seems to follow the closing price\n",
      "Closing price versus sentiment score \n",
      "Much better 21\n",
      "You can start to see how the closing price of the stock actually does seem to\n",
      "move with our sentiment 21\n",
      "Let's take this one step further and attempt to apply a supervised\n",
      "learning model 16\n",
      "For this to work, we need to define our features and our response 14\n",
      "Recall\n",
      "that our response is the value that we wish to predict, and our features are values that we\n",
      "will use to predict the response 28\n",
      "\n",
      "If we look at each row of our data, we have a sentiment and closing price for that day 21\n",
      "\n",
      "However, we wish to use today's sentiment to predict tomorrow's stock price and whether\n",
      "it increased or not 23\n",
      "Think about it; it would be kind of cheating because today's sentiment\n",
      "will include tweets from after the closing price was finalized 25\n",
      "To simplify this, we will\n",
      "ignore any tweet as a feature for the prediction of today's price 20\n",
      "\n",
      "\n",
      "So, for each row, our response should be today's closing price, while our feature should be\n",
      "yesterday's sentiment of the stock 29\n",
      "To do this, I will use a built-in function in Pandas called\n",
      "shift  to shift our sentiment column one item backward:\n",
      "# Shift the sentiment column backwards one item\n",
      "merged['yesterday_sentiment'] = merged['sentiment'] 49\n",
      "shift(1)\n",
      "merged 5\n",
      "head()\n",
      "Dataframe with yesterday's sentiment included\n",
      "Ah good, so now, for each  day we have  our true feature, which is yesterday_sentiment 32\n",
      "\n",
      "Note that in our heads (first five rows), we have a new null value 17\n",
      "This is because, on the\n",
      "first day, we don't have a value from yesterday, so we will have to remove it 26\n",
      "But before we\n",
      "do, let's define our response column 12\n",
      "\n",
      "We have two options:\n",
      "Keep our response quantitative and use a regression analysis\n",
      "Convert our response to a qualitative state and use classification\n",
      "Which route to choose is up to the data scientist and depends on the situation 42\n",
      "If you\n",
      "merely wish to associate sentiment with a movement in price, then I recommend using the\n",
      "classification route 23\n",
      "If you wish to associate sentiment with the amount of movement, I\n",
      "recommend a regression 17\n",
      "I will do both 4\n",
      "\n",
      "\n",
      "Regression route\n",
      "We are already good to go on this front 13\n",
      "We have our response and our single feature 8\n",
      "We\n",
      "will first have to remove  that one null value  before continuing:\n",
      "# Make a new dataframe for our regression and drop the null values\n",
      "regression_df = merged[['yesterday_sentiment', 'Close']]\n",
      "regression_df 47\n",
      "dropna(inplace=True)\n",
      "regression_df 8\n",
      "head()\n",
      "Let's use both a random forest  and a linear regression and see which performs better, using\n",
      "root-mean-square error  (RMSE ) as our metric:\n",
      "# Imports for our regression\n",
      "from sklearn 43\n",
      "linear_model import LinearRegression\n",
      "from sklearn 8\n",
      "ensemble import RandomForestRegressor\n",
      "from sklearn 7\n",
      "cross_validation import cross_val_score\n",
      "import numpy as np\n",
      "We will use a cross-validated RMSE in order to compare our two models:\n",
      "# Our RMSE as a result of cross validation linear regression\n",
      "linreg = LinearRegression()\n",
      "rmse_cv = np 53\n",
      "sqrt(abs(cross_val_score(linreg,\n",
      "regression_df[['yesterday_sentiment']], regression_df['Close'], cv=3,\n",
      "scoring='mean_squared_error') 35\n",
      "mean()))\n",
      "rmse_cv\n",
      "3 7\n",
      "49837\n",
      "# Our RMSE as a result of cross validation random forest\n",
      "rf = RandomForestRegressor()\n",
      "\n",
      "rmse_cv = np 27\n",
      "sqrt(abs(cross_val_score(rf,\n",
      "regression_df[['yesterday_sentiment']], regression_df['Close'], cv=3,\n",
      "scoring='mean_squared_error') 34\n",
      "mean()))\n",
      "rmse_cv\n",
      "3 7\n",
      "30603\n",
      "Look at our RMSE; it's about 3 15\n",
      "5 for both models, meaning that on average, our model is off\n",
      "by about 3 20\n",
      "5 dollars, which is actually a big deal considering our stock price likely doesn't\n",
      "move that much:\n",
      "regression_df['Close'] 28\n",
      "describe()\n",
      "count     14 6\n",
      "000000\n",
      "mean     128 8\n",
      "132858\n",
      "std        2 8\n",
      "471810    # Our standard deviation is less than our RMSE (bad\n",
      "sign)\n",
      "min      125 23\n",
      "010002\n",
      "25%      125 9\n",
      "905003\n",
      "50%      128 9\n",
      "195003\n",
      "75%      130 9\n",
      "067505\n",
      "max      132 8\n",
      "539993\n",
      "Another way to test the validity of our model is by comparing our RMSE to the null\n",
      "model's RMSE 27\n",
      "The null model for a regression model is predicting the average value for\n",
      "each value:\n",
      "# null model for regression\n",
      "mean_close = regression_df['Close'] 31\n",
      "mean()\n",
      "preds = [mean_close]*regression_df 12\n",
      "shape[0]\n",
      "preds\n",
      "from sklearn 9\n",
      "metrics import mean_squared_error\n",
      "null_rmse = np 11\n",
      "sqrt(mean_squared_error(preds, regression_df['Close']))\n",
      "null_rmse\n",
      "2 17\n",
      "381895\n",
      "Because our model did not beat the null model, perhaps regression isn't the best way to go 23\n",
      "\n",
      "\n",
      "Classification route\n",
      "For classification, we have a bit more  work to do because  we don't have a categorical\n",
      "response yet 27\n",
      "To make one, we need to transform the closing column into some categorical\n",
      "option 16\n",
      "I will choose to make the following response 8\n",
      "I will make a new column called\n",
      "change_close_big_deal , defined as follows:\n",
      "So, our response will be 1 if our response changed significantly, and 0 if the change in stock\n",
      "was negligible:\n",
      "# Imports for our classification\n",
      "from sklearn 52\n",
      "linear_model import LogisticRegression\n",
      "from sklearn 8\n",
      "ensemble import RandomForestClassifier\n",
      "from sklearn 7\n",
      "cross_validation import cross_val_score\n",
      "import numpy as np\n",
      "# Make a new dataframe for our classification and drop the null values\n",
      "classification_df = merged[['yesterday_sentiment', 'Close']]\n",
      "# variable to represent yesterday's closing price\n",
      "classification_df['yesterday_close'] = classification_df['Close'] 61\n",
      "shift(1)\n",
      "# column that represents the precent change in price since yesterday\n",
      "classification_df['percent_change_in_price'] = (classification_df['Close']-\n",
      "classification_df['yesterday_close']) /\n",
      "classification_df['yesterday_close']\n",
      "# drop any null values\n",
      "classification_df 56\n",
      "dropna(inplace=True)\n",
      "classification_df 7\n",
      "head()\n",
      "# Our new classification response\n",
      "classification_df['change_close_big_deal'] =\n",
      "abs(classification_df['percent_change_in_price'] ) > 30\n",
      "01\n",
      "classification_df 5\n",
      "head()\n",
      "\n",
      "\n",
      "Our DataFrame with a new column called change_close_big_deal  is either True  or\n",
      "False 22\n",
      "\n",
      "Let's now perform the same cross-validation as we did with our regression, but this time,\n",
      "we will be using the accuracy  feature of our cross-validation module and, instead of a \n",
      "regression  module, we will be using  two classification machine learning algorithms:\n",
      "# Our accuracy as a result of cross validation random forest\n",
      "rf = RandomForestClassifier()\n",
      "accuracy_cv = cross_val_score(rf,\n",
      "classification_df[['yesterday_sentiment']],\n",
      "classification_df['change_close_big_deal'], cv=3,\n",
      "scoring='accuracy') 107\n",
      "mean()\n",
      "accuracy_cv\n",
      "0 6\n",
      "1777777\n",
      "Gosh 7\n",
      "Not so good, so let's try logistic regression instead:\n",
      "# Our accuracy as a result of cross validation logistic regression\n",
      "logreg = LogisticRegression()\n",
      "accuracy_cv = cross_val_score(logreg,\n",
      "classification_df[['yesterday_sentiment']],\n",
      "classification_df['change_close_big_deal'], cv=3,\n",
      "scoring='accuracy') 65\n",
      "mean()\n",
      "accuracy_cv\n",
      "0 6\n",
      "5888\n",
      "\n",
      "Better 5\n",
      "But, of course, we should check  it with our null model's accuracy:\n",
      "# null model for classification\n",
      "null_accuracy = 1 - classification_df['change_close_big_deal'] 38\n",
      "mean()\n",
      "null_accuracy\n",
      "0 6\n",
      "5833333\n",
      "Whoa, our model can beat the null accuracy,  meaning that our machine learning algorithm\n",
      "can predict the movement of a stock price using social media sentiment better than just\n",
      "randomly guessing 43\n",
      "\n",
      "Going beyond with this example\n",
      "There are many ways that we could have  enhanced this example to make a more robust\n",
      "prediction 26\n",
      "We could have included more features, including a moving average of\n",
      "sentiment, instead of looking simply at the previous day's sentiment 26\n",
      "We could have also\n",
      "brought in more examples to enhance our idea of sentiment 16\n",
      "We could have looked at\n",
      "Facebook, the media, and so on, for more information on how we believe the stock will\n",
      "perform in the future 30\n",
      "\n",
      "We really only had 14 data points, which is far from sufficient to make a production-ready\n",
      "algorithm 22\n",
      "Of course, for the purposes of this book, this is enough, but if we are serious\n",
      "about making a financial algorithm that can effectively predict stock price movement, we\n",
      "will have to obtain many more days of media coverage and prices 47\n",
      "\n",
      "We could have spent more time optimizing our parameters in our models by utilizing the\n",
      "gridsearchCV  module in the sklearn  package to get the most out of our models 35\n",
      "There\n",
      "are other models  that exist that deal specifically with time series data (data that changes\n",
      "over time), including a model called AutoRegressive Integrated Moving Average\n",
      "(ARIMA ) 38\n",
      "Models such as ARIMA and similar ones attempt to focus and zero in on specific\n",
      "time series features 20\n",
      "\n",
      "\n",
      "Case study 2 – Why do some people cheat\n",
      "on their spouses 15\n",
      "\n",
      "In 1978, a survey was conducted  on housewives in order to discern factors that led them to\n",
      "pursue extramarital affairs 31\n",
      "This study became the basis for many future studies of both\n",
      "men and women, all attempting to focus on features of people and marriages that led either\n",
      "partner to seek partners elsewhere behind their spouse's back 40\n",
      "\n",
      "Supervised learning is not always about prediction 9\n",
      "In this case study, we will purely\n",
      "attempt to identify a few factors of the many that we believe might be the most important\n",
      "factors that lead someone to pursue an affair 36\n",
      "\n",
      "First, let's read in the data:\n",
      "# Using dataset of a 1978 survey conducted to measure likliehood of women\n",
      "to perform extramarital affairs\n",
      "# http://statsmodels 40\n",
      "sourceforge 2\n",
      "net/stable/datasets/generated/fair 8\n",
      "html\n",
      "import statsmodels 5\n",
      "api as sm\n",
      "affairs_df = sm 9\n",
      "datasets 1\n",
      "fair 1\n",
      "load_pandas() 4\n",
      "data\n",
      "affairs_df 5\n",
      "head()\n",
      "The statsmodels  website provides a data dictionary, as follows:\n",
      "rate_marriage : The rating given to the marriage (given by the wife); 1 = very\n",
      "poor, 2 = poor , 3 = fair , 4 = good , 5 = very good ; ordinal level\n",
      "age: Age of the wife; ratio level\n",
      "yrs_married : Number of years married: ratio level\n",
      "\n",
      "children : Number of children between husband and wife: ratio level\n",
      "religious : How religious the wife is; 1 = not , 2 = mildly , 3 = fairly , 4 = strongly ;\n",
      "ordinal level\n",
      "educ : Level of education; 9 = grade school , 12 = high school , 14 = some college , 16 =\n",
      "college graduate , 17 = some graduate school , 20 = advanced degree ; ratio level\n",
      "occupation : 1 = student;  2 = farming, agriculture; semi-skilled, or unskilled worker ; 3\n",
      "= white-collar ; 4 = teacher, counselor, social worker, nurse; artist, writer; technician,\n",
      "skilled worker;  5 = managerial, administrative, business;  6 = professional with advanced\n",
      "degree ; nominal level\n",
      "occupation_husb : Husband's occupation 260\n",
      "Same as occupation; nominal level\n",
      "affairs : Measure of time spent in extramarital affairs; ratio level\n",
      "Okay, so we have a quantitative response, but my question is simply what factors cause\n",
      "someone to have an affair 46\n",
      "The exact number of minutes or hours does not really matter\n",
      "that much 14\n",
      "For this reason, let's make a new categorical variable called affair_binary ,\n",
      "which is either true  (they had an affair for more than 0 minutes) or false  (they had an\n",
      "affair for 0 minutes):\n",
      "# Create a categorical variable\n",
      "affairs_df['affair_binary'] = (affairs_df['affairs'] > 0)\n",
      "Again, this column has either a true  or a false  value 88\n",
      "The value is true  if the person had\n",
      "an extramarital affair for more than 0 minutes 21\n",
      "The value is false  otherwise 6\n",
      "From now on,\n",
      "let's use this binary  response as our primary response 15\n",
      "Now, we are trying to find which of\n",
      "these variables are associated with our response, so let's begin 22\n",
      "\n",
      "Let's start with a simple correlation matrix 9\n",
      "Recall that this matrix shows us linear\n",
      "correlations between our quantitative variables and our response 17\n",
      "I will show the\n",
      "correlation matrix as both a matrix of decimals and also as a heat map 20\n",
      "Let's see the\n",
      "numbers first:\n",
      "# find linear correlations between variables and affair_binary\n",
      "affairs_df 21\n",
      "corr()\n",
      "\n",
      "\n",
      "Correlation matrix for extramarital aﬀairs data from a Likert survey conducted in 1978\n",
      "Remember that we ignore the diagonal series of 1s because they are merely telling us that\n",
      "every quantitative variable is correlated with itself 52\n",
      "Note the other correlated variables,\n",
      "which are the values closest to 1 and -1 in the last row or column (the matrix is always\n",
      "symmetrical across the diagonal) 35\n",
      "\n",
      "We can see a few standout variables:\n",
      "affairs\n",
      "age\n",
      "yrs_married\n",
      "children\n",
      "These are the top four variables with the largest magnitude (absolute value) 35\n",
      "However, one\n",
      "of these variables is cheating 9\n",
      "The affairs  variable is the largest in magnitude, but is\n",
      "obviously correlated to affair_binary  because we made the affair_binary\n",
      "variable directly based on affairs 32\n",
      "So let's ignore that one 6\n",
      "\n",
      "\n",
      "Let's take a look at our correlation heat map to see whether our views can be seen there:\n",
      "import seaborn as sns\n",
      "sns 27\n",
      "heatmap(affairs_df 5\n",
      "corr())\n",
      "Correlation matrix\n",
      "The same correlation matrix, but this time  as a heat map 19\n",
      "Note the colors close to dark red\n",
      "and dark blue (excluding the diagonal) 16\n",
      "\n",
      "We are looking for the dark red and dark blue areas of the heat map 16\n",
      "These colors are\n",
      "associated with the most correlated features 10\n",
      "\n",
      "Remember correlations are not the only way to identify which features are associated with\n",
      "our response 18\n",
      "This method shows us how linearly correlated the variables are with each\n",
      "other 15\n",
      "We may find another variable that affects affairs by evaluating the coefficients of a\n",
      "decision tree classifier 18\n",
      "These methods might reveal new variables that are associated with\n",
      "our variables, but not in a linear fashion 20\n",
      "\n",
      "\n",
      "Also notice that there are two variables here that don't actually belong …\n",
      "Can you spot them 20\n",
      "These are the occupation  and occupation_husb\n",
      "variables 11\n",
      "Recall earlier that we deemed them as nominal and therefore\n",
      "have no right to be included in this correlation matrix 21\n",
      "This is because\n",
      "Pandas, unknowingly, casts them as integers and now considers them as\n",
      "quantitative variables 23\n",
      "Don't worry, we will fix this soon 9\n",
      "\n",
      "First, let's make ourselves an X and a y DataFrame:\n",
      "affairs_X = affairs_df 20\n",
      "drop(['affairs', 'affair_binary'], axis=1)\n",
      "# data without the affairs or affair_binary column\n",
      "affairs_y = affairs_df['affair_binary']\n",
      "Now, we will instantiate a decision tree classifier and cross-validate our model in order to\n",
      "determine whether or not the model is doing an okay job at fitting our data:\n",
      "from sklearn 72\n",
      "tree import DecisionTreeClassifier\n",
      "model = DecisionTreeClassifier()\n",
      "# instantiate the model\n",
      "from sklearn 19\n",
      "cross_validation import cross_val_score\n",
      "# import our cross validation module\n",
      "# check the accuracy on the training set\n",
      "scores = cross_val_score(model, affairs_X, affairs_y, cv=10)\n",
      "print( scores 43\n",
      "mean(), \"average accuracy\" )\n",
      "0 8\n",
      "659756806845 average accuracy\n",
      "print( scores 11\n",
      "std(), \"standard deviation\") # very low, meaning variance of\n",
      "the model is low\n",
      "0 20\n",
      "0204081732291 standard deviation\n",
      "# Looks ok on the cross validation side\n",
      "Because our standard deviation is low, we may make  the assumption that the variance of\n",
      "our model is low (because variance is the square of standard deviation) 50\n",
      "This is good\n",
      "because that means that our model is not fitting wildly differently on each fold of the cross\n",
      "validation and that it is generally a reliable model 31\n",
      "\n",
      "\n",
      "Because we agree that our decision tree classifier is generally a reliable model, we can fit the\n",
      "tree to our entire dataset and use the importance metric to identify which variables our tree\n",
      "deems the most important:\n",
      "# Explore individual features that make the biggest impact\n",
      "# rate_marriage, yrs_married, and occupation_husb 66\n",
      "But one of these\n",
      "variables doesn't quite make sense right 12\n",
      "\n",
      "# Its the occupation variable, because they are nominal, their\n",
      "interpretations\n",
      "model 18\n",
      "fit(affairs_X, affairs_y)\n",
      "pd 10\n",
      "DataFrame({'feature':affairs_X 7\n",
      "columns,\n",
      "'importance':model 7\n",
      "feature_importances_}) 5\n",
      "sort_values('importance') 6\n",
      "tail(3)\n",
      "So, yrs_married  and rate_marriage  both are important, but the most important\n",
      "variable is occupation_husb 29\n",
      "But that doesn't make sense because that variable is\n",
      "nominal 13\n",
      "So, let's apply our dummy variable technique wherein we create new columns\n",
      "that represent each option for occupation_husb  and also for occupation 28\n",
      "\n",
      "Firstly, for the occupation  column:\n",
      "# Dummy Variables:\n",
      "# Encoding qualitiative (nominal) data using separate columns (see slides\n",
      "for linear regression for more)\n",
      "occuptation_dummies = pd 44\n",
      "get_dummies(affairs_df['occupation'],\n",
      "prefix='occ_') 15\n",
      "iloc[:, 1:]\n",
      "# concatenate the dummy variable columns onto the original DataFrame\n",
      "(axis=0 means rows, axis=1 means columns)\n",
      "affairs_df = pd 34\n",
      "concat([affairs_df, occuptation_dummies], axis=1)\n",
      "affairs_df 19\n",
      "head()\n",
      "\n",
      "This new DataFrame has many new columns:\n",
      "Remember, these new columns, occ_2 19\n",
      "0 , occ_4 6\n",
      "0 , and so on, represent a binary variable\n",
      "that represents whether  or not the wife holds job 2, or 4, and so on:\n",
      "# Now for the husband's job\n",
      "occuptation_dummies = pd 48\n",
      "get_dummies(affairs_df['occupation_husb'],\n",
      "prefix='occ_husb_') 19\n",
      "iloc[:, 1:]\n",
      "# concatenate the dummy variable columns onto the original DataFrame\n",
      "(axis=0 means rows, axis=1 means columns)\n",
      "affairs_df = pd 34\n",
      "concat([affairs_df, occuptation_dummies], axis=1)\n",
      "affairs_df 19\n",
      "head()\n",
      "(6366, 15)\n",
      "Now we have 15 new columns 16\n",
      "Let's run our tree again and find the most important\n",
      "variables:\n",
      "# remove appropiate columns for feature set affairs_X =\n",
      "affairs_df 29\n",
      "drop(['affairs', 'affair_binary', 'occupation',\n",
      "'occupation_husb'], axis=1) affairs_y = affairs_df['affair_binary'] model =\n",
      "DecisionTreeClassifier() from sklearn 40\n",
      "cross_validation import\n",
      "cross_val_score # check the accuracy on the training set scores =\n",
      "cross_val_score(model, affairs_X, affairs_y, cv=10) print scores 34\n",
      "mean(),\n",
      "\"average accuracy\"\n",
      "print (scores 9\n",
      "std(), \"standard deviation\") # very low, meaning variance of\n",
      "the model is low # Still looks ok # Explore individual features that make\n",
      "the biggest impact model 33\n",
      "fit(affairs_X, affairs_y)\n",
      "\n",
      "pd 10\n",
      "DataFrame({'feature':affairs_X 7\n",
      "columns,\n",
      "'importance':model 7\n",
      "feature_importances_}) 5\n",
      "sort_values('importance') 6\n",
      "tail(10\n",
      ")\n",
      "And there you have it:\n",
      "rate_marriage : The rating of the marriage, as told by the decision tree\n",
      "children : The number of children they had, as told by the decision tree and our\n",
      "correlation matrix\n",
      "yrs_married : The number of years they  had been married, as told by the\n",
      "decision tree and our correlation matrix\n",
      "educ : The level of education the women had, as told by the decision tree\n",
      "age: The age of the women, as told by the decision tree and our correlation\n",
      "matrix\n",
      "These seem to be the top five most important variables in determining whether or not a\n",
      "woman from the 1978 survey would be involved in an extramarital affair 147\n",
      "\n",
      "Case study 3 – Using TensorFlow\n",
      "I would like to finish off our time  together by looking at a somewhat more modern machine\n",
      "learning module called TensorFlow 32\n",
      "\n",
      "\n",
      "TensorFlow is an open source machine learning module that is used primarily for its\n",
      "simplified deep learning and neural network abilities 25\n",
      "I would like to take some time  to\n",
      "introduce the module and solve a few quick problems using TensorFlow 22\n",
      "The syntax for\n",
      "TensorFlow (like PyBrain in Chapter 12 , Beyond the Essentials ) is a bit different than our\n",
      "normal scikit-learn  syntax, so I will be going over it step by step 44\n",
      "Let's start with some\n",
      "imports:\n",
      "from sklearn import datasets, metrics\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "from sklearn 27\n",
      "cross_validation import train_test_split\n",
      "%matplotlib inline\n",
      "Our imports from sklearn  include train_test_split , datasets , and metrics 25\n",
      "We will\n",
      "be utilizing our train test splits to reduce overfitting, we will use datasets in order to import\n",
      "our iris  classification data, and we'll use the metrics module in order to calculate some\n",
      "simple metrics for our learning models 49\n",
      "\n",
      "TensorFlow learns in a different way in that it is always trying to minimize an error\n",
      "function 20\n",
      "It does this by iteratively going through our entire dataset and, every so often,\n",
      "updates our model to better fit the data 25\n",
      "\n",
      "It is important to note that TensorFlow doesn't just implement neural networks, but can\n",
      "implement even simpler models as well 24\n",
      "For example, let's implement a classic logistic\n",
      "regression using TensorFlow:\n",
      "# Our data set of iris flowers\n",
      "iris = datasets 26\n",
      "load_iris()\n",
      "# Load datasets and split them for training and testing\n",
      "X_train, X_test, y_train, y_test = train_test_split(iris 32\n",
      "data, iris 3\n",
      "target)\n",
      "####### TENSORFLOW #######\n",
      "# Here is tensorflow's syntax for defining features 17\n",
      "\n",
      "# We must specify that all features have real-value data\n",
      "feature_columns = [tf 18\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\", dimension=4)]\n",
      "# notice the dimension is set to four because we have four columns\n",
      "# We set our \"learning rate\" which is a decimal that tells the network\n",
      "# how quickly to learn\n",
      "optimizer = tf 49\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1)\n",
      "# A learning rate closer to 0 means the network will learn slower\n",
      "\n",
      "# Build a linear classifier (logistic regression)\n",
      "# note we have to tell tensorflow the number of classes we are looking for\n",
      "# which are 3 classes of iris\n",
      "classifier =\n",
      "tf 56\n",
      "contrib 1\n",
      "learn 1\n",
      "LinearClassifier(feature_columns=feature_columns,\n",
      "                                             optimizer=optimizer,\n",
      "                                                      n_classes=3)\n",
      "# Fit model 22\n",
      "Uses error optimization techniques like stochastic gradient\n",
      "descent\n",
      "classifier 12\n",
      "fit(x=X_train,\n",
      "               y=y_train,\n",
      "               steps=1000)  # number of iterations\n",
      "I will point out the key lines of code from the preceding snippet to really solidify what is\n",
      "happening during training:\n",
      "feature_columns = [tf 53\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\",1 6\n",
      "\n",
      "dimension=4)] :\n",
      "Here, I am creating four input  columns that we know correlate to the\n",
      "flowers' sepal length, sepal width, petal length, and petal width 40\n",
      "\n",
      "optimizer =2 4\n",
      "\n",
      "tf 2\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1) :\n",
      "Here, I am telling TensorFlow to optimize using something called\n",
      "gradient descent , which means that we will define an error function\n",
      "(which will happen in the next step) and, little by little, we will work\n",
      "our way to minimize this error function 54\n",
      "\n",
      "Our learning rate should hover close to 0 because we want our model\n",
      "to learn slowly 19\n",
      "If our model learns too quickly, it might \"skip over\" the\n",
      "correct answer 17\n",
      "\n",
      "classifier =3 4\n",
      "\n",
      "tf 2\n",
      "contrib 1\n",
      "learn 1\n",
      "LinearClassifier(feature_columns=feature_colum\n",
      "ns, optimizer=optimizer, n_classes=3) :\n",
      "When we specify LinearClassifier , we are denoting the same error\n",
      "function that logistic regression is minimizing, meaning that this\n",
      "classifier is attempting to work as a logistic regression classifier 56\n",
      "\n",
      "We give the model our feature_columns  as defined in Step 1 15\n",
      "\n",
      "The optimizer  is the method of minimizing our error function; in this\n",
      "case, we chose gradient descent 22\n",
      "\n",
      "We must also specify our number of classes as being 3 13\n",
      "We know that\n",
      "we have three different iris flowers that the model could choose from 16\n",
      "\n",
      "\n",
      "classifier 2\n",
      "fit(x=X_train, y=y_train, steps=1000) : 4 17\n",
      "\n",
      "The training looks similar to a scikit-learn model, with an added\n",
      "parameter called steps 20\n",
      "Steps tell us how many times we would like\n",
      "to go over our dataset 15\n",
      "So, when we specify 1000 , we are iterating over\n",
      "our dataset 16\n",
      "The more steps we take, the more the model gets a chance\n",
      "to learn 16\n",
      "\n",
      "Phew 3\n",
      "When we run the preceding  code, a linear classifier (logistic regression) model is\n",
      "being fit, and when it is done, it is ready to be tested:\n",
      "# Evaluate accuracy 38\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=X_test,\n",
      "                                     y=y_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 20\n",
      "format(accuracy_score))\n",
      "Accuracy: 0 9\n",
      "973684\n",
      "Excellent 5\n",
      "It is worth noting that when using TensorFlow, we may also utilize a similarly\n",
      "simple predict  function:\n",
      "# Classify two new flower samples 28\n",
      "\n",
      "new_samples = np 5\n",
      "array(\n",
      "    [[6 5\n",
      "4, 3 5\n",
      "2, 4 5\n",
      "5, 1 5\n",
      "5], [5 5\n",
      "8, 3 5\n",
      "1, 5 5\n",
      "0, 1 5\n",
      "7]], dtype=float)\n",
      "y = classifier 9\n",
      "predict(new_samples)\n",
      "print('Predictions: {}' 11\n",
      "format(str(y)))\n",
      "Predictions: [1 2]\n",
      "Now, let's compare this with a standard scikit-learn logistic regression to see who won:\n",
      "from sklearn 34\n",
      "linear_model import LogisticRegression\n",
      "# compare our result above to a simple scikit-learn logistic regression\n",
      "logreg = LogisticRegression()\n",
      "# instantiate the model\n",
      "logreg 34\n",
      "fit(X_train, y_train)\n",
      "# fit it to our training set\n",
      "y_predicted = logreg 20\n",
      "predict(X_test)\n",
      "# predict on our test set, to avoid overfitting 16\n",
      "\n",
      "accuracy = metrics 4\n",
      "accuracy_score(y_predicted, y_test)\n",
      "# get our accuracy score\n",
      "\n",
      "accuracy\n",
      "# It's the same thing 22\n",
      "\n",
      "Wow, so it seems that with 1,000 steps, a gradient descent-optimized TensorFlow model is\n",
      "no better than a simple sklearn logistic regression 31\n",
      "OK, that's fine, but what if we allowed\n",
      "the model to iterate over the iris  dataset even more 23\n",
      "\n",
      "feature_columns = [tf 6\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\", dimension=4)]\n",
      "optimizer = tf 12\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1)\n",
      "classifier =\n",
      "tf 6\n",
      "contrib 1\n",
      "learn 1\n",
      "LinearClassifier(feature_columns=feature_columns,\n",
      "                                               optimizer=optimizer,\n",
      "                                            n_classes=3)\n",
      "classifier 20\n",
      "fit(x=X_train,\n",
      "               y=y_train,\n",
      "               steps=2000)  # number of iterations is 2000 now\n",
      "Our code is exactly the same as before, but now we have 2000  steps instead of 1000 :\n",
      "# Evaluate accuracy 54\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=X_test,\n",
      "                                     y=y_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 20\n",
      "format(accuracy_score))\n",
      "Accuracy: 0 9\n",
      "973684\n",
      "And now we have even better accuracy 11\n",
      "\n",
      "Note that you need to be very careful in choosing the number of steps 15\n",
      "As\n",
      "you increase this number, you increase the number of times your model\n",
      "sees the exact same training points over and over again 27\n",
      "We do have a\n",
      "chance of overfitting 11\n",
      "To remedy this, I would recommend choosing\n",
      "multiple train/test splits and running the model on each one ( k-fold cross\n",
      "validation ) 27\n",
      "\n",
      "It is also worth mentioning that TensorFlow implements very low bias, high-variance\n",
      "models, meaning that running  the preceding code again for TensorFlow might result in a\n",
      "different answer 36\n",
      "This is one of the caveats of deep learning 10\n",
      "They might converge to a very\n",
      "great low bias model, but that model will have a high variance and, therefore, amazingly\n",
      "might not generalize to all of the sample data 35\n",
      "As mentioned before, cross validation would\n",
      "be helpful in order to mitigate this 15\n",
      "\n",
      "\n",
      "TensorFlow and neural networks\n",
      "Now, let's point a more powerful model  at our iris  dataset 22\n",
      "Let's create a neural network \n",
      "whose  goal it is to classify iris flowers (because why not 20\n",
      ":\n",
      "# Specify that all features have real-value data\n",
      "feature_columns = [tf 16\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\", dimension=4)]\n",
      "optimizer = tf 12\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1)\n",
      "# Build 3 layer DNN with 10, 20, 10 units respectively 21\n",
      "\n",
      "classifier =\n",
      "tf 4\n",
      "contrib 1\n",
      "learn 1\n",
      "DNNClassifier(feature_columns=feature_columns,\n",
      "                                        hidden_units=[10, 20, 10],\n",
      "                                        optimizer=optimizer,\n",
      "                                        n_classes=3)\n",
      "# Fit model 35\n",
      "\n",
      "classifier 2\n",
      "fit(x=X_train,\n",
      "               y=y_train,\n",
      "               steps=2000)\n",
      "Notice that our code  really hasn't changed  from the last segment 30\n",
      "We still have our\n",
      "feature_columns  from before, but now we introduce, instead of a linear classifier, a\n",
      "DNNClassifier , which stands for Deep Neural Network Classifier 35\n",
      "\n",
      "This is TensorFlow's syntax for implementing a neural network 11\n",
      "Let's take a closer look:\n",
      "tf 8\n",
      "contrib 1\n",
      "learn 1\n",
      "DNNClassifier(feature_columns=feature_columns,\n",
      "                                     hidden_units=[10, 20, 10],\n",
      "                                     optimizer=optimizer,\n",
      "                                     n_classes=3)\n",
      "We see that we are inputting the same feature_columns , n_classes , and optimizer ,\n",
      "but see how we have a new parameter called hidden_units 61\n",
      "This list represents the\n",
      "number of nodes to have in each layer between the input and the output layer 20\n",
      "\n",
      "\n",
      "All in all, this neural network will have five layers:\n",
      "The first layer will have four nodes, one for each of the iris feature variables, and\n",
      "this layer is the input layer\n",
      "A hidden layer of 10 nodes\n",
      "A hidden layer of 20 nodes\n",
      "A hidden layer of 10 nodes\n",
      "The final layer will have three nodes, one for each possible outcome of the\n",
      "network, and this is called our output layer\n",
      "Now that we've trained  our model, let's evaluate  it on our test set:\n",
      "# Evaluate accuracy 111\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=X_test,\n",
      "                                     y=y_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 20\n",
      "format(accuracy_score))\n",
      "Accuracy: 0 9\n",
      "921053\n",
      "Hmm, our neural network didn't do so well on this dataset, but perhaps it is because the\n",
      "network is a bit too complicated for such a simple dataset 36\n",
      "Let's introduce a new dataset\n",
      "that has a bit more to it 14\n",
      " 1\n",
      " 1\n",
      "\n",
      "The mnist  dataset consists of over 50,000 handwritten digits (0-9) and the goal is to\n",
      "recognize the handwritten digits and output which letter they are writing 37\n",
      "TensorFlow has\n",
      "a built-in mechanism for downloading and loading these images 13\n",
      "We've seen these images\n",
      "before, but on a much smaller scale in Chapter 12 , Beyond the Essentials :\n",
      "from tensorflow 25\n",
      "examples 1\n",
      "tutorials 1\n",
      "mnist import input_data\n",
      "mnist = input_data 10\n",
      "read_data_sets(\"MNIST_data/\", one_hot=False)\n",
      "Extracting MNIST_data/train-images-idx3-ubyte 24\n",
      "gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte 15\n",
      "gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte 16\n",
      "gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte 17\n",
      "gz\n",
      "Notice that one of our inputs for downloading mnist  is called one_hot 16\n",
      "This parameter\n",
      "either brings in the dataset's target variable (which is the digit itself) as a single number, or\n",
      "has a dummy variable 29\n",
      "\n",
      "\n",
      "For example, if the first digit were a 7, the target would be either of these:\n",
      "7: If one_hot  was false\n",
      "0 0 0 0 0 0 0 1 0 0 : If one_hot  was true  (notice that starting from 0, the seventh\n",
      "index is a 1)\n",
      "We will encode our target the former way, as this is what our TensorFlow neural network\n",
      "and our sklearn logistic regression will expect 99\n",
      "\n",
      "The dataset is split up already into a training and test set, so let's create new variables to\n",
      "hold them:\n",
      "x_mnist = mnist 30\n",
      "train 1\n",
      "images\n",
      "y_mnist = mnist 7\n",
      "train 1\n",
      "labels 1\n",
      "astype(int)\n",
      "For the y_mnist  variable, I specifically cast every target as an integer (by default, they\n",
      "come in as floats) because otherwise, TensorFlow would throw an error at us 41\n",
      "\n",
      "Out of curiosity, let's take a look  at a single image:\n",
      "import matplotlib 18\n",
      "pyplot as plt\n",
      "plt 5\n",
      "imshow(x_mnist[10] 7\n",
      "reshape(28, 28))\n",
      "The number 0 in the MNIST dataset\n",
      "\n",
      "And, hopefully, our target variable matches at the 10th index as well:\n",
      "y_mnist[10]\n",
      "0\n",
      "Excellent 43\n",
      "Let's now take a peek at how big our dataset is:\n",
      "x_mnist 16\n",
      "shape\n",
      "(55000, 784)\n",
      "y_mnist 12\n",
      "shape\n",
      "(55000,)\n",
      "Our training size then is 55000  images and target variables 19\n",
      "\n",
      "Let's fit a deep neural network to our images and see whether it will be able to pick up on\n",
      "the patterns in our inputs:\n",
      "# Specify that all features have real-value data feature_columns =\n",
      "[tf 43\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\", dimension=784)] optimizer =\n",
      "tf 12\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1) # Build 3 layer DNN\n",
      "with 10, 20, 10 units respectively 22\n",
      "classifier =\n",
      "tf 3\n",
      "contrib 1\n",
      "learn 1\n",
      "DNNClassifier(feature_columns=feature_columns,\n",
      "hidden_units=[10, 20, 10], optimizer=optimizer, n_classes=10) # Fit model 32\n",
      "\n",
      "classifier 2\n",
      "fit(x=x_mnist, y=y_mnist, steps=1000) # Warning this is\n",
      "veryyyyyyyy slow\n",
      "This code is very similar to our previous  segment using DNNClassifier ; however, look\n",
      "how, in our first line of code, I have changed the number of columns to be 784 while, in the\n",
      "classifier itself, I changed the number of output classes to be 10 84\n",
      "These are manual  inputs\n",
      "that TensorFlow must be given to work 13\n",
      "\n",
      "The preceding code runs very slowly 7\n",
      "It is adjusting itself, little by little,  in order to get the\n",
      "best possible performance from our training set 23\n",
      "Of course, we know that the ultimate test\n",
      "here is testing our network on an unknown test set, which is also given to us by\n",
      "TensorFlow:\n",
      "x_mnist_test = mnist 38\n",
      "test 1\n",
      "images\n",
      "y_mnist_test = mnist 8\n",
      "test 1\n",
      "labels 1\n",
      "astype(int)\n",
      "x_mnist_test 8\n",
      "shape\n",
      "(10000, 784)\n",
      "y_mnist_test 13\n",
      "shape\n",
      "(10000,)\n",
      "\n",
      "So we have 10,000 images to test on; let's see how our network was able to adapt to the\n",
      "dataset:\n",
      "# Evaluate accuracy 37\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=x_mnist_test,\n",
      "                                     y=y_mnist_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 24\n",
      "format(accuracy_score))\n",
      "Accuracy: 0 9\n",
      "920600\n",
      "Not bad, 92% accuracy on our dataset 14\n",
      "Let's take a second and compare this\n",
      "performance to a standard sklearn logistic regression now:\n",
      "logreg = LogisticRegression()\n",
      "logreg 26\n",
      "fit(x_mnist, y_mnist)\n",
      "# Warning this is slow\n",
      "y_predicted = logreg 20\n",
      "predict(x_mnist_test)\n",
      "from sklearn 8\n",
      "metrics import accuracy_score\n",
      "# predict on our test set, to avoid overfitting 17\n",
      "\n",
      "accuracy = accuracy_score(y_predicted, y_mnist_test)\n",
      "# get our accuracy score\n",
      "accuracy\n",
      "0 22\n",
      "91969\n",
      "Success 5\n",
      "Our neural network performed better  than the standard logistic regression 11\n",
      "This is\n",
      "likely because the network is attempting to find relationships between the pixels\n",
      "themselves and using these relationships to map them to what digit we are writing down 33\n",
      "\n",
      "In logistic regression, the model assumes that every single input is independent of one\n",
      "another, and therefore has a tough time finding relationships between them 29\n",
      "\n",
      "\n",
      "There are ways of making our neural network learn differently:\n",
      "We could make our network wider, that is, increase the number of nodes in the\n",
      "hidden layers instead of having several layers of a smaller number of nodes:\n",
      "Artiﬁcial neural network (Source: http://electronicdesign 59\n",
      "com/site-ﬁles/electronicdesign 10\n",
      "com/ﬁles/uploads/2015/02/0816_Development_Tools_F1_0 22\n",
      "gif)\n",
      "# A wider network\n",
      "feature_columns = [tf 12\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\",\n",
      "dimension=784)]\n",
      "optimizer = tf 13\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1)\n",
      "# Build 3 layer DNN with 10, 20, 10 units respectively 21\n",
      "\n",
      "classifier =\n",
      "tf 4\n",
      "contrib 1\n",
      "learn 1\n",
      "DNNClassifier(feature_columns=feature_columns,\n",
      "                                     hidden_units=[1500],\n",
      "                                     optimizer=optimizer,\n",
      "                                            n_classes=10)\n",
      "# Fit model 30\n",
      "\n",
      "classifier 2\n",
      "fit(x=x_mnist,\n",
      "               y=y_mnist,\n",
      "               steps=100)\n",
      "# Warning this is veryyyyyyyy slow\n",
      "# Evaluate accuracy 29\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=x_mnist_test,\n",
      "                                     y=y_mnist_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 24\n",
      "format(accuracy_score))\n",
      "      Accuracy: 0 10\n",
      "898400\n",
      "\n",
      "We could increase our learning rate, forcing the network to attempt to converge\n",
      "on an answer faster 23\n",
      "As I mentioned before, we run the risk of the model\n",
      "skipping the answer entirely if we go down this route 24\n",
      "It is usually better  to stick\n",
      "with a smaller learning rate 13\n",
      "\n",
      "We can change the method  of optimization 9\n",
      "Gradient descent is very popular;\n",
      "however, there are other algorithms for doing this 15\n",
      "One example is called the\n",
      "Adam Optimizer 9\n",
      "The difference is in the way they traverse the error function,\n",
      "and therefore the way that they approach the optimization point 22\n",
      "Different\n",
      "problems in different domains call for different optimizers 11\n",
      "\n",
      "There is no replacement for a good old fashioned feature selection phase, instead\n",
      "of attempting to let the network figure everything out for us 27\n",
      "We can take the\n",
      "time to find relevant and meaningful features that actually will allow our\n",
      "network to find an answer quicker 24\n",
      "\n",
      "Summary\n",
      "In this chapter, we've seen three different case studies from three different domains using\n",
      "many different statistical and machine learning methods 27\n",
      "However, what all of them have\n",
      "in common is that in order to solve them properly, we had to implement a data science\n",
      "mindset 29\n",
      "We had to solve problems in an interesting way, obtain data, clean the data,\n",
      "visualize the data, and finally model the data and evaluate our thinking process 32\n",
      "\n",
      "I do hope that you have found the contents of this book to be interesting and not just the\n",
      "final chapter 23\n",
      "I leave it unto you to keep exploring the world of data science 13\n",
      "Keep learning\n",
      "Python 4\n",
      "Keep learning statistics and probability 5\n",
      "Keep your minds open 4\n",
      "It is my hope that\n",
      "this book has been a catalyst for you to go out and find even more on the subject 24\n",
      "\n",
      "For further reading beyond this book, I highly recommend looking into well-known data\n",
      "science books and blogs, such as:\n",
      "Dataschool 27\n",
      "io, a blog by Kevin Markham\n",
      "Python for Data Scientists,  by Packt\n",
      "If you would like to contact me for any reason, please feel free to reach out to\n",
      "sinan 40\n",
      "u 1\n",
      "ozdemir@gmail 4\n",
      "com 1\n",
      "\n",
      "\n",
      "4\n",
      "Microsoft Azure Databricks\n",
      "The main purpose of this chapter is to highlight the Microsoft Data Environment and how\n",
      "we can utilize the many tools provided to us, especially Azure Databricks: a fullyfledged,\n",
      "powerful analytics platform powered by Apache Spark 54\n",
      "\n",
      "We have three main case studies in this chapter that join together the principles of data\n",
      "science and machine learning that we have learned about in this book with the ease and\n",
      "power of the Microsoft Data Environment, specifically Azure Databricks 47\n",
      "Each case study\n",
      "will highlight different features of using Azure Databricks, as well as aspects of machine\n",
      "learning that we have learned in this text 30\n",
      "\n",
      "The Microsoft data science environment\n",
      "Microsoft offers many tools and environments to the data scientist, and this offering\n",
      "consists of many parts 27\n",
      "The three main components are as follows:\n",
      "Microsoft Azure : This is an  enterprise-grade cloud computing platform that\n",
      "provides a great  deal of access and support for production-ready scalable\n",
      "systems 38\n",
      "\n",
      "Microsoft Azure Machine Learning Studio : This is a GUI-based environment for\n",
      "creating  and operationalizing a machine learning workflow that is optimized for\n",
      "Azure 30\n",
      "\n",
      "Azure Databricks : This is an Apache Spark-based analytics and machine\n",
      "learning platform  optimized for the Microsoft Azure platform 25\n",
      "\n",
      "\n",
      "This text will focus heavily  on the Azure  Databricks  environment as it provides access to\n",
      "many tools, including these:\n",
      "Spark DataFrames : Spark DataFrames are distributed collections of data\n",
      "organized into rows and columns 46\n",
      "They are conceptually equivalent to a data\n",
      "frame in Python 12\n",
      "\n",
      "Notebooks : Lik e Jupyter, the Azure Databricks notebook tool provides a cell-\n",
      "based code environment for writing logically separated code that is easy to read\n",
      "and replicate 36\n",
      "\n",
      "Clusters/workers : Using Microsoft Azure (or another cloud computing\n",
      "platform), we can spin  up resources in order to massively optimize and\n",
      "parallelize our machine learning and data analysis to allow for faster iteration 42\n",
      "\n",
      "MLib : This is a scalable machine  learning library consisting of common learning\n",
      "algorithms and utilities, including classification, regression, and more 29\n",
      "\n",
      "Azure Databricks provides many more capabilities other than what we have listed here 16\n",
      "We\n",
      "will focus on the tools that are relevant to us as data scientists and machine learning\n",
      "engineers 21\n",
      "\n",
      "For more information on Azure Databricks, check out https:/ ​/​docs 18\n",
      "\n",
      "microsoft 3\n",
      "​com/ ​en-​us/​azure/ ​azure- ​databricks/ ​what- ​is-​azure-\n",
      "databricks 32\n",
      "\n",
      "What exactly are Spark and PySpark 8\n",
      "\n",
      "The backbone of Azure Databricks  is Apache Spark 12\n",
      "Spark  is an analytics engine for big\n",
      "data processing 11\n",
      "It has built-in modules for data streaming, SQL, machine learning, and\n",
      "more 17\n",
      "PySpark  is the Python API for Spark 9\n",
      "It allows us to invoke the power  of Spark using\n",
      "Python code 14\n",
      "Azure Databricks brings all of this at our fingertips and makes setting up\n",
      "clusters running Spark and PySpark within seconds possible 25\n",
      "Let's see it in action 6\n",
      "\n",
      "Basic Azure Databricks use\n",
      "Before we begin with our case studies, it is important to get our bearings in  Azure\n",
      "Databricks 30\n",
      "We will begin by setting up our first cluster and some notebooks to write our\n",
      "Python code in 19\n",
      "\n",
      "\n",
      "Setting up our first cluster\n",
      "To get started in Azure Databricks, we have to set up our first cluster 24\n",
      "This will spin up\n",
      "(initialize) resources running the Azure Databricks Runtime Environment (including\n",
      "Spark) 22\n",
      "This is where all of the action takes place 9\n",
      "Whenever we run code in our notebooks,\n",
      "the code is sent to our cluster to actually run it 19\n",
      "This means that no code will ever actually\n",
      "run on our local machine 14\n",
      "This is great for many reasons, one of the main ones being that it\n",
      "provides data scientists with sub-optimal  equipment at home/work a chance to use\n",
      "production-quality resources for a fraction of the cost 43\n",
      "\n",
      "To set up a cluster, navigate to the Clusters  option on the left-hand pane and click on\n",
      "Create cluster 25\n",
      "There, you will see a form with three basic pieces of information: Cluster\n",
      "Name , Azure Databricks Runtime Version , and Python Version 28\n",
      "Make your cluster name\n",
      "whatever your heart desires 9\n",
      "Try to use the most recent Azure Databricks version (this is\n",
      "the default) and select Python version 3 24\n",
      "Once that's finished, click Create cluster  again and\n",
      "you are done 15\n",
      "\n",
      "When spinning up a cluster, there are many optional advanced settings\n",
      "that we have the ability to set 21\n",
      "For our purposes, we will not need to 9\n",
      "\n",
      "The page should look something like this:\n",
      "\n",
      "\n",
      "Once we have a cluster, it is time to set up a notebook 23\n",
      "This can be done from the home\n",
      "page 9\n",
      "When creating a new notebook, all we have to do is select the language we wish to\n",
      "use (Python for now, but more are  available) 31\n",
      "The notebooks in Azure Databricks are nearly\n",
      "identical to Jupyter notebooks in functionality and use (nifty 23\n",
      " 1\n",
      "This comes in handy for\n",
      "data scientists who are used to this environment :\n",
      "Creating a notebook is even easier than spinning up a new cluster 27\n",
      "All of the code that we write in here will be run on our cluster and not on our local machine\n",
      "Once we have our cluster up and running and we are able to create notebooks, it is time to\n",
      "jump right into using the Azure Databricks environment and seeing first-hand how the\n",
      "power of Apache Spark and the Microsoft Data Environment will affect the way that we\n",
      "write data-driven code 80\n",
      "\n",
      "Case study 1 – bike-sharing usage prediction\n",
      "using parallelization in Azure Databricks\n",
      "Our first case study will focus on setting up a simple  notebook in Azure Databricks and\n",
      "running some basic data visualization and machine learning code in order to get used to\n",
      "the Azure Databricks environment 62\n",
      "Azure Databricks comes with a built-in filesystem that\n",
      "is preloaded with data for us to use 21\n",
      "We can upload our own files to the system (which we\n",
      "will do in the second case study),  but for now, we will import a dataset that came pre-\n",
      "loaded with Azure Databricks 41\n",
      "We will also make heavy use of the built-in Spark-based\n",
      "visualization tools to analyze our data to the fullest extent that we can 26\n",
      "\n",
      "\n",
      "The aspects of Azure Databricks that we will be highlighting in this case study include the\n",
      "following:\n",
      "The collection of open data that is easily accessible by the Azure Databricks\n",
      "filesystem\n",
      "Converting our pandas  DataFrames to Spark equivalents and generating\n",
      "visualizations\n",
      "Parallelizing some simple hyperparameter tuning\n",
      "Broadcasting variables to workers to enhance parallelization further\n",
      "The data that we will be using involves predicting the amount of bikes being rented via a\n",
      "bike-share system 96\n",
      "Our goal  is to predict the usage of the system based on daily/hourly\n",
      "corresponding weather, time-based, and seasonal information 29\n",
      "Let's get right to it and see\n",
      "our first Azure Databricks-specific programming 17\n",
      "We can access this through the dbutils\n",
      "module:\n",
      "# display is a reserved function in Databricks that allows us to view and\n",
      "manipulate Dataframes inline\n",
      "# dbutils is a library full of ready-to-use datasets for us to use\n",
      "# Let's start by displaying all of the directories in the main data folder\n",
      "# \"databricks-datasets\"\n",
      "display(dbutils 79\n",
      "fs 1\n",
      "ls(\"/databricks-datasets\"))\n",
      "The output is a DataFrame of available folders to look in for data 21\n",
      "Here is a snippet:\n",
      "path                                         name                 size\n",
      "dbfs:/databricks-datasets/README 21\n",
      "md          README 3\n",
      "md            976\n",
      "dbfs:/databricks-datasets/Rdatasets/         Rdatasets/           0\n",
      "dbfs:/databricks-datasets/SPARK_README 37\n",
      "md    SPARK_README 6\n",
      "md      3359\n",
      "dbfs:/databricks-datasets/adult/             adult/               0\n",
      "dbfs:/databricks-datasets/airlines/          airlines/            0\n",
      "dbfs:/databricks-datasets/amazon/            amazon/              0\n",
      "dbfs:/databricks-datasets/asa/               asa/                 0\n",
      "dbfs:/databricks-datasets/atlas_higgs/       atlas_higgs/         0\n",
      "dbfs:/databricks-datasets/bikeSharing/       bikeSharing/         0\n",
      "dbfs:/databricks-datasets/cctvVideos/        cctvVideos/          0\n",
      "dbfs:/databricks-datasets/credit-card-fraud/ credit-card-fraud/   0\n",
      "dbfs:/databricks-datasets/cs100/             cs100/               0\n",
      "dbfs:/databricks-datasets/cs110x/            cs110x/              0\n",
      " 208\n",
      " 1\n",
      " 1\n",
      "\n",
      "\n",
      "Every time we run a command in our notebook, the time that it took our cluster to execute\n",
      "that code is shown at the bottom 28\n",
      "For example, this code block took my cluster 0 11\n",
      "88 seconds 3\n",
      "\n",
      "In general, the format for this statement is as follows:\n",
      "Command took  <<elapsed_time>>  -- by <<username/email>>  at <<date>>,\n",
      "<<time>>  on <<cluster_name>>\n",
      "We can view the contents of a particular file:\n",
      "# Let's check out the general README of the date folder by opening the\n",
      "markdown folder and printing out the result of \"readlines\"\n",
      "# which is a way to print out the contents of a file\n",
      "with open(\"/dbfs/databricks-datasets/README 105\n",
      "md\") as f:\n",
      " x = '' 8\n",
      "join(f 2\n",
      "readlines())\n",
      "print(x)\n",
      "Databricks Hosted Datasets\n",
      "==========================\n",
      "The data contained within this directory is hosted for users to build data\n",
      "pipelines using Apache Spark and Databricks 39\n",
      "\n",
      "License\n",
      "-------\n",
      " 4\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "Let's take a deeper look at the bikeSharing  directory:\n",
      "# Let's list the contents of the directory corresponding to the data that\n",
      "we want to import\n",
      "display(dbutils 37\n",
      "fs 1\n",
      "ls(\"dbfs:/databricks-datasets/bikeSharing/\"))\n",
      "Running the preceding code will list out the contents of the bikeSharing  directory:\n",
      "path                                                name       size\n",
      "dbfs:/databricks-datasets/bikeSharing/README 50\n",
      "md     README 3\n",
      "md  5016\n",
      "dbfs:/databricks-datasets/bikeSharing/data-001/     data-001/  0\n",
      "We have a README file in markdown format and another sub-directory 42\n",
      "In the  data-001\n",
      "folder, we can list out the contents and see that there are two CSV files:\n",
      "# the data is given to use in both an hourly and a daily format 39\n",
      "\n",
      "display(dbutils 4\n",
      "fs 1\n",
      "ls(\"dbfs:/databricks-datasets/bikeSharing/data-001/\"))\n",
      "\n",
      "The output is as follows:\n",
      "path                                                     name       size\n",
      "dbfs:/databricks-datasets/bikeSharing/data-001/day 45\n",
      "csv   day 3\n",
      "csv    57569\n",
      "dbfs:/databricks-datasets/bikeSharing/data-001/hour 22\n",
      "csv  hour 3\n",
      "csv   1156736\n",
      "We will use the hourly format for our study 16\n",
      "Before we do, note that there is a README in\n",
      "the directory that will give us context and information about the data 24\n",
      "We can view the file\n",
      "by opening it and printing our the line contents as we did before:\n",
      "# Note that we had to change the format from dbfs:/ to /dbfs/\n",
      "with open(\"/dbfs/databricks-datasets/bikeSharing/README 53\n",
      "md\") as f:\n",
      " x = '' 8\n",
      "join(f 2\n",
      "readlines())\n",
      "print(x)\n",
      "## Dataset\n",
      "Bike-sharing rental process is highly correlated to the environmental and\n",
      "seasonal settings 25\n",
      "For instance, weather conditions, precipitation, day of\n",
      "week, season, hour of the day, etc 21\n",
      "can affect the rental behaviors 5\n",
      "The\n",
      "core data set is related to the two-year historical log corresponding to\n",
      "years 2011 and 2012 from Capital Bikeshare system, Washington D 33\n",
      "C 1\n",
      ", USA\n",
      "which is publicly available in http://capitalbikeshare 15\n",
      "com/system-data 3\n",
      "We\n",
      "aggregated the data on two hourly and daily basis and then extracted and\n",
      "added the corresponding weather and seasonal information 24\n",
      "Weather\n",
      "information are extracted from http://www 9\n",
      "freemeteo 4\n",
      "com 1\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "The README goes on to say that this dataset is primarily used to do regression (predicting\n",
      "a continuous response) 24\n",
      "We will treat the problem as a classification by bucketing our\n",
      "response to classes 16\n",
      "More on this later 4\n",
      "Let's import our data into a good old-fashioned\n",
      "pandas DataFrame:\n",
      "# Load data into a Pandas dataframe (note that pandas, sklearn, etc come\n",
      "with the environment 36\n",
      "That's pretty neat 4\n",
      "\n",
      "import pandas\n",
      "bike_data = pandas 8\n",
      "read_csv(\"/dbfs/databricks-\n",
      "datasets/bikeSharing/data-001/hour 18\n",
      "csv\") 2\n",
      "iloc[:,1:] # remove line number\n",
      "# view the dataframe\n",
      "#look at the first row\n",
      "bike_data 24\n",
      "loc[0]\n",
      "\n",
      "The output is as follows:\n",
      "dteday 2011-01-01\n",
      "season 1\n",
      "yr 0 mnth 1 hr 0 holiday 0 weekday 6 workingday 0 weathersit 1 temp 0 53\n",
      "24\n",
      "atemp 0 7\n",
      "2879 hum 0 6\n",
      "81 windspeed 0 casual 3 registered 13 cnt 16\n",
      "Descriptions for each variable are in the README:\n",
      "holiday : A Boolean that  is 1 (True ) means the  day is a holiday and 0 (False )\n",
      "means it is not  (extracted from http:/ ​/​dchr 66\n",
      "​dc 2\n",
      "​gov/ ​page/ ​holiday- ​schedule )\n",
      "workingday : A Boolean that is 1 (True ) means  the day is neither a weekend nor\n",
      "a holiday, otherwise 0 (False )\n",
      "cnt: Count of total rental bikes, including both casual and registered\n",
      "The README included in the Azure Databricks filesystem is very helpful for context and\n",
      "information about the datasets 78\n",
      "Let's see how many observations we have:\n",
      "# 17,379 observations\n",
      "bike_data 18\n",
      "shape\n",
      "(17379, 16)\n",
      "It is nice to stop and notice that everything that we have done so far (except for the Azure\n",
      "Databricks filesystem module) would be exactly the same if we were to do it in a normal\n",
      "Jupyter environment 54\n",
      "Let's see whether our dataset has any missing data that we would\n",
      "have to deal with:\n",
      "# no missing data, great 25\n",
      "\n",
      "bike_data 3\n",
      "isnull() 3\n",
      "sum()\n",
      "dteday 0\n",
      "season 0\n",
      "yr 0\n",
      "mnth 0 hr 0\n",
      "holiday 0\n",
      "weekday 0\n",
      "workingday 0\n",
      "weathersit 0\n",
      "temp 0\n",
      "atemp 0\n",
      "hum 0\n",
      "windspeed 0\n",
      "casual 0\n",
      "registered 0\n",
      "cnt 0\n",
      "\n",
      "No missing data 78\n",
      "Excellent 1\n",
      "Let's see a quick histogram of the cnt column, as it represents\n",
      "the total count of bikes reserved in that hour:\n",
      "# not everything will carry over, but that is ok\n",
      "%matplotlib inline\n",
      "bike_data 43\n",
      "hist(\"cnt\")\n",
      "# matplotlib inline is not supported in Databricks 14\n",
      "\n",
      "# You can display matplotlib figures using display() 10\n",
      "For an example, see #\n",
      "https://docs 9\n",
      "databricks 2\n",
      "com/user-guide/visualizations/matplotlib-and-ggplot\n",
      " 13\n",
      "html\n",
      "We get an error when we run this code, even though we have run this command  in this\n",
      "book before 25\n",
      "Unfortunately, matplotlib  does not work exactly the same in this\n",
      "environment as it does in Jupyter 20\n",
      "That is fine, though, because Azure Databricks provides\n",
      "a visualization tool built on top of Spark's version of a DataFrame that puts even more\n",
      "capabilities at our fingertips 35\n",
      "To get access to these visualizations, we will have to first\n",
      "convert our pandas DataFrame to its Spark equivalent:\n",
      "# converting to a Spark DataFrame allows us to make multiple types of plots\n",
      "using either a sample of data or the entire data population as the source\n",
      "sparkDataframe = spark 58\n",
      "createDataFrame(bike_data)\n",
      "# display is a simple way to view your data either via a table or through\n",
      "graphs\n",
      "display(sparkDataframe)\n",
      "Running the preceding code yields a snippet of the Dataframe for us to inspect:\n",
      "\n",
      "\n",
      "The display  command shows the Spark DataFrame inline with our notebook 59\n",
      "\n",
      "Azure Databricks allows for extremely powerful graphing capabilities thanks to\n",
      "Spark 16\n",
      "When using the display  command, on the bottom-left of the cell, a widget will\n",
      "appear, allowing us to graph using the data 28\n",
      "Let's click on the Histogram :\n",
      "Azure Databricks oﬀers a multitude of graphing options to get the best picture of our data\n",
      "Another button will appear, called Plot Options 39\n",
      " 1\n",
      " 1\n",
      ", which allows us to customize our graph 8\n",
      "\n",
      "We can drag and drop our columns into one of three fields:\n",
      "Keys  will, in general, represent our x-axis 25\n",
      "\n",
      "Values  will, in general, represent our y-axis 12\n",
      "\n",
      "Series groupings  will separate our data into groupings in order to get a bigger\n",
      "picture 20\n",
      "\n",
      "\n",
      "By default, display and visualizations will only aggregate over the first\n",
      "1,000 rows for convenience 21\n",
      "We can force Azure Databricks to utilize the\n",
      "entire dataset in our graph by setting the appropriate option 22\n",
      "\n",
      "Creating a simple histogram in Azure Databricks couldn't be simpler 14\n",
      "\n",
      "We can see how easy to use the system is and that our distribution is right-skewed 19\n",
      "\n",
      "Interesting 2\n",
      "Let's try something else now 6\n",
      "Let's say we also want to visualize the hourly\n",
      "usage, separated by the binary variable workingday ; the idea being that we are curious to\n",
      "see how the total amount of bikes reserved changes by the hour and if that distribution\n",
      "changes depending on whether it's a working day or not 58\n",
      "We can achieve this by selecting\n",
      "cnt as our value, hr as our key, and workingday  as our grouping 24\n",
      "\n",
      "\n",
      "It should look something like this:\n",
      "Hitting Apply  will apply this to the entire dataset, instead of the first-1,000-row sample that\n",
      "it shows us on the right:\n",
      "\n",
      "\n",
      "It is extremely easy to use Azure Databricks to generate beautiful and interpretable graphs\n",
      "based on our data 61\n",
      "From this, we can see that bike sharing appears to be somewhat\n",
      "normally distributed on weekends while workdays have large spikes in the morning and\n",
      "evening (which makes sense) 36\n",
      "\n",
      "In this dataset, there are three possible  regression response candidates: casual ,\n",
      "registered , and cnt 20\n",
      "We will turn our problem into a classification problem by bucketing\n",
      "the cnt column into one of two buckets 21\n",
      "Our response will either be 100 or fewer bikes\n",
      "reserved that hour ( False ) or over 100 ( True ):\n",
      "# Seperate into X, and y (features and label)\n",
      "# we will turn our regression problem into a classification problem by\n",
      "bucketing the column \"cnt\" as being either over 100 / 100 or under 68\n",
      "\n",
      "features, labels = bike_data[[\"season\", \"yr\", \"mnth\", \"hr\", \"holiday\",\n",
      "\"weekday\", \"workingday\", \"weathersit\", \"atemp\", \"hum\", \"windspeed\"]],\n",
      "bike_data[\"cnt\"] > 100\n",
      "# See the distribution of our labels\n",
      "labels 66\n",
      "value_counts()\n",
      "True 10344\n",
      "False 7035\n",
      "Let's now create a simple function that will do a few things: take in a param choice for a\n",
      "random forest classifier, fit and test our model on our data, and return the results:\n",
      "from sklearn 55\n",
      "ensemble import RandomForestClassifier\n",
      "from sklearn 7\n",
      "model_selection import train_test_split\n",
      "from sklearn 9\n",
      "metrics import accuracy_score\n",
      "# Create a function that will\n",
      "# 0 15\n",
      "Take in a parameter for our Random Forest's n_estimators param\n",
      "# 1 16\n",
      "Instantiate a Random Forest algorithm\n",
      "# 2 9\n",
      "Split our data into a training and testing split\n",
      "# 3 13\n",
      "Fit on our training data\n",
      "# 4 9\n",
      "evaluate on our testing data\n",
      "# 5 9\n",
      "return a tuple with the n_estimators param and corresponding accuracy\n",
      "def runRandomForest(c):\n",
      " rf = RandomForestClassifier(n_estimators=c)\n",
      " # Split into train and test using\n",
      "sklearn 36\n",
      "cross_validation 2\n",
      "train_test_split\n",
      " X_train, X_test, y_train, y_test = train_test_split(features, labels,\n",
      "test_size=0 27\n",
      "2, random_state=1)\n",
      "\n",
      " # Build the model\n",
      " rf 14\n",
      "fit(X_train, y_train)\n",
      " # Calculate predictions and accuracy\n",
      " predictions = rf 16\n",
      "predict(X_test)\n",
      " accuracy = accuracy_score(predictions, y_test)\n",
      " # return param and accuracy score\n",
      " return (c, accuracy)\n",
      "This function takes in a single number as an input, sets that number as a random forest's\n",
      "n_estimator  parameter, and returns the accuracy associated with that param choice on a\n",
      "train-test split:\n",
      "runRandomForest(1)\n",
      "(1, 0 78\n",
      "90218642117376291)\n",
      "Command took  0 13\n",
      "13 seconds\n",
      "This took a very short amount of time because we were only training a single decision tree\n",
      "in our forest 25\n",
      "Let's now iteratively try a varying number of estimators and see how this\n",
      "affects our accuracy:\n",
      "for i in [1, 10, 100, 1000, 10000]:\n",
      "  print(runRandomForest(i))\n",
      "(1, 0 54\n",
      "89700805523590332)\n",
      "(10, 0 13\n",
      "939873417721519)\n",
      "(100, 0 12\n",
      "94677790563866515)\n",
      "(1000, 0 14\n",
      "94677790563866515)\n",
      "(10000, 0 14\n",
      "94792865362485612)\n",
      "Command took  4 13\n",
      "16 minutes\n",
      "It took my cluster over 4 minutes to try these five n_estimator  options 20\n",
      "Most of the time\n",
      "was taken up by the final two options as it took a very long time to train thousands of\n",
      "decision trees 27\n",
      "\n",
      "As we add more combinations of parameters  and more parameter\n",
      "options, the time it will take to iteratively go through these options will\n",
      "explode 30\n",
      "We will see how we can utilize Databrick's environment to\n",
      "optimize this in the third case study 21\n",
      "\n",
      "\n",
      "Let's make use of Spark to parallelize our for loop 13\n",
      "Every notebook has a special variable\n",
      "called sc that represents Spark:\n",
      "# every notebook has a variable called \"sc\" that represents the Spark\n",
      "context in our cluster\n",
      "sc\n",
      "There are a few ways of performing this parallelization, but in general, it will look like this:\n",
      "Create a dataset that will be sent to our cluster (in this case, parameter options) 74\n",
      "1 2\n",
      "\n",
      "Map a function to each element of the dataset (our runRandomForest  function) 18\n",
      "2 2\n",
      "\n",
      "Collect the results:3 6\n",
      "\n",
      "# 1 4\n",
      "set up 5 tasks in our Spark Cluster by parallelizing a\n",
      "dataset (list) of five elements (n_estimator options)\n",
      "k = sc 29\n",
      "parallelize([1, 10, 100, 1000, 10000])\n",
      "# 2 22\n",
      "map our function to our 5 tasks\n",
      "# The code will not be sent to our cluster until we run the\n",
      "next command\n",
      "results = k 30\n",
      "map(runRandomForest)\n",
      "Command took  0 10\n",
      "13 seconds\n",
      "Here, we are introduced to our first Apache Spark-specific syntax 16\n",
      "Step 1  will return a\n",
      "distributed dataset that is optimized for parallel computation, and we will call that dataset\n",
      "k 25\n",
      "The values of k represent different possible arguments for our function,\n",
      "runRandomForest 15\n",
      "Step 2  tells our cluster to run the function across our distributed\n",
      "dataset 16\n",
      "\n",
      "It is important to note that while Step 1  and Step 2  are Spark-specific commands, up until\n",
      "now, our function has not actually been sent to our cluster for execution 39\n",
      "We have just set\n",
      "the stage to do so by setting up the appropriate variables 16\n",
      "Step 3  will collect our results by\n",
      "running the function in parallel across the different values in k:\n",
      "# 3 25\n",
      "the collect method actually sends the five tasks to our cluster for\n",
      "execution\n",
      "# Faster (1 19\n",
      "5x) because we aren't doing each task one after the other 15\n",
      "We\n",
      "are doing them in parallel\n",
      "# This becomes much more noticeable when doing more params (we will get to\n",
      "this in a later case study)\n",
      "results 32\n",
      "collect()\n",
      "Command took  2 7\n",
      "73 minutes\n",
      "\n",
      "Immediately, we can see the value of parallelizing functions using Spark 17\n",
      "By doing nothing\n",
      "more than relying on Azure Databricks and Spark, we are able to perform our for loop 1 25\n",
      "5x\n",
      "faster 6\n",
      "If we use a variable in a function (like our dataset), Spark will automatically send the\n",
      "dataset to the workers 23\n",
      "This is usually fine 4\n",
      "We can send it to workers  more efficiently by\n",
      "broadcasting  it 15\n",
      "By broadcasting data, a copy of the data is sent to our workers, which are\n",
      "used when running tasks 22\n",
      "This is much more efficient when dealing with extremely large\n",
      "datasets with large values in them 17\n",
      "We can rewrite our previous function using broadcast\n",
      "variables:\n",
      "# Broadcast dataset\n",
      "# If we use a variable in a function, Spark will automatically send the\n",
      "dataset to the workers 35\n",
      "This is usually fine 4\n",
      "\n",
      "# We can send it to workers more efficiently by broadcasting it 13\n",
      "By\n",
      "broadasting data, a copy is sent to our workers which are used when running\n",
      "tasks 21\n",
      "\n",
      "# For more info on broadcast variables, see the Spark programming guide 14\n",
      "\n",
      "You can create a Broadcast variable using sc 9\n",
      "broadcast() 2\n",
      "\n",
      "# To access the value of a broadcast variable, you need to use 15\n",
      "value\n",
      "# broadcast the variables to our workers\n",
      "featuresBroadcast = sc 14\n",
      "broadcast(features)\n",
      "labelsBroadcast = sc 7\n",
      "broadcast(labels)\n",
      "# reboot of the previous function\n",
      "def runRandomForestBroadcast(c):\n",
      "  rf = RandomForestClassifier(n_estimators=c)\n",
      "  # Split into train and test using\n",
      "sklearn 37\n",
      "cross_validation 2\n",
      "train_test_split\n",
      "  # ** This part of the function is the only difference from the previous\n",
      "version **\n",
      "  X_train, X_test, y_train, y_test =\n",
      "train_test_split(featuresBroadcast 40\n",
      "value, labelsBroadcast 4\n",
      "value,\n",
      "test_size=0 6\n",
      "2, random_state=1)\n",
      "  # Build the model\n",
      "  rf 16\n",
      "fit(X_train, y_train)\n",
      "  # Calculate predictions and accuracy\n",
      "  predictions = rf 18\n",
      "predict(X_test)\n",
      "  accuracy = accuracy_score(predictions, y_test)\n",
      "  return (c, accuracy)\n",
      "\n",
      "Once our new function using broadcast variables is complete, running it in parallel is no\n",
      "different:\n",
      "# set up 5 tasks in our Spark Cluster\n",
      "k = sc 54\n",
      "parallelize([1, 10, 100, 1000, 10000])\n",
      "# map our function to our five tasks\n",
      "results = k 31\n",
      "map(runRandomForestBroadcast)\n",
      "# the real work begins here 12\n",
      "\n",
      "results 2\n",
      "collect()\n",
      "The timing is not very different from  our last run with non-broadcast data 18\n",
      "This is because\n",
      "our data and logic are not large enough to see a noticeable difference yet 18\n",
      "Once done with\n",
      "our variables, we can unpersist (unbroadcast) them like so:\n",
      "# Since we are done with our broadcast variables, we can clean them up 34\n",
      "\n",
      "# (This will happen automatically, but we can make it happen earlier by\n",
      "explicitly unpersisting the broadcast variables 25\n",
      "\n",
      "featuresBroadcast 3\n",
      "unpersist()\n",
      "labelsBroadcast 5\n",
      "unpersist()\n",
      "Hyperparameter tuning would be very difficult to do if you wanted to do a grid search\n",
      "across multiple parameters and multiple options 28\n",
      "We could enhance our function in order\n",
      "to accommodate this; however, it would be better to rely on existing frameworks in scikit-\n",
      "learn to do so 31\n",
      "We will see how to remedy this conundrum in a later case study 15\n",
      "\n",
      "We can see how we can utilize Databrick's easy-to-use environment, clusters, and\n",
      "notebooks in order to enhance our data analysis and machine learning with minimal\n",
      "changes to our coding style 41\n",
      "In the next case study, we will examine Spark's scalable\n",
      "machine learning library, MLlib, for optimized machine learning speed 25\n",
      "\n",
      "Case study 2 – Using MLlib in Azure Databricks\n",
      "to predict credit card fraud\n",
      "Our second case study will focus  on predicting credit card fraud and will make use of\n",
      "MLlib, Apache Spark's scalable machine learning library 49\n",
      "MLlib comes standard with our\n",
      "Azure Databricks environment and allows us to write scalable machine learning code 21\n",
      "This\n",
      "case study will focus on MLlib syntax while we draw parallels (no pun intended) to its\n",
      "scikit-learn cousins 27\n",
      "\n",
      "\n",
      "The aspects of Azure Databricks that we will be highlighting in this case study include\n",
      "these:\n",
      "Importing a CSV that is uploaded to the Azure Databricks filesystem\n",
      "Using MLlib's pipeline, feature pre-processing, and machine learning library to\n",
      "write scalable machine learning code\n",
      "Using MLlib metric evaluation modules that mirror scikit-learn's components\n",
      "The data in question is predicting credit card fraud 82\n",
      "The data represents credit card\n",
      "transactions and contains 28 anonymized continuous features (called V1 19\n",
      " 1\n",
      "V28) plus the\n",
      "amount that the transaction was for and the time at which the transaction occurred 20\n",
      "We will\n",
      "not dive too deeply into feature engineering in this case study, but will focus  mainly on\n",
      "using the MLlib modules that come with Azure Databricks to predict the response label\n",
      "(which is a binary variable) 46\n",
      "Let's get right into it and start by importing a CSV that we have\n",
      "uploaded using the Azure Databricks data import feature:\n",
      "# File location and type\n",
      "# This dataset also exists in the DBFS of Databricks 46\n",
      "This is just to show\n",
      "how to import a CSV that has been previously uploaded so that\n",
      "# you can upload your own 25\n",
      "\n",
      "file_location = \"/FileStore/tables/creditcard 12\n",
      "csv\"\n",
      "file_type = \"csv\"\n",
      "# CSV options\n",
      "# will automatically cast columns as appropiate types (float, string, etc)\n",
      "infer_schema = \"true\"\n",
      "first_row_is_header = \"true\"\n",
      "delimiter = \",\"\n",
      "# read a csv file and convert to a Spark dataframe\n",
      "df = spark 61\n",
      "read 1\n",
      "format(file_type) \\\n",
      " 5\n",
      "option(\"inferSchema\", infer_schema) \\\n",
      " 9\n",
      "option(\"header\", first_row_is_header) \\\n",
      " 10\n",
      "option(\"sep\", delimiter) \\\n",
      " 7\n",
      "load(file_location)\n",
      "# show us the Spark Dataframe\n",
      "display(df)\n",
      "\n",
      "Our goal is to build a pipeline that will do this:\n",
      "Assemble the columns that we wish to use as features1 39\n",
      "\n",
      "Scale our features using a standard z-score function2 11\n",
      "\n",
      "Encode our label as 0 or 1 (it already is but it is good to see this functionality)3 24\n",
      "\n",
      "Run a logistic regression across the training data to fit coefficients4 13\n",
      "\n",
      "Evaluate binary metrics on the testing set5 9\n",
      "\n",
      "Use an MLlib cross-validating grid searching module to find the best parameters6 17\n",
      "\n",
      "for our logistic regression model\n",
      "Phew 9\n",
      "That's a lot, so let's go one step at a time 14\n",
      "The following code block will handle Step 1 ,\n",
      "Step 2 , and Step 3  by setting up a pipeline with three steps in it:\n",
      "# import the pipeline module from pyspark\n",
      "from pyspark 42\n",
      "ml import Pipeline\n",
      "from pyspark 7\n",
      "ml 1\n",
      "feature import VectorAssembler, StandardScaler,\n",
      "StringIndexer\n",
      "# will hold the steps of our pipeline\n",
      "stages = []\n",
      "The first stage of our pipeline will assemble the features that we want to use in our machine\n",
      "learning procedure 46\n",
      "Let's use all 28 entries from the anonymized data (V1 - V28) plus the\n",
      "amount  column 25\n",
      "For this case study, we will not use the time  column:\n",
      "# Transform all features into a vector using VectorAssembler\n",
      "# create list of [\"V1\", \"V2\", \"V3\", 41\n",
      " 1\n",
      " 1\n",
      "\n",
      "numericCols = [\"V{}\" 8\n",
      "format(i) for i in range(1, 29)]\n",
      "# Add \"Amount\" to the list of features\n",
      "assemblerInputs = numericCols + [\"Amount\"]\n",
      "# VectorAssembler acts like scikit-learn's \"FeatureUnion\" to put together\n",
      "the feature columns and adding the label \"features\"\n",
      "assembler = VectorAssembler(inputCols=assemblerInputs,\n",
      "outputCol=\"features\")\n",
      "# add the VectorAssembler to the stages of our MLLib Pipeline\n",
      "stages 96\n",
      "append(assembler)\n",
      "The second stage of our pipeline will take the assembled features and scale them:\n",
      "# MLLib's StandardScaler acts like scikit-learn's StandardScaler\n",
      "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
      " withStd=True, withMean=True)\n",
      "# add StandardScaler to our pipeline\n",
      "stages 70\n",
      "append(scaler)\n",
      "\n",
      "Once we've gathered our features and scaled them, the final step of our pipeline encodes\n",
      "our class label  to ensure consistency:\n",
      "# Convert label into label indices using the StringIndexer (like scikit-\n",
      "learn's LabelEncoder)\n",
      "label_stringIdx = StringIndexer(inputCol=\"Class\", outputCol=\"label\")\n",
      "# add the StringIndexer to our pipeline\n",
      "stages 81\n",
      "append(label_stringIdx)\n",
      "That was a lot, but note that every step of the pipeline has a scikit-learn equivalent 25\n",
      "As a\n",
      "data scientist, the most important thing to remember is that as long as you know the theory \n",
      "and the proper steps of data science and machine learning, you can transfer your skills\n",
      "across many languages, platforms, and technologies 48\n",
      "Now that we have created our list of\n",
      "three stages, let's instantiate a pipeline  object:\n",
      "# create our pipeline with three stages\n",
      "pipeline = Pipeline(stages=stages)\n",
      "To make sure that we are training a model that is able to predict unseen cases, we should\n",
      "split up our data into a training and testing set, and fit our pipeline to the training set while\n",
      "using the trained pipeline to transform the testing set:\n",
      "# We need to split our data into training and test sets 100\n",
      "This is like\n",
      "scikit-learn's train_test_split function\n",
      "# We will also set a random number seed for reproducibility\n",
      "(trainingData, testData) = df 35\n",
      "randomSplit([0 4\n",
      "7, 0 5\n",
      "3], seed=1)\n",
      "print(trainingData 10\n",
      "count())\n",
      "print(testData 5\n",
      "count())\n",
      "199470  # elements in the training set\n",
      "85337  # elements in the testing set\n",
      "Let's now fit our pipeline to the training set 32\n",
      "This will learn the features as well as the\n",
      "parameters to scale future unseen testing data:\n",
      "# fit and transform to our training data\n",
      "pipelineModel = pipeline 31\n",
      "fit(trainingData)\n",
      "trainingDataTransformed = pipelineModel 11\n",
      "transform(trainingData)\n",
      "\n",
      "If we take a look at our DataFrame, we will notice that we have three new columns:\n",
      "features , scaledFeatures , and label 30\n",
      "These three columns were added by our pipeline\n",
      "and those names can be found exactly in the preceding code where we set the three stages\n",
      "of the pipeline 30\n",
      "Note that the data types of the features and scaledFeatures  column\n",
      "are vector 16\n",
      "This indicates that they represent observations to be learned by our machine\n",
      "learning model in the future:\n",
      "# note the new columns \"features\", \"scaledFeatures\", and \"label\" at the end\n",
      "trainingDataTransformed\n",
      "Time:decimal(10,0)\n",
      "V1:double\n",
      "V2:double\n",
      "V3:double\n",
      "V4:double\n",
      "V5:double\n",
      "V6:double\n",
      "V7:double\n",
      "V8:double\n",
      "V9:double\n",
      "V10:double\n",
      "V11:double\n",
      "V12:double\n",
      "V13:double\n",
      "V14:double\n",
      "V15:double\n",
      "V16:double\n",
      "V17:double\n",
      "V18:double\n",
      "V19:double\n",
      "V20:double\n",
      "V21:double\n",
      "V22:double\n",
      "V23:double\n",
      "V24:double\n",
      "V25:double\n",
      "V26:double\n",
      "V27:double\n",
      "V28:double\n",
      "Amount:double\n",
      "Class:integer\n",
      "features: udt\n",
      "scaledFeatures: udt\n",
      "label:double\n",
      "\n",
      "Just like we do in scikit-learn, we will have to import out logistic regression model,\n",
      "instantiate it, and fit it to our training data:\n",
      "# Import the logistic regression module from pyspark\n",
      "from pyspark 260\n",
      "ml 1\n",
      "classification import LogisticRegression\n",
      "# Create initial LogisticRegression model\n",
      "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"scaledFeatures\")\n",
      "# Train model with Training Data\n",
      "lrModel = lr 38\n",
      "fit(trainingDataTransformed)\n",
      "This process should look very familiar to us as it is nearly identical to scikit-learn 24\n",
      "The main\n",
      "difference is that when we instantiate our model, we have to tell the object the names of the\n",
      "label and features, instead of feeding the features and label separately into the fit method\n",
      "(like we do in scikit-learn) 50\n",
      "\n",
      "Once we fit our model, we can transform and gather predictions from our testing data:\n",
      "# transform (not fit) to the testing data\n",
      "testDataTransformed = pipelineModel 36\n",
      "transform(testData)\n",
      "# run our logistic regression over the transformed testing set\n",
      "predictions = lrModel 19\n",
      "transform(testDataTransformed)\n",
      "Transforming our testing data using logistic regression will actually add three new columns\n",
      "(like the pipeline did) 27\n",
      "We can see this by running this:\n",
      "predictions:\n",
      "Time: decimal(10,0)\n",
      "V1: double\n",
      "V2: double\n",
      "V3: double\n",
      "V4: double\n",
      "V5: double\n",
      "V6: double\n",
      "V7: double\n",
      "V8: double\n",
      "V9: double\n",
      "V10: double\n",
      "V11: double\n",
      "V12: double\n",
      "V13: double\n",
      "V14: double\n",
      "V15: double\n",
      "V16: double\n",
      "V17: double\n",
      "V18: double\n",
      "\n",
      "V19: double\n",
      "V20: double\n",
      "V21: double\n",
      "V22: double\n",
      "V23: double\n",
      "V24: double\n",
      "V25: double\n",
      "V26: double\n",
      "V27: double\n",
      "V28: double\n",
      "Amount: double\n",
      "Class: integer\n",
      "features: udt\n",
      "scaledFeatures: udt\n",
      "label: double\n",
      "rawPrediction: udt\n",
      "probability: udt\n",
      "prediction: double\n",
      "Let's take a look at the label  column as well  as two of the new columns,  probability  and\n",
      "prediction 222\n",
      "We can do this by invoking the filter method of the Spark DataFrame:\n",
      "selected = predictions 17\n",
      "select(\"label\", \"prediction\", \"probability\")\n",
      "display(selected)\n",
      "The output is as follows:\n",
      "label prediction probability\n",
      "0     0          [1,2,[],[0 35\n",
      "9998645390717447,0 9\n",
      "000135460928255428]]\n",
      "0     0          [1,2,[],[0 20\n",
      "9998821292706751,0 9\n",
      "00011787072932487872]]\n",
      "0     0          [1,2,[],[0 21\n",
      "9994714991454193,0 9\n",
      "000528500854580697]]\n",
      "0     0          [1,2,[],[0 20\n",
      "9991193503498385,0 9\n",
      "0008806496501614437]]\n",
      "0     0          [1,2,[],[0 21\n",
      "9997043818469743,0 9\n",
      "00029561815302580084]]\n",
      "0     0          [1,2,[],[0 21\n",
      "9998106820888389,0 9\n",
      "00018931791116114655]]\n",
      "0     0          [1,2,[],[0 21\n",
      "9995735877526569,0 9\n",
      "0004264122473429876]]\n",
      " 9\n",
      " 1\n",
      " 1\n",
      "\n",
      "The label  and prediction  columns show each observation's ground truth and our\n",
      "model's estimate, while the probability  column holds a vector that contains the\n",
      "predicted probability (think scikit-learn's predict_proba  functionality) 47\n",
      "Let's now bring in\n",
      "PySpark's metric evaluation module in order to get some basic metrics 19\n",
      "we will start with\n",
      "BinaryClassificationEvaluator , which can tell us the testing AUC (area under the\n",
      "ROC curve):\n",
      "# like scikit-learn's metric module\n",
      "from pyspark 38\n",
      "ml 1\n",
      "evaluation import BinaryClassificationEvaluator\n",
      "\n",
      "# Evaluate model using either area under Precision Recall curev or the area\n",
      "under the ROC\n",
      "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n",
      "metricName=\"areaUnderROC\")\n",
      "evaluator 47\n",
      "evaluate(predictions)\n",
      "0 4\n",
      "984986959421\n",
      "This is helpful, but we also want some of the more familiar and interpretable metrics such\n",
      "as accuracy , precision , recall , and more 34\n",
      "We can get these using\n",
      "PySpark's MulticlassMetrics  module:\n",
      "# Get some deeper metrics out of our predictions\n",
      "from pyspark 28\n",
      "mllib 2\n",
      "evaluation import MulticlassMetrics\n",
      "# must turn DF into RDD (Resilient Distributed Dataset)\n",
      "predictionAndLabels = predictions 24\n",
      "select(\"label\",\n",
      "\"prediction\") 7\n",
      "rdd 2\n",
      "map(tuple)\n",
      "# instantiate a MulticlassMetrics object\n",
      "metrics = MulticlassMetrics(predictionAndLabels)\n",
      "# Overall statistics\n",
      "accuracy = metrics 27\n",
      "accuracy\n",
      "precision = metrics 5\n",
      "precision()\n",
      "recall = metrics 5\n",
      "recall()\n",
      "f1Score = metrics 7\n",
      "fMeasure()\n",
      "print(\"Summary Stats\")\n",
      "print(\"Accuracy = %s\" % accuracy)\n",
      "print(\"Precision = %s\" % precision)\n",
      "print(\"Recall = %s\" % recall)\n",
      "print(\"F1 Score = %s\" % f1Score)\n",
      "The output is as follows:\n",
      "Summary Stats\n",
      "Accuracy = 0 66\n",
      "999203159239\n",
      "Precision = 0 10\n",
      "999203159239\n",
      "Recall = 0 11\n",
      "999203159239\n",
      "F1 Score = 0 12\n",
      "999203159239\n",
      "Note that accuracy is an attribute of the MulticlassMetrics  object and\n",
      "not a method, so we do not need the parentheses 32\n",
      "\n",
      "\n",
      "Great 2\n",
      "We can also calculate our true positive rate, false negative rate, and other by using\n",
      "the predictions vector 21\n",
      "This will give us a better sense of how our machine learning model\n",
      "performs on particular examples of positive and negative fraud cases:\n",
      "tp = predictions[(predictions 31\n",
      "label == 1) & (predictions 8\n",
      "prediction ==\n",
      "1)] 4\n",
      "count()\n",
      "tn = predictions[(predictions 7\n",
      "label == 0) & (predictions 8\n",
      "prediction ==\n",
      "0)] 4\n",
      "count()\n",
      "fp = predictions[(predictions 7\n",
      "label == 0) & (predictions 8\n",
      "prediction ==\n",
      "1)] 4\n",
      "count()\n",
      "fn = predictions[(predictions 7\n",
      "label == 1) & (predictions 8\n",
      "prediction ==\n",
      "0)] 4\n",
      "count()\n",
      "print (\"True Positives:\", tp)\n",
      "print (\"True Negatives:\", tn)\n",
      "print (\"False Positives:\", fp)\n",
      "print (\"False Negatives:\", fn)\n",
      "The output is as follows:\n",
      "True Positives: 93\n",
      "True Negatives: 85176\n",
      "False Positives: 18\n",
      "False Negatives: 50\n",
      "Let's consider this our baseline logistic regression model and try to optimize our results by\n",
      "tweaking our logistic regression parameters 93\n",
      "\n",
      "Using the MLlib Grid Search module to tune\n",
      "hyperparameters\n",
      "To optimize our parameters, it would help  to know exactly what they were and how to use\n",
      "them 35\n",
      "Luckily, we can see the README in each model by running the explainParams\n",
      "method 17\n",
      "This will generate a list of the available parameters, a description as to what they\n",
      "represent, and usually a guide to the acceptable values:\n",
      "# explain the parameters that are included in MLLib's Logistic Regression\n",
      "print(lr 45\n",
      "explainParams())\n",
      "The output is as follows:\n",
      "aggregationDepth : suggested depth for treeAggregate (>= 2) 23\n",
      "(default: 2)\n",
      "elasticNetParam : the ElasticNet mixing parameter, in range [0, 1] 24\n",
      "For\n",
      "alpha = 0, the penalty is an L2 penalty 14\n",
      "For alpha = 1, it is an L1\n",
      "\n",
      "penalty 14\n",
      "(default: 0 5\n",
      "0)\n",
      "family: The name of family which is a description of the label distribution\n",
      "to be used in the model 24\n",
      "Supported options: auto, binomial, multinomial\n",
      "(default: auto) featuresCol : features column name 21\n",
      "(default: features,\n",
      "current: scaledFeatures) fitIntercept : whether to fit an intercept term 20\n",
      "\n",
      "(default: True)\n",
      "labelCol : label column name 11\n",
      "(default: label, current: label)\n",
      " 9\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "Let's build a parameter grid for Spark to gridsearch  across 14\n",
      "Let's choose three parameters\n",
      "to start with: maxIter , regParam,  and elasticNetParam 21\n",
      "We first need to build a\n",
      "parameter grid using PySpark's version of scikit-learn's GridSearchCV :\n",
      "# pyspark's version of GridSearchCV\n",
      "from pyspark 37\n",
      "ml 1\n",
      "tuning import ParamGridBuilder, CrossValidator\n",
      "# Create ParamGrid for Cross Validation\n",
      "paramGrid = (ParamGridBuilder()\n",
      " 25\n",
      "addGrid(lr 3\n",
      "regParam, [0 5\n",
      "0, 0 5\n",
      "01, 0 5\n",
      "5, 2 5\n",
      "0])\n",
      " 3\n",
      "addGrid(lr 3\n",
      "elasticNetParam, [0 6\n",
      "0, 0 5\n",
      "5, 1 5\n",
      "0])\n",
      " 3\n",
      "addGrid(lr 3\n",
      "maxIter, [5, 10])\n",
      " 9\n",
      "build())\n",
      "# Create 5-fold CrossValidator that can also test multiple parameters (like\n",
      "scikit-learn's GridSearchCV)\n",
      "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid,\n",
      "evaluator=evaluator, numFolds=5)\n",
      "Let's send our grid search to the cluster for the efficient parallelization of tasks:\n",
      "# Run cross validations on our cluster\n",
      "cvModel = cv 84\n",
      "fit(trainingDataTransformed)\n",
      "# this will likely take a fair amount of time because of the amount of\n",
      "models that we're creating and testing\n",
      "The variable, cvModel , holds the logistic regression model with the optimized parameters 45\n",
      "\n",
      "From our optimized model, we can extract the learned coefficients to gain a deeper\n",
      "understanding as to how the features correlate to the label:\n",
      "# extract the weights from our model\n",
      "weights = cvModel 41\n",
      "bestModel 2\n",
      "coefficients\n",
      "# convert from numpy type to float\n",
      "weights = [[float(w)] for w in weights]\n",
      "weightsDF = sqlContext 26\n",
      "createDataFrame(weights, [\"Feature Weight\"])\n",
      "display(weightsDF)\n",
      "\n",
      "Running the preceding code yields a single-column DataFrame with the logistic regression\n",
      "coefficients 28\n",
      "As we've seen before, the weights represent the importance and correlation\n",
      "between the features and the response:\n",
      "Feature Weight\n",
      "-0 26\n",
      "009310428799214699\n",
      "0 9\n",
      "024089617982041803\n",
      "-0 10\n",
      "07660344812402071\n",
      "0 9\n",
      "13879375587420806\n",
      "0 9\n",
      "03389644146602658\n",
      "-0 10\n",
      "03197804822203382\n",
      "-0 10\n",
      "026671134727093863\n",
      "-0 10\n",
      "05963860645699601\n",
      "-0 10\n",
      "07157334685249503\n",
      "-0 10\n",
      "12739634200985744\n",
      "0 9\n",
      "11271203988538568\n",
      "-0 10\n",
      "16991941687681994\n",
      "-0 10\n",
      "022382161065846975\n",
      "-0 10\n",
      "2967372927422323\n",
      "-0 10\n",
      "002525484797586701\n",
      "-0 10\n",
      "08661888759753078\n",
      "-0 10\n",
      "09428351861530046\n",
      "-0 10\n",
      "010145267312697291\n",
      "0 9\n",
      "000474661023205239\n",
      "0 9\n",
      "003985147929831393\n",
      "0 9\n",
      "031230406955806467\n",
      "0 9\n",
      "011362000753207976\n",
      "-0 10\n",
      "014548646956536248\n",
      "-0 10\n",
      "011270335506048019\n",
      "-0 10\n",
      "004390342109545349\n",
      "0 9\n",
      "008722583938741943\n",
      "0 9\n",
      "01390573346423987\n",
      "0 9\n",
      "014176539525542918\n",
      "0 9\n",
      "021489763526114244\n",
      "We can grab our predictions in the same way we did previously to compare our results:\n",
      "# Use test set to get the best params\n",
      "predictions = cvModel 39\n",
      "transform(testDataTransformed)\n",
      "# must turn DF into RDD (Resilient Distributed Dataset)\n",
      "predictionAndLabels = predictions 24\n",
      "select(\"label\",\n",
      "\"prediction\") 7\n",
      "rdd 2\n",
      "map(tuple)\n",
      "metrics = MulticlassMetrics(predictionAndLabels)\n",
      "# Overall statistics\n",
      "\n",
      "accuracy = metrics 19\n",
      "accuracy\n",
      "precision = metrics 5\n",
      "precision()\n",
      "recall = metrics 5\n",
      "recall()\n",
      "f1Score = metrics 7\n",
      "fMeasure()\n",
      "print(\"Summary Stats\")\n",
      "print(\"Accuracy = %s\" % accuracy)\n",
      "print(\"Precision = %s\" % precision)\n",
      "print(\"Recall = %s\" % recall)\n",
      "print(\"F1 Score = %s\" % f1Score)\n",
      "The output is as follows:\n",
      "Summary Stats\n",
      "Accuracy = 0 66\n",
      "998839893598\n",
      "Precision = 0 10\n",
      "998839893598\n",
      "Recall = 0 11\n",
      "998839893598\n",
      "F1 Score = 0 12\n",
      "998839893598\n",
      "Run the following code:\n",
      "tp = predictions[(predictions 16\n",
      "label == 1) & (predictions 8\n",
      "prediction ==\n",
      "1)] 4\n",
      "count()\n",
      "tn = predictions[(predictions 7\n",
      "label == 0) & (predictions 8\n",
      "prediction ==\n",
      "0)] 4\n",
      "count()\n",
      "fp = predictions[(predictions 7\n",
      "label == 0) & (predictions 8\n",
      "prediction ==\n",
      "1)] 4\n",
      "count()\n",
      "fn = predictions[(predictions 7\n",
      "label == 1) & (predictions 8\n",
      "prediction ==\n",
      "0)] 4\n",
      "count()\n",
      "print \"True Positives:\", tp\n",
      "print \"True Negatives:\", tn\n",
      "print \"False Positives:\", fp\n",
      "print \"False Negatives:\", fn\n",
      "# False positive went from 18 to 16 (win) but False Negative jumped to 83\n",
      "(opposite of win)\n",
      "The output is as follows:\n",
      "True Positives: 67\n",
      "True Negatives: 85178\n",
      "False Positives: 16\n",
      "False Negatives: 76\n",
      "It's easy to see how Azure Databricks' environment makes it easy to utilize Spark and\n",
      "MLlib to create scalable machine  learning pipelines that are similar in construction and\n",
      "usage to scikit-learn 137\n",
      "\n",
      "\n",
      "Case study 3 – Using Azure Databricks to\n",
      "optimize our hyperparameter tuning\n",
      "Our final case study is the shortest and will showcase how we can combine the best of\n",
      "scikit-learn, Spark, and Azure Databricks to build simple yet powerful machine learning\n",
      "models 58\n",
      "We will be using the MNIST dataset, which we used earlier, and we will be fitting\n",
      "a fairly simple RandomForestClassifier  to the data 29\n",
      "The interesting bit will come when\n",
      "we import a third-party tool called spark_sklearn  to help us out 22\n",
      "\n",
      "The aspects of Azure Databricks that we will be highlighting in this case study include\n",
      "these:\n",
      "Importing third-party packages into our Azure Databricks environment\n",
      "Enabling Spark's parallelization within scikit-learn's easy-to-use syntax\n",
      "How to add Python libraries to your cluster\n",
      "Up until now, all of the packages that we have used come with the Azure Databricks\n",
      "environment 81\n",
      "Now we need to add a new package, called spark_sklearn 13\n",
      "To add a third-\n",
      "party package to our Azure Databricks cluster, we simply click on Import Library  from our\n",
      "main dashboard, and we will see a window like the following:\n",
      "Type in any package that you wish to use in the PyPi Name ﬁeld\n",
      "\n",
      "We simply type in spark_sklearn , hit Install Library , and we are done 72\n",
      "The cluster will\n",
      "now let us import the library from any existing or new notebook 16\n",
      "Now, spark_sklearn  is\n",
      "a handy tool that allows the use of some  scikit-learn packages with the backend swapped\n",
      "out for PySpark 32\n",
      "This means that we can use existing code that we have already written,\n",
      "and only have to tweak it slightly to make it compatible with Spark 27\n",
      "\n",
      "Using spark_sklearn to build an MNIST classifier\n",
      "We have already seen that MNIST is a handwritten digit detection dataset 25\n",
      "Without\n",
      "spending too much time  on the data itself, let's jump right into how we can use\n",
      "spark_sklearn  for our benefit 30\n",
      "Let's bring in our data using the standard scikit-learn\n",
      "dataset module:\n",
      "from sklearn import datasets\n",
      "from sklearn 24\n",
      "ensemble import RandomForestClassifier\n",
      "from sklearn 7\n",
      "model_selection import GridSearchCV as original_grid_search\n",
      "digits = datasets 14\n",
      "load_digits()\n",
      "X, y = digits 8\n",
      "data, digits 3\n",
      "target\n",
      "From here, we can import our handy GridSearchCV  module to do some hyperparameter\n",
      "tuning:\n",
      "param_grid = {\"max_depth\": [3, None],\n",
      " \"max_features\": [1, 3, 10],\n",
      " \"min_samples_leaf\": [1, 3, 10],\n",
      " \"bootstrap\": [True, False],\n",
      " \"criterion\": [\"gini\", \"entropy\"],\n",
      " \"n_estimators\": [10, 100, 1000]}\n",
      "gs = original_grid_search(RandomForestClassifier(), param_grid=param_grid)\n",
      "gs 112\n",
      "fit(X, y)\n",
      "gs 6\n",
      "best_params_, gs 4\n",
      "best score\n",
      "({'bootstrap': False,\n",
      "'criterion': 'gini',\n",
      "'max_depth': None,\n",
      "'max_features': 3,\n",
      "'min_samples_leaf': 1,\n",
      "'n_estimators': 1000},\n",
      "0 45\n",
      "95436839176405119)\n",
      "Command took 24 12\n",
      "69 minutes\n",
      "\n",
      "The preceding code is a standard grid search that takes nearly 25 minutes to run 20\n",
      "As in our\n",
      "first case study, we could write a custom function to run this in parallel, but that would\n",
      "take a lot of custom code 30\n",
      "As in our second case study, we use could MLlib to write a\n",
      "scalable grid search using the MLlib standard models, but that would  take a while as well 36\n",
      "\n",
      "spark_sklearn  provides a third option 9\n",
      "We can import their GridSearchCV  and swap\n",
      "out our module for theirs to get extreme gains in speed with minimal code intervention:\n",
      "# the new gridsearch module\n",
      "from spark_sklearn import GridSearchCV as spark_grid_search\n",
      "# the only difference is passing in the SparkContext objecr as the first\n",
      "parameter of the grid search\n",
      "gs = spark_grid_search(sc, RandomForestClassifier(), param_grid=param_grid)\n",
      "gs 87\n",
      "fit(X, y)\n",
      "gs 6\n",
      "best_params_, gs 4\n",
      "bestscore\n",
      "({'bootstrap': False,\n",
      "'criterion': 'gini',\n",
      "'max_depth': None,\n",
      "'max_features': 3,\n",
      "'min_samples_leaf': 1,\n",
      "'n_estimators': 100},\n",
      "0 44\n",
      "95436839176405119)\n",
      "Command took  5 13\n",
      "29 minute\n",
      "By doing nothing more than importing a new grid search module and passing the\n",
      "SparkContext  variable into the new  module, we can get a 5x speed boost 38\n",
      "Always be on\n",
      "the lookout for third-party modules that can be used to enhance the already easy-to-use\n",
      "Azure Databricks environment 27\n",
      "\n",
      "More about spark_sklearn  can be found on their GitHub page: https:/\n",
      "/​github 21\n",
      "​com/ ​databricks/ ​spark- ​sklearn 14\n",
      "\n",
      "\n",
      "Summary\n",
      "Azure Databricks provides an environment that allows us to create scalable machine\n",
      "learning pipelines with ease 22\n",
      "By using the notebooks in Azure Databricks, we can run our\n",
      "data analytics and machine learning code in the cloud with powerful Azure resources in\n",
      "the backend 32\n",
      "By implementing MLlib, we can create scalable machine learning pipelines\n",
      "behind Apache Spark 17\n",
      "Utilizing third-party tools is a great way to bring everything together\n",
      "to build extremely powerful learning algorithms and train them much faster than on our\n",
      "local machines 31\n",
      "\n",
      "\n",
      "ther Books You May Enjoy\n",
      "If you enjoyed this book, you may be interested in these other books by Packt:\n",
      "Data Science Algorithms in a Week, Second Edition\n",
      "Dávid Natingga\n",
      "ISBN:  978-1-78980-607-6\n",
      "Understand how to identify a data science problem correctly\n",
      "Implement well-known machine learning algorithms efficiently using Python\n",
      "Classify your datasets using Naive Bayes, decision trees, and random forest with\n",
      "accuracy\n",
      "Devise an appropriate prediction solution using regression\n",
      "Work with time series data to identify relevant data events and trends\n",
      "Cluster your data using the k-means algorithm\n",
      "\n",
      "\n",
      "Practical Data Science Cookbook, Second Edition\n",
      "Tony Ojeda, Sean Patrick Murphy, Et al\n",
      "ISBN: 978-1-78712-962-7\n",
      "Learn and understand the installation procedure and environment required for R\n",
      "and Python on various platforms\n",
      "Prepare data for analysis by implement various data science concepts such as\n",
      "acquisition, cleaning and munging through R and Python\n",
      "Build a predictive model and an exploratory model\n",
      "Analyze the results of your model and create reports on the acquired data\n",
      "Build various tree-based methods and Build random forest\n",
      "\n",
      "Leave a review - let other readers know what\n",
      "you think\n",
      "Please share your thoughts on this book with others by leaving a review on the site that you\n",
      "bought it from 280\n",
      "If you purchased the book from Amazon, please leave us an honest review\n",
      "on this book's Amazon page 21\n",
      "This is vital so that other potential readers can see and use\n",
      "your unbiased opinion to make purchasing decisions, we can understand what our\n",
      "customers think about our products, and our authors can see your feedback on the title that\n",
      "they have worked with Packt to create 53\n",
      "It will only take a few minutes of your time, but is\n",
      "valuable to other potential customers, our authors, and Packt 27\n",
      "Thank you 2\n",
      "\n",
      "\n",
      "ndex\n",
      "A\n",
      "Adam Optimizer  374\n",
      "arithmetic mean  48, 144\n",
      "arithmetic symbols\n",
      "   about  78\n",
      "   dot product  79, 81\n",
      "   proportional  79\n",
      "   summation  78\n",
      "AutoRegressive Integrated Moving Average\n",
      "(ARIMA)  355\n",
      "average observation  44\n",
      "B\n",
      "back-propagation  332\n",
      "bar charts  193\n",
      "Bayes' formula  79\n",
      "Bayes' theorem  112, 113, 115\n",
      "   applications  117\n",
      "   example  117, 119\n",
      "Bayesian approach\n",
      "   about  97\n",
      "   versus Frequentist approach  97\n",
      "Bayesian\n",
      "   about  113\n",
      "bias/variance tradeoff\n",
      "   about  298\n",
      "   error functions  309, 310\n",
      "   errors, due to bias  298\n",
      "   errors, due to variance  298, 299\n",
      "   example  299, 300, 301, 302, 303, 304, 306,\n",
      "307\n",
      "   overfitting  308\n",
      "   underfitting  308\n",
      "big data  21\n",
      "binary classifier  110\n",
      "binomial random variable  127, 129Bootstrap aggregation (bagging)  323\n",
      "box plots  197, 199\n",
      "C\n",
      "Cartesian graph  82\n",
      "causation  201\n",
      "chi-square goodness of fit test\n",
      "   about  182\n",
      "   assumptions  182\n",
      "   example  183\n",
      "   for association/independence  185\n",
      "chi-square independence test\n",
      "   assumptions  185\n",
      "classification  219\n",
      "classification route  353, 354, 355\n",
      "classification tree\n",
      "   fitting  266, 268, 270, 271\n",
      "cluster validation\n",
      "   optimal number, selecting for  282\n",
      "clustering  221\n",
      "coefficient of variation  149\n",
      "collectively exhaustive\n",
      "   about  112\n",
      "   events  112\n",
      "comma separated value (CSV)  39\n",
      "communication matter  189\n",
      "compound events  101\n",
      "conditional probability  104\n",
      "confidence intervals  170, 172, 174\n",
      "confounding factor  142\n",
      "confusion matrices  110\n",
      "confusion matrix  110\n",
      "continuous random variable  133, 136\n",
      "correlation  201\n",
      "correlation coefficients  157\n",
      "correlation\n",
      "   causation, lacking  206\n",
      "\n",
      "   versus causation  201, 203\n",
      "cross-validation error\n",
      "   versus training error  318, 320\n",
      "D\n",
      "data levels\n",
      "   about  42\n",
      "   checking  46\n",
      "   interval level  47\n",
      "   nominal level  43\n",
      "   ordinal level  44\n",
      "   ratio level  51\n",
      "data pre-processing\n",
      "   example  34\n",
      "   special characters  35\n",
      "   text, relative length  36\n",
      "   topics, picking out  36\n",
      "   word/phrase counts  35\n",
      "data science, case studies\n",
      "   about  22\n",
      "   dollars, marketing  25\n",
      "   government paper pushing, automating  23\n",
      "   job description  27, 29\n",
      "   performance  24\n",
      "data science\n",
      "   about  9, 55\n",
      "   computer programming  12, 16\n",
      "   consideration  10\n",
      "   data mining  21\n",
      "   data, exploring  56\n",
      "   data, modeling  57\n",
      "   data, obtaining  56\n",
      "   domain knowledge  12, 20\n",
      "   exploratory data analysis (EDA)  21\n",
      "   machine learning  21\n",
      "   math  13\n",
      "   math/statistics  12\n",
      "   organized data  9\n",
      "   overview  56\n",
      "   Python  16\n",
      "   question  56\n",
      "   results, communicating  57\n",
      "   results, visualizing  57\n",
      "   terminology  9, 21\n",
      "   unorganized data  9   Venn diagram  12, 13\n",
      "   xyz123 Technologies  11\n",
      "data\n",
      "   about  42, 53\n",
      "   experimental  139\n",
      "   experimentation  139\n",
      "   exploring  57, 58\n",
      "   observational  139\n",
      "   obtaining  139\n",
      "   probability sampling  142\n",
      "   random sampling  142\n",
      "   sampling  139, 141\n",
      "   titanic dataset  68\n",
      "   types  32, 33\n",
      "   unequal probability sampling  143\n",
      "   yelp dataset  59\n",
      "DataFrame  61\n",
      "decision trees\n",
      "   about  264, 266\n",
      "   classification tree, fitting  266, 268, 270, 271\n",
      "   comparing, with random forests  329\n",
      "   regression tree, building  266\n",
      "Deep Neural Network Classifier  368\n",
      "dimension reduction  221\n",
      "discrete random variable\n",
      "   about  121, 124, 126\n",
      "   binomial random variables  127, 129\n",
      "   continuous random variable  133, 136\n",
      "   geometric random variable  129\n",
      "   poisson random variable  131\n",
      "   types  127\n",
      "DNNClassifier  368\n",
      "domain knowledge  20\n",
      "dummy variables  248, 249, 250, 251, 252\n",
      "E\n",
      "effective visualizations\n",
      "   identifying  189\n",
      "empirical rule  159, 160\n",
      "ensembling\n",
      "   about  320, 321, 322\n",
      "   random forests  323, 324, 325, 328\n",
      "error functions  309, 310\n",
      "Euler's number  241\n",
      "event  96\n",
      "\n",
      "exploratory data analysis (EDA)  338, 345, 349,\n",
      "350\n",
      "   about  339, 340, 341, 343\n",
      "   classification route  353, 354, 355\n",
      "   exploratory data analysis  339\n",
      "   regression route  351\n",
      "exponent  83, 84\n",
      "F\n",
      "false positive  181\n",
      "feature extraction  284, 286, 288, 291, 293, 295\n",
      "Frequentist approach\n",
      "   about  97, 98\n",
      "   law of large num  99\n",
      "   versus Bayesian approach  97\n",
      "G\n",
      "geometric mean  51\n",
      "geometric random variable  130\n",
      "gradient descent  365\n",
      "graphs  82, 201\n",
      "grid searching\n",
      "   about  315, 316, 317, 318\n",
      "   training error, versus cross-validation error  318,\n",
      "320\n",
      "H\n",
      "histograms  195\n",
      "hypothesis test  175\n",
      "   conducting  176\n",
      "   one sample t-tests  177\n",
      "hypothesis tests\n",
      "   chi-square goodness of fit test  182\n",
      "   for categorical variables  182\n",
      "   type I errors  181\n",
      "   type II errors  181\n",
      "I\n",
      "ineffective visualizations\n",
      "   identifying  189\n",
      "interval level\n",
      "   about  47\n",
      "   example  47\n",
      "   mathematical operations  48\n",
      "   measures of center  48   measures of variation  49\n",
      "J\n",
      "Jaccard measure  89\n",
      "K\n",
      "k folds cross-validation\n",
      "   about  310, 311, 313, 314\n",
      "   features  312\n",
      "k-fold cross validation  367\n",
      "k-means clustering\n",
      "   about  272, 274\n",
      "   beer, illustrative example  279, 281\n",
      "   data points, illustrative example  274, 276, 278\n",
      "K-Nearest Neighbors (KNN) algorithm  310\n",
      "K\n",
      "   optimal number, selecting for  282\n",
      "key performance indicator (KPI)  206\n",
      "L\n",
      "labeled data  215\n",
      "likelihood  256\n",
      "likert  44\n",
      "Likert scale  124\n",
      "line graphs  191\n",
      "linear algebra  82\n",
      "   about  90\n",
      "   matrix multiplication  90\n",
      "linear regression  226, 227, 228, 229, 230, 231\n",
      "   predictors, adding  231, 232, 233, 234\n",
      "   regression metrics  234, 235, 237, 239, 240\n",
      "log odds  242, 243, 244, 245\n",
      "logarithm  83, 84\n",
      "logistic regression  240, 241, 242\n",
      "   math  245, 247, 248\n",
      "M\n",
      "machine learning\n",
      "   about  15, 211\n",
      "   data  220\n",
      "   facial recognition  211\n",
      "   limitations  213\n",
      "   predictions  218\n",
      "   probabilistic model  21\n",
      "\n",
      "   reinforcement learning  225\n",
      "   statistical model  21\n",
      "   supervised learning  215\n",
      "   supervised learning, example  216, 217\n",
      "   supervised learning, types  219\n",
      "   supervised machine learning  224\n",
      "   types  214, 215\n",
      "   types, overview  224\n",
      "   unsupervised learning  221, 222\n",
      "   unsupervised machine learning  225\n",
      "   working  214\n",
      "math  74\n",
      "   about  14\n",
      "   example  14\n",
      "mathematics  74, 75\n",
      "   symbols  75\n",
      "   terminology  75\n",
      "matrices\n",
      "   about  75\n",
      "   answers  78\n",
      "   multiplication  91, 93, 94\n",
      "matrix  76\n",
      "matrix multiplication  90\n",
      "mean  48\n",
      "Mean Squared Error (MSE)  266\n",
      "measures of center  43\n",
      "measures of relative standing  151\n",
      "   correlations, in data  156\n",
      "measures of variation  49, 151\n",
      "measures of variation, interval level\n",
      "   standard deviation  49\n",
      "measures of variation, statistics\n",
      "   defining  149\n",
      "   example  150\n",
      "median  45, 145\n",
      "microsoft data science environment\n",
      "   about  375\n",
      "   Microsoft Azure  375\n",
      "   Microsoft Azure Machine Learning Studio  375\n",
      "   Microsoft Databricks  375\n",
      "Microsoft Databricks\n",
      "   about  376\n",
      "   bike-sharing usage prediction, parallelization in\n",
      "Databricks  378, 379, 380, 381, 382, 383,\n",
      "385, 386, 387, 389, 390, 391   cluster, setting up  377\n",
      "   clusters/workers  376\n",
      "   MLib  376\n",
      "   MLlib Grid Search module, used to tune\n",
      "hyperparameters  399, 402\n",
      "   MLlib, used in Databricks to predict credit card \n",
      "391, 392, 393, 394, 395, 397, 399\n",
      "   Notebooks  376\n",
      "   Python libraries, adding to cluster  403\n",
      "   reference link  376\n",
      "   Spark DataFrames  376\n",
      "   spark_sklearn, used to build MNIST classifier \n",
      "404, 405\n",
      "   used, to optimize hyperparameter tuning  403,\n",
      "404\n",
      "   using  376\n",
      "model coefficients  229\n",
      "models  13\n",
      "multilayer perceptron (MLP)  332\n",
      "mutually exhaustive  112\n",
      "N\n",
      "Naive Bayes\n",
      "   classification  255, 259, 262, 263\n",
      "Naïve Bayes algorithm  112\n",
      "neural networks\n",
      "   about  329\n",
      "   basic structure  330\n",
      "   for anomaly detection  330\n",
      "   for entity movement  330\n",
      "   for pattern recognition  330\n",
      "   structure  331, 332, 333, 335, 336\n",
      "nominal level  60\n",
      "   about  43\n",
      "   data  44\n",
      "   mathematical operations  43\n",
      "   measure of center  43\n",
      "normalizing constant  256\n",
      "notation  96\n",
      "null model  239\n",
      "O\n",
      "odds  242, 243, 244, 245\n",
      "one sample t-tests\n",
      "   about  177\n",
      "\n",
      "   assumptions  178, 180\n",
      "   example  178\n",
      "ordinal level  44, 60\n",
      "   examples  44\n",
      "   mathematical operations  44\n",
      "   measure of center  45\n",
      "out-of-sample (OOS) error  311\n",
      "overfitting  308\n",
      "P\n",
      "parameter  138\n",
      "perceptrons  330\n",
      "point estimates  162, 165, 166\n",
      "poisson random variable  131\n",
      "pre-processing  34\n",
      "prediction  217\n",
      "predictive analytics models  215\n",
      "presentation\n",
      "   strategy  208\n",
      "principal component analysis  284, 286, 288, 291,\n",
      "293, 295\n",
      "Principal Component Analysis (PCA)  288\n",
      "prior probability  256\n",
      "probability  96, 242, 243, 244, 245\n",
      "probability density function (PDF)  133\n",
      "probability mass function (PMF)  122, 127\n",
      "probability\n",
      "   addition rule  104\n",
      "   complementary events  108\n",
      "   event independence  108\n",
      "   multiplication rule  107\n",
      "   mutual exclusivity  106\n",
      "   rules  104\n",
      "procedure  95\n",
      "proper subset  88\n",
      "PySpark  376\n",
      "Python\n",
      "   about  16\n",
      "   example  18\n",
      "   practices  17\n",
      "   single tweet, parsing  19\n",
      "Q\n",
      "qualitative data\n",
      "   about  37   example  37, 39\n",
      "   exploration tips  62\n",
      "   filtering in pandas  64\n",
      "   nominal level columns  62\n",
      "   ordinal level columns  67\n",
      "   versus quantitative data  36\n",
      "quantitative data\n",
      "   about  37\n",
      "   continuous data  41\n",
      "   discrete data  41\n",
      "   example  37, 39\n",
      "   overview  41\n",
      "R\n",
      "random forests\n",
      "   about  323, 324, 325, 328\n",
      "   comparing, with decision trees  329\n",
      "random variable  120\n",
      "   discrete random variable  121, 124, 126\n",
      "random variables  112\n",
      "ratio level\n",
      "   about  51\n",
      "   example  51\n",
      "   measures of center  51\n",
      "   problems  52\n",
      "regression  219\n",
      "regression metrics  234, 235, 237, 239, 240\n",
      "regression route  351\n",
      "regression tree\n",
      "   building  266\n",
      "reinforcement learning  223, 224, 225\n",
      "relative frequency  98\n",
      "root-mean-square error (RMSE)  351\n",
      "S\n",
      "sample space  96\n",
      "sampling bias  142\n",
      "sampling distributions  167, 169\n",
      "scalar  80\n",
      "scatter plots  190\n",
      "set theory  86\n",
      "Silhouette Coefficient  282, 284\n",
      "simpson's paradox  204\n",
      "social media\n",
      "   exploratory data analysis (EDA)  340, 341, 343,\n",
      "\n",
      "45, 349, 350\n",
      "   survey  356, 357, 359, 360, 362, 363\n",
      "Spark  376\n",
      "spark_sklearn\n",
      "   reference link  405\n",
      "square matrix  77\n",
      "standard deviation  146\n",
      "standard normal distribution  133\n",
      "statistical modeling  226\n",
      "statistics  137, 201\n",
      "   measures of center  144\n",
      "   measures of relative standing  150, 153, 156\n",
      "   measures of variation  145, 148\n",
      "   measuring  144\n",
      "stock price\n",
      "   example  355\n",
      "stock prices\n",
      "   predicting, on social media  338\n",
      "   text sentiment analysis  338, 339\n",
      "structured data\n",
      "   about  33\n",
      "   versus unstructured data  33\n",
      "superset  87\n",
      "supervised learning  215\n",
      "supervised learning models  271\n",
      "supervised learning\n",
      "   classification  219\n",
      "   regression  219\n",
      "   types  219\n",
      "supervised machine learning  224\n",
      "T\n",
      "TensorFlow\n",
      "   about  368, 369, 370, 371, 372, 374\n",
      "   neural networks  368, 369, 370, 371, 372, 374   using  363, 364, 366, 367\n",
      "titanic dataset  68\n",
      "training error\n",
      "   versus cross-validation error  318, 320\n",
      "U\n",
      "underfitting  308\n",
      "unstructured data\n",
      "   about  33\n",
      "   versus structured data  33\n",
      "unsupervised learning  222\n",
      "   about  221, 271\n",
      "   reinforcement learning  223, 224\n",
      "   using  271\n",
      "unsupervised machine learning  225\n",
      "V\n",
      "vectors\n",
      "   about  75\n",
      "   answers  78\n",
      "   exercises  78\n",
      "verbal communication\n",
      "   about  206\n",
      "   data findings, presentation  207\n",
      "W\n",
      "World Health Organization (WHO)  39\n",
      "Y\n",
      "yelp dataset\n",
      "   about  59\n",
      "   DataFrame  61\n",
      "   qualitative data, exploration tips  62\n",
      "   series  62 3657\n",
      "476.69642857142856\n"
     ]
    }
   ],
   "source": [
    "split = overlapping_chunks(principles_of_ds, overlapping_factor=0)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77e5475-e2d6-40a4-b2f6-7e3e92ddef85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping chunking approach has 280 documents with average length 476.7 tokens\n"
     ]
    }
   ],
   "source": [
    "# 비중첩 청킹의 문서수와 평균 토큰 길이\n",
    "print(f'non-overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffefd5e8-2ebf-4db9-9ae2-5f2bcc917545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rinciples of Data Science\n",
      "Second Edition\n",
      "A beginner's guide to statistical techniques and theory to\n",
      "build eﬀective data-driven applications\n",
      "Sinan Ozdemir\n",
      "Sunil Kakade\n",
      "BIRMINGHAM - MUMBAI\n",
      "\n",
      "rinciples of Data Science\n",
      "Second Edition\n",
      "Copyright © 2018 Packt Publishing\n",
      "All rights reserved 74\n",
      "No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form\n",
      "or by any means, without the prior written permission of the publisher, except in the case of brief quotations\n",
      "embedded in critical articles or reviews 49\n",
      "\n",
      "Every effort has been made in the preparation of this book to ensure the accuracy of the information presented 20\n",
      "\n",
      "However, the information contained in this book is sold without warranty, either express or implied 18\n",
      "Neither the\n",
      "authors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to\n",
      "have been caused directly or indirectly by this book 36\n",
      "\n",
      "Packt Publishing has endeavored to provide trademark information about all of the companies and products\n",
      "mentioned in this book by the appropriate use of capitals 29\n",
      "However, Packt Publishing cannot guarantee the accuracy\n",
      "of this information 13\n",
      "\n",
      "Commissioning Editor:  Amey Varangoankar\n",
      "Acquisition Editor:  Dayne Castelino\n",
      "Content Development Editor:  Chris D'cruz\n",
      "Technical Editor:  Sneha Hanchate\n",
      "Copy Editor:  Safis Editing\n",
      "Project Coordinator:  Namrata Swetta\n",
      "Proofreader:  Safis Editing\n",
      "Indexer:  Pratik Shirodkar\n",
      "Graphics:  Tom Scaria\n",
      "Production Coordinator:  Nilesh Mohite\n",
      "First published: December 2016\n",
      "Second edition: November 2018\n",
      "Production reference: 1111218\n",
      "Published by Packt Publishing Ltd 129\n",
      "\n",
      "Livery Place\n",
      "35 Livery Street\n",
      "Birmingham\n",
      "B3 2PB, UK 20\n",
      "\n",
      "ISBN 978-1-78980-454-6\n",
      "www 15\n",
      "packtpub 3\n",
      "com\n",
      "\n",
      "\n",
      "mapt 4\n",
      "io\n",
      "Mapt is an online digital library that gives you full access to over 5,000 books and videos, as\n",
      "well as industry leading tools to help you plan your personal development and advance\n",
      "your career 43\n",
      "For more information, please visit our website 8\n",
      "\n",
      "Why subscribe 3\n",
      "\n",
      "Spend less time learning and more time coding with practical eBooks and Videos\n",
      "from over 4,000 industry professionals\n",
      "Improve your learning with Skill Plans built especially for you\n",
      "Get a free eBook or video every month\n",
      "Mapt is fully searchable\n",
      "Copy and paste, print, and bookmark content\n",
      "Packt 64\n",
      "com\n",
      "Did you know that Packt offers eBook versions of every book published, with PDF and\n",
      "ePub files available 24\n",
      "You can upgrade to the eBook version at www 9\n",
      "packt 2\n",
      "com  and as a print\n",
      "book customer, you are entitled to a discount on the eBook copy 20\n",
      "Get in touch with us at\n",
      "customercare@packtpub 14\n",
      "com  for more details 5\n",
      "\n",
      "At www 3\n",
      "packt 2\n",
      "com , you can also read a collection of free technical articles, sign up for a\n",
      "range of free newsletters, and receive exclusive discounts and offers on Packt books and\n",
      "eBooks 37\n",
      "\n",
      "\n",
      "ontributors\n",
      "About the authors\n",
      "Sinan Ozdemir  is a data scientist, start-up founder, and educator living in the San\n",
      "Francisco Bay Area 35\n",
      "He studied pure mathematics at Johns Hopkins University 8\n",
      "He then\n",
      "spent several years conducting lectures on data science at Johns Hopkins University before\n",
      "founding his own start-up, Kylie 25\n",
      "ai, which uses artificial intelligence to clone brand\n",
      "personalities and automate customer service communications 17\n",
      "\n",
      "Sinan is also the author of Principles of Data Science, First Edition  available through Packt 20\n",
      "\n",
      "Sunil Kakade  is a technologist, educator, and senior leader with expertise in creating data-\n",
      "and AI-driven organizations 26\n",
      "He is in the adjunct faculty at Northwestern University,\n",
      "Evanston, IL, where he teaches graduate courses of data science and big data 27\n",
      "He has\n",
      "several research papers to his credit and has presented his work in big data applications at\n",
      "reputable conferences 25\n",
      "He has US patents in areas of big data and retail processes 12\n",
      "He is\n",
      "passionate about applying data science to improve business outcomes and save patients'\n",
      "lives 19\n",
      "At present, Sunil leads the information architecture and analytics team for a large\n",
      "healthcare organization focused on improving healthcare outcomes and lives with his wife,\n",
      "Pratibha, and daughter, Preeti, in Scottsdale, Arizona 47\n",
      "\n",
      "I would like to thank my mother, Subhadra, wife; Pratibha; and daughter, Preeti, for\n",
      "supporting me during my education and career and for supporting my passion for\n",
      "learning 45\n",
      "Many thanks to my mentors, Prof 7\n",
      "Faisal Akkawi, Northwestern University; Bill\n",
      "Guise, Sr 15\n",
      "Director, Dr 3\n",
      "Joseph Colorafi, CMIO, and Deanna Wise, CIO at Dignity\n",
      "Health for supporting my passion for big data, data science, and artificial intelligence 34\n",
      "\n",
      "Special thanks to Sinan Ozdemir and Packt Publishing for giving me the opportunity to\n",
      "co-author this book 24\n",
      "I appreciate the incredible support of my team at Dignity Health\n",
      "Insights in my journey in data science 22\n",
      "Finally, I'd like to thank my friend, Anand\n",
      "Deshpande, who inspired me to take on this project 26\n",
      "\n",
      "\n",
      "bout the reviewers\n",
      "Oleg Okun  got his PhD from the Institute of Engineering Cybernetics, National Academy\n",
      "of Sciences (Minsk, Belarus) in 1996 37\n",
      "Since 1998 he has worked abroad, doing both academic\n",
      "research (in Belarus and Finland) and industrial research (in Sweden and Germany) 29\n",
      "His\n",
      "research experience includes document image analysis, cancer prediction by analyzing\n",
      "gene expression profiles (bioinformatics), fingerprint verification and identification\n",
      "(biometrics), online and offline marketing analytics, credit scoring (microfinance), and text\n",
      "search and summarization (natural language processing) 54\n",
      "He has 80 publications, including\n",
      "one IGI Global-published book and three co-edited books published by Springer-Verlag, as\n",
      "well as book chapters, journal articles, and numerous conference papers 42\n",
      "He has also been a\n",
      "reviewer of several books published by Packt Publishing 16\n",
      "\n",
      "Jared James Thompson , PhD, is a graduate of Purdue University and has held both\n",
      "academic and industrial appointments teaching programming, algorithms, and big data\n",
      "technology 33\n",
      "He is a machine learning enthusiast and has a particular love of optimization 13\n",
      "\n",
      "Jared is currently employed as a machine learning engineer at Atomwise, a start-up that\n",
      "leverages artificial intelligence to design better drugs faster 29\n",
      "\n",
      "Packt is searching for authors like you\n",
      "If you're interested in becoming an author for Packt, please visit authors 25\n",
      "packtpub 3\n",
      "com\n",
      "and apply today 5\n",
      "We have worked with thousands of developers and tech professionals,\n",
      "just like you, to help them share their insight with the global tech community 26\n",
      "You can\n",
      "make a general application, apply for a specific hot topic that we are recruiting an author\n",
      "for, or submit your own idea 28\n",
      "\n",
      "\n",
      "able of Contents\n",
      "Preface 1\n",
      "Chapter 1: How to Sound Like a Data Scientist 6\n",
      "What is data science 28\n",
      "9\n",
      "Basic terminology 9\n",
      "Why data science 11\n",
      "10\n",
      "Example – xyz123 Technologies 11\n",
      "The data science Venn diagram 12\n",
      "The math 13\n",
      "Example – spawner-recruit models 14\n",
      "Computer programming 16\n",
      "Why Python 42\n",
      "16\n",
      "Python practices 17\n",
      "Example of basic Python 18\n",
      "Example – parsing a single tweet 19\n",
      "Domain knowledge 20\n",
      "Some more terminology 21\n",
      "Data science case studies 22\n",
      "Case study – automating government paper pushing 23\n",
      "Fire all humans, right 58\n",
      "24\n",
      "Case study – marketing dollars 25\n",
      "Case study – what's in a job description 20\n",
      "27\n",
      "Summary 30\n",
      "Chapter 2: Types of Data 32\n",
      "Flavors of data 32\n",
      "Why look at these distinctions 29\n",
      "33\n",
      "Structured versus unstructured data 33\n",
      "Example of data pre-processing 34\n",
      "Word/phrase counts 35\n",
      "Presence of certain special characters 35\n",
      "The relative length of text 35\n",
      "Picking out topics 36\n",
      "Quantitative versus qualitative data 36\n",
      "Example – coffee shop data 37\n",
      "Example – world alcohol consumption data 39\n",
      "Digging deeper 41\n",
      "The road thus far 42\n",
      "The four levels of data 42\n",
      "The nominal level 43\n",
      "Mathematical operations allowed 43\n",
      "\n",
      "Measures of center 43\n",
      "What data is like at the nominal level 44\n",
      "The ordinal level 44\n",
      "Examples 44\n",
      "Mathematical operations allowed 44\n",
      "Measures of center 45\n",
      "Quick recap and check 46\n",
      "The interval level 47\n",
      "Example 47\n",
      "Mathematical operations allowed 48\n",
      "Measures of center 48\n",
      "Measures of variation 49\n",
      "Standard deviation 49\n",
      "The ratio level 51\n",
      "Examples 51\n",
      "Measures of center 51\n",
      "Problems with the ratio level 52\n",
      "Data is in the eye of the beholder 53\n",
      "Summary 53\n",
      "Chapter 3: The Five Steps of Data Science 55\n",
      "Introduction to data science 55\n",
      "Overview of the five steps 56\n",
      "Asking an interesting question 56\n",
      "Obtaining the data 56\n",
      "Exploring the data 56\n",
      "Modeling the data 57\n",
      "Communicating and visualizing the results 57\n",
      "Exploring the data 57\n",
      "Basic questions for data exploration 58\n",
      "Dataset 1 – Yelp 59\n",
      "DataFrames 61\n",
      "Series 62\n",
      "Exploration tips for qualitative data 62\n",
      "Nominal level columns 62\n",
      "Filtering in pandas 64\n",
      "Ordinal level columns 67\n",
      "Dataset 2 – Titanic 68\n",
      "Summary 73\n",
      "Chapter 4: Basic Mathematics 74\n",
      "Mathematics as a discipline 74\n",
      "Basic symbols and terminology 75\n",
      "Vectors and matrices 75\n",
      "Quick exercises 78\n",
      "Answers 78\n",
      "Arithmetic symbols 78\n",
      "Summation 78\n",
      "\n",
      "Proportional 79\n",
      "Dot product 79\n",
      "Graphs 82\n",
      "Logarithms/exponents 83\n",
      "Set theory 86\n",
      "Linear algebra 90\n",
      "Matrix multiplication 90\n",
      "How to multiply matrices 91\n",
      "Summary 94\n",
      "Chapter 5: Impossible or Improbable – A Gentle Introduction to\n",
      "Probability 95\n",
      "Basic definitions 95\n",
      "Probability 96\n",
      "Bayesian versus Frequentist 97\n",
      "Frequentist approach 98\n",
      "The law of large numbers 99\n",
      "Compound events 101\n",
      "Conditional probability 104\n",
      "The rules of probability 104\n",
      "The addition rule 104\n",
      "Mutual exclusivity 106\n",
      "The multiplication rule 107\n",
      "Independence 108\n",
      "Complementary events 108\n",
      "A bit deeper 110\n",
      "Summary 111\n",
      "Chapter 6: Advanced Probability 112\n",
      "Collectively exhaustive events 112\n",
      "Bayesian ideas revisited 113\n",
      "Bayes' theorem 113\n",
      "More applications of Bayes' theorem 117\n",
      "Example – Titanic 117\n",
      "Example  – medical studies 119\n",
      "Random variables 120\n",
      "Discrete random variables 121\n",
      "Types of discrete random variables 127\n",
      "Binomial random variables 127\n",
      "Geometric random variables 129\n",
      "Poisson random variable 131\n",
      "Continuous random variables 133\n",
      "Summary 136\n",
      "Chapter 7: Basic Statistics 137\n",
      "What are statistics 708\n",
      "137\n",
      "How do we obtain and sample data 10\n",
      "139\n",
      "\n",
      "Obtaining data 139\n",
      "Observational 139\n",
      "Experimental 139\n",
      "Sampling data 141\n",
      "Probability sampling 142\n",
      "Random sampling 142\n",
      "Unequal probability sampling 143\n",
      "How do we measure statistics 46\n",
      "144\n",
      "Measures of center 144\n",
      "Measures of variation 145\n",
      "Definition 149\n",
      "Example – employee salaries 150\n",
      "Measures of relative standing 150\n",
      "The insightful part – correlations in data 156\n",
      "The empirical rule 159\n",
      "Summary 161\n",
      "Chapter 8: Advanced Statistics 162\n",
      "Point estimates 162\n",
      "Sampling distributions 167\n",
      "Confidence intervals 170\n",
      "Hypothesis tests 175\n",
      "Conducting a hypothesis test 176\n",
      "One sample t-tests 177\n",
      "Example of a one-sample t-test 178\n",
      "Assumptions of the one-sample t-test 178\n",
      "Type I and type II errors 181\n",
      "Hypothesis testing for categorical variables 182\n",
      "Chi-square goodness of fit test 182\n",
      "Assumptions of the chi-square goodness of fit test 182\n",
      "Example of a chi-square test for goodness of fit 183\n",
      "Chi-square test for association/independence 185\n",
      "Assumptions of the chi-square independence test 185\n",
      "Summary 187\n",
      "Chapter 9: Communicating Data 188\n",
      "Why does communication matter 221\n",
      "189\n",
      "Identifying effective and ineffective visualizations 189\n",
      "Scatter plots 190\n",
      "Line graphs 191\n",
      "Bar charts 193\n",
      "Histograms 195\n",
      "Box plots 197\n",
      "When graphs and statistics lie 201\n",
      "Correlation versus causation 201\n",
      "Simpson's paradox 204\n",
      "If correlation doesn't imply causation, then what does 74\n",
      "206\n",
      "\n",
      "Verbal communication 206\n",
      "It's about telling a story 206\n",
      "On the more formal side of things 207\n",
      "The why/how/what strategy of presenting 208\n",
      "Summary 208\n",
      "Chapter 10: How to Tell If Your Toaster Is Learning – Machine Learning\n",
      "Essentials 210\n",
      "What is machine learning 69\n",
      "211\n",
      "Example – facial recognition 211\n",
      "Machine learning isn't perfect 213\n",
      "How does machine learning work 23\n",
      "214\n",
      "Types of machine learning 214\n",
      "Supervised learning 215\n",
      "Example – heart attack prediction 216\n",
      "It's not only about predictions 218\n",
      "Types of supervised learning 219\n",
      "Regression 219\n",
      "Classification 219\n",
      "Data is in the eyes of the beholder 220\n",
      "Unsupervised learning 221\n",
      "Reinforcement learning 223\n",
      "Overview of the types of machine learning 224\n",
      "How does statistical modeling fit into all of this 93\n",
      "226\n",
      "Linear regression 226\n",
      "Adding more predictors 231\n",
      "Regression metrics 234\n",
      "Logistic regression 240\n",
      "Probability, odds, and log odds 242\n",
      "The math of logistic regression 245\n",
      "Dummy variables 248\n",
      "Summary 253\n",
      "Chapter 11: Predictions Don't Grow on Trees – or Do They 67\n",
      "255\n",
      "Naive Bayes classification 255\n",
      "Decision trees 264\n",
      "How does a computer build a regression tree 24\n",
      "266\n",
      "How does a computer fit a classification tree 11\n",
      "266\n",
      "Unsupervised learning 271\n",
      "When to use unsupervised learning 271\n",
      "k-means clustering 272\n",
      "Illustrative example – data points 274\n",
      "Illustrative example – beer 41\n",
      "279\n",
      "Choosing an optimal number for K and cluster validation 282\n",
      "The Silhouette Coefficient 282\n",
      "Feature extraction and principal component analysis 284\n",
      "\n",
      "Summary 296\n",
      "Chapter 12: Beyond the Essentials 297\n",
      "The bias/variance tradeoff 298\n",
      "Errors due to bias 298\n",
      "Error due to variance 298\n",
      "Example – comparing body and brain weight of mammals 299\n",
      "Two extreme cases of bias/variance tradeoff 308\n",
      "Underfitting 308\n",
      "Overfitting 308\n",
      "How bias/variance play into error functions 309\n",
      "K folds cross-validation 310\n",
      "Grid searching 315\n",
      "Visualizing training error versus cross-validation error 318\n",
      "Ensembling techniques 320\n",
      "Random forests 323\n",
      "Comparing random forests with decision trees 329\n",
      "Neural networks 329\n",
      "Basic structure 330\n",
      "Summary 337\n",
      "Chapter 13: Case Studies 338\n",
      "Case study 1 – Predicting stock prices based on social media 338\n",
      "Text sentiment analysis 338\n",
      "Exploratory data analysis 339\n",
      "Regression route 351\n",
      "Classification route 353\n",
      "Going beyond with this example 355\n",
      "Case study 2 – Why do some people cheat on their spouses 245\n",
      "356\n",
      "Case study 3 – Using TensorFlow 363\n",
      "TensorFlow and neural networks 368\n",
      "Summary 374\n",
      "Chapter 14: Microsoft Azure Databricks 375\n",
      "The Microsoft data science environment 375\n",
      "What exactly are Spark and PySpark 52\n",
      "376\n",
      "Basic Azure Databricks use 376\n",
      "Setting up our first cluster 377\n",
      "Case study 1 – bike-sharing usage prediction using parallelization in Azure\n",
      "Databricks 378\n",
      "Case study 2 – Using MLlib in Azure Databricks to predict credit card fraud 391\n",
      "Using the MLlib Grid Search module to tune hyperparameters 399\n",
      "Case study 3 – Using Azure Databricks to optimize our hyperparameter\n",
      "tuning 403\n",
      "How to add Python libraries to your cluster 403\n",
      "Using spark_sklearn to build an MNIST classifier 404\n",
      "Summary 406\n",
      "\n",
      "Other Books You May Enjoy 407\n",
      "Index 410\n",
      "\n",
      "reface\n",
      "The topic of this book is data science, which is a field of study that has been growing\n",
      "rapidly for the past few decades 167\n",
      "Today, more companies than ever before are investing in\n",
      "big data and data science to improve business performance, drive innovation, and create\n",
      "new revenue streams by building data products 34\n",
      "According to LinkedIn's 2017 US\n",
      "Emerging Jobs Report, machine learning engineer, data scientist, and big data engineer\n",
      "rank among the top emerging jobs, and companies in a wide range of industries are seeking\n",
      "people with the requisite skills for those roles 52\n",
      "\n",
      "We will dive into topics from all three areas  and solve complex problems 15\n",
      "We will clean,\n",
      "explore, and analyze data in order to derive scientific and accurate conclusions 18\n",
      "Machine\n",
      "learning and deep learning techniques will be applied to solve complex data tasks 15\n",
      "\n",
      "Who this book is for\n",
      "This book is for people who are looking to understand and utilize the basic practices of data\n",
      "science for any domain 29\n",
      "The reader should be fairly well acquainted with basic mathematics\n",
      "(algebra, and perhaps probability) and should feel comfortable reading snippets in\n",
      "R/Python as well as pseudo code 34\n",
      "The reader is not expected to have worked in a data field;\n",
      "however, they should have the urge to learn and apply the techniques put forth in this book\n",
      "to either their own datasets or those provided to them 42\n",
      "\n",
      "What this book covers\n",
      "Chapter 1 , How to Sound Like a Data Scientist , introduces the basic terminology used by data\n",
      "scientists and looks at the types of problem we will be solving throughout this book 42\n",
      "\n",
      "Chapter 2 , Types of Data , looks at the different levels and types of data out there and shows\n",
      "how to manipulate each type 28\n",
      "This chapter will begin to deal with the mathematics needed\n",
      "for data science 14\n",
      "\n",
      "Chapter 3 , The Five Steps of Data Science , uncovers the five basic steps of performing data\n",
      "science, including data manipulation and cleaning, and shows examples of each step in\n",
      "detail 39\n",
      "\n",
      "\n",
      "Chapter 4 , Basic Mathematics , explains the basic mathematical principles that guide the\n",
      "actions of data scientists by presenting and solving examples in calculus, linear algebra, and\n",
      "more 35\n",
      "\n",
      "Chapter 5 , Impossible or Improbable – a Gentle Introduction to Probability , is a beginner's guide\n",
      "to probability theory and how it is used to gain an understanding of our random universe 39\n",
      "\n",
      "Chapter 6 , Advanced Probability , uses principles from the previous chapter and introduces\n",
      "and applies theorems, such as Bayes' Theorem, in the hope of uncovering the hidden\n",
      "meaning in our world 44\n",
      "\n",
      "Chapter 7 , Basic Statistics , deals with the types of problem that statistical inference attempts\n",
      "to explain, using the basics of experimentation, normalization, and random sampling 33\n",
      "\n",
      "Chapter 8 , Advanced Statistics , uses hypothesis testing and confidence intervals to gain\n",
      "insight from our experiments 22\n",
      "Being able to pick which test is appropriate and how to\n",
      "interpret p-values and other results is very important as well 23\n",
      "\n",
      "Chapter 9 , Communicating Data , explains how correlation and causation affect our\n",
      "interpretation of data 22\n",
      "We will also be using visualizations in order to share our results with\n",
      "the world 17\n",
      "\n",
      "Chapter 10 , How to Tell Whether Your Toaster Is Learning – Machine Learning Essentials ,\n",
      "focuses on the definition of machine learning and looks at real-life examples of how and\n",
      "when machine learning is applied 42\n",
      "A basic understanding of the relevance of model\n",
      "evaluation is introduced 12\n",
      "\n",
      "Chapter 11 , Predictions Don't Grow on Trees, or Do They 16\n",
      ", looks at more complicated\n",
      "machine learning models, such as decision trees and Bayesian predictions, in order to solve\n",
      "more complex data-related tasks 28\n",
      "\n",
      "Chapter 12 , Beyond the Essentials , introduces some of the mysterious forces guiding data\n",
      "science, including bias and variance 24\n",
      "Neural networks are introduced as a modern deep\n",
      "learning technique 11\n",
      "\n",
      "Chapter 13 , Case Studies , uses an array of case studies in order to solidify the ideas of data\n",
      "science 25\n",
      "We will be following the entire data science workflow from start to finish multiple\n",
      "times for different examples, including stock price prediction and handwriting detection 27\n",
      "\n",
      "Chapter 14 , Microsoft Databricks Case Studies , will harness the power of the Microsoft data\n",
      "environment as well as Apache Spark to put our machine learning in high gear 35\n",
      "This\n",
      "chapter makes use of parallelization and advanced visualization software to get the most\n",
      "out of our data 21\n",
      "\n",
      "\n",
      "To get the most out of this book\n",
      "This book will attempt to bridge the gap between math, programming, and domain\n",
      "expertise 28\n",
      "Most people today have expertise in at least one of these (maybe two), but proper\n",
      "data science requires a little bit of all three 27\n",
      "\n",
      "Download the example code files\n",
      "You can download the example code files for this book from your account at\n",
      "www 23\n",
      "packt 2\n",
      "com 1\n",
      "If you purchased this book elsewhere, you can visit\n",
      "www 12\n",
      "packt 2\n",
      "com/support  and register to have the files emailed directly to you 13\n",
      "\n",
      "You can download the code files by following these steps:\n",
      "Log in or register at www 18\n",
      "packt 2\n",
      "com 1\n",
      "1 2\n",
      "\n",
      "Select the SUPPORT  tab 6\n",
      "2 2\n",
      "\n",
      "Click on Code Downloads & Errata 8\n",
      "3 2\n",
      "\n",
      "Enter the name of the book in the Search  box and follow the onscreen 4 19\n",
      "\n",
      "instructions 2\n",
      "\n",
      "Once the file is downloaded, please make sure that you unzip or extract the folder using the\n",
      "latest version of:\n",
      "WinRAR/7-Zip for Windows\n",
      "Zipeg/iZip/UnRarX for Mac\n",
      "7-Zip/PeaZip for Linux\n",
      "The code bundle for the book is also hosted on GitHub\n",
      "at https://github 71\n",
      "com/PacktPublishing/ Principles-of-Data-Science-Second-\n",
      "Edition 17\n",
      "In case there's an update to the code, it will be updated on the existing GitHub\n",
      "repository 20\n",
      "\n",
      "We also have other code bundles from our rich catalog of books and videos available\n",
      "at https:/ ​/​github 24\n",
      "​com/ ​PacktPublishing/ ​ 10\n",
      "Check them out 3\n",
      "\n",
      "Download the color images\n",
      "We also provide a PDF file that has color images of the screenshots/diagrams used in this\n",
      "book 27\n",
      "You can download it here: https:/ ​/​www 12\n",
      "​packtpub 4\n",
      "​com/ ​sites/ ​default/ ​files/\n",
      "downloads/ ​9781789804546_ ​ColorImages 24\n",
      "​pdf 2\n",
      "\n",
      "\n",
      "Conventions used\n",
      "There are a number of text conventions used throughout this book 16\n",
      "\n",
      "CodeInText : Indicates c ode words in text, database table names, folder names, filenames,\n",
      "file extensions, pathnames, dummy URLs, user input, and Twitter handles 36\n",
      "Here is an\n",
      "example:  \"Mount the downloaded WebStorm-10* 16\n",
      "dmg  disk image file as another disk in\n",
      "your system 12\n",
      "\n",
      "A block of code is set as follows:\n",
      "dict = {\"dog\": \"human's best friend\", \"cat\": \"destroyer of world\"}\n",
      "dict[\"dog\"]# == \"human's best friend\"\n",
      "len(dict[\"cat\"]) # == 18\n",
      "# but if we try to create a pair with the same key as an existing key\n",
      "dict[\"dog\"] = \"Arf\"\n",
      "When we wish to draw your attention to a particular part of a code block, the relevant lines\n",
      "or items are set in bold:\n",
      "def jaccard(user1, user2):\n",
      "  stores_in_common = len(user1 & user2)\n",
      "  stores_all_together = len(user1 | user2)\n",
      "  return stores / float(stores_all_together)\n",
      "Any command-line input or output is written as follows:\n",
      "import numpy as np\n",
      "Bold : Indicates a new term, an important word, or w ords that you see onscreen 188\n",
      "For\n",
      "example, words in menus or dialog boxes appear in the text like this 16\n",
      "Here is an example:\n",
      "\"Select System info  from the Administration  panel 15\n",
      "\"\n",
      "Warnings or important notes appear like this 8\n",
      "\n",
      "Tips and tricks appear like this 7\n",
      "\n",
      "\n",
      "Get in touch\n",
      "Feedback from our readers is always welcome 12\n",
      "\n",
      "General feedback : If you have questions about any aspect of this book, mention the book\n",
      "title in the subject of your message and  email us at customercare@packtpub 38\n",
      "com 1\n",
      "\n",
      "Errata : Although we have taken every care to ensure the accuracy of our content, mistakes\n",
      "do happen 22\n",
      "If you have found a mistake in this book, we would be grateful if you would\n",
      "report this to us 22\n",
      "Please visit www 3\n",
      "packt 2\n",
      "com/submit-errata , selecting your book, clicking\n",
      "on the Errata Submission Form link, and entering the details 25\n",
      "\n",
      "Piracy : If you come across any illegal copies of our works in any form on the Internet, we\n",
      "would be grateful if you would provide us with the location address or website name 39\n",
      "\n",
      "Please contact us at copyright@packt 9\n",
      "com  with a link to the material 8\n",
      "\n",
      "If you are interested in becoming an author : If there is a topic that you have expertise in\n",
      "and you are interested in either writing or contributing to a book, please visit\n",
      "authors 38\n",
      "packtpub 3\n",
      "com 1\n",
      "\n",
      "Reviews\n",
      "Please leave a review 7\n",
      "Once you have read and used this book, why not leave a review on\n",
      "the site that you purchased it from 23\n",
      "Potential readers can then see and use your unbiased\n",
      "opinion to make purchase decisions, we at Packt can understand what you think about our\n",
      "products, and our authors can see your feedback on their book 41\n",
      "Thank you 2\n",
      "\n",
      "For more information about Packt, please visit packt 12\n",
      "com 1\n",
      "\n",
      "\n",
      "\n",
      "How to Sound Like a Data\n",
      "Scientist\n",
      "No matter which industry you work in —IT, fashion, food, or finance —there is no doubt\n",
      "that data affects your life and work 40\n",
      "At some point this week, you will either have or hear a\n",
      "conversation about data 17\n",
      "News outlets are covering more and more stories about data leaks,\n",
      "cybercrimes, and how data can give us a glimpse into our lives 28\n",
      "But why now 3\n",
      "What makes\n",
      "this era such a hotbed of data-related industries 13\n",
      "\n",
      "In the nineteenth century, the world was in the grip of the I ndustrial Age 18\n",
      "Mankind was\n",
      "exploring its place in the industrial world, working with giant mechanical inventions 18\n",
      "\n",
      "Captains of industry, such as Henry Ford, recognized that using these machines could open\n",
      "major market opportunities, enabling industries to achieve previously unimaginable\n",
      "profits 32\n",
      "Of course, the Industrial Age had its pros and cons 11\n",
      "While mass production placed\n",
      "goods in the hands of more consumers, our battle with pollution also began at around this\n",
      "time 24\n",
      "\n",
      "By the twentie th century, we were quite skilled at making huge machines; the goal now\n",
      "was to make them smaller and faster 29\n",
      "The Industrial Age was over and was replaced by\n",
      "what we now refer to as the I nformation Age 21\n",
      "We started using machines to gather and\n",
      "store information (data) about ourselves and our environment for the purpose of\n",
      "understanding our universe 27\n",
      "\n",
      "\n",
      "Beginning in the 1940s, machines such as ENIAC  (considered one of the first —if not the\n",
      "first—computers) were computing math equations and running models and simulations\n",
      "like never before 46\n",
      "The following photograph shows ENIAC:\n",
      "ENIAC —The world's ﬁrst electronic digital computer (Ref: http://ftp 27\n",
      "arl 2\n",
      "mil/ftp/historic-computers/)\n",
      "We finally had a decent lab assistant who could run the numbers better than we could 26\n",
      "As\n",
      "with the Industrial Age, the Information Age brought us both the good and the bad 18\n",
      "The\n",
      "good was the extraordinary works of technology, including mobile phones and televisions 16\n",
      "\n",
      "The bad was not as bad as worldwide pollution, but still left us with a problem in the\n",
      "twenty-fir st century —so much data 30\n",
      "\n",
      "\n",
      "That's right —the Information Age, in its quest to procure data, has exploded the production\n",
      "of electronic data 24\n",
      "Estimates show that we created about 1 8\n",
      "8 trillion gigabytes of data in\n",
      "2011 (take a moment to just think about how much that is) 24\n",
      "Just one year later, in 2012, we\n",
      "created over 2 16\n",
      "8 trillion gigabytes of data 7\n",
      "This number is only going to explode further to\n",
      "hit an estimated 40 trillion gigabytes of created data in just one year by 2020 29\n",
      "People\n",
      "contribute to this every time they tweet, post on Facebook, save a new resume on Microsoft\n",
      "Word, or just send their mom a picture by text message 34\n",
      "\n",
      "Not only are we creating data at an unprecedented rate, but we are also consuming it at an\n",
      "accelerated pace as well 27\n",
      "Just five years ago, in 2013, the average cell phone user used under\n",
      "1 GB of data a month 24\n",
      "Today, that number is estimated to be well over 2 GB a month 15\n",
      "We\n",
      "aren't just looking for the next personality quiz —what we are looking for is insight 19\n",
      "With all\n",
      "of this data out there, some of it has to be useful to me 18\n",
      "And it can be 4\n",
      "\n",
      "So we, in the twenty-fir st century, are left with a problem 17\n",
      "We have so much data and we\n",
      "keep making more 11\n",
      "We have built insanely tiny machines that collect data 24/7, and it's our\n",
      "job to make sense of it all 26\n",
      "Enter the D ata Age 5\n",
      "This is the age when we take machines\n",
      "dreamed up by our nineteenth century ancestors and the data created by our twentieth\n",
      "century counterparts and create insights and sources of knowledge that every human on\n",
      "Earth can benefit from 44\n",
      "The United States created an entirely new role in the government of\n",
      "chief data scientist 16\n",
      "Many companies are now investing in data science departments and\n",
      "hiring data scientists 15\n",
      "The benefit is quite obvious —using data to make accurate predictions\n",
      "and simulations gives us insight into our world like never before 24\n",
      "\n",
      "Sounds great, but what's the catch 9\n",
      "\n",
      "This chapter will explore the terminology and vocabulary of the modern data scientist 14\n",
      "We\n",
      "will learn keywords and phrases that will be essential in our discussion of data science\n",
      "throughout this book 22\n",
      "We will also learn why we use data science and learn about the three\n",
      "key domains that data science is derived from before we begin to look at the code in\n",
      "Python, the primary language used in this book 42\n",
      "This chapter will cover the following\n",
      "topics:\n",
      "The basic terminology of data science\n",
      "The three domains of data science\n",
      "The basic Python syntax\n",
      "\n",
      "What is data science 32\n",
      "\n",
      "Before we go any further, let's look at some  basic definitions that we will use throughout\n",
      "this book 23\n",
      "The great/awful thing about this field is that it is so young that these definitions\n",
      "can differ from textbook to newspaper to whitepaper 28\n",
      "\n",
      "Basic terminology\n",
      "The definitions that follow are general  enough to be used in daily conversations, and work\n",
      "to serve the purpose of this book, an introduction to the principles of data science 38\n",
      "\n",
      "Let's start by defining what data is 9\n",
      "This might seem like a silly first definition to look at,\n",
      "but it is very important 17\n",
      "Whenever we use the word \"data,\" we refer to a collection of\n",
      "information in either an organized  or unorganized  format 26\n",
      "These formats have the\n",
      "following qualities:\n",
      "Organized data : This refers to data  that is sorted into a row/column structure,\n",
      "where every row represents a single observation  and the columns represent the\n",
      "characteristics  of that observation 46\n",
      "\n",
      "Unorganized data : This is the type of data  that is in a free form, usually text or\n",
      "raw audio/signals that must be parsed further to become organized 35\n",
      "\n",
      "Whenever you open Excel (or any other spreadsheet program), you are looking at a blank\n",
      "row/column structure waiting for organized data 26\n",
      "These programs don't do well with\n",
      "unorganized data 11\n",
      "For the most part, we will deal with organized data as it is the easiest to\n",
      "glean insights from, but we will not shy away from looking at raw text and methods of\n",
      "processing unorganized forms of data 44\n",
      "\n",
      "Data science is the art and science of acquiring knowledge through data 13\n",
      "\n",
      "What a small definition for such a big topic, and rightfully so 14\n",
      "Data science covers so many\n",
      "things that it would take pages to list it all out (I should know —I tried and got told to edit it\n",
      "down) 33\n",
      "\n",
      "Data science is all about how we take data, use it to acquire knowledge, and then use that\n",
      "knowledge to do the following:\n",
      "Make decisions\n",
      "Predict the future\n",
      "Understand the past/present\n",
      "Create new industries/products\n",
      "\n",
      "This book is all about the methods of data science, including how to process data, gather\n",
      "insights, and use those insights to make informed decisions and predictions 79\n",
      "\n",
      "Data science is about using data in order to gain new insights that you would otherwise\n",
      "have missed 20\n",
      "\n",
      "As an example, using data science, clinics can identify patients who are likely to not show\n",
      "up for an appointment 24\n",
      "This can help improve margins, and providers can give other\n",
      "patients available slots 15\n",
      "\n",
      "That's why data science won't replace the human brain, but complement it, working\n",
      "alongside it 22\n",
      "Data science should not be thought of as an end-all solution to our data woes;\n",
      "it is merely an opinion —a very informed opinion, but an opinion nonetheless 32\n",
      "It deserves a\n",
      "seat at the table 8\n",
      "\n",
      "Why data science 4\n",
      "\n",
      "In this Data Age, it's clear that we have  a surplus of data 17\n",
      "But why should that necessitate\n",
      "an entirely new set of vocabulary 13\n",
      "What was wrong with our previous forms of analysis 9\n",
      "\n",
      "For one, the sheer volume of data makes it literally impossible for a human to parse it in a\n",
      "reasonable time frame 25\n",
      "Data is collected in various forms and from different sources, and\n",
      "often comes in a very unorganized format 21\n",
      "\n",
      "Data can be missing, incomplete, or just flat out wrong 13\n",
      "Oftentimes, we will have data on\n",
      "very different scales, and that makes it tough to compare it 22\n",
      "Say that we are looking at data\n",
      "in relation to pricing used cars 14\n",
      "One characteristic of a car is the year it was made, and\n",
      "another might be the number of miles on that car 24\n",
      "Once we clean our data (which we will\n",
      "spend a great deal of time looking at in this book), the relationships between the data\n",
      "become more obvious, and the knowledge that was once buried deep in millions of rows of\n",
      "data simply pops out 52\n",
      "One of the main goals of data science is to make explicit practices and\n",
      "procedures to discover and apply these relationships in the data 26\n",
      "\n",
      "Earlier, we looked at data science in a more historical perspective, but let's take a minute to\n",
      "discuss its role in business today using a very simple example 34\n",
      "\n",
      "\n",
      "Example – xyz123 Technologies\n",
      "Ben Runkle, the CEO of xyz123 Technologies,  is trying to solve a huge problem 27\n",
      "The\n",
      "company is consistently losing long-time customers 9\n",
      "He does not know why they are\n",
      "leaving, but he must do something fast 17\n",
      "He is convinced that in order to reduce  his churn,\n",
      "he must create new products and features, and consolidate existing technologies 24\n",
      "To be\n",
      "safe, he calls in his chief data scientist, Dr 14\n",
      "Hughan 2\n",
      "However, she is not convinced that new\n",
      "products and features alone will save the company 17\n",
      "Instead, she turns to the transcripts of\n",
      "recent customer service tickets 13\n",
      "She shows Ben the most recent transcripts and finds\n",
      "something surprising:\n",
      "\" 14\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "Not sure how to export this; are you 9\n",
      "\n",
      "\"Where is the button that makes a new list 11\n",
      "\n",
      "\"Wait, do you even know where the slider is 12\n",
      "\n",
      "\"If I can't figure this out today, it's a real problem 15\n",
      " 1\n",
      " 1\n",
      "\n",
      "It is clear that customers were having problems with the existing UI/UX, and weren't upset\n",
      "because of a lack of features 27\n",
      "Runkle and Hughan organized a mass UI/UX overhaul and\n",
      "their sales have never been better 21\n",
      "\n",
      "Of course, the science  used in the last example was minimal, but it makes a point 20\n",
      "We tend\n",
      "to call people like Runkle drivers 11\n",
      "Today's common stick-to-your-gut CEO wants to make\n",
      "all decisions quickly and iterate over solutions until something works 23\n",
      "Dr 1\n",
      "Hughan is much\n",
      "more analytical 7\n",
      "She wants to solve the problem just as much as Runkle, but she turns to\n",
      "user-generated data instead of her gut feeling for answers 29\n",
      "Data science is about applying\n",
      "the skills of the analytical mind and using them as a driver would 19\n",
      "\n",
      "Both of these mentalities have their place in today's enterprises; however, it is Hughan's\n",
      "way of thinking that dominates the ideas of data science —using data generated by the\n",
      "company as her source of information, rather than just picking up a solution and going\n",
      "with it 58\n",
      "\n",
      "\n",
      "The data science Venn diagram\n",
      "It is a common misconception that only those with a PhD or geniuses can understand the\n",
      "math/programming behind data science 33\n",
      "This is absolutely false 4\n",
      "Understanding data\n",
      "science begins with three basic areas:\n",
      "Math/statistics : This is the use of equations and formulas to perform analysis 25\n",
      "\n",
      "Computer programming : This is the ability to use code to create outcomes on a\n",
      "computer 18\n",
      "\n",
      "Domain knowledge : This refers  to understanding the problem domain\n",
      "(medicine, finance, social science, and so on) 25\n",
      "\n",
      "The following Venn diagram provides a visual representation of how these three areas of\n",
      "data science intersect:\n",
      "The Venn diagram of data science\n",
      "Those with hacking skills can conceptualize and program complicated algorithms using\n",
      "computer languages 44\n",
      "Having a math and statistics background allows you to theorize and\n",
      "evaluate algorithms and tweak the existing procedures to fit specific situations 24\n",
      "Having\n",
      "substantive expertise (domain expertise) allows you to apply concepts and results in a\n",
      "meaningful and effective way 25\n",
      "\n",
      "\n",
      "While having only two of these three qualities can make you intelligent, it will also leave a\n",
      "gap 21\n",
      "Let's say that you are very skilled in coding and have formal training in day trading 17\n",
      "\n",
      "You might create an automated system to trade in your place, but lack the math skills to\n",
      "evaluate your algorithms 23\n",
      "This will mean that you end up losing money in the long run 13\n",
      "It\n",
      "is only when you boost your skills in coding, math, and domain knowledge that you can\n",
      "truly perform data science 26\n",
      "\n",
      "The quality that was probably a surprise  for you was domain knowledge 14\n",
      "It is really just\n",
      "knowledge of the area you are working in 13\n",
      "If a financial analyst started analyzing data\n",
      "about heart attacks, they might need the help of a cardiologist to make sense of a lot of the\n",
      "numbers 31\n",
      "\n",
      "Data science  is the intersection of the three key areas mentioned earlier 14\n",
      "In order to gain\n",
      "knowledge from data, we must be able to utilize computer programming to access the data,\n",
      "understand the mathematics behind the models we derive, and, above all, understand our\n",
      "analyses' place in the domain we are in 50\n",
      "This includes the presentation of data 6\n",
      "If we are\n",
      "creating a model to predict heart attacks in patients, is it better to create a PDF of\n",
      "information, or an app where you can type in numbers and get a quick prediction 39\n",
      "All\n",
      "these decisions must be made by the data scientist 11\n",
      "\n",
      "The intersection of math and coding is machine learning 10\n",
      "This book will\n",
      "look at machine learning in great detail later on, but it is important to note\n",
      "that without the explicit ability to generalize any models or results to a\n",
      "domain, machine learning algorithms remain just that —algorithms sitting\n",
      "on your computer 51\n",
      "You might have the best algorithm to predict cancer 9\n",
      "\n",
      "You could be able to predict cancer with over 99% accuracy based on past\n",
      "cancer patient data, but if you don't understand how to apply this model\n",
      "in a practical sense so that doctors and nurses can easily use it, your\n",
      "model might be useless 55\n",
      "\n",
      "Both computer programming and math are covered extensively in this book 12\n",
      "Domain\n",
      "knowledge comes with both the practice of data science and reading examples of other\n",
      "people's analyses 20\n",
      "\n",
      "The math\n",
      "Most people stop listening once  someone says the word \" math 16\n",
      "They'll nod along in an\n",
      "attempt to hide their utter disdain for the topic 16\n",
      "This book will guide you through the math\n",
      "needed for data science, specifically statistics and probability 18\n",
      "We will use these\n",
      "subdomains of mathematics to create what are called models 15\n",
      "\n",
      "\n",
      "A data model  refers to an organized and formal relationship between elements of data,\n",
      "usually meant to simulate a real-world phenomenon 25\n",
      "\n",
      "Essentially, we will use math  in order to formalize relationships between variables 17\n",
      "As a\n",
      "former pure mathematician and current math teacher, I know how difficult this can be 19\n",
      "I\n",
      "will do my best to explain everything as clearly as I can 14\n",
      "Between the three areas of data\n",
      "science, math is what allows us to move from domain to domain 20\n",
      "Understanding the theory\n",
      "allows us to apply a model that we built for the fashion industry to a financial domain 21\n",
      "\n",
      "The math covered in this book ranges from basic algebra to advanced probabilistic and\n",
      "statistical modeling 20\n",
      "Do not skip over these chapters, even if you already know these topics\n",
      "or you're afraid of them 21\n",
      "Every mathematical concept that I will introduce will be\n",
      "introduced with care and purpose, using examples 19\n",
      "The math in this book is essential for\n",
      "data scientists 11\n",
      "\n",
      "Example – spawner-recruit models\n",
      "In biology, we use, among many  other models, a model known as the spawner-recruit\n",
      "model to judge the biological health of a species 40\n",
      "It is a basic relationship between the\n",
      "number of healthy parental units of a species and the number of new units in the group of\n",
      "animals 28\n",
      "In a public dataset of the number of salmon spawners and recruits, the graph\n",
      "further down (titled spawner-recruit model)  was formed to visualize the relationship\n",
      "between the two 41\n",
      "We can see that there definitely is some sort of positive relationship (as\n",
      "one goes up, so does the other) 24\n",
      "But how can we formalize this relationship 8\n",
      "For example, if\n",
      "we knew the number of spawners in a population, could we predict the number of recruits\n",
      "that the group would obtain, and vice versa 34\n",
      "\n",
      "Essentially, models allow us to plug in one variable to get the other 16\n",
      "Consider the following\n",
      "example :\n",
      "In this example, let's say we knew that a group of salmon had 1 23\n",
      "15 (in thousands) spawners 9\n",
      "\n",
      "Then, we would have the following:\n",
      " (in thousands)\n",
      "\n",
      "\n",
      "This result can be very beneficial to estimate how the health of a population is changing 29\n",
      "If\n",
      "we can create these models, we can visually observe how the relationship between the two\n",
      "variables can change 22\n",
      "\n",
      "There are many types of data models, including probabilistic and statistical models 15\n",
      "Both of\n",
      "these are subsets  of a larger paradigm, called machine learning 15\n",
      "The essential idea behind\n",
      "these three topics is that we use data in order to come up with the best model possible 23\n",
      "We\n",
      "no longer rely on human instincts —rather, we rely on data, such as that displayed in the\n",
      "following graph:\n",
      "The spawner-recruit model visualized\n",
      "The purpose of this example is to show how we can define relationships between data\n",
      "elements using mathematical equations 55\n",
      "The fact that I used salmon health data was\n",
      "irrelevant 12\n",
      "Throughout this book, we will look at relationships involving marketing dollars,\n",
      "sentiment data, restaurant reviews, and much more 23\n",
      "The main reason for this is that I\n",
      "would like you (the reader) to be exposed to as many domains as possible 25\n",
      "\n",
      "Math and coding are vehicles that allow data scientists to step back and apply their skills\n",
      "virtually anywhere 21\n",
      "\n",
      "\n",
      "Computer programming\n",
      "Let's be honest: you probably think  computer science is way cooler than math 20\n",
      "That's ok, I\n",
      "don't blame you 10\n",
      "The news isn't filled with math news like it is with news on technology 15\n",
      "\n",
      "You don't turn on the TV to see a new theory on primes —rather, you will see investigative\n",
      "reports on how the latest smartphone can take better  photos of cats, or something 39\n",
      "\n",
      "Computer languages are how we communicate with machines and tell them to do our\n",
      "bidding 18\n",
      "A computer speaks many languages and, like a book, can be written in many\n",
      "languages; similarly, data science can also be done in many languages 30\n",
      "Python,  Julia,  and R\n",
      "are some of the many languages that are available to us 20\n",
      "This book will focus exclusively on\n",
      "using Python 9\n",
      "\n",
      "Why Python 3\n",
      "\n",
      "We will use Python  for a variety  of reasons, listed as follows:\n",
      "Python is an extremely simple language to read and write, even if you've never\n",
      "coded before, which will make future examples easy to understand and read\n",
      "later on, even after you have read this book 58\n",
      "\n",
      "It is one of the most common languages, both in production and in the academic\n",
      "setting (one of the fastest growing, as a matter of fact) 32\n",
      "\n",
      "The language's online community is vast and friendly 10\n",
      "This means that a quick\n",
      "search for the solution to a problem should yield many people who have faced\n",
      "and solved similar (if not exactly the same) situations\n",
      "Python has prebuilt data science modules that both the novice and the veteran\n",
      "data scientist can utilize 52\n",
      "\n",
      "The last point is probably the biggest reason we will focus on Python 14\n",
      "These prebuilt\n",
      "modules are not only powerful, but also easy to pick up 16\n",
      "By the end of the first few\n",
      "chapters, you will be very comfortable with these modules 19\n",
      "Some of these modules include\n",
      "the following:\n",
      "pandas\n",
      "scikit-learn\n",
      "seaborn\n",
      "numpy/scipy\n",
      "requests  (to mine data from the web)\n",
      "BeautifulSoup  (for web –HTML parsing)\n",
      "\n",
      "Python practices\n",
      "Before we move on, it is important  to formalize many of the requisite coding skills in\n",
      "Python 69\n",
      "\n",
      "In Python, we have variables  that are placeholders for objects 13\n",
      "We will focus on just a few\n",
      "types of basic objects at first, as shown in the following table :\n",
      "Object Type Example\n",
      "int  (an integer) 3, 6, 99, -34, 34, 11111111\n",
      "float  (a decimal) 3 60\n",
      "14159, 2 6\n",
      "71, -0 5\n",
      "34567\n",
      "boolean  (either  True  or False )• The statement \"Sunday is a weekend\" is True\n",
      "• The statement \"Friday is a weekend\" is False\n",
      "• The statement \"pi is exactly the ratio of a circle's circumference to its\n",
      "diameter\" is True  (crazy, right 66\n",
      "\n",
      "string  (text or words made up of\n",
      "characters)\"I love hamburgers\" ( by the way, who doesn't 26\n",
      ")\n",
      "\"Matt is awesome\"\n",
      "A tweet is a string\n",
      "list  (a collection of objects) [1, 5 25\n",
      "4, True, \"apple\"]\n",
      "We will also have to understand some basic logistical operators 18\n",
      "For these operators, keep\n",
      "the Boolean datatype in mind 11\n",
      "Every operator will evaluate to either True  or False 10\n",
      "Let's\n",
      "take a look at the following operators :\n",
      "Operators Example\n",
      "==Evaluates to  True  if both sides are equal; otherwise, it evaluates to\n",
      "False , as shown in the following examples:\n",
      "• 3 + 4 == 7 (will evaluate to True )\n",
      "• 3 - 2 == 7 (will evaluate to False )\n",
      "< (less than)• 3 < 5 (True )\n",
      "• 5 < 3 (False )\n",
      "\n",
      "<= (less than or equal to)• 3 <= 3 (True )\n",
      "• 5 <= 3 (False )\n",
      "> (greater than)• 3 > 5 (Fals e)\n",
      "• 5 > 3 (True )\n",
      ">= (greater than or equal to)• 3 >= 3 (True )\n",
      "• 5 >= 7 (False )\n",
      "When coding in Python, I will use a pound sign ( #) to create a \"comment,\" which will not\n",
      "be processed as code, but is merely there to communicate with the reader 211\n",
      "Anything to the\n",
      "right of a  # sign is a comment on the code being executed 18\n",
      "\n",
      "Example of basic Python\n",
      "In Python, we use spaces/tabs to denote  operations that belong to other lines of code 25\n",
      "\n",
      "The print True  statement belongs to the if x + y == 15 16\n",
      "3:  line\n",
      "preceding it because it is tabbed right under it 17\n",
      "This means that the print\n",
      "statement will be executed if, and only if, x + y  equals 15 23\n",
      "3 2\n",
      "\n",
      "Note that the following list variable, my_list , can hold multiple types of objects 17\n",
      "This one\n",
      "has an int, a float , a boolean , and string  inputs (in that order):\n",
      "my_list = [1, 5 30\n",
      "7, True, \"apples\"]\n",
      "len(my_list) == 4  # 4 objects in the list\n",
      "my_list[0] == 1    # the first object\n",
      "my_list[1] == 5 47\n",
      "7    # the second object\n",
      "\n",
      "In the preceding code, I used the  len command to get the length of the list (which was 4) 32\n",
      "\n",
      "Also, n ote the zero-indexing of Python 12\n",
      "Most computer languages start counting at zero\n",
      "instead of one 11\n",
      "So if I want the first element, I call index 0, and if I want the 95th  element, I\n",
      "call index 94 31\n",
      "\n",
      "Example – parsing a single tweet\n",
      "Here is some more Python code 14\n",
      "In this example, I will be parsing some tweets about stock\n",
      "prices (one of the important case studies in this book will be trying to predict market\n",
      "movements based  on popular sentiment regarding stocks on social media):\n",
      "tweet = \"RT @j_o_n_dnger: $TWTR now top holding for Andor, unseating $AAPL\"\n",
      "words_in_tweet = tweet 77\n",
      "split(' ') # list of words in tweet\n",
      "for word in words_in_tweet:             # for each word in list\n",
      "  if \"$\" in word:                       # if word has a \"cashtag\"\n",
      "  print(\"THIS TWEET IS ABOUT\", word)  # alert the user\n",
      "I will point out a few things about this code snippet line by line, as follows:\n",
      "First, we set a variable to hold some text (known as a string in Python) 96\n",
      "In this\n",
      "example, the tweet in question is \"RT @robdv: $TWTR now top holding for\n",
      "Andor, unseating $AAPL \" 35\n",
      "\n",
      "The words_in_tweet  variable tokenizes  the tweet (separates it by word) 20\n",
      "If you\n",
      "were to print this variable, you would see the following:\n",
      "['RT',\n",
      "'@robdv:',\n",
      "'$TWTR',\n",
      "'now',\n",
      "'top',\n",
      "'holding',\n",
      "'for',\n",
      "'Andor,',\n",
      "'unseating',\n",
      "'$AAPL']\n",
      "We iterate through this list of words; this is called a for loop 67\n",
      "It just means that\n",
      "we go through a list one by one 13\n",
      "\n",
      "\n",
      "Here, we have another if statement 8\n",
      "For each word in this tweet, if the word\n",
      "contains the $ character which represents stock tickers on Twitter 22\n",
      "\n",
      "If the preceding if statement is True  (that is, if the tweet contains a cashtag),\n",
      "print it and show it to the user 29\n",
      "\n",
      "The output of this code will be as follows:\n",
      "THIS TWEET IS ABOUT $TWTR\n",
      "THIS TWEET IS ABOUT $AAPL\n",
      "We get this output as these are the only words in the tweet that use the cashtag 49\n",
      "Whenever I\n",
      "use Python in this book, I will ensure that I am as explicit as possible about what I am\n",
      "doing in each line of code 30\n",
      "\n",
      "Domain knowledge\n",
      "As I mentioned earlier, domain knowledge focuses  mainly on having knowledge of the\n",
      "particular topic you are working on 27\n",
      "For example, if you are a financial analyst working on\n",
      "stock market data, you have a lot of domain knowledge 23\n",
      "If you are a journalist looking at\n",
      "worldwide adoption rates, you might benefit from  consulting an expert in the field 24\n",
      "This\n",
      "book will attempt to show examples from several problem domains, including medicine,\n",
      "marketing, finance, and even UFO sightings 24\n",
      "\n",
      "Does this mean that if you're not a doctor, you can't work with medical data 19\n",
      "Of course not 3\n",
      "\n",
      "Great data scientists can apply their skills to any area, even if they aren't fluent in it 20\n",
      "Data\n",
      "scientists can adapt to the field and contribute meaningfully when their analysis is\n",
      "complete 19\n",
      "\n",
      "A big part of domain knowledge is a presentation 10\n",
      "Depending on your audience, it can\n",
      "matter greatly on how you present your findings 16\n",
      "Your results are only as good as your\n",
      "vehicle of communication 12\n",
      "You can predict the movement of the market with 99 11\n",
      "99%\n",
      "accuracy, but if your program is impossible to execute, your results will go unused 19\n",
      "\n",
      "Likewise, if your vehicle is inappropriate for the field, your results will go equally unused 19\n",
      "\n",
      "\n",
      "Some more terminology\n",
      "This is a good time to define some more vocabulary 15\n",
      "By this point, you're probably\n",
      "excitedly looking up a lot of data  science material and seeing words and phrases I haven't\n",
      "used yet 31\n",
      "Here are some common terms that you are likely to encounter 11\n",
      "\n",
      "Machine learning : This refers to giving computers the ability to learn from data\n",
      "without explicit \"rules\" being given by a programmer 26\n",
      "We have seen the concept\n",
      "of machine learning earlier in this chapter as the union of someone who has both\n",
      "coding and math skills 26\n",
      "Here, we are attempting to formalize this definition 10\n",
      "\n",
      "Machine learning combines the power of computers with intelligent learning\n",
      "algorithms in order to automate the discovery of relationships in data and create\n",
      "powerful data models 31\n",
      "Speaking of data models, in this book, we will concern\n",
      "ourselves with the following two basic types of data model:\n",
      "Probabilistic model : This refers to using  probability to find a\n",
      "relationship between elements that includes a degree of\n",
      "randomness 51\n",
      "\n",
      "Statistical model : This refers to taking  advantage of statistical\n",
      "theorems to formalize relationships between data elements in a\n",
      "(usually) simple mathematical formula 33\n",
      "\n",
      "While both the statistical and probabilistic models\n",
      "can be run on computers and might be considered\n",
      "machine learning in that regard, we will keep these\n",
      "definitions separate, since machine learning\n",
      "algorithms generally attempt to learn  relationships\n",
      "in different ways 50\n",
      "We will take a look at the\n",
      "statistical and probabilistic models in later chapters 17\n",
      "\n",
      "Exploratory data analysis  (EDA ): This refers to preparing data in order to\n",
      "standardize results and gain quick insights 26\n",
      "EDA is concerned with data\n",
      "visualization and preparation 10\n",
      "This is where we turn unorganized data into\n",
      "organized data and clean up missing/incorrect data points 20\n",
      "During EDA, we will\n",
      "create many types of plots and use these plots to identify key features and\n",
      "relationships to exploit in our data models 29\n",
      "\n",
      "Data mining:  This is the process of finding  relationships between elements of\n",
      "data 18\n",
      "Data mining is the part of data science where we try to find relationships\n",
      "between variables (think the spawn-recruit model) 25\n",
      "\n",
      "I have tried pretty hard  not to use the term big data  up until now 18\n",
      "This is because I think\n",
      "this term is misused, a lot 14\n",
      "Big data is data that is too large to be processed by a single\n",
      "machine (if your laptop crashed, it might be suffering from a case of big data) 33\n",
      "\n",
      "\n",
      "The following diagram shows the relationship between these data science concepts:\n",
      "The state of data science (so far)\n",
      "The preceding diagram is incomplete and is meant for visualization purposes only 34\n",
      "\n",
      "Data science case studies\n",
      "The combination of math, computer programming, and domain knowledge is what makes\n",
      "data science so powerful 25\n",
      "Oftentimes, it is difficult for a single person to master all three  of\n",
      "these areas 20\n",
      "That's why it's very common for companies to hire teams of data scientists\n",
      "instead of a single person 21\n",
      "Let's look at a few powerful examples of data science in action\n",
      "and their outcomes 17\n",
      "\n",
      "\n",
      "Case study – automating government paper\n",
      "pushing\n",
      "Social security claims are known  to be a major hassle for both the agent reading it and the\n",
      "person who wrote the claim 37\n",
      "Some claims take over two years to get resolved in their\n",
      "entirety, and that's absurd 20\n",
      "Let's look at the following diagram, which shows what goes into\n",
      "a claim:\n",
      "Sample social security form\n",
      "\n",
      "Not bad 24\n",
      "It's mostly just text, though 7\n",
      "Fill this in, then that, then this, and so on 13\n",
      "You can see\n",
      "how it would be difficult for an agent to read these all day, form after form 21\n",
      "There must be a\n",
      "better way 7\n",
      "\n",
      "Well, there is 5\n",
      "Elder Research Inc 3\n",
      "parsed this unorganized data and was able to automate\n",
      "20% of all disability social security forms 19\n",
      "This means that a computer could look at 20% of\n",
      "these written forms and give its opinion on the approval 23\n",
      "\n",
      "Not only that —the third-party company that is hired to rate the approvals of the forms\n",
      "actually gave the machine-graded forms a higher grade than the human forms 34\n",
      "So not only\n",
      "did the computer handle 20% of the load on average, it also did better than a human 24\n",
      "\n",
      "Fire all humans, right 6\n",
      "\n",
      "Before I get a load of angry emails  claiming that data science is bringing about the end of\n",
      "human workers, keep in mind that the computer was only  able to handle 20% of the load 42\n",
      "\n",
      "This means that it probably performed terribly on 80% of the forms 15\n",
      "This is because the\n",
      "computer was probably great at simple forms 12\n",
      "The claims that would have taken a human\n",
      "minutes to compute took the computer seconds 16\n",
      "But these minutes add up, and before you\n",
      "know it, each human is being saved over an hour a day 23\n",
      "\n",
      "Forms that might be easy for a human to read are also likely easy for the computer 18\n",
      "It's\n",
      "when the forms are very terse, or when the writer starts deviating from the usual grammar,\n",
      "that the computer starts to fail 28\n",
      "This model is great because it lets the humans spend more\n",
      "time on those difficult claims and gives them more attention without getting distracted by\n",
      "the sheer volume of papers 32\n",
      "\n",
      "Note that I used the word \"model 9\n",
      "Remember that a model is a\n",
      "relationship between elements 10\n",
      "In this case, the relationship is between\n",
      "written words and the approval status of a claim 18\n",
      "\n",
      "\n",
      "Case study – marketing dollars\n",
      "A dataset shows the relationships between TV, radio, and newspaper sales 20\n",
      "The goal is to\n",
      "analyze the relationships between the three different marketing mediums and how they \n",
      "affect  the sale of a product 26\n",
      "In this case, our data is displayed in the form of a table 14\n",
      "Each\n",
      "row represents a sales region, and the columns tell us how much money was spent on each\n",
      "medium, as well as the profit that was gained in that region 34\n",
      "F or example, from the\n",
      "following table, we can see that in the third region, we spent $17,200 on TV advertising and\n",
      "sold 9,300 widgets:\n",
      "Usually, the data scientist must ask for units and the scale 49\n",
      "In this case, I\n",
      "will tell you that the TV, radio, and newspaper categories are measured in\n",
      "\"thousands of dollars\" and the sales in \"thousands of widgets sold 38\n",
      "This\n",
      "means that in the first region, $230,100 was spent on TV advertising,\n",
      "$37,800 on radio advertising, and $69,200 on newspaper advertising 35\n",
      "In\n",
      "the same region, 22,100 items were sold 13\n",
      "\n",
      "Advertising budgets' data\n",
      "If we plot each variable against the sales, we get the following graph:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "%matplotlib inline\n",
      "data = pd 38\n",
      "read_csv('http://www-bcf 8\n",
      "usc 2\n",
      "edu/~gareth/ISL/Advertising 9\n",
      "csv',\n",
      "index_col=0)\n",
      "data 8\n",
      "head()\n",
      "sns 3\n",
      "pairplot(data, x_vars=['TV','radio','newspaper'], y_vars='sales',\n",
      "height=4 23\n",
      "5, aspect=0 6\n",
      "7)\n",
      "\n",
      "\n",
      "Results – Graphs of advertising budgets\n",
      "Note how none of these variables form a very strong line, and that therefore they might not\n",
      "work well in predicting sales on their own 38\n",
      "TV comes closest in forming an obvious\n",
      "relationship, but even that isn't great 16\n",
      "In this case, we will have to create a more complex\n",
      "model than the one we used in the spawner-recruiter model and combine all three variables\n",
      "in order to model sales 38\n",
      "\n",
      "This type of problem is very common in data science 11\n",
      "In this example, we are attempting to\n",
      "identify key features that are associated with the sales of a product 21\n",
      "If we can isolate these\n",
      "key features, then we can exploit these relationships and change how much we spend on\n",
      "advertising in different places with the hope of increasing our sales 35\n",
      "\n",
      "\n",
      "Case study – what's in a job description 10\n",
      "\n",
      "Looking for a job in data  science 9\n",
      "Great 1\n",
      "Let me help 3\n",
      "In this case study, I have \"scraped\"\n",
      "(taken from the web) 1,000 job descriptions for companies that are actively hiring data\n",
      "scientists 34\n",
      "The goal here is to look at some of the most common keywords that people use in\n",
      "their job descriptions, as shown in the following screenshot:\n",
      "An example of data scientist job listings 36\n",
      "\n",
      "Note the second one asking for core Python libraries; we will talk about\n",
      "these later on in this book 22\n",
      "\n",
      "\n",
      "In the following Python code, the first two imports are used to grab web data from the\n",
      "website http:/ ​/​indeed 28\n",
      "​com/ ​, and the third import is meant to simply count the number of\n",
      "times a word or phrase appears, as shown in the following code:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from sklearn 43\n",
      "feature_extraction 2\n",
      "text import CountVectorizer\n",
      "# grab postings from the web\n",
      "texts = []\n",
      "for i in range(0,1000,10): # cycle through 100 pages  of indeed job\n",
      "resources\n",
      " soup =\n",
      "BeautifulSoup(requests 46\n",
      "get('http://www 5\n",
      "indeed 1\n",
      "com/jobs 3\n",
      "q=data+scientist&sta\n",
      "rt='+str(i)) 13\n",
      "text)\n",
      " texts += [a 6\n",
      "text for a in soup 5\n",
      "findAll('span', {'class':'summary'})]\n",
      "print(type(texts))\n",
      "print(texts[0]) # first job description\n",
      "Okay, before I lose you, all that this loop is doing is going through 100 pages  of job\n",
      "descriptions, and for each page, grabbing each job description 61\n",
      "The important variable here\n",
      "is texts , which is a list of over 1,000 job descriptions, as shown in the following code:\n",
      "type(texts) # == list\n",
      "vect = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
      "# Get basic counts of one and two word phrases\n",
      "matrix = vect 67\n",
      "fit_transform(texts)\n",
      "# fit and learn to the vocabulary in the corpus\n",
      "print len(vect 20\n",
      "get_feature_names())  # how many features are there\n",
      "# There are 10,587 total one and two words phrases in my case 28\n",
      " 1\n",
      "\n",
      "Since web pages are scraped in real-time and these pages may change since\n",
      "this code is run, you may get different number than 10587 30\n",
      "\n",
      "\n",
      "I have omitted some code here, but it exists in the GitHub repository for this book 18\n",
      "The\n",
      "results are as follows (represented as the phrase and then the number of times it occurred ):\n",
      "\\\n",
      "The following list shows some  things that we should mention:\n",
      "\"Machine learning\" and \"experience\" are at the top of the list 48\n",
      "Experience comes\n",
      "with practice 5\n",
      "A basic idea of machine learning comes with this book 10\n",
      "\n",
      "These words are followed closely by statistical words implying a knowledge of\n",
      "math and theory 17\n",
      "\n",
      "The word \" team\"  is very high up, implying that you will need to work with a\n",
      "team of data scientists; you won't be a lone wolf 34\n",
      "\n",
      "Computer science words such as \" algorithms\"  and \" programming\"  are prevalent 17\n",
      "\n",
      "The words \" techniques\" , \"understanding\" , and \" methods\"  imply a more\n",
      "theoretical approach, unrelated to any single domain 30\n",
      "\n",
      "The word \" business\"  implies a particular problem domain 12\n",
      "\n",
      "\n",
      "There are many interesting things to note about this case study, but the biggest take away is\n",
      "that there are many keywords and phrases that make up a data science role 34\n",
      "It isn't just\n",
      "math, coding, or domain knowledge; it truly is a combination of these three ideas (whether\n",
      "exemplified in a single person or across a multiperson team) that makes data science\n",
      "possible and powerful 47\n",
      "\n",
      "Summary\n",
      "At the beginning of this chapter, I posted a simple question: what's the catch of data\n",
      "science 24\n",
      "Well, there is one 5\n",
      "It isn't all fun, games, and modeling 10\n",
      "There must be a price for\n",
      "our quest to create ever-smarter machines and algorithms 17\n",
      "As we seek new and innovative\n",
      "ways to discover data trends, a beast lurks in the shadows 20\n",
      "I'm not talking about the\n",
      "learning curve of mathematics or programming, nor am I referring to the surplus of data 23\n",
      "\n",
      "The Industrial Age left us with an ongoing battle against pollution 12\n",
      "The subsequent\n",
      "Information Age left behind a trail of big data 12\n",
      "So, what dangers might the Data Age bring\n",
      "us 11\n",
      "\n",
      "The Data Age can lead to something much more sinister —the dehumanization of the\n",
      "individual through mass data 23\n",
      "\n",
      "More and more people are jumping head first into the field of data science, most with no\n",
      "prior experience of math or CS, which, on the surface, is great 35\n",
      "Average data scientists have\n",
      "access to millions of dating profiles' data, tweets, online reviews, and much more in order\n",
      "to jump start their education 30\n",
      "\n",
      "However, if you jump into data science without the proper exposure to theory or coding\n",
      "practices, and without respect for the domain you are working in, you face the risk of\n",
      "oversimplifying the very phenomenon you are trying to model 49\n",
      "\n",
      "For example, let's say you want to automate your sales pipeline by building a simplistic\n",
      "program that looks at LinkedIn for very specific keywords in a person's LinkedIn profile 34\n",
      "\n",
      "You could use the following code to do this:\n",
      "keywords = [\"Saas\", \"Sales\", \"Enterprise\"]\n",
      "Great 24\n",
      "Now you can scan LinkedIn quickly to find people who match your criteria 13\n",
      "But\n",
      "what about that person who spells out \"Software as a Service\", instead of \"SaaS,\" or\n",
      "misspells \"enterprise\" (it happens to the best of us; I bet someone will find a typo in my\n",
      "book) 50\n",
      "How will your model figure out that these people are also a good match 14\n",
      "They\n",
      "should not be left behind just because the cut-corners data scientist has overgeneralized\n",
      "people in such an easy way 26\n",
      "\n",
      "\n",
      "The programmer chose to simplify their search for another human by looking for three\n",
      "basic keywords and ended up with a lot of missed opportunities left on the table 31\n",
      "\n",
      "In the next chapter, we will explore the different types of data that exist in the world,\n",
      "ranging from free-form text to highly structured row/column files 32\n",
      "We will also look at the\n",
      "mathematical operations that are allowed for different types of data, as well as deduce\n",
      "insights based on the form of the data that comes in 38\n",
      "\n",
      "\n",
      "\n",
      "Types of Data\n",
      "Now that we have a basic introduction to the world of data science and understand why\n",
      "the field is so important, let's take a look at the various ways in which data can be formed 43\n",
      "\n",
      "Specifically, in this chapter, we will look at the following topics:\n",
      "Structured versus unstructured data\n",
      "Quantitative versus qualitative data\n",
      "The four levels of data\n",
      "We will dive further into each of these topics by showing examples of how data scientists\n",
      "look at and work with data 57\n",
      "The aim of this chapter is to familiarize ourselves with the\n",
      "fundamental ideas underpinning data science 21\n",
      "\n",
      "Flavors of data\n",
      "In the field, it is important to understand the different flavors of data for several reasons 23\n",
      "\n",
      "Not only will the type of data  dictate the methods used to analyze and extract results, but\n",
      "knowing whether the data is unstructured, or perhaps quantitative, can also tell you a lot\n",
      "about the real-world phenomenon being measured 48\n",
      "\n",
      "The first thing to note is my use of the word data 13\n",
      "In the last chapter, I defined data as\n",
      "merely being a collection of information 17\n",
      "This vague definition exists because we may\n",
      "separate data into different categories and need our definition to be loose 21\n",
      "\n",
      "The next thing to remember while we go through this chapter is that for the most part,\n",
      "when I talk about the type of data I will refer to either a specific characteristic  of a dataset or\n",
      "to the entire dataset  as a whole 49\n",
      "I will be very clear about which one I refer to at any given\n",
      "time 16\n",
      "\n",
      "\n",
      "Why look at these distinctions 6\n",
      "\n",
      "It might seem worthless to stop and think about what type of data we have before getting\n",
      "into the fun stuff, such as statistics and machine learning, but this is arguably one of the\n",
      "most important steps you need to take to perform data science 50\n",
      "\n",
      "The same principle applies to data  science 9\n",
      "When given a dataset, it is tempting to jump\n",
      "right into exploring, applying statistical models, and researching the applications of\n",
      "machine learning in order to get results faster 33\n",
      "However, if you don't understand the type\n",
      "of data that you are working with, then you might waste a lot of time applying models that\n",
      "are known to be ineffective with that specific type of data 41\n",
      "\n",
      "When given a new dataset, I always recommend taking about an hour (usually less) to\n",
      "make the distinctions mentioned in the following sections 28\n",
      "\n",
      "Structured versus unstructured data\n",
      "The distinction between structured  and unstructured data  is usually the first question you\n",
      "want to ask yourself about the entire  dataset 33\n",
      "The answer to this question can mean the\n",
      "difference between needing three days or three weeks of time to perform a proper analysis 24\n",
      "\n",
      "The basic breakdown is as follows (this is a rehashed definition of organized and\n",
      "unorganized data in the first chapter):\n",
      "Structured (organized) data : This is data that can be thought of as observations\n",
      "and characteristics 45\n",
      "It is usually organized using a table method (rows and\n",
      "columns) 14\n",
      "\n",
      "Unstructured (unorganized) data : This data exists  as a free entity and does not\n",
      "follow any standard organization hierarchy 26\n",
      "\n",
      "Here are a few examples that could help you differentiate between the two:\n",
      "Most data that exists in text form, including server logs and Facebook posts, is\n",
      "unstructured\n",
      "Scientific observations, as recorded by careful scientists, are kept in a very neat\n",
      "and organized ( structured ) format\n",
      "A genetic sequence of chemical nucleotides (for example, ACGTATTGCA) is\n",
      "unstructured,  even if the order of the nucleotides matters, as we cannot form\n",
      "descriptors of the sequence using a row/column format without taking a further\n",
      "look\n",
      "\n",
      "Structured data is generally thought of as being much easier to work with and analyze 132\n",
      "\n",
      "Most statistical and machine learning models were built with structured data in mind and\n",
      "cannot work on the loose interpretation of unstructured data 26\n",
      "The natural row and column\n",
      "structure is easy to digest for human and machine eyes 16\n",
      "So, why even talk about\n",
      "unstructured data 10\n",
      "Because it is so common 5\n",
      "Most estimates place unstructured data as\n",
      "80-90% of the world's data 17\n",
      "This data exists in many forms and, for the most part, goes\n",
      "unnoticed by humans as a potential source of data 25\n",
      "Tweets, emails, literature, and server\n",
      "logs are generally unstructured forms of data 17\n",
      "\n",
      "While a data scientist likely prefers structured data, they must be able to deal with the\n",
      "world's massive amounts of unstructured data 27\n",
      "If 90% of the world's data is unstructured,\n",
      "that implies that about 90% of the world's information is trapped in a difficult format 31\n",
      "\n",
      "So, with most of our data  existing in this free-form format, we must turn to pre-analysis\n",
      "techniques, called pre-processing , in order to apply structure to at least a part of the data\n",
      "for further analysis 47\n",
      "The next chapter will deal with pre-processing in great detail; for now,\n",
      "we will consider the part of pre-processing wherein we attempt to apply transformations to\n",
      "convert unstructured data into a structured counterpart 39\n",
      "\n",
      "Example of data pre-processing\n",
      "When looking at text data (which is almost always considered unstructured), we have\n",
      "many options to transform the set into a structured format 34\n",
      "We may do this by applying\n",
      "new characteristics that describe the data 13\n",
      "A few such characteristics are as follows:\n",
      "Word/phrase count\n",
      "The existence of certain special characters\n",
      "The relative length of text\n",
      "Picking out topics\n",
      "I will use the following tweet as a quick example of unstructured data, but you may use\n",
      "any unstructured free-form text that you like, including tweets and Facebook posts:\n",
      "\"This Wednesday morn, are you early to rise 77\n",
      "Then look East 3\n",
      "The Crescent Moon joins Venus\n",
      "& Saturn 8\n",
      "Afloat in the dawn skies 6\n",
      "\n",
      "It is important to reiterate that pre-processing is necessary for this tweet because a vast\n",
      "majority of learning algorithms require numerical data (which we will get into after this\n",
      "example) 38\n",
      "\n",
      "\n",
      "More than requiring a certain type of data, pre-processing allows us to explore features that\n",
      "have been created from the existing features 26\n",
      "For example, we can extract features such as\n",
      "word count and special characters from the mentioned tweet 19\n",
      "Now, let's take a look at a few\n",
      "features that we can extract from the text 19\n",
      "\n",
      "Word/phrase counts\n",
      "We may break down a tweet  into its word/phrase count 19\n",
      "The word this appears in the tweet\n",
      "once, as does every other word 15\n",
      "We can represent this tweet in a structured format as\n",
      "follows, thereby converting the unstructured set of words into a row/column format:\n",
      "this wednesday morn are you\n",
      "Word count 1 1 1 1 1\n",
      "Note that to obtain this format, we can utilize scikit-learn's CountVectorizer , which we\n",
      "saw in the previous chapter 76\n",
      "\n",
      "Presence of certain special characters\n",
      "We may also look at the presence of special characters, such as the question mark and\n",
      "exclamation mark 28\n",
      "The appearance of these characters might imply certain ideas about the\n",
      "data that are otherwise difficult to know 19\n",
      "For example, the fact that this tweet contains a\n",
      "question mark might strongly imply that this tweet contains a question for the reader 25\n",
      "We\n",
      "might append the preceding table with a new column, as shown:\n",
      "this wednesday morn are you 22\n",
      "\n",
      "Word Count 1 1 1 1 1 1\n",
      "The relative length of text\n",
      "This tweet is 125 characters long:\n",
      "len(\"This Wednesday morn, are you early to rise 42\n",
      "Then look East 3\n",
      "The\n",
      "Crescent Moon joins Venus & Saturn 10\n",
      "Afloat in the dawn skies 6\n",
      "\n",
      "# get the length of this text (number of characters for a string)\n",
      "# 125\n",
      "\n",
      "The average tweet, as discovered by analysts, is about 30 characters in length 36\n",
      "So, we might\n",
      "impose a new  characteristic, called relative length  (which is the length of the tweet divided\n",
      "by the average length), telling us the length of this tweet as compared with the average\n",
      "tweet 45\n",
      "This tweet is actually 4 6\n",
      "03 times longer than the average tweet, as shown:\n",
      "We can add yet another column to our table using this method:\n",
      "this wednesday morn are you 32\n",
      "Relative length\n",
      "Word count 1 1 1 1 1 1 4 19\n",
      "03\n",
      "Picking out topics\n",
      "We can pick out some topics of the tweet  to add as columns 22\n",
      "This tweet is about astronomy,\n",
      "so we can add another column, as illustrated:\n",
      "this wednesday morn are you 23\n",
      "Relative length Topic\n",
      "Word count 1 1 1 1 1 1 4 20\n",
      "03 astronomy\n",
      "And just like that, we can convert a piece of text into structured/organized data ready for\n",
      "use in our models and exploratory analysis 32\n",
      "\n",
      "The topic is the only extracted feature we looked at that is not automatically derivable from\n",
      "the tweet 21\n",
      "Looking at word count and tweet length in Python is easy 11\n",
      "However, more\n",
      "advanced models (called topic models) are able to derive and predict topics of natural text\n",
      "as well 24\n",
      "\n",
      "Being able to quickly recognize whether your data is structured or unstructured can save\n",
      "hours or even days of work in the future 26\n",
      "Once you are able to discern the organization of\n",
      "the data presented to you, the next question is aimed at the individual characteristics of the\n",
      "dataset 29\n",
      "\n",
      "Quantitative versus qualitative data\n",
      "When you ask a data scientist, \"what type of data is this 21\n",
      ", they will usually assume that you\n",
      "are asking them whether or not it is mostly quantitative or qualitative 20\n",
      "It is likely the most\n",
      "common way of describing the specific  characteristics of a dataset 17\n",
      "\n",
      "\n",
      "For the most part, when talking about quantitative data, you are usually  (not always)\n",
      "talking about a structured dataset with a strict row/column structure (because we don't\n",
      "assume unstructured data even has any characteristics) 47\n",
      "All the more reason why the pre-\n",
      "processing step is so important 13\n",
      "\n",
      "These two data types can be defined as follows:\n",
      "Quantitative data : This data  can be described using numbers, and basic\n",
      "mathematical procedures, including addition, are possible on the set 40\n",
      "\n",
      "Qualitative data : This data  cannot be described using numbers and basic\n",
      "mathematics 18\n",
      "This data is generally thought of as being described using natural\n",
      "categories and language 15\n",
      "\n",
      "Example – coffee shop data\n",
      "Say that we were processing  observations of coffee  shops in a major city using the\n",
      "following five descriptors (characteristics):\n",
      "Data: Coffee Shop\n",
      "Name of coffee shop\n",
      "Revenue (in thousands of dollars)\n",
      "Zip code\n",
      "Average monthly customers\n",
      "Country of coffee origin\n",
      "Each of these characteristics can be classified as either quantitative or qualitative, and that\n",
      "simple distinction can change everything 82\n",
      "Let's take a look at each one:\n",
      "Name of a coffee shop : Qualitative\n",
      "The name of a coffee shop is not expressed as a number and we cannot perform\n",
      "mathematical operations on the name of the shop 45\n",
      "\n",
      "Revenue : Quantitative\n",
      "How much money a coffee shop brings in can definitely be described using a\n",
      "number 22\n",
      "Also, we can do basic operations, such as adding up the revenue for 12\n",
      "months to get a year's worth of revenue 27\n",
      "\n",
      "\n",
      "Zip code : Qualitative\n",
      "This one is tricky 11\n",
      "A zip code is always represented using numbers, but what\n",
      "makes it qualitative is that it does not fit the second part of the definition of\n",
      "quantitative —we cannot perform basic mathematical operations on a zip code 42\n",
      "If\n",
      "we add together two zip codes, it is a nonsensical measurement 16\n",
      "We don't\n",
      "necessarily get a new zip code and we definitely don't get \"double the zip code 22\n",
      "\n",
      "Average monthly customers : Quantitative\n",
      "Again, describing this factor using numbers and addition makes sense 19\n",
      "Add up\n",
      "all of your monthly customers and you get your yearly customers 14\n",
      "\n",
      "Country of coffee origin : Qualitative\n",
      "We will assume this is a very small coffee shop with coffee from a single origin 25\n",
      "\n",
      "This country is described using a name (Ethiopian, Colombian), and not\n",
      "numbers 19\n",
      "\n",
      "A couple of important things to note:\n",
      "Even though a zip code is being described using numbers, it is not quantitative 24\n",
      "\n",
      "This is because you can't talk about the sum of all zip codes or an average  zip\n",
      "code 22\n",
      "These are nonsensical descriptions 6\n",
      "\n",
      "Pretty much whenever a word is used to describe a characteristic, it is a\n",
      "qualitative factor 20\n",
      "\n",
      "If you are having trouble identifying which is which, basically, when trying to decide\n",
      "whether or not the data is qualitative or quantitative, ask yourself a few basic questions\n",
      "about the data characteristics:\n",
      "Can you describe it using numbers 46\n",
      "\n",
      "No 2\n",
      "It is qualitative\n",
      "Yes 5\n",
      "Move on to the next question\n",
      "Does it still make sense after you add them together 17\n",
      "\n",
      "No 2\n",
      "They are qualitative\n",
      "Yes 5\n",
      "You probably have quantitative  data\n",
      "This method will help you to classify most, if not all, data into one of these two categories 27\n",
      "\n",
      "\n",
      "The difference between these two categories defines the types of questions you may ask\n",
      "about each column 19\n",
      "For a quantitative column, you may ask questions such as the\n",
      "following:\n",
      "What is the average value 20\n",
      "\n",
      "Does this quantity increase or decrease over time (if time is a factor) 16\n",
      "\n",
      "Is there a threshold where if this number became too high or too low, it would\n",
      "signal trouble for the company 24\n",
      "\n",
      "For a qualitative column, none of the preceding questions can be answered 14\n",
      "However, the\n",
      "following questions only apply to qualitative values:\n",
      "Which value occurs the most and the least 20\n",
      "\n",
      "How many unique values are there 7\n",
      "\n",
      "What are these unique values 6\n",
      "\n",
      "Example – world alcohol consumption data\n",
      "The World Health Organization  (WHO ) released a dataset describing  the average drinking\n",
      "habits of people  in countries across  the world 36\n",
      "We will use Python and the data exploration\n",
      "tool, pandas, in order to gain a better look:\n",
      "import pandas as pd\n",
      "# read in the CSV file from a URL\n",
      "drinks =\n",
      "pd 40\n",
      "read_csv('https://raw 6\n",
      "githubusercontent 2\n",
      "com/sinanuozdemir/principles_of_\n",
      "data_science/master/data/chapter_2/drinks 23\n",
      "csv')\n",
      "# examine the data's first five rows\n",
      "drinks 13\n",
      "head()           # print the first 5 rows\n",
      "These three lines have  done the following:\n",
      "Imported pandas , which will be referred to as pd in the future\n",
      "Read in a comma separated value  (CSV ) file as a variable called drinks\n",
      "Called a method, head , that reveals the first five rows of the dataset\n",
      "Note the neat row/column structure a CSV comes in 78\n",
      "\n",
      "\n",
      "\n",
      "The preceding table lists the first five rows of data from the drink 14\n",
      "csv  file 3\n",
      "We have six\n",
      "different columns that we are working within this example:\n",
      "country : Qualitative\n",
      "beer_servings : Quantitative\n",
      "spirit_servings : Quantitative\n",
      "wine_servings : Quantitative\n",
      "total_litres_of_pure_alcohol : Quantitative\n",
      "continent : Qualitative\n",
      "Let's look at the qualitative column continent 65\n",
      "We can use Pandas in order to get some\n",
      "basic summary statistics about this non-numerical characteristic 21\n",
      "The describe()  method\n",
      "is being used here, which first identifies whether the column is likely to be quantitative or\n",
      "qualitative, and then gives basic information about the column as a whole 38\n",
      "This is done as\n",
      "follows:\n",
      "drinks['continent'] 13\n",
      "describe()\n",
      ">> count     170\n",
      ">> unique      5\n",
      ">> top        AF\n",
      ">> freq       53\n",
      "It reveals that the WHO has gathered data about five unique continents, the most frequent\n",
      "being AF (Africa), which occurred 53 times in the 193 observations 57\n",
      "\n",
      "\n",
      "If we take a look at one of the quantitative columns and call the same method, we can see\n",
      "the difference in output, as shown:\n",
      "drinks['beer_servings'] 37\n",
      "describe()\n",
      "The output is as follows:\n",
      "count    193 12\n",
      "000000\n",
      "mean     106 8\n",
      "160622\n",
      "std      101 8\n",
      "143103\n",
      "min        0 8\n",
      "000000\n",
      "25%       20 9\n",
      "000000\n",
      "50%       76 9\n",
      "000000\n",
      "75%      188 9\n",
      "000000\n",
      "max      376 8\n",
      "000000\n",
      "Now, we can look at the mean (average) beer serving per person per country (106 23\n",
      "2\n",
      "servings), as well as the lowest beer serving, zero, and the highest beer serving recorded,\n",
      "376 (that's more than a beer a day) 34\n",
      "\n",
      "Digging deeper\n",
      "Quantitative data can be broken down  one step further into discrete  and continuous\n",
      "quantities 24\n",
      "\n",
      "These can be defined as follows:\n",
      "Discrete data : This describes data  that is counted 19\n",
      "It can only take on certain\n",
      "values 8\n",
      "\n",
      "Examples of discrete quantitative data include a dice roll, because it can only take\n",
      "on six values, and the number of customers in a coffee shop, because you can't\n",
      "have a real range of people 42\n",
      "\n",
      "Continuous data : This describes data  that is measured 11\n",
      "It exists on an infinite\n",
      "range of values 9\n",
      "\n",
      "A good example of continuous data would be a person's weight, because it can\n",
      "be 150 pounds or 197 25\n",
      "66 pounds (note the decimals) 8\n",
      "The height of a person or\n",
      "building is a continuous number because an infinite scale of decimals is possible 20\n",
      "\n",
      "Other examples of continuous data would be time and temperature 11\n",
      "\n",
      "\n",
      "The road thus far\n",
      "So far in this chapter, we have looked  at the differences between structured and\n",
      "unstructured data, as well as between qualitative and quantitative characteristics 35\n",
      "These\n",
      "two simple distinctions can have drastic effects on the analysis that is performed 15\n",
      "Allow me\n",
      "to summarize before moving on the second half of the chapter 14\n",
      "\n",
      "Data as a whole can either be structured  or unstructured , meaning that the data can either\n",
      "take on an organized row/column structure with distinct features that describe each row of\n",
      "the dataset, or exist in a free-form state that usually must be pre-processed into a form that\n",
      "is easily digestible 63\n",
      "\n",
      "If data is structured, we can look at each column (feature) of the dataset as being either\n",
      "quantitative  or qualitative 27\n",
      "Basically, can the column be described using mathematics and\n",
      "numbers or not 14\n",
      "The next part of this chapter will break down data into four very specific\n",
      "and detailed levels 18\n",
      "At each order, we will apply more complicated rules of mathematics,\n",
      "and in turn, we can gain a more intuitive and quantifiable understanding of the data 30\n",
      "\n",
      "The four levels of data\n",
      "It is generally understood that a specific characteristic (feature/column) of structured data\n",
      "can be broken down into one of four levels of data 34\n",
      "The levels are as follows:\n",
      "The nominal level\n",
      "The ordinal level\n",
      "The interval level\n",
      "The ratio level\n",
      "As we move down the list, we gain more structure and, therefore, more returns from our\n",
      "analysis 43\n",
      "Each level comes with its own accepted practice in measuring the center  of the\n",
      "data 17\n",
      "We usually think of the mean/average as being an acceptable form of a center 16\n",
      "\n",
      "However, this is only true for a specific type of data 13\n",
      "\n",
      "\n",
      "The nominal level\n",
      "The first level of data, the nominal  level, consists of data  that is described purely by name\n",
      "or category 29\n",
      "Basic examples include gender, nationality, species, or yeast strain  in a beer 16\n",
      "\n",
      "They are not described by numbers and are therefore qualitative 11\n",
      "The following are some\n",
      "examples:\n",
      "A type of animal is on the nominal level of data 18\n",
      "We may also say that if you are\n",
      "a chimpanzee, then you belong to the mammalian class as well 23\n",
      "\n",
      "A part of speech is also considered on the nominal level of data 14\n",
      "The word she is a\n",
      "pronoun, and it is also a noun 15\n",
      "\n",
      "Of course, being qualitative, we cannot perform any quantitative mathematical operations,\n",
      "such as addition or division 20\n",
      "These would not make any sense 6\n",
      "\n",
      "Mathematical operations allowed\n",
      "We cannot perform mathematics on the nominal  level of data except the basic equality  and\n",
      "set membership  functions, as shown in the following two examples:\n",
      "Being a tech entrepreneur  is the same as being in the tech industry , but not the other\n",
      "way around\n",
      "A figure described as a square falls under the description of being a rectangle,\n",
      "but not the other way around\n",
      "Measures of center\n",
      "A measure of center  is a number that describes what the data tends to 103\n",
      "It is sometimes\n",
      "referred to as the balance point  of the data 15\n",
      "Common examples include the mean, median,\n",
      "and mode 10\n",
      "\n",
      "In order to find the center  of nominal data, we generally turn to the mode  (the most\n",
      "common element) of the dataset 29\n",
      "For example, look back at the WHO alcohol consumption\n",
      "data 12\n",
      "The most common  continent surveyed was Africa, making that a possible choice for\n",
      "the center of the continent column 22\n",
      "\n",
      "Measures of the center, such as the mean and median, do not make sense at this level as we\n",
      "cannot order the observations or even add them together 33\n",
      "\n",
      "\n",
      "What data is like at the nominal level\n",
      "Data at the nominal level is mostly  categorical in nature 21\n",
      "Because we generally can only use\n",
      "words to describe the data, it can be lost in translation between countries, or can even be\n",
      "misspelled 30\n",
      "\n",
      "While data at this level can certainly be useful, we must be careful about what insights we\n",
      "may draw from them 24\n",
      "With only the mode as a basic measure of center, we are unable to \n",
      "draw  conclusions about an average  observation 24\n",
      "This concept does not exist at this level 8\n",
      "It\n",
      "is only at the next level that we may begin to perform true mathematics on our\n",
      "observations 20\n",
      "\n",
      "The ordinal level\n",
      "The nominal level did not provide us with much flexibility in terms of mathematical\n",
      "operations due to one seemingly unimportant fact: we could not order the observations in\n",
      "any natural way 40\n",
      "Data in the ordinal  level provides us with a rank order, or the means to\n",
      "place one observation before the other 24\n",
      "However, it does not provide us with relative\n",
      "differences between observations, meaning that while we may order the observations from\n",
      "first to last, we cannot add or subtract them to get any real meaning 40\n",
      "\n",
      "Examples\n",
      "The Likert  is among the most common  ordinal level scales 16\n",
      "Whenever you are given a \n",
      "survey  asking you to rate your satisfaction on a scale from 1 to 10, you are providing data at\n",
      "the ordinal level 33\n",
      "Your answer, which must fall between 1 and 10, can be ordered: eight is\n",
      "better than seven while three is worse than nine 29\n",
      "\n",
      "However, differences between the numbers do not make much sense 12\n",
      "The difference\n",
      "between a seven and a six might be different from the difference between a two and a one 21\n",
      "\n",
      "Mathematical operations allowed\n",
      "We are allowed much more freedom  on this level in mathematical operations 20\n",
      "We inherit all\n",
      "mathematics from the ordinal level (equality and set membership) and we can also add the\n",
      "following to the list of operations allowed in the nominal level:\n",
      "Ordering\n",
      "Comparison\n",
      "\n",
      "Ordering refers to the natural order provided to us by the data 53\n",
      "However, this can be tricky\n",
      "to figure out sometimes 11\n",
      "When speaking about the spectrum of visible light, we can refer to\n",
      "the names of colors —Red, Orange , Yellow , Green , Blue , Indigo , and Violet 34\n",
      "Naturally, as\n",
      "we move from left to right, the light is gaining energy and other properties 19\n",
      "We may refer to\n",
      "this as a natural order:\n",
      "The natural order of color\n",
      "However, if needed, an artist may impose another order on the data, such as sorting the\n",
      "colors based on the cost of the material to make said color 49\n",
      "This could change the order of\n",
      "the data, but as long as we are consistent in what defines the order, it does not matter what\n",
      "defines it 31\n",
      "\n",
      "Comparisons are another new operation allowed at this level 11\n",
      "At the ordinal level, it would\n",
      "not make sense to say that one country was naturally  better than another or that one part of\n",
      "speech is worse than another 33\n",
      "At the ordinal level, we can make these comparisons 10\n",
      "For\n",
      "example, we can talk about how putting a \"7\" on a survey is worse than putting a \"10 24\n",
      "\n",
      "Measures of center\n",
      "At the ordinal level, the median  is usually an appropriate  way of defining the center of the\n",
      "data 28\n",
      "The mean, however, would be impossible because the division is not allowed  at this\n",
      "level 19\n",
      "We can also use the mode as we could at the nominal level 13\n",
      "\n",
      "We will now look at an example of using the median 12\n",
      "\n",
      "Imagine you have conducted a survey among your employees asking \" how happy are you to\n",
      "be working here on a scale from 1-5 29\n",
      ",\" and your results are as follows:\n",
      "5, 4, 3, 4, 5, 3, 2, 5, 3, 2, 1, 4, 5, 3, 4, 4, 5, 4, 2, 1, 4, 5, 4, 3, 2,\n",
      "4, 4, 5, 4, 3, 2, 1\n",
      "\n",
      "Let's use Python to find the median of this data 113\n",
      "It is worth noting that most people would\n",
      "argue that the mean of these scores would work just fine 21\n",
      "The reason that the mean would\n",
      "not be as mathematically viable is that if we subtract/add two scores, say a score of four\n",
      "minus a score of two, the difference of two does not really mean anything 44\n",
      "If\n",
      "addition/subtraction among the scores doesn't make sense, the mean won't make sense\n",
      "either:\n",
      "import numpy\n",
      "results = [5, 4, 3, 4, 5, 3, 2, 5, 3, 2, 1, 4, 5, 3, 4, 4, 5, 4, 2, 1, 4,\n",
      "5, 4, 3, 2, 4, 4, 5, 4, 3, 2, 1]\n",
      "sorted_results = sorted(results)\n",
      "print(sorted_results)\n",
      "'''\n",
      "[1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "5, 5, 5, 5, 5, 5, 5]\n",
      "'''\n",
      "print(numpy 232\n",
      "mean(results)) # == 3 7\n",
      "4375\n",
      "print(numpy 6\n",
      "median(results)) # == 4 7\n",
      "0\n",
      "The ''' (triple apostrophe) denotes a longer (over two lines) comment 20\n",
      "It\n",
      "acts in a way similar to # 9\n",
      "\n",
      "It turns out that the median is not only more sound but makes the survey results look much\n",
      "better 21\n",
      "\n",
      "Quick recap and check\n",
      "So far, we have seen half of the levels of data:\n",
      "The nominal level\n",
      "The ordinal level\n",
      "\n",
      "At the nominal level, we deal with data usually described using vocabulary (but sometimes\n",
      "with numbers), with no order and little use of mathematics 55\n",
      "\n",
      "At the ordinal level, we have data that can be described with numbers and we also have a\n",
      "\"natural\" order, allowing us to put one in front of the other 36\n",
      "\n",
      "Let's try to classify the following example as either ordinal or nominal ( answers are at the\n",
      "end of the chapter ):\n",
      "The origin of the beans in your cup of coffee\n",
      "The place someone receives after completing a foot race\n",
      "The metal used to make the medal that they receive after placing in said race\n",
      "The telephone number of a client\n",
      "How many cups of coffee you drink in a day\n",
      "The interval level\n",
      "Now, we are getting somewhere interesting 91\n",
      "At the interval  level, we are beginning to look\n",
      "at data that can be expressed through very quantifiable means, and where much more\n",
      "complicated mathematical  formulas are allowed 36\n",
      "The basic difference between the ordinal\n",
      "level and the interval level is, well, just that difference 19\n",
      "\n",
      "Data at the interval level allows meaningful subtraction between data points 12\n",
      "\n",
      "Example\n",
      "Temperature is a great example of data  at the interval level 15\n",
      "If it is 100 degrees Fahrenheit in\n",
      "Texas and 80 degrees Fahrenheit in Istanbul, Turkey, then Texas is 20 degrees warmer than\n",
      "Istanbul 31\n",
      "This simple example allows for so much more manipulation at this level than\n",
      "previous examples 16\n",
      "\n",
      "(Non) Example\n",
      "It seems as though the example in the ordinal level (using the one to five survey) fits the bill\n",
      "of the interval level 32\n",
      "However, remember that the difference  between the scores (when you\n",
      "subtract them) does not make sense; therefore, this data cannot be called at the interval\n",
      "level 34\n",
      "\n",
      "\n",
      "Mathematical operations allowed\n",
      "We can use all the operations allowed  on the lower levels (ordering, comparisons, and so\n",
      "on), along with two other notable operations:\n",
      "Addition\n",
      "Subtraction\n",
      "The allowance of these two operations allows us to talk about data at this level in a whole\n",
      "new way 63\n",
      "\n",
      "Measures of center\n",
      "At this level, we can use the median  and mode to describe this data 22\n",
      "However, usually the\n",
      "most accurate description of the center of data would be the arithmetic mean , more\n",
      "commonly referred to as simply the mean 29\n",
      "Recall that the definition of the mean  requires us\n",
      "to add together all the measurements 17\n",
      "At the previous levels, an addition was meaningless 9\n",
      "\n",
      "Therefore, the mean would have lost extreme value 10\n",
      "It is only at the interval level and\n",
      "above that the arithmetic mean makes sense 16\n",
      "\n",
      "We will now look at an example of using the mean 12\n",
      "\n",
      "Suppose we look at the temperature of a fridge containing a pharmaceutical company's\n",
      "new vaccine 19\n",
      "We measure the temperature every hour with the following data points (in\n",
      "Fahrenheit):\n",
      "31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26\n",
      "Using Python again, let's find the mean and median of the data:\n",
      "import numpy\n",
      "temps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\n",
      "print(numpy 128\n",
      "mean(temps))    # == 30 9\n",
      "73\n",
      "print(numpy 5\n",
      "median(temps))  # == 31 9\n",
      "0\n",
      "\n",
      "Note how the mean and median are quite close to each other and both are around 31\n",
      "degrees 23\n",
      "The question, on average, how cold is the fridge 11\n",
      ",  has an answer of about 31 9\n",
      "\n",
      "However, the vaccine comes with a warning:\n",
      "\"Do not keep this vaccine at a temperature under 29 degrees 22\n",
      "\n",
      "Note that at least twice the temperature dropped below 29 degrees, but you ended up\n",
      "assuming that it isn't enough for it to be detrimental 30\n",
      "\n",
      "This is where the measure of variation can help us understand how bad the fridge situation\n",
      "can be 20\n",
      "\n",
      "Measures of variation\n",
      "This is something new that we have  not yet discussed 17\n",
      "It is one thing to talk about the\n",
      "center of the data but, in data science, it is also very important to mention how \"spread out\"\n",
      "the data is 34\n",
      "The measures that describe this phenomenon are called measures of variation 11\n",
      "\n",
      "You have  likely heard of \"standard deviation\" from your statistics classes 15\n",
      "This idea is\n",
      "extremely important and I would like to address it briefly 15\n",
      "\n",
      "A measure of variation (such as the standard deviation) is a number that attempts to\n",
      "describe how spread out the data is 26\n",
      "\n",
      "Along with a measure of center, a measure of variation can almost entirely describe a\n",
      "dataset with only two numbers 23\n",
      "\n",
      "Standard deviation\n",
      "Arguably, the standard deviation is the most common measure  of variation of data at the\n",
      "interval level and beyond 27\n",
      "The standard deviation can be thought of as the \"average\n",
      "distance a data point is at from the mean 21\n",
      "While this description is technically and\n",
      "mathematically incorrect, it is a good way to think about it 21\n",
      "The formula for standard\n",
      "deviation can be broken down into the following steps:\n",
      "Find the mean of the data1 23\n",
      "\n",
      "For each number in the dataset, subtract it from the mean and then square it2 18\n",
      "\n",
      "Find the average of each square difference3 9\n",
      "\n",
      "Take the square root of the number obtained in Step 3  and this is the standard 4 21\n",
      "\n",
      "deviation\n",
      "Notice how, in the steps, we do actually take an arithmetic mean as one of the steps 23\n",
      "\n",
      "\n",
      "For example, look back at the temperature dataset 10\n",
      "Let's find the standard deviation of the\n",
      "dataset using Python:\n",
      "import numpy\n",
      "temps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\n",
      "mean = numpy 66\n",
      "mean(temps)    # == 30 9\n",
      "73\n",
      "squared_differences = []\n",
      "# empty list o squared differences\n",
      "for temperature in temps:\n",
      "    difference = temperature - mean\n",
      " # how far is the point from the mean\n",
      "    squared_difference = difference**2\n",
      "    # square the difference\n",
      "    squared_differences 56\n",
      "append(squared_difference)\n",
      "    # add it to our list\n",
      "average_squared_difference = numpy 18\n",
      "mean(squared_differences)\n",
      "# This number is also called the \"Variance\"\n",
      "standard_deviation = numpy 22\n",
      "sqrt(average_squared_difference)\n",
      "# We did it 10\n",
      "\n",
      "print(standard_deviation)  # == 2 12\n",
      "5157\n",
      "All of this code led to us find out that the standard deviation of the dataset is around 2 24\n",
      "5,\n",
      "meaning that \"on average,\" a data point is 2 15\n",
      "5 degrees off from the average temperature of\n",
      "around 31 degrees, meaning that the temperature could likely dip below 29 degrees again\n",
      "in the near future 32\n",
      "\n",
      "The reason we want the \"square difference\" between each point and the\n",
      "mean and not the \"actual difference\" is because squaring the value\n",
      "actually puts emphasis on outliers —data points that are abnormally far\n",
      "away 46\n",
      "\n",
      "Measures of variation give us a very clear picture of how spread out or dispersed our data\n",
      "is 21\n",
      "This is especially important when we are concerned with ranges of data and how data\n",
      "can fluctuate (think percentage return on stocks) 26\n",
      "\n",
      "\n",
      "The big difference between data at this level and at the next level lies in something that is\n",
      "not obvious 22\n",
      "\n",
      "Data at the interval level does not have a \"natural starting point or a natural zero 18\n",
      "\n",
      "However, being at zero degrees Celsius does not mean that you have \"no temperature\" 18\n",
      "\n",
      "The ratio level\n",
      "Finally, we will take a look  at the ratio level 17\n",
      "After moving through three different levels\n",
      "with differing levels of allowed mathematical operations, the ratio level proves to be the\n",
      "strongest of the four 28\n",
      "\n",
      "Not only can we define order  and difference, but the ratio level also allows us to multiply\n",
      "and divide  as well 26\n",
      "This might seem like not much to make a fuss over but it changes almost\n",
      "everything about the way we view data at this level 26\n",
      "\n",
      "Examples\n",
      "While Fahrenheit and Celsius are stuck  in the interval level, the Kelvin scale of temperature\n",
      "boasts a natural zero 26\n",
      "A measurement of zero Kelvin literally means the absence of heat 11\n",
      "It\n",
      "is a non-arbitrary starting zero 9\n",
      "We can actually scientifically say that 200 Kelvin is twice as\n",
      "much heat as 100 Kelvin 19\n",
      "\n",
      "Money in the bank is at the ratio level 10\n",
      "You can have \"no money in the bank\" and it makes\n",
      "sense that $200,000 is \"twice as much as\" $100,000 32\n",
      "\n",
      "Many people may argue that Celsius and Fahrenheit also have a starting\n",
      "point (mainly because we can convert from Kelvin to either of the two) 30\n",
      "\n",
      "The real difference here might seem silly, but because the conversion to\n",
      "Celsius and Fahrenheit make the calculations go into the negative, it does\n",
      "not define a clear and \"natural\" zero 39\n",
      "\n",
      "Measures of center\n",
      "The arithmetic mean still holds meaning  at this level, as does a new type of mean called the\n",
      "geometric mean 30\n",
      "This measure is generally not used as much, even at the ratio  level, but is\n",
      "worth mentioning 21\n",
      "It is the square root of the product of all the values 12\n",
      "\n",
      "\n",
      "For example, in our fridge temperature data, we can calculate the geometric mean as shown\n",
      "here:\n",
      "import numpy\n",
      "temps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\n",
      "num_items = len(temps)\n",
      "product = 1 82\n",
      "\n",
      "for temperature in temps:\n",
      "    product *= temperature\n",
      "geometric_mean = product**(1 18\n",
      "/num_items)\n",
      "print(geometric_mean)   # == 30 15\n",
      "634\n",
      "Note again how it is close to the arithmetic mean and median as calculated before 18\n",
      "This is\n",
      "not always the case and will be talked about at great length in the statistics chapter of this\n",
      "book 23\n",
      "\n",
      "Problems with the ratio level\n",
      "Even with all of this added  functionality at this level, we must generally also make a very\n",
      "large assumption that actually makes the ratio level a bit restrictive 39\n",
      "\n",
      "Data at the ratio level is usually non-negative 10\n",
      "\n",
      "For this reason alone, many data scientists prefer the interval level to the ratio level 17\n",
      "The\n",
      "reason for this restrictive property is because if we allowed negative values, the ratio might\n",
      "not always make sense 23\n",
      "\n",
      "Consider that we allowed debt to occur in our money in the bank example 15\n",
      "If we had a\n",
      "balance of $50,000, the following ratio would not really make sense at all:\n",
      "\n",
      "\n",
      "Data is in the eye of the beholder\n",
      "It is possible to impose structure  on data 42\n",
      "For example, while I said that you technically\n",
      "cannot use a mean for the one to five data at the ordinal scale, many statisticians would not\n",
      "have a problem using this number as a descriptor of the dataset 43\n",
      "\n",
      "The level at which you are interpreting data is a huge  assumption that should be made at\n",
      "the beginning of any analysis 25\n",
      "If you are looking at data that is generally thought of at the\n",
      "ordinal level and applying tools such as the arithmetic mean and standard deviation, this is\n",
      "something that data scientists must be aware of 39\n",
      "This is mainly because if you continue to\n",
      "hold these assumptions as valid in your analysis, you may encounter problems 22\n",
      "For\n",
      "example, if you also assume divisibility at the ordinal level by mistake, you are imposing a\n",
      "structure where the structure may not exist 29\n",
      "\n",
      "Summary\n",
      "The type of data that you are working with is a very large piece of data science 20\n",
      "It must\n",
      "precede most of your analysis because the type of data you have impacts the type of\n",
      "analysis that is even possible 26\n",
      "\n",
      "Whenever you are faced with a new dataset, the first three questions you should ask about\n",
      "it are the following:\n",
      "Is the data organized or unorganized 31\n",
      "For example, does our data exist in a nice,\n",
      "clean row/column structure 15\n",
      "\n",
      "Is each column quantitative or qualitative 7\n",
      "For example, are the values\n",
      "numbers, strings, or do they represent quantities 16\n",
      "\n",
      "At what level is the data in each column 10\n",
      "For example, are the values at the\n",
      "nominal, ordinal, interval, or ratio level 19\n",
      "\n",
      "The answers to these questions will not only impact your knowledge of the data at the end\n",
      "but will also dictate the next steps of your analysis 29\n",
      "They will dictate the types of graphs\n",
      "you are able to use and how you interpret them in your upcoming data models 23\n",
      "Sometimes,\n",
      "we will have to convert from one level to another in order to gain more perspective 18\n",
      "In the\n",
      "coming chapters, we will take a much deeper look at how to deal with and explore data at\n",
      "different levels 25\n",
      "\n",
      "\n",
      "By the end of this book, we will be able to not only recognize data at different levels, but\n",
      "will also know how to deal with it at these levels 34\n",
      "In the next chapter, we will review how\n",
      "types of data are used by data scientists to do data discovery and visualization 24\n",
      "\n",
      "Answers for classification of the following example as either ordinal or nominal:\n",
      "The origin of the beans in your cup of coffee : Nominal\n",
      "The place someone receives after completing a foot race : Ordinal\n",
      "The metal used to make the medal that they receive after placing in the race :\n",
      "Nominal\n",
      "The telephone number of a client : Nominal\n",
      "How many cups of coffee you drink in a day : Ordinal\n",
      "\n",
      "\n",
      "The Five Steps of Data Science\n",
      "We have spent much time looking at the preliminaries of data science, including outlining\n",
      "the types of data and how to approach datasets depending on their type 122\n",
      "\n",
      "In this chapter, in addition to the introduction of data science, we will focus on the\n",
      "following topics:\n",
      "Steps to perform data science\n",
      "Data exploration\n",
      "Data visualization\n",
      "We will use the Python packages pandas  and matplotlib  to explore different datasets 50\n",
      "\n",
      "Introduction to data science\n",
      "Many people ask me the biggest difference between data science and data analytics 19\n",
      "While\n",
      "some can argue  that there is no difference between the two, many will argue that there are\n",
      "hundreds 24\n",
      "I believe that regardless of how many differences there are between the two\n",
      "terms, the biggest is that data science follows a structured, step-by-step process that, when\n",
      "followed, preserves the integrity of the results 43\n",
      "\n",
      "Like any other scientific endeavor, this process must be adhered to, or else the analysis and\n",
      "the results are in danger of scrutiny 28\n",
      "On a simpler level, following a strict process can make\n",
      "it much easier for amateur data scientists to obtain results faster than if they were exploring\n",
      "data with no clear vision 34\n",
      "\n",
      "While these steps are a guiding lesson for amateur analysts, they also provide the\n",
      "foundation for all data scientists, even those in the highest levels of business and academia 33\n",
      "\n",
      "Every data scientist recognizes the value of these steps and follows them in some way or\n",
      "another 19\n",
      "\n",
      "\n",
      "Overview of the five steps\n",
      "The five essential steps to perform data  science are as follows:\n",
      "Asking an interesting question1 26\n",
      "\n",
      "Obtaining the data2 7\n",
      "\n",
      "Exploring the data3 6\n",
      "\n",
      "Modeling the data4 6\n",
      "\n",
      "Communicating and visualizing the results5 9\n",
      "\n",
      "First, let's look at the five steps with reference to the big picture 16\n",
      "\n",
      "Asking an interesting question\n",
      "This is probably my favorite step 13\n",
      "As an entrepreneur, I ask myself (and others) interesting\n",
      "questions every day 16\n",
      "I would treat this step as you would treat a brainstorming session 13\n",
      "\n",
      "Start writing down questions regardless of whether or not you think the data  to answer\n",
      "these questions even exists 22\n",
      "The reason for this is twofold 8\n",
      "First off, you don't want to start\n",
      "biasing yourself even before searching for data 18\n",
      "Secondly, obtaining data might involve\n",
      "searching in both public and private locations and, therefore, might not be very\n",
      "straightforward 26\n",
      "You might ask a question and immediately tell yourself \"Oh, but I bet\n",
      "there's no data out there that can help me\" and cross it off your list 33\n",
      "Don't do that 4\n",
      "Leave it\n",
      "on your list 6\n",
      "\n",
      "Obtaining the data\n",
      "Once you have selected the question  you want to focus on, it is time to scour the world for\n",
      "the data that might be able to answer that question 39\n",
      "As mentioned before, the data can\n",
      "come from a variety of sources; so, this step can be very creative 23\n",
      "\n",
      "Exploring the data\n",
      "Once we have the data, we use the lessons  learned in Chapter 2 , Types of Data , and begin to\n",
      "break down the types of data that we are dealing with 42\n",
      "This is a pivotal step in the process 8\n",
      "\n",
      "Once this step is completed, the analyst has generally  spent several hours learning about\n",
      "the domain, using code or other tools to manipulate and explore the data, and has a very\n",
      "good sense of what the data might be trying to tell them 50\n",
      "\n",
      "\n",
      "Modeling the data\n",
      "This step involves the use of statistical  and machine learning models 18\n",
      "In this step, we are\n",
      "not only fitting and choosing models, but we are also implanting mathematical validation\n",
      "metrics in order to quantify the models and their effectiveness 33\n",
      "\n",
      "Communicating and visualizing the results\n",
      "This is arguably the most important step 16\n",
      "While it might seem obvious and simple, the\n",
      "ability to conclude your  results in a digestible format is much more difficult than it seems 28\n",
      "\n",
      "We will look at different  examples of cases when results were communicated poorly and\n",
      "when they were displayed very well 23\n",
      "\n",
      "In this book, we will focus mainly on steps 3, 4, and 5 20\n",
      "\n",
      "Why are we skipping steps 1 and 2 in this book 14\n",
      "\n",
      "While the first two steps are undoubtedly imperative to the process, they\n",
      "generally precede statistical and programmatic systems 24\n",
      "Later in this\n",
      "book, we will touch upon the different ways to obtain data; however, for\n",
      "the purpose of focusing on the more scientific aspects of the process, we\n",
      "will begin with exploration right away 42\n",
      "\n",
      "Exploring the data\n",
      "The process of exploring data is not simply defined 15\n",
      "It involves the ability to recognize the\n",
      "different types of data, transform data types, and use code to systemically improve the \n",
      "quality  of the entire dataset to prepare it for the modeling stage 39\n",
      "In order to best represent\n",
      "and teach the art of exploration, I will present several different datasets and use the Python\n",
      "package pandas  to explore the data 31\n",
      "Along the way, we will run into different tips and\n",
      "tricks on how to handle data 19\n",
      "\n",
      "There are three basic questions we should ask ourselves when dealing with a new dataset\n",
      "that we have not seen before 23\n",
      "Keep in mind that these questions are not the beginning and\n",
      "the end of data science; they are some guidelines that should be followed when exploring a\n",
      "newly obtained set of data 36\n",
      "\n",
      "\n",
      "Basic questions for data exploration\n",
      "When looking at a new dataset, whether it is familiar to you or not, it is important to use\n",
      "the following questions  as guidelines for your preliminary analysis:\n",
      "Is the data organized or not 46\n",
      ": We are checking for whether or not the data is\n",
      "presented in a row/column structure 19\n",
      "For the most part, data will be presented in\n",
      "an organized fashion 14\n",
      "In this book, over 90% of our examples will begin with\n",
      "organized data 17\n",
      "Nevertheless, this is the most basic question that we can answer\n",
      "before diving any deeper into our analysis 20\n",
      "A general rule of thumb is that if we\n",
      "have unorganized data, we want to transform it into a row/column structure 25\n",
      "For\n",
      "example, earlier in this book, we looked at ways to transform text into a\n",
      "row/column structure by counting the number of words/phrases 30\n",
      "\n",
      "What does each row represent 6\n",
      ": Once we have an answer to how the data is\n",
      "organized and are looking at a nice row/column-based dataset, we should\n",
      "identify what each row actually represents 33\n",
      "This step is usually very quick and\n",
      "can help put things into perspective much more quickly 17\n",
      "\n",
      "What does each column represent 6\n",
      ": We should identify each column by the level\n",
      "of data and whether or not it is quantitative/qualitative, and so on 26\n",
      "This\n",
      "categorization might change as our analysis progresses, but it is important to\n",
      "begin this step as early as possible 25\n",
      "\n",
      "Are there any missing data points 7\n",
      ": Data isn't perfect 5\n",
      "Sometimes, we might be\n",
      "missing data because of human or mechanical error 14\n",
      "When this happens, we, as\n",
      "data scientists, must make decisions about how to deal with these discrepancies 21\n",
      "\n",
      "Do we need to perform any transformations on the columns 11\n",
      ": Depending on\n",
      "the level/type of data in each column, we might need to perform certain types of\n",
      "transformation 24\n",
      "For example, generally speaking, for the sake of statistical\n",
      "modeling and machine learning, we would like each column to be numerical, so\n",
      "would use Python to make any transformations 36\n",
      "\n",
      "All the while, we are asking ourselves the overall question, what can we infer from the\n",
      "preliminary inferential statistics 26\n",
      "We want to be able to understand our data better than when\n",
      "we first found it 17\n",
      "\n",
      "Enough talk, let's see an example in the following section 13\n",
      "\n",
      "\n",
      "Dataset 1 – Yelp\n",
      "The first dataset we will look at is a public  dataset made available by the restaurant review\n",
      "site, Yelp 29\n",
      "All personally identifiable information has been removed 7\n",
      "Let's read  in the data\n",
      "first, as shown here:\n",
      "import pandas as pd\n",
      "yelp_raw_data = pd 25\n",
      "read_csv(\"yelp 5\n",
      "csv\")\n",
      "yelp_raw_data 6\n",
      "head()\n",
      "A quick recap of what the preceding code does:\n",
      "Imports the pandas  package and nickname it pd 1 23\n",
      "\n",
      "Reads in the 5\n",
      "csv  from the web; call is yelp_raw_data 2 14\n",
      "\n",
      "Looks at the head  of the data (just the first few rows) 3 18\n",
      "\n",
      "We get the following:\n",
      "Is the data organized or not 12\n",
      ":\n",
      "Because we have a nice row/column structure, we can conclude that this data\n",
      "seems pretty organized 21\n",
      "\n",
      "\n",
      "What does each row represent 6\n",
      ":\n",
      "It seems pretty obvious that each row represents a user giving a review of a\n",
      "business 18\n",
      "The next thing we should do is examine each row and label it by the\n",
      "type of data it contains 21\n",
      "At this point, we can also use Python to figure out just\n",
      "how big our dataset is 19\n",
      "We can use the shape  quality of a DataFrame to find this\n",
      "out, as shown:\n",
      "yelp_raw_data 23\n",
      "shape\n",
      "# (10000,10)\n",
      "It tells us that this dataset has 10000  rows and 10 columns 25\n",
      "Another way to say\n",
      "this is that this dataset has 10,000 observations and 10 characteristics 20\n",
      "\n",
      "What does each column represent 6\n",
      "(Note that we have 10 columns) :\n",
      "business_id : This is likely to be a unique identifier for the business the review\n",
      "is for 29\n",
      "This would  be at the nominal level  because there is no natural order to this\n",
      "identifier 19\n",
      "\n",
      "date : This is probably the date on which the review was posted 14\n",
      "Note that it\n",
      "seems to be only specific to the day, month, and year 18\n",
      "Even though time is \n",
      "usually  considered continuous, this column would likely be considered discrete\n",
      "and at the ordinal level  because of the natural order that dates have 32\n",
      "\n",
      "review_id : This is likely to be a unique identifier for the review that each post\n",
      "represents 21\n",
      "This would be at the nominal level  because, again, there is no natural\n",
      "order to this identifier 21\n",
      "\n",
      "stars : From a quick look (don't worry; we will perform some further analysis\n",
      "soon), we can see that this is an ordered column that represents what the\n",
      "reviewer gave the restaurant as a final score 44\n",
      "This is ordered and qualitative, so is\n",
      "at the ordinal level 13\n",
      "\n",
      "text : This is probably the raw text that each reviewer wrote 13\n",
      "As with most text,\n",
      "we place this at the nominal level 12\n",
      "\n",
      "type : In the first five columns, all we see is the word review 16\n",
      "This might be a\n",
      "column that identifies that each row is a review, implying that there might be\n",
      "another type of row other than a review 29\n",
      "We will take a look at this later 8\n",
      "We\n",
      "place this at the nominal level 8\n",
      "\n",
      "user_id : This is likely to be a unique identifier for the user who is writing the\n",
      "review 21\n",
      "Just like the other unique IDs, we place this data at the nominal level 15\n",
      "\n",
      "\n",
      "Note that after we have looked at all of the columns, and found that all of\n",
      "the data is either at the ordinal or nominal level, we have to look at the\n",
      "following things 39\n",
      "This is not uncommon, but it is worth mentioning 10\n",
      "\n",
      "Are there any missing data points 7\n",
      ":\n",
      "Perform an isnull  operation 7\n",
      "For example, if your DataFrame is called\n",
      "awesome_dataframe , then try the Python command\n",
      "awesome_dataframe 20\n",
      "isnull() 3\n",
      "sum() , which will show the number of\n",
      "missing values in each column 15\n",
      "\n",
      "Do we need to perform any transformations on the columns 11\n",
      ":\n",
      "At this point, we are looking for a few things 12\n",
      "For example, will we need to\n",
      "change the scale of some of the quantitative data, or do we need to create dummy\n",
      "variables for the qualitative variables 31\n",
      "As this dataset only has qualitative\n",
      "columns, we can only focus on transformations at the ordinal and nominal scale 21\n",
      "\n",
      "Before starting, let's go over some quick terminology for pandas, the Python data\n",
      "exploration module 21\n",
      "\n",
      "DataFrames\n",
      "When we read in a dataset, pandas creates a custom  object called a DataFrame 20\n",
      "Think of\n",
      "this as the Python version of a spreadsheet (but way better) 16\n",
      "In this case, the variable,\n",
      "yelp_raw_data , is a DataFrame 15\n",
      "\n",
      "To check whether this is true in Python, type in the following code:\n",
      "type(yelp_raw_data)\n",
      "# pandas 24\n",
      "core 1\n",
      "frame 1\n",
      "DataFrame\n",
      "DataFrames are two-dimensional in nature, meaning that they are organized in a\n",
      "row/column structure just as spreadsheets are 26\n",
      "The main benefits of using DataFrames over,\n",
      "say, spreadsheet software would be that a DataFrame can handle much larger data than\n",
      "most common spreadsheet software 29\n",
      "If you are familiar with the R language, you might\n",
      "recognize the word DataFrame 17\n",
      "This is because the name was actually borrowed from the\n",
      "language 12\n",
      "\n",
      "As most of the data that we will deal with is organized, DataFrames are likely to be the\n",
      "most used object in pandas, second only to the Series  object 35\n",
      "\n",
      "\n",
      "Series\n",
      "The Series  object is simply a DataFrame, but only with one dimension 17\n",
      "Essentially, it is a\n",
      "list of data  points 11\n",
      "Each column of a DataFrame is considered to be a Series  object 13\n",
      "Let's\n",
      "check this —the first thing we need to do is grab a single column from our DataFrame; we\n",
      "generally use what is known as bracket notation 33\n",
      "The following is an example:\n",
      "yelp_raw_data['business_id'] # grabs a single column of the Dataframe\n",
      "We will list the first and last few rows:\n",
      "0     9yKzy9PApeiPPOUJEtnvkg\n",
      "1     ZRJwVLyzEJq1VAihDhYiow\n",
      "2     6oRAC4uyJCsJl1X0WZpVSA\n",
      "3     _1QQZuf4zZOyFCvXc0o6Vg\n",
      "4     6ozycU1RpktNG2-1BroVtw\n",
      "5     -yxfBYGB6SEqszmxJxd97A\n",
      "6     zp713qNhx8d9KCJJnrw1xA\n",
      "Let's use the type function to check that this column is a Series :\n",
      "type(yelp_raw_data['business_id'])\n",
      "# pandas 194\n",
      "core 1\n",
      "series 1\n",
      "Series\n",
      "Exploration tips for qualitative data\n",
      "Using these two pandas objects, let's start performing  some preliminary data exploration 24\n",
      "\n",
      "For qualitative  data, we will specifically look at the nominal and ordinal levels 16\n",
      "\n",
      "Nominal level columns\n",
      "As we are at the nominal level, let's recall that at this level, data is qualitative and is\n",
      "described purely by name 33\n",
      "In this dataset, this refers to business_id , review_id , text ,\n",
      "type , and user_id 20\n",
      "Let's use pandas in order to dive a bit deeper, as shown here:\n",
      "yelp_raw_data['business_id'] 24\n",
      "describe()\n",
      "The output is as follows:\n",
      "# count                      10000\n",
      "# unique                      4174\n",
      "# top       ntN85eu27C04nwyPa8IHtw\n",
      "# freq                          37\n",
      "\n",
      "The describe  function will give us some quick stats about the column whose name we\n",
      "enter into the quotation marks 67\n",
      "Note how pandas automatically recognized that\n",
      "business_id  was a qualitative column and gave us stats that make sense 21\n",
      "When\n",
      "describe  is called on a qualitative column, we will always get the following four items:\n",
      "count : How many values are filled in\n",
      "unique : How many unique values are filled in\n",
      "top: The name of the most common item in the dataset\n",
      "freq : How often the most common item appears in the dataset\n",
      "At the nominal level, we are usually looking for a few things, which would signal a\n",
      "transformation:\n",
      "Do we have a reasonable number (usually under 20) of unique items 102\n",
      "\n",
      "Is this column free text 6\n",
      "\n",
      "Is this column completely unique across all rows 9\n",
      "\n",
      "So, for the business_id  column, we have a count of 10000 18\n",
      "Don't be fooled though 5\n",
      "This\n",
      "does not mean that we have 10,000 businesses being reviewed here 16\n",
      "It just means that of the\n",
      "10,000 rows of reviews, the business_id  column is filled in all 10,000 times 28\n",
      "The next\n",
      "qualifier, unique , tells us that we have 4174  unique businesses being reviewed in this\n",
      "dataset 25\n",
      "The most reviewed business is business JokKtdXU7zXHcr20Lrk29A , which was\n",
      "reviewed 37 times:\n",
      "yelp_raw_data['review_id'] 40\n",
      "describe()\n",
      "The output is as follows:\n",
      "# count                      10000\n",
      "# unique                     10000\n",
      "# top       M3jTv5NIipi_N4mgmZiIEg\n",
      "# freq                           1\n",
      "We have a count  of 10000  and a unique  of 10000 65\n",
      "Think for a second, does this make\n",
      "sense 10\n",
      "Think about what each row represents and what this column represents 11\n",
      "\n",
      "(Insert Jeopardy theme song here)\n",
      "\n",
      "Of course, it does 15\n",
      "Each row of this dataset is supposed to represent a single, unique review\n",
      "of a business and this column is meant to serve as a unique identifier for a review; so, it\n",
      "makes sense that the review_id  column has 10000  unique items in it 54\n",
      "So, why is eTa5KD-\n",
      "LTgQv6UT1Zmijmw  the most common  review 26\n",
      "This is just a random choice from the 10,000\n",
      "and means nothing:\n",
      "yelp_raw_data['text'] 24\n",
      "describe()\n",
      "The output is as follows:\n",
      "count                                                 10000\n",
      "unique                                                 9998\n",
      "top       This review is for the chain in general 30\n",
      "The l 2\n",
      " 1\n",
      " 1\n",
      "\n",
      "freq                                                      2\n",
      "This column, which represents the actual text people wrote, is interesting 19\n",
      "We would\n",
      "imagine that this should also be similar to review_id  in that each one should contain\n",
      "unique text, because it would be weird if two people wrote exactly the same thing; but we\n",
      "have two reviews with the exact same text 50\n",
      "Let's take a second to learn about DataFrame\n",
      "filtering to examine this further 16\n",
      "\n",
      "Filtering in pandas\n",
      "Let's talk a bit about how  filtering works 16\n",
      "Filtering rows based on certain criteria is quite\n",
      "easy in pandas 12\n",
      "In a DataFrame, if we wish to filter out rows based on some search criteria,\n",
      "we will need to go row by row and check whether or not a row satisfies that particular\n",
      "condition; pandas handles this by passing in a Series  of True  and False  (Booleans) 57\n",
      "\n",
      "We literally pass into the DataFrame a list of True  and False  data that mean the following:\n",
      "True : This row satisfies the condition\n",
      "False : This row does not satisfy the condition\n",
      "So, first, let's make the conditions 48\n",
      "In the following lines of code, I will grab the text that\n",
      "occurs twice:\n",
      "yelp_raw_data['text'] 25\n",
      "describe()['top']\n",
      "\n",
      "Here is a snippet of the text:\n",
      "\"This review is for the chain in general 20\n",
      "The location we went to is new so\n",
      "it isn't in Yelp yet 15\n",
      "Once it is I will put this review there as\n",
      "well 12\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "Right off the bat, we can guess that this might actually be one person who went to review\n",
      "two businesses that belong to the same chain and wrote the exact same review 35\n",
      "However,\n",
      "this is just a guess right now 9\n",
      "\n",
      "The duplicate_text  variable is of  string  type 12\n",
      "\n",
      "Now that we have this text, let's use some magic to create that Series  of True  and False :\n",
      "duplicate_text = yelp_raw_data['text'] 34\n",
      "describe()['top']\n",
      "text_is_the_duplicate = yelp_raw_data['text'] == duplicate_text\n",
      "Right away you might be confused 26\n",
      "What we have done here is take the text column of the\n",
      "DataFrame and compared it to the string, duplicate_text 23\n",
      "This is strange because we\n",
      "seem to be comparing a list of 10,000 elements to a single string 23\n",
      "Of course, the answer\n",
      "should be a straight false, right 13\n",
      "\n",
      "Series has a very interesting feature in that if you compare the Series  to an object, it will\n",
      "return another Series of Booleans of the same length where each True  and False  is the\n",
      "answer to the question is this element the same as the element you are comparing it to 59\n",
      "Very\n",
      "handy 4\n",
      "\n",
      "type(text_is_the_duplicate) # it is a Series of Trues and Falses\n",
      "text_is_the_duplicate 24\n",
      "head() # shows a few Falses out of the Series\n",
      "In Python, we can add and subtract true and false as if they were 1 and 0, respectively 36\n",
      "For\n",
      "example, True + False - True + False + True == 1 16\n",
      "So, we can verify that this Series  is correct\n",
      "by adding up all of the values 19\n",
      "As only two of these rows should contain the duplicate text,\n",
      "the sum of the Series should only be 2, which it is 26\n",
      "This is as follows:\n",
      "sum(text_is_the_duplicate) # == 2\n",
      "\n",
      "Now that we have our Series  of Booleans, we can pass it directly into our DataFrame,\n",
      "using bracket notation, and get our filtered rows, as illustrated:\n",
      "filtered_dataframe = yelp_raw_data[text_is_the_duplicate]\n",
      "# the filtered Dataframe\n",
      "filtered_dataframe\n",
      "It seems that our suspicions were correct and one person, on the same day, gave the exact\n",
      "same review to two different business_id , presumably a part of the same chain 105\n",
      "Let's\n",
      "keep moving along to the rest of our columns:\n",
      "yelp_raw_data['type'] 20\n",
      "describe()\n",
      "# count      10000\n",
      "# unique         1\n",
      "# top       review\n",
      "# freq       10000\n",
      "Remember this column 30\n",
      "Turns out they are all the exact same type, namely review :\n",
      "yelp_raw_data['user_id'] 21\n",
      "describe()\n",
      "# count                      10000\n",
      "# unique                      6403\n",
      "# top       fczQCSmaWF78toLEmb0Zsw\n",
      "# freq                          38\n",
      "\n",
      "Similar to the business_id  column, all the 10000  values are filled in with 6403  unique\n",
      "users and one user reviewing 38 times 72\n",
      "\n",
      "In this example, we won't have to perform any transformations 13\n",
      "\n",
      "Ordinal level columns\n",
      "As far as ordinal columns go, we are looking  at date and stars 20\n",
      "For each of these columns,\n",
      "let's look at what the described method brings back:\n",
      "yelp_raw_data['stars'] 24\n",
      "describe()\n",
      "# count    10000 8\n",
      "000000\n",
      "# mean         3 9\n",
      "777500\n",
      "# std          1 9\n",
      "214636\n",
      "# min          1 9\n",
      "000000\n",
      "# 25%          3 11\n",
      "000000\n",
      "# 50%          4 11\n",
      "000000\n",
      "# 75%          5 11\n",
      "000000\n",
      "# max          5 9\n",
      "000000\n",
      "Woah 6\n",
      "Even though this column is ordinal, the describe  method returned stats that we\n",
      "might expect for a quantitative column 22\n",
      "This is because the software saw a bunch of\n",
      "numbers and just assumed that we wanted stats like the mean  or the min and max 27\n",
      "This is\n",
      "not a problem 6\n",
      "Let's use a method called value_counts  to see the count distribution, as\n",
      "shown here:\n",
      "yelp_raw_data['stars'] 27\n",
      "value_counts()\n",
      "# 4    3526\n",
      "# 5    3337\n",
      "# 3    1461\n",
      "# 2     927\n",
      "# 1     749\n",
      "\n",
      "The value_counts  method will return the distribution of values for any column 55\n",
      "In this\n",
      "case, we see that the star rating 4 is the most common, with 3526  values, followed closely\n",
      "by rating 5 32\n",
      "We can also plot this data to get a nice visual 11\n",
      "First, let's sort by star rating, and\n",
      "then use the prebuilt plot  method to make a bar chart:\n",
      "import datetime\n",
      "dates = yelp_raw_data['stars'] 37\n",
      "value_counts()\n",
      "dates 4\n",
      "sort_values\n",
      "dates 4\n",
      "plot(kind='bar')\n",
      "From this graph, we can conclude that people are definitely more likely to give good star\n",
      "ratings over bad ones 27\n",
      "We can follow this procedure for the date column 9\n",
      "I will leave you to\n",
      "try it on your own 11\n",
      "For now, let's look at a new dataset 10\n",
      "\n",
      "Dataset 2 – Titanic\n",
      "The titanic  dataset contains a sample  of people who were  on the Titanic when it struck\n",
      "an iceberg in 1912 34\n",
      "Let's go ahead and import it, as shown here:\n",
      "titanic = pd 17\n",
      "read_csv('short_titanic 7\n",
      "csv')\n",
      "titanic 5\n",
      "head()\n",
      "\n",
      "\n",
      "This table represents the DataFrame for the dataset short_titanic 14\n",
      "csv 1\n",
      "This data is\n",
      "definitely organized in a row/column structure, as is most spreadsheet data 18\n",
      "Let's take a\n",
      "quick peek at its size, as shown here:\n",
      "titanic 18\n",
      "shape\n",
      "# (891, 5)\n",
      "So, we have 891 rows and 5 columns 20\n",
      "Each row seems to represent a single passenger on\n",
      "the ship and as far as columns are concerned, the following list tells us what they indicate:\n",
      "Survived : This is a binary variable that indicates whether or not the passenger\n",
      "survived the accident ( 1 if they survived, 0 if they died) 63\n",
      "This is likely to be at the\n",
      "nominal level because there are only two options 17\n",
      "\n",
      "Pclass : This is the class that the passenger was traveling in ( 3 for third class,\n",
      "and so on) 25\n",
      "This is at the ordinal level 6\n",
      "\n",
      "Name : This is the name of the passenger, and it is definitely at the nominal level 19\n",
      "\n",
      "Sex: This indicates the gender of the passenger 10\n",
      "It is at the nominal level 6\n",
      "\n",
      "Age: This one is a bit tricky 9\n",
      "Arguably, you may place age at either a qualitative\n",
      "or quantitative level; however, I think that age belongs to a quantitative state,\n",
      "and thus, to the ratio level 35\n",
      "\n",
      "As far as transformations are concerned, usually, we want all columns to be numerical,\n",
      "regardless of their qualitative state 24\n",
      "This means that Name  and Sex will have to be\n",
      "converted into numerical columns somehow 17\n",
      "For Sex, we can change the column to hold 1\n",
      "if the passenger was female and 0 if they were male 25\n",
      "Let's use pandas  to make the change 9\n",
      "\n",
      "We will have to import another Python module, called numpy  or numerical Python, as\n",
      "illustrated:\n",
      "import numpy as np\n",
      "titanic['Sex'] = np 35\n",
      "where(titanic['Sex']=='female', 1, 0)\n",
      "\n",
      "The np 17\n",
      "where  method takes in three things:\n",
      "A list of Booleans ( True  or False )\n",
      "A new value\n",
      "A backup value\n",
      "The method will replace all true  with the first value (in this case 1) and the false  with the\n",
      "second value (in this case 0), leaving us with a new numerical column that represents the\n",
      "same thing as the original Sex column:\n",
      "titanic['Sex']\n",
      "# 0     0\n",
      "# 1     1\n",
      "# 2     1\n",
      "# 3     1\n",
      "# 4     0\n",
      "# 5     0\n",
      "# 6     0\n",
      "# 7     0\n",
      "Let's use a shortcut and describe all the columns at once, as shown:\n",
      "titanic 161\n",
      "describe()\n",
      "\n",
      "\n",
      "This table lists descriptive statistics of the titanic  dataset 13\n",
      "Note how our qualitative\n",
      "columns are being treated as quantitative; however, I'm looking for something irrelevant to\n",
      "the data type 25\n",
      "Note the count row: Survived , Pclass , and Sex all have 891 values (the\n",
      "number of rows), but Age only has 714 values 32\n",
      "Some are missing 3\n",
      "To double verify, let's use\n",
      "the pandas functions, called isnull  and sum, as shown:\n",
      "titanic 25\n",
      "isnull() 3\n",
      "sum()\n",
      "Survived      0\n",
      "Pclass        0\n",
      "Name          0\n",
      "Sex           0\n",
      "Age         177\n",
      "This will show us the number of missing values in each column 41\n",
      "So, Age is the only column\n",
      "with missing values to deal with 14\n",
      "\n",
      "When dealing with missing values, you usually have the following two options:\n",
      "Drop the row with the missing value\n",
      "Try to fill it in\n",
      "Dropping the row is the easy choice; however, you run the risk of losing valuable data 48\n",
      "For\n",
      "example, in this case, we have 177 missing age values (891-714), which are nearly 20% of\n",
      "the data 30\n",
      "To fill in the data, we could either go back to the history books, find each person\n",
      "one by one, and fill in their age, or we can fill in the age with a placeholder value 41\n",
      "\n",
      "Let's fill in each missing value of the Age column with the overall average age of the people\n",
      "in the dataset 24\n",
      "For this, we will use two new methods, called mean  and fillna 16\n",
      "We use\n",
      "isnull  to tell us which values are null and the mean  function to give us the average value\n",
      "of the Age column 29\n",
      "The fillna  method is a pandas method that replaces null values with a\n",
      "given value:\n",
      "print sum(titanic['Age'] 27\n",
      "isnull()) # == 177 missing values\n",
      "average_age = titanic['Age'] 18\n",
      "mean() # get the average age\n",
      "titanic['Age'] 14\n",
      "fillna(average_age, inplace = True) #use the fillna method\n",
      "to remove null values\n",
      "print sum(titanic['Age'] 30\n",
      "isnull()) # == 0 missing values\n",
      "\n",
      "We're done 13\n",
      "We have replaced each value with 26 8\n",
      "69 , the average age in the dataset 9\n",
      "The\n",
      "following code now confirms that no null values exist:\n",
      "titanic 15\n",
      "isnull() 3\n",
      "sum()\n",
      "Survived      0\n",
      "Pclass        0\n",
      "Name          0\n",
      "Sex           0\n",
      "Age           0\n",
      "Great 30\n",
      "Nothing is missing, and we did not have to remove any rows:\n",
      "titanic 17\n",
      "head()\n",
      "At this point, we could start getting a bit more complicated with our questions, for\n",
      "example, what is the average age for a female or a male 33\n",
      "To answer this, we can filter by each\n",
      "gender and take the mean age; pandas has a built-in function for this, called groupby , as\n",
      "illustrated here:\n",
      "titanic 39\n",
      "groupby('Sex')['Age'] 7\n",
      "mean()\n",
      "This means group the data by the Sex column, and then give me the mean age for each group 22\n",
      "This\n",
      "gives us the following output:\n",
      "Sex\n",
      "0      30 15\n",
      "505824\n",
      "1      28 8\n",
      "216730\n",
      "We will ask more of these difficult and complex questions and will be able to answer them\n",
      "with Python and statistics 26\n",
      "\n",
      "\n",
      "Summary\n",
      "Although this is only our first look at data exploration, don't worry, this is definitely not the\n",
      "last time we will follow these steps for data science and exploration 36\n",
      "From now on, every\n",
      "time we look at a new piece of data, we will use our steps of exploration to transform, break\n",
      "down, and standardize our data 35\n",
      "The steps outlined in this chapter, while they are only\n",
      "guidelines, form a standard practice that any data scientist can follow in their work 28\n",
      "The\n",
      "steps can also be applied to any dataset that requires analysis 13\n",
      "\n",
      "We are rapidly approaching the section of the book that deals with statistical, probabilistic,\n",
      "and machine learning models 22\n",
      "Before we can truly jump into these models, we have to look\n",
      "at some of the basics of mathematics 21\n",
      "In the next chapter, we will take a look at some of the\n",
      "math necessary to perform some of the more complicated operations in modeling, but don't\n",
      "worry, the math required for this process is minimal, and we will go through it step by\n",
      "step 54\n",
      "\n",
      "\n",
      "\n",
      "Basic Mathematics\n",
      "It's time to start looking at some basic mathematics principles that are handy when dealing\n",
      "with data science 24\n",
      "The word math  tends to strike fear in the hearts of many, but I aim to \n",
      "make  this as enjoyable as possible 26\n",
      "In this chapter, we will go over the basics of the\n",
      "following topics:\n",
      "Basic symbols/terminology\n",
      "Logarithms/exponents\n",
      "Set theory\n",
      "Calculus\n",
      "Matrix (linear) algebra\n",
      "We will also cover other fields of mathematics 48\n",
      "Moreover, we will see how to apply each of\n",
      "these to various aspects of data science, as well as other scientific endeavors 25\n",
      "\n",
      "Recall that, in a previous chapter, we identified math as being one of the three key\n",
      "components of data science 25\n",
      "In this chapter, I will introduce concepts that will become\n",
      "important later on in this book – when looking at probabilistic and statistical models – and\n",
      "we will also be looking at concepts that will be useful in this chapter 44\n",
      "Regardless of this, all\n",
      "of the concepts in this chapter should be considered fundamental to your quest to become a\n",
      "data scientist 25\n",
      "\n",
      "Mathematics as a discipline\n",
      "Mathematics, as a science, is one of the oldest known forms of logical thinking 24\n",
      "Since\n",
      "ancient Mesopotamia (3,000 BCE), and probably before, humans have been relying on \n",
      "arithmetic  and more challenging forms of math to answer life's biggest questions 39\n",
      "\n",
      "\n",
      "Today, we rely on math for most  aspects of our daily lives; yes, I know that sounds like a\n",
      "cliche, but I mean it 32\n",
      "Whether you are watering your plants or feeding your dog, your\n",
      "internal mathematical engine is constantly working —calculating how much water the plant\n",
      "had per day over the last week and predicting the next time your dog will be hungry given\n",
      "that it is eating right now 53\n",
      "Whether or not you are consciously using the principles of math,\n",
      "the concepts live deep inside everyone's brains 20\n",
      "It's my job as a math teacher to get you to\n",
      "realize it 16\n",
      "\n",
      "Basic symbols and terminology\n",
      "In the following section, we will review  the mathematical concepts of vectors, matrices,\n",
      "arithmetic symbols, and linear algebra, as well as some more subtle notations used  by data\n",
      "scientists 46\n",
      "\n",
      "Vectors and matrices\n",
      "A vector  is defined as an object with  both magnitude and direction 19\n",
      "This definition,\n",
      "however, is a bit complicated 9\n",
      "For our purpose, a vector is simply a 1-dimensional array\n",
      "representing a series of numbers 20\n",
      "Put another way, a vector is a list of numbers 11\n",
      "\n",
      "It is generally represented using an arrow or bold font, shown as follows:\n",
      "Vectors are broken into components, which are individual members of the vector 29\n",
      "We use\n",
      "index notations to denote the element that we are referring to, illustrated as follows:\n",
      "In math, we generally refer to the first element as index  1, as opposed to\n",
      "computer science, where we generally refer to the first element as index  0 56\n",
      "\n",
      "It is important to remember which index system you are using 12\n",
      "\n",
      "\n",
      "In Python, we can represent arrays in many ways 11\n",
      "We could simply use a Python list to\n",
      "represent the preceding array:\n",
      "x = [3, 6, 8]\n",
      "However, it is better to use the numpy  array type to represent arrays, as shown, because it\n",
      "gives us much more utility when performing vector operations:\n",
      "import numpy as np\n",
      "x = np 66\n",
      "array([3, 6, 8])\n",
      "Regardless of the Python representation, vectors give us a simple way of storing multiple\n",
      "dimensions  of a single data point/observation 35\n",
      "\n",
      "If we measure the average satisfaction rating (0-100) of employees in three departments of a\n",
      "company as being 57 for HR, 89 for engineering, and 94 for management 39\n",
      "We can represent\n",
      "this as a vector with the following formula:\n",
      "This vector holds three different bits of information about our data 24\n",
      "This is the perfect use\n",
      "of a vector in data science 12\n",
      "\n",
      "You can also think of a vector as being the theoretical generalization of the pandas Series\n",
      "object 20\n",
      "So, naturally, we need something to represent the DataFrame 11\n",
      "\n",
      "We can extend our notion of an array to move beyond a single dimension and represent\n",
      "data in multiple dimensions 22\n",
      "\n",
      "A matrix  is a two-dimensional representation of arrays of numbers 13\n",
      "Matrices (plural) have\n",
      "two main characteristics that we need to be aware of 17\n",
      "The dimension of a matrix, denoted\n",
      "by n x m (n by m ), tells us that the matrix has n rows and m columns 29\n",
      "Matrices are generally\n",
      "denoted by a capital, bold-faced letter, such as X 18\n",
      "Consider the following example:\n",
      "\n",
      "\n",
      "This is a 3 x 2 (3 by 2 ) matrix because it has three rows and two columns 28\n",
      "\n",
      "If a matrix has the same number of rows and columns, it is called a square\n",
      "matrix 20\n",
      "\n",
      "The matrix is our generalization of the pandas DataFrame 11\n",
      "It is arguably one of the most\n",
      "important mathematical objects in our toolkit 14\n",
      "It is used to hold organized information, in\n",
      "our case, data 14\n",
      "\n",
      "Revisiting our previous example, let's say we have three offices in different locations, each\n",
      "with the same three departments: HR, engineering, and management 33\n",
      "We could make three\n",
      "different vectors, each holding a different office's satisfaction scores, as shown:\n",
      "However, this is not only cumbersome, but also unscalable 33\n",
      "What if you have 100 different\n",
      "offices 10\n",
      "Then you would need to have 100 different one-dimensional arrays to hold this\n",
      "information 17\n",
      "\n",
      "This is where a matrix alleviates this problem 10\n",
      "Let's make a matrix where each row\n",
      "represents a different department and each column represents a different office, as shown :\n",
      "This is much more natural 30\n",
      "Now, let's strip away the labels, and we are left with a matrix 16\n",
      "\n",
      "\n",
      "\n",
      "Quick exercises\n",
      "If we added a fourth office, would we need  a new row or column 20\n",
      "1 2\n",
      "\n",
      "What would the dimension of the matrix  be after we added the fourth office 16\n",
      "2 2\n",
      "\n",
      "If we eliminate the management department from the original X matrix, what 3 16\n",
      "\n",
      "would the dimension of the new matrix be 9\n",
      "\n",
      "What is the general formula to find out the number of elements in the matrix 16\n",
      "4 2\n",
      "\n",
      "Answers\n",
      "Column1 5\n",
      "\n",
      "3 x 4 2 7\n",
      "\n",
      "2 x 3 3 7\n",
      "\n",
      "n × m (n being the number  of rows and m being the number  of columns) 4 23\n",
      "\n",
      "Arithmetic symbols\n",
      "In this section, we will go over some  symbols associated with basic arithmetic that appear\n",
      "in most, if not all, data science tutorials and books 35\n",
      "\n",
      "Summation\n",
      "The uppercase sigma ∑ symbol is a universal symbol for addition 17\n",
      "Whatever is to the right\n",
      "of the sigma symbol is usually  something iterable, meaning that we can go over it one by\n",
      "one (for example, a vector) 34\n",
      "\n",
      "For example, let's create the representation of a vector:\n",
      "X = [1, 2, 3, 4, 5]\n",
      "To find the sum of the content, we can use the following formula:\n",
      "In Python, we can use the following formula:\n",
      "sum(x) # == 15\n",
      "\n",
      "For example, the formula for calculating the mean of a series of numbers is quite common 80\n",
      "\n",
      "If we have a vector ( x) of length n, the mean of the vector can be calculated as follows:\n",
      "This means that we will add up each element of x, denoted by xi, and then multiply the sum\n",
      "by 1/n, otherwise known as dividing by n, (the length of the vector) 66\n",
      "\n",
      "Proportional\n",
      "The lowercase alpha symbol, α, represents values  that are proportional to each other 21\n",
      "This\n",
      "means that as one value changes, so does the other 13\n",
      "The direction in which the values move\n",
      "depends on how the values are proportional 15\n",
      "Values can either vary directly or indirectly 7\n",
      "If\n",
      "values vary directly, they both move in the same direction (as one goes up, so does the\n",
      "other) 25\n",
      "If they vary indirectly, they move in opposite directions (if one goes down, the other\n",
      "goes up) 23\n",
      "\n",
      "Consider the following examples:\n",
      "The sales of a company vary directly with the number of customers 18\n",
      "This can be\n",
      "written as Sales α Customers 9\n",
      "\n",
      "Gas prices vary (usually) indirectly with oil availability, meaning that as the\n",
      "availability of oil goes down (it's more scarce), gas prices go up 32\n",
      "This can be\n",
      "denoted as Gas α Availability 10\n",
      "\n",
      "Later on, we will see a very  important formula called the Bayes' formula , which includes a\n",
      "variation symbol 25\n",
      "\n",
      "Dot product\n",
      "The dot product is an operator like addition and multiplication 14\n",
      "It is used to combine two\n",
      "vectors, as shown:\n",
      "\n",
      "\n",
      "So, what does this mean 19\n",
      "Let's say we have a vector that represents a customer's sentiments\n",
      "toward three genres of movies: comedy, romance, and action 27\n",
      "\n",
      "When using a dot product, note that an answer is a single number, known\n",
      "as a scalar 21\n",
      "\n",
      "On a scale of 1-5, a customer loves  comedies, hates romantic movies, and is alright with\n",
      "action movies 28\n",
      "We might represent this as follows:\n",
      "Here, 5 denotes their love for comedies, 1 their hatred of romantic movies, and 3 the\n",
      "customer's indifference toward action movies 37\n",
      "\n",
      "Now, let's assume that we have two new movies, one of which is a romantic comedy and\n",
      "the other is a funny action movie 29\n",
      "The movies would have their own vector of qualities, as\n",
      "shown:\n",
      "Here, m1 is our romantic comedy and m2 is our funny action movie 30\n",
      "\n",
      "In order to make a recommendation, we will apply the dot product between the customer's\n",
      "preferences for each movie 23\n",
      "The higher value will win and, therefore, will be recommended\n",
      "to the user 16\n",
      "\n",
      "Let's compute the recommendation score for each movie 10\n",
      "For movie 1, we want to compute\n",
      "the following:\n",
      "\n",
      "\n",
      "We can think of this problem like as follows:\n",
      "The answer we obtain is 28, but what does this number mean 37\n",
      "On what scale is it 5\n",
      "Well, the\n",
      "best score anyone can ever get is when all values are 5, making the outcome as follows:\n",
      "The lowest possible score is when all values are 1, as shown:\n",
      "So, we must think about 28 on a scale from 3 to 75 56\n",
      "To do this, imagine a number line from 3\n",
      "to 75 and where 28 would be on it 23\n",
      "This is illustrated as follows:\n",
      "Not that far 9\n",
      "Let's try for movie 2:\n",
      "\n",
      "\n",
      "This is higher than 28 14\n",
      "Putting this number on the same timeline as before, we can also\n",
      "visually observe that it is a much better score, as shown:\n",
      "So, between movie 1 and movie 2, we would definitely recommend movie 2 to our user 49\n",
      "\n",
      "This is, in essence, how most movie prediction engines work 13\n",
      "They build a customer profile,\n",
      "which is represented as a vector 12\n",
      "They then take a vector representation of each movie they\n",
      "have to offer, combine them with the customer profile (perhaps with a dot product), and\n",
      "make recommendations from there 34\n",
      "Of course, most companies must do this on a much\n",
      "larger scale, which is where  a particular field of mathematics, called linear algebra , can be\n",
      "very useful; we will look at it later in the chapter 45\n",
      "\n",
      "Graphs\n",
      "No doubt you have encountered dozens, if not hundreds, of graphs in your life so far 22\n",
      "I'd\n",
      "like to mostly talk about conventions with regard to graphs and notations 16\n",
      "\n",
      "The following is a basic Cartesian graph  (x and y coordinates) 15\n",
      "The x and y notations are\n",
      "very standard but sometimes do not entirely explain the big picture 19\n",
      "We sometimes refer  to\n",
      "the x variable as being the independent variable and the y as the dependent variable 21\n",
      "This is\n",
      "because when we write functions, we tend to speak about them as being y is a function of x ,\n",
      "meaning that the value of y is dependent on the value of x 37\n",
      "This is what a graph is trying to\n",
      "show 10\n",
      "\n",
      "Suppose we have two points on a graph, as shown:\n",
      "We refer to the points as (x1, y1) and (x2, y2) 35\n",
      "\n",
      "\n",
      "The slope  between these two points is defined as follows:\n",
      "You have probably seen this formula before, but it is worth mentioning, if only for its\n",
      "significance 34\n",
      "The slope defines the rate of change between the two points 11\n",
      "Rates of change\n",
      "can be very important in data science, specifically in areas involving differential equations\n",
      "and calculus 21\n",
      "\n",
      "Rates of change are a way of representing how variables move together and to what degree 17\n",
      "\n",
      "Imagine we are modeling the temperature of your coffee in relation to the time that it has\n",
      "been sitting outside 22\n",
      "Perhaps we have a rate of change as follows:\n",
      "This rate of change is telling us that for every single minute, our coffee's temperature is\n",
      "dropping by two degrees Fahrenheit 35\n",
      "\n",
      "Later on in this book,  we will look at a machine learning algorithm called linear regression 19\n",
      "\n",
      "In linear regression, we are concerned with the rates of change between variables, as they\n",
      "allow us to exploit this relationship for predictive purposes 28\n",
      "\n",
      "Think of the Cartesian plane as being an infinite plane of vectors with two\n",
      "elements 17\n",
      "When people refer to higher dimensions, such as 3D or 4D, they\n",
      "are merely referring to an infinite space that holds vectors with more\n",
      "elements 33\n",
      "A 3D space holds vectors of length three while a 7D space holds\n",
      "vectors with seven elements in them 25\n",
      "\n",
      "Logarithms/exponents\n",
      "An exponent  tells you how many  times you have  to multiply a number by itself, as\n",
      "illustrated:\n",
      "\n",
      "\n",
      "A logarithm  is the number that answers the question \"what exponent gets me from the base\n",
      "to this other number 55\n",
      "This can be denoted as follows:\n",
      "If these two concepts seem similar, then you are correct 19\n",
      "Exponents and logarithms are\n",
      "heavily related 11\n",
      "In fact, the words exponent and logarithm actually mean the same thing 14\n",
      "A\n",
      "logarithm is an exponent 8\n",
      "The preceding two equations are actually two versions of the\n",
      "same thing 13\n",
      "The basic idea is that 2 times 2 times 2 times 2 is 16 19\n",
      "\n",
      "The following is a depiction of how we can use both versions to say the same thing 18\n",
      "Note\n",
      "how I use arrows to move from the log formula to the exponent formula:\n",
      "Consider the following examples:\n",
      "Note something interesting 25\n",
      "Let's rewrite the first equation:\n",
      "We then replace 81 with the equivalent statement,  , as follows:\n",
      "Something interesting to note : the 3s seem to cancel out 35\n",
      "This is actually very important\n",
      "when dealing with numbers more difficult to work with than 3s and 4s 23\n",
      "\n",
      "Exponents and logarithms  are most important when  dealing with growth 15\n",
      "More often than\n",
      "not, if a quantity is growing (or declining in growth), an exponent/logarithm can help\n",
      "model this behavior 28\n",
      "\n",
      "\n",
      "For example, the number e is around 2 11\n",
      "718 and has many practical applications 7\n",
      "A very\n",
      "common application is interest  calculation for saving 11\n",
      "Suppose you have $5,000 deposited\n",
      "in a bank with continuously compounded interest at the rate of 3%, then we can use the\n",
      "following formula to model the growth of your deposit:\n",
      "In this formula:\n",
      "A denotes the final amount\n",
      "P denotes the principal investment ( 5000 )\n",
      "e denotes a constant ( 2 66\n",
      "718 )\n",
      "r denotes the rate of growth ( 10\n",
      "03)\n",
      "t denotes the time (in years)\n",
      "We are curious; when will our investment double 20\n",
      "How long would I have to have my\n",
      "money in this investment to achieve 100% growth 19\n",
      "We can write this in mathematical form,\n",
      "as follows:\n",
      " (divided by 5000 on both sides)\n",
      "At this point, we have a variable in the exponent that we want to solve 38\n",
      "When this happens,\n",
      "we can use the logarithm notation to figure it out:\n",
      "This leaves us with  \n",
      "When we take the logarithm of a number with a base of e, it is called a natural logarithm 43\n",
      "We\n",
      "rewrite the logarithm as follows:\n",
      "\n",
      "\n",
      "Using a calculator (or Python), we find that \n",
      "This means that it would take 2 28\n",
      "31 years to double our money 7\n",
      "\n",
      "Set theory\n",
      "Set theory involves mathematical operations at the set level 13\n",
      "It is sometimes thought of as a\n",
      "basic fundamental group  of theorems that governs the rest of mathematics 23\n",
      "For our\n",
      "purpose, we'll use set theory to manipulate groups of elements 15\n",
      "\n",
      "A set is a collection of distinct objects 9\n",
      "\n",
      "That's it 4\n",
      "A set can be thought of as a list in Python, but with no repeat objects 17\n",
      "In fact, there\n",
      "is even a set of objects in Python:\n",
      "s = set()\n",
      "s = set([1, 2, 2, 3, 2, 1, 2, 2, 3, 2])\n",
      "# will remove duplicates from a list\n",
      "s == {1, 2, 3}\n",
      "Note that, in Python, curly braces {, } can denote either a set or a\n",
      "dictionary 90\n",
      "\n",
      "Remember that a dictionary in Python is a set of key-value pairs, for\n",
      "example:\n",
      "dict = {\"dog\": \"human's best friend\", \"cat\": \"destroyer of world\"}\n",
      "dict[\"dog\"]# == \"human's best friend\"\n",
      "len(dict[\"cat\"]) # == 18\n",
      "# but if we try to create a pair with the same key as an existing key\n",
      "dict[\"dog\"] = \"Arf\"\n",
      "dict\n",
      "{\"dog\": \"Arf\", \"cat\": \"destroyer of world\"}\n",
      "# It will override the previous value\n",
      "# dictionaries cannot have two values for one key 123\n",
      "\n",
      "\n",
      "They share this notation because they share a quality in that sets cannot have duplicate\n",
      "elements, just as dictionaries cannot have duplicate keys 26\n",
      "\n",
      "The magnitude  of a set is the number of elements in the set and is represented as follows:\n",
      "s  # == {1,2,3}\n",
      "len(s) == 3 # magnitude of s\n",
      "The concept of an empty set exists and is denoted by the character  {} 59\n",
      "\n",
      "This null  set is said to have a magnitude of 0 14\n",
      "\n",
      "If we wish to denote that an element is within a set, we use the epsilon notation, as shown:\n",
      "2 ∈ {1,2,3}\n",
      "This means that the  2 element exists in the set of 1, 2, and 3 54\n",
      "If one set is entirely inside\n",
      "another set, we say that it is a subset  of its larger counterpart:\n",
      "A= {1,5,6}, B={1,5,6,7,8}\n",
      "A ⊆ B                                                                                                   \n",
      " \n",
      "So, A is a subset of B and B is called the superset  of A 69\n",
      "If A is a subset of B but A does not\n",
      "equal B (meaning that there is at least one element in B that is not in A), then A is called a\n",
      "proper subset  of B 42\n",
      "\n",
      "Consider the following examples:\n",
      "A set of even numbers is a subset of all integers\n",
      "Every set is a subset, but not a proper subset, of itself\n",
      "A set of all tweets is a superset of English tweets\n",
      "In data science, we use sets (and lists) to represent a list of objects and, often, to generalize\n",
      "the behavior of consumers 74\n",
      "It is common to reduce a customer to a set of characteristics 12\n",
      "\n",
      "\n",
      "Imagine we are a marketing firm  trying to predict where a person wants to shop for clothes 19\n",
      "\n",
      "We are given a set of clothing brands the user has previously visited, and our goal is to\n",
      "predict a new store that they would also enjoy 30\n",
      "Suppose a specific user has previously\n",
      "shopped at the following stores:\n",
      "user1 = {\"Target\",\"Banana Republic\",\"Old Navy\"}\n",
      "# note that we use {} notation to create a set\n",
      "# compare that to using [] to make a list\n",
      "So, user1  has previously shopped at Target , Banana Republic , and Old Navy 68\n",
      "Let's\n",
      "also look at a different user, called user2 , as shown:\n",
      "user2 = {\"Banana Republic\",\"Gap\",\"Kohl's\"}\n",
      "Suppose we are wondering how similar these users are 41\n",
      "With the limited information we\n",
      "have, one way to define similarity is to see how many stores there are that they both shop\n",
      "at 27\n",
      "This is called an intersection :\n",
      "The intersection of two sets is a set whose elements appear in both sets 20\n",
      "It is denoted using\n",
      "the ∩ symbo l, as shown:\n",
      "The intersection of the two users is just one store 26\n",
      "So, right away, that doesn't seem great 10\n",
      "\n",
      "However, each user only has three elements in their set, so having 1/3 does not seem as bad 24\n",
      "\n",
      "Suppose we are curious about how many stores are represented between the two of them;\n",
      "this is called a union 23\n",
      "\n",
      "The union of two sets is a set whose elements appear in either set 15\n",
      "It is denoted using the\n",
      "symbol ∪, as shown:\n",
      "\n",
      "\n",
      "When looking at the similarities between user1  and user2 , we should use a combination of\n",
      "the union and the intersection of their sets 42\n",
      "user1  and user2  have one element in common\n",
      "out of a total of five distinct elements between them 23\n",
      "So, we can define the similarity\n",
      "between the two users as follows:\n",
      "                                                     \n",
      "          \n",
      "In fact, this has a name in set theory 28\n",
      "It is called the Jaccard measure 8\n",
      "In general, for the A\n",
      "and B sets, the Jaccard  measure (Jaccard similarity) between the two sets is defined as\n",
      "follows:\n",
      "                                                          \n",
      "It can also be defined as the magnitude of the intersection of the two sets divided by the\n",
      "magnitude of the union of the two sets 63\n",
      "\n",
      "This gives us a way to quantify similarities between elements represented with sets 14\n",
      "\n",
      "Intuitively, the Jaccard measure is a number between 0 and 1, such that when the number is\n",
      "closer to 0, people are more dissimilar and when the measure is closer to 1, people are\n",
      "considered similar to each other 57\n",
      "\n",
      "If we think about the definition, then it actually makes sense 13\n",
      "Take a look at the measure\n",
      "once more:\n",
      "Here, the numerator represents the number of stores that the users have in common (in the\n",
      "sense that they like shopping there), while the denominator represents the unique number\n",
      "of stores that they like put together 51\n",
      "\n",
      "We can represent this in Python using some simple code, as shown:\n",
      "user1 = {\"Target\",\"Banana Republic\",\"Old Navy\"}\n",
      "user2 = {\"Banana Republic\",\"Gap\",\"Kohl's\"}\n",
      "def jaccard(user1, user2):\n",
      "  stores_in_common = len(user1 & user2)\n",
      "  stores_all_together = len(user1 | user2)\n",
      "\n",
      "  return stores / float(stores_all_together)\n",
      "# I cast stores_all_together as a float to return a decimal answer instead\n",
      "of python's default integer division\n",
      "# so\n",
      "jaccard(user1, user2) == # 0 128\n",
      "2 or 1/5\n",
      "Set theory becomes highly prevalent when we enter the world of probability and also when\n",
      "dealing with high-dimensional data 30\n",
      "We will use sets to represent real-world events taking\n",
      "place, and probability becomes set theory with vocabulary on top of it 24\n",
      "\n",
      "Linear algebra\n",
      "Remember the movie recommendation engine  we looked at earlier 14\n",
      "What if we had 10,000\n",
      "movies to recommend and we had to choose only 10 to give to the user 25\n",
      "We'd have to take\n",
      "a dot product between the user profile and each of the 10,000 movies 22\n",
      "Linear algebra\n",
      "provides the tools to make these calculations much more efficient 14\n",
      "\n",
      "It is an area of mathematics that deals with the math of matrices and vectors 16\n",
      "It has the aim\n",
      "of breaking down these objects and reconstructing them in order to provide practical\n",
      "applications 21\n",
      "Let's look at a few linear algebra rules before proceeding 11\n",
      "\n",
      "Matrix multiplication\n",
      "Like numbers, we can multiply  matrices together 13\n",
      "Multiplying matrices is, in essence, a\n",
      "mass-produced way of taking several dot products at once 20\n",
      "Let's, for example, try to \n",
      "multiply  the following matrices:\n",
      "We need to consider a couple of things:\n",
      "Unlike numbers, multiplication of matrices is not commutative , meaning that the\n",
      "order in which you multiply matrices matters a great deal 50\n",
      "\n",
      "In order to multiply matrices, their dimensions must match up 12\n",
      "This means that\n",
      "the first matrix must have the same number of columns as the second matrix has\n",
      "rows 21\n",
      "\n",
      "\n",
      "To remember this, write out the dimensions of the matrices 12\n",
      "In this case, we have a 3 x 2\n",
      "times a 2 x 2 matrix 21\n",
      "You can multiply matrices together if the second number in the first\n",
      "dimension pair is the same as the first number in the second dimension pair:\n",
      "                                                                          \n",
      "The resulting matrix will always have dimensions equal to the outer numbers in the\n",
      "dimension pairs (the ones you did not circle) 55\n",
      "In this case, the resulting matrix will have a\n",
      "dimension of 3 x 2 18\n",
      "\n",
      "How to multiply matrices\n",
      "To multiply matrices, there is actually quite  a simple procedure 18\n",
      "Essentially, we are\n",
      "performing a bunch of dot products 12\n",
      "\n",
      "Recall our earlier sample problem, which was as follows:\n",
      "We know that our resulting matrix will have a dimension of 3 x 2 29\n",
      "So, we know it will look\n",
      "something like the following:\n",
      "Note that each element of the matrix is indexed using a double index 26\n",
      "The\n",
      "first number represents the row, and the second number represents the\n",
      "column 16\n",
      "So, the m32 element is the element in the third row of the second\n",
      "column 18\n",
      "Each element is the result of a dot product between rows and\n",
      "columns of the original matrices 18\n",
      "\n",
      "\n",
      "The mxy element is the result of the dot product of the xth row of the first matrix and the yth\n",
      "column of the second matrix 31\n",
      "Let's solve a few:\n",
      "Moving on, we will eventually get a resulting matrix that looks as follows:\n",
      "Way to go 24\n",
      "Let's come back to the movie recommendation example 9\n",
      "Recall the user's movie\n",
      "genre preferences of comedy, romance, and action, which are illustrated as follows:\n",
      "Now suppose we have 10,000 movies, all with a rating for these three categories 40\n",
      "To make a\n",
      "recommendation, we need to take the dot product of the preference vector with each of the\n",
      "10,000 movies 27\n",
      "We can use matrix multiplication to represent this 8\n",
      "\n",
      "Instead of writing them all out, let's express them using the matrix notation 16\n",
      "We already\n",
      "have U, defined here as the user's preference vector (it can also be thought of as a 3 x 1\n",
      "matrix), and we also need a movie matrix:\n",
      "M = movies = 3 x 10,000\n",
      "So, now we have two matrices; one is 3 x 1 and the other is 3 x 10,000 77\n",
      "We can't multiply\n",
      "these matrices as they are, because the dimensions do not work out 18\n",
      "We will have to change\n",
      "U a bit 9\n",
      "We can take the transpose  of the matrix (turning all rows into columns and columns\n",
      "into rows) 22\n",
      "This will switch the dimensions around:\n",
      "\n",
      "\n",
      "So, now we have two matrices  that can be multiplied together 20\n",
      "Let's visualize what this\n",
      "looks like:\n",
      "                                                                                     1  x  3                     3  x  1000\n",
      "The resulting matrix will be a 1 x 1,000  matrix (a vector) of 10,000 predictions for each\n",
      "individual movie 58\n",
      "Let's try this out in Python:\n",
      "import numpy as np\n",
      "# create user preferences\n",
      "user_pref = np 22\n",
      "array([5, 1, 3])\n",
      "# create a random movie matrix of 10,000 movies\n",
      "movies = np 26\n",
      "random 1\n",
      "randint(5,size=(3,1000))+1\n",
      "# Note that the randint will make random integers from 0-4\n",
      "# so I added a 1 at the end to increase the scale from 1-5\n",
      "We are using the numpy  array function to create our matrices 59\n",
      "We will have both a\n",
      "user_pref  and a movies  matrix to represent our data 18\n",
      "\n",
      "To check our dimensions, we can use the numpy shape  variable, as shown:\n",
      "print(user_pref 21\n",
      "shape) # (1, 3)\n",
      "print(movies 12\n",
      "shape) # (3, 1000)\n",
      "This checks out 13\n",
      "Last but not least, let's use the matrix multiplication method of numpy\n",
      "(called dot) to perform the operation, as illustrated:\n",
      "# np 29\n",
      "dot does both dot products and matrix multiplication\n",
      "np 10\n",
      "dot(user_pref, movies)\n",
      "The result is an array of integers that represents the recommendations for each movie 20\n",
      "\n",
      "\n",
      "For a quick extension of this, let's run some code  that predicts across more than 10,000\n",
      "movies, as shown:\n",
      "import numpy as np\n",
      "import time\n",
      "for num_movies in (10000, 100000, 1000000, 10000000, 100000000):\n",
      "   movies = np 68\n",
      "random 1\n",
      "randint(5,size=(3, num_movies))+1\n",
      "    now = time 16\n",
      "time()\n",
      "    np 4\n",
      "dot(user_pref, movies)\n",
      "    print((time 10\n",
      "time() - now), \"seconds to run\", num_movies, \"movies\")\n",
      "0 17\n",
      "000160932540894 seconds to run 10000 movies\n",
      "0 15\n",
      "00121188163757 seconds to run 100000 movies\n",
      "0 15\n",
      "0105860233307 seconds to run 1000000 movies\n",
      "0 16\n",
      "096577167511 seconds to run 10000000 movies\n",
      "4 15\n",
      "16197991371 seconds to run 100000000 movies\n",
      "It took only a bit over four seconds to run through 100,000,000 movies using matrix\n",
      "multiplication 37\n",
      "\n",
      "Summary\n",
      "In this chapter, we took a look at some basic mathematical principles that will become very\n",
      "important as we progress through this book 28\n",
      "Between logarithms/exponents, matrix\n",
      "algebra, and proportionality, mathematics clearly has a big role not just in the analysis of\n",
      "data but in many aspects of our lives 36\n",
      "\n",
      "The coming chapters will take a much deeper dive into two big areas of mathematics:\n",
      "probability and statistics 20\n",
      "Our goal will be to define and interpret the smallest and biggest\n",
      "theorems in these two giant fields of mathematics 23\n",
      "\n",
      "It is in the next few chapters that everything will start to come together 15\n",
      "So far in this book,\n",
      "we have looked at math examples, data exploration guidelines, and basic insights into\n",
      "types of data 25\n",
      "It is time to begin to tie all of these concepts together 12\n",
      "\n",
      "\n",
      "\n",
      "Impossible or Improbable – A\n",
      "Gentle Introduction to\n",
      "Probability\n",
      "Over the next few chapters, we will explore both probability and statistics as methods of\n",
      "examining both data-driven situations and real-world scenarios 44\n",
      "The rules of probability\n",
      "govern the basics of prediction 11\n",
      "We use probability to define the chances of the occurrence\n",
      "of an event 14\n",
      "\n",
      "In this chapter, we will look at the following topics:\n",
      "What is the probability 17\n",
      "\n",
      "The differences between the Frequentist approach and the Bayesian approach\n",
      "How to visualize probability\n",
      "How to utilize the rules of probability\n",
      "Using confusion matrices to look at the basic metrics\n",
      "Probability will help us model real-life events that include a sense of randomness and\n",
      "chance 55\n",
      "Over the next two chapters, we will look at the terminology behind probability\n",
      "theorems and how to apply them to model situations that can appear unexpectedly 30\n",
      "\n",
      "Basic definitions\n",
      "One of the most basic concepts of probability is the concept of a procedure 18\n",
      "A procedure  is\n",
      "an act that leads  to a result, for example, throwing a die or visiting a website 24\n",
      "\n",
      "\n",
      "An event  is a collection of the outcomes of a procedure, such as getting a head on a coin flip\n",
      "or leaving a website after only 4 seconds 33\n",
      "A simple event is an outcome/event of a\n",
      "procedure that cannot be broken down further 17\n",
      "For example, rolling two dice can be broken\n",
      "down into two simple events: rolling die 1 and rolling die 2 25\n",
      "\n",
      "The sample space  of a procedure is the set of all possible simple events 16\n",
      "For example, an\n",
      "experiment is performed in which a coin is flipped three times in succession 18\n",
      "What is the\n",
      "size of the sample space for this experiment 12\n",
      "\n",
      "The answer is eight because the results could be any one of the possibilities in the following\n",
      "sample space: {HHH, HHT, HTT, HTH, TTT, TTH, THH, or THT} 48\n",
      "\n",
      "Probability\n",
      "The probability  of an event represents the frequency, or chance, that the event will happen 21\n",
      "\n",
      "For notation , if A is an event, P(A)  is the probability of the occurrence of the event 23\n",
      "\n",
      "We can define the actual probability of an event, A, as follows:\n",
      "Here, A is the event in question 24\n",
      "Think of an entire universe of events where anything is\n",
      "possible, and let's represent it as a circle 21\n",
      "We can think of a single event, A, as being a\n",
      "smaller circle within that larger universe, as shown in the following diagram:\n",
      "\n",
      "\n",
      "Let's now pretend that our universe involves a research study on humans and the A event\n",
      "is people in that study who have cancer 55\n",
      "\n",
      "If our study has 100 people and A has 25 people, the probability of A or P(A)  is 25/100 29\n",
      "\n",
      "The maximum probability of any event is 1 10\n",
      "This can be understood as the red circle grows\n",
      "so large that it is the size of the universe (the larger circle) 25\n",
      "\n",
      "The most basic examples (I promise they will get more interesting) are coin flips 17\n",
      "Let's say\n",
      "we have two coins and we want the probability that we will get two heads 19\n",
      "We can very\n",
      "easily count the number of ways two coins could end up being two heads 19\n",
      "There's only one 4\n",
      "\n",
      "Both coins have to be heads 7\n",
      "But how many options are there 6\n",
      "It could either be two heads,\n",
      "two tails, or a heads/tails combination 16\n",
      "\n",
      "First, let's define A 7\n",
      "It is the event in which two heads occur 9\n",
      "The number of ways that A can\n",
      "occur is 1 13\n",
      "\n",
      "The sample space of the experiment is {HH, HT, TH, TT} , where each two-letter word\n",
      "indicates the outcome of the first and second coin simultaneously 35\n",
      "The size of the sample\n",
      "space is four 9\n",
      "So, P(getting two heads) = 1/4 13\n",
      "\n",
      "Let's refer to a quick visual table to prove it 12\n",
      "The following table denotes the options for\n",
      "coin 1 as the columns, and the options for coin 2 as the rows 25\n",
      "In each cell, there is\n",
      "either True  or False 12\n",
      "A True  value indicates that it satisfies the condition (both heads),\n",
      "and False  indicates otherwise:\n",
      "Coin 1 is heads Coin 1 is tails\n",
      "Coin 2 is heads True False\n",
      "Coin 2 is tails False False\n",
      "So, we have one out of a total of four possible outcomes 60\n",
      "\n",
      "Bayesian versus Frequentist\n",
      "The preceding example was almost  too easy 16\n",
      "In practice, we can hardly ever truly count the\n",
      "number of ways something can happen 17\n",
      "For example, let's say that we want to know the\n",
      "probability of a random  person smoking cigarettes at least once a day 26\n",
      "If we wanted  to\n",
      "approach this problem using  the classical way (the previous formula), we would need to\n",
      "figure out how many different ways a person is a smoker —someone who smokes at least\n",
      "once a day —which is not possible 52\n",
      "\n",
      "\n",
      "When faced with such a problem, two main schools of thought are considered when it\n",
      "comes to calculating probabilities in practice: the Frequentist approach  and the Bayesian\n",
      "approach 37\n",
      "This chapter will focus heavily on the Frequentist approach, while the\n",
      "subsequent chapter will dive into the Bayesian approach 24\n",
      "\n",
      "Frequentist approach\n",
      "In a Frequentist approach, the probability of an event  is calculated through\n",
      "experimentation 25\n",
      "It uses the past in order to predict the future chance of an event 14\n",
      "The basic\n",
      "formula is as follows:\n",
      "Basically, we observe several instances of the event and count the number of times A was\n",
      "satisfied 28\n",
      "The division of these numbers is an approximation of the probability 11\n",
      "\n",
      "The Bayesian approach differs by dictating that probabilities must be discerned using\n",
      "theoretical means 19\n",
      "Using the Bayes approach, we would have to think a bit more critically\n",
      "about events and why they occur 22\n",
      "Neither methodology is wholly the correct answer all of\n",
      "the time 12\n",
      "Usually, it comes down to the problem and the difficulty of using either approach 15\n",
      "\n",
      "The crux of the Frequentist approach is the relative frequency 14\n",
      "\n",
      "The relative frequency  of an event  is how often an event occurs divided by the total\n",
      "number of observations 23\n",
      "\n",
      "Example – marketing stats\n",
      "Let's say that you are interested in ascertaining how often a person who visits your website\n",
      "is likely to return on a later date 34\n",
      "This is sometimes called the rate of repeat visitors 9\n",
      "In the\n",
      "previous definition, we would define our A event as being a visitor coming back to the site 21\n",
      "\n",
      "We would then have to calculate the number of ways a person can come back, which\n",
      "doesn't really make sense at all 26\n",
      "In this case, many people would turn to a Bayesian\n",
      "approach; however, we can calculate what is known as relative frequency 26\n",
      "\n",
      "So, in this case, we can take the visitor logs and calculate the relative frequency of event A\n",
      "(repeat visitors) 26\n",
      "Let's say, of the 1,458  unique visitors in the past week, 452 were repeat\n",
      "visitors 25\n",
      "We can calculate this as follows:\n",
      "\n",
      "\n",
      "So, about 31% of your visitors are repeat visitors 19\n",
      "\n",
      "The law of large numbers\n",
      "The reason that even the Frequentist approach  can do this is that of the law of large\n",
      "numbers, which states that if we repeat a procedure over and over, the relative frequency\n",
      "probability will approach the actual probability 52\n",
      "Let's try to demonstrate this using Python 8\n",
      "\n",
      "If I were to ask you the average of the numbers 1 and 10, you would very quickly answer\n",
      "around 5 27\n",
      "This question is identical to asking you to pick the average  number between 1 and\n",
      "10 19\n",
      "\n",
      "Python will choose n random numbers between 1 and 10 and find their average 17\n",
      "\n",
      "We will repeat this experiment several times using a larger n each time, and then we will\n",
      "graph the outcome 23\n",
      "The steps are as follows:\n",
      "Pick a random number between 1 and 10 and find the average1 21\n",
      "\n",
      "Pick two random numbers between 1 and 10 and find their average2 16\n",
      "\n",
      "Pick three random numbers between 1 and 10 and find their average3 16\n",
      "\n",
      "Pick 10,000 random numbers between 1 and 10 and find their average4 19\n",
      "\n",
      "Graph the results5 5\n",
      "\n",
      "Let's take a look at the code:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "results = []\n",
      "for n in range(1,10000):\n",
      "    nums = np 48\n",
      "random 1\n",
      "randint(low=1,high=10, size=n) # choose n numbers\n",
      "between 1 and 10\n",
      "    mean = nums 28\n",
      "mean()                              # find the average of\n",
      "these numbers\n",
      "    results 14\n",
      "append(mean)                            # add the average to a\n",
      "running list\n",
      "# POP QUIZ: How large is the list results 25\n",
      "\n",
      "len(results) # 9999\n",
      "# This was tricky because I took the range from 1 to 10000 and usually we\n",
      "do from 0 to 10000\n",
      "df = pd 41\n",
      "DataFrame({ 'means' : results})\n",
      "print (df 11\n",
      "head()) # the averages in the beginning are all over the place 13\n",
      "\n",
      "# means\n",
      "\n",
      "# 9 7\n",
      "0\n",
      "# 5 6\n",
      "0\n",
      "# 6 6\n",
      "0\n",
      "# 4 6\n",
      "5\n",
      "# 4 6\n",
      "0\n",
      "print (df 6\n",
      "tail()) # as n, our size of the sample size, increases, the\n",
      "averages get closer to 5 24\n",
      "\n",
      "# means\n",
      "# 4 7\n",
      "998799\n",
      "# 5 7\n",
      "060924\n",
      "# 4 7\n",
      "990597\n",
      "# 5 7\n",
      "008802\n",
      "# 4 7\n",
      "979198\n",
      "df 5\n",
      "plot(title='Law of Large Numbers')\n",
      "plt 9\n",
      "xlabel(\"Number of throws in sample\")\n",
      "plt 9\n",
      "ylabel(\"Average Of Sample\")\n",
      "Cool, right 9\n",
      "What this is essentially showing us is that as we increase the sample size of our\n",
      "relative frequency, the frequency approaches the actual average (probability) of 5 32\n",
      "\n",
      "In our statistics chapters, we will work to define this law much more rigorously, but for\n",
      "now, just know that it is used to link the relative frequency of an event to its actual\n",
      "probability 42\n",
      "\n",
      "\n",
      "Compound events\n",
      "Sometimes, we need to deal with two or more events 15\n",
      "These are called compound events 5\n",
      "A\n",
      "compound event is an event that combines two or more simple events 14\n",
      "When this happens,\n",
      "we need a special notation 9\n",
      "\n",
      "Given events A and B:\n",
      "The probability that A and B occur is P(A ∩ B) = P(A and B)\n",
      "The probability that either A or B occurs is P(A ∪ B) = P(A or B)\n",
      "Understanding why we use the set notation for these compound events is very important 62\n",
      "\n",
      "Remember how we represented events in a universe using circles earlier 12\n",
      "Let's say that our\n",
      "universe  is 100 people who showed up for an experiment in which a new test for cancer is\n",
      "being developed:\n",
      "In the preceding diagram, the red circle, A, represents 25 people who actually have cancer 50\n",
      "\n",
      "Using the relative frequency approach, we can say that P(A) = number of people with\n",
      "cancer/number of people in study , that is, 25/100 = ¼ = 40\n",
      "25 2\n",
      "This means that there is a 25%\n",
      "chance that someone has cancer 15\n",
      "\n",
      "\n",
      "Let's introduce a second event, called B, as shown, which contains people for whom the test\n",
      "was positive (it claimed that they had cancer) 32\n",
      "Let's say that this is for 30 people 10\n",
      "So, P(B) =\n",
      "30/100 = 3/10 = 15\n",
      "3 2\n",
      "This means that there is a 30% chance that the test said positive for any\n",
      "given person:\n",
      "These are two separate events, but they interact with each other 33\n",
      "Namely, they might\n",
      "intersect  or have people in common, as shown here:\n",
      "\n",
      "\n",
      "Anyone in the space that both A and B occupies, otherwise known as A intersect B  or A ∩ B,\n",
      "are people for whom the test claimed they were positive for cancer ( A), and they actually\n",
      "do have cancer 64\n",
      "Let's say that's 20 people 8\n",
      "The test said positive for 20 people, that is, they\n",
      "have cancer, as shown here:\n",
      "This means that P(A and B) = 20/100 = 1/5 = 40\n",
      "2 = 20% 6\n",
      "\n",
      "If we want to say that someone has cancer, or the test came back positive, this would be the\n",
      "total sum (or union) of the two events, namely, the sum of 5, 20, and 10, which is 35 53\n",
      "So,\n",
      "35/100 people either have cancer or had a positive test outcome 15\n",
      "That means, P(A or B) =\n",
      "35/100 = 13\n",
      "35 = 35% 6\n",
      "\n",
      "All in all, we have people in the following four different classes:\n",
      "Pink : This refers to those people who have cancer and had a negative test\n",
      "outcome\n",
      "Purple ( A intersect B): These people have cancer and had a positive test outcome\n",
      "Blue : This refers to those people with no cancer and a positive test outcome\n",
      "White : This refers to those people with no cancer and a negative test outcome\n",
      "So, effectively, the only times the test was accurate  was in the white and purple regions 101\n",
      "In\n",
      "the blue and pink regions, the test was incorrect 12\n",
      "\n",
      "\n",
      "Conditional probability\n",
      "Let's pick an arbitrary person from  this study of 100 people 18\n",
      "Let's also assume that you are\n",
      "told that their test result was positive 16\n",
      "What is the probability of them actually having\n",
      "cancer 11\n",
      "So, we are told that event B has already taken place, and that their test came back\n",
      "positive 21\n",
      "The question now is: what is the probability that they have cancer, that is P(A) 19\n",
      "\n",
      "This is called a conditional probability of A given B or P(A|B) 17\n",
      "Effectively, it is asking you\n",
      "to calculate the probability of an event given that another event has already happened 22\n",
      "\n",
      "You can think of conditional probability as changing the relevant universe 12\n",
      "P(A|B)  (called\n",
      "the probability of A given B) is a way of saying, given that my entire universe is now B,\n",
      "what is the probability of A 37\n",
      "This is also known as transforming the sample space 9\n",
      "\n",
      "Zooming in on our previous diagram, our universe is now B, and we are concerned with\n",
      "AB (A and B) inside B 29\n",
      "\n",
      "The formula can be given as follows:\n",
      "P(A|B) = P(A and B) / P(B) = (20/100) / (30/100) = 20/30 = 42\n",
      "66 = 66%\n",
      "There is a 66%  chance that if a test result came back positive, that person had cancer 27\n",
      "In\n",
      "reality, this is the main probability that the experimenters want 15\n",
      "They want to know how\n",
      "good the test is at predicting cancer 13\n",
      "\n",
      "The rules of probability\n",
      "In probability, we have some rules  that become very useful when visualization gets too\n",
      "cumbersome 25\n",
      "These rules help us calculate compound probabilities with ease 9\n",
      "\n",
      "The addition rule\n",
      "The addition rule is used to calculate the probability of either or  events 19\n",
      "To calculate P(A ∪ B)\n",
      "= P(A or B) , we use the following formula:\n",
      "P(A ∪ B) = P(A) + P(B) − P(A ∩ B)\n",
      "\n",
      "The first part of the formula (P(A) + P(B))  makes complete sense 60\n",
      "To get the union of the\n",
      "two events, we have to add together the area of the circles in the universe 23\n",
      "But why the\n",
      "subtraction of P(A and B) 12\n",
      "This is because when we add the two circles, we are adding the\n",
      "area of intersection twice, as shown in the following diagram:\n",
      "See how both the red circles include the intersection of A and B 40\n",
      "So, when we add them, we\n",
      "need to subtract just one of them to account for this, leaving us with our formula 26\n",
      "\n",
      "You will recall that we wanted the number of people who either had cancer or had a\n",
      "positive test result 22\n",
      "If A is the event that someone has cancer, and B is that the test result\n",
      "was positive, we have the following:\n",
      "P(A or B) = P(A) + P(B) - P(A and B) = 46\n",
      "25 + 3\n",
      "30 - 3\n",
      "2 = 3\n",
      "35\n",
      "\n",
      "This is represented visually in the following diagram:\n",
      "Mutual exclusivity\n",
      "We say that two events  are mutually exclusive if they cannot occur at the same time 34\n",
      "This\n",
      "means that A∩B=∅, or just that the intersection of the events is the empty set 24\n",
      "When this\n",
      "happens, P(A ∩ B) = P(A and B) = 0 22\n",
      "\n",
      "If two events are mutually exclusive, then the following applies:\n",
      "P(A ∪ B) = P(A or B)\n",
      " ",
      "= P(A) + P(B) − P(A ∩ B) = P(A) + P(B)\n",
      "This makes the addition rule much easier 57\n",
      "Some examples of mutually exclusive events\n",
      "include the following:\n",
      "A customer seeing your site for the first time on both Twitter and Facebook\n",
      "Today is Saturday and today is Wednesday\n",
      "I failed Econ 101 and I passed Econ 101\n",
      "None of these events can occur simultaneously 53\n",
      "\n",
      "\n",
      "The multiplication rule\n",
      "The multiplication rule is used  to calculate the probability of and events 18\n",
      "To calculate P(A ∩\n",
      "B) = P(A and B) , we use the following formula:\n",
      "P(A ∩ B) = P(A and B) = P(A) P(B|A)\n",
      "Why do we use B|A instead of B 53\n",
      "This is because it is possible that B depends on A 11\n",
      "If this\n",
      "is the case, then just multiplying P(A)  and P(B)  does not give us the whole picture 26\n",
      "\n",
      "In our cancer trial example, let's find P(A and B) 15\n",
      "To do this, let's redefine A to be the event\n",
      "that the trial is positive, and B to be the person having cancer (because it doesn't matter\n",
      "what we call the events) 40\n",
      "The equation will be as follows:\n",
      "P(A ∩ B) = P(A and B) = P(A) P(B|A) = 29\n",
      "3 * 3\n",
      "6666 = 4\n",
      "2 = 20%\n",
      "It's difficult to see the true necessity of using conditional probability, so let's try another,\n",
      "more difficult problem 28\n",
      "\n",
      "For example, of a randomly selected set of 10 people, 6 have iPhones and 4 have Androids 24\n",
      "\n",
      "What is the probability that if I randomly select 2 people, they both will have iPhones 19\n",
      "This\n",
      "example can be retold using event spaces, as follows:\n",
      "I have the following two events:\n",
      "A: This event shows the probability that I choose a person with an iPhone first\n",
      "B: This event shows the probability that I choose a person with an iPhone second\n",
      "So, basically, I want the following:\n",
      "P(A and B) : P(I choose a person with an iPhone and a person with an iPhone )\n",
      "So, I can use my P(A and B) = P(A) P(B|A)  formula 107\n",
      "\n",
      "P(A)  is simple, right 9\n",
      "People with iPhones are 6 out of 10, so, I have a 6/10 = 3/5 = 0 28\n",
      "6  chance\n",
      "of A 7\n",
      "This means P(A) = 0 8\n",
      "6 2\n",
      "\n",
      "So, if I have a 0 9\n",
      "6 chance of choosing someone with an iPhone, the probability of choosing\n",
      "two should just be 0 21\n",
      "6 * 0 5\n",
      "6 , right 4\n",
      "\n",
      "But wait 3\n",
      "We only have 9 people left to choose our second person from, because one was\n",
      "taken away 20\n",
      "So, in our new transformed sample space, we have 9 people in total, 5 with\n",
      "iPhones and 4 with Androids, making P(B) = 5/9 = 40\n",
      "555 2\n",
      "\n",
      "\n",
      "So, the probability of choosing two people with iPhones is 0 14\n",
      "6 * 0 5\n",
      "555 = 0 5\n",
      "333 = 33% 6\n",
      "\n",
      "I have a 1/3 chance of choosing two people with iPhones out of 10 19\n",
      "The conditional\n",
      "probability is very important in the multiplication rule as it can drastically alter your\n",
      "answer 19\n",
      "\n",
      "Independence\n",
      "Two events are independent if one event does not affect the outcome of the other, that is\n",
      "P(B|A) = P(B)  and P(A|B) = P(A) 43\n",
      "\n",
      "If two events are independent, then the following applies:\n",
      "P(A ∩ B) = P(A) P(B|A) = P(A) P(B)\n",
      "Some examples of independent events are as follows:\n",
      "It was raining in San Francisco, and a puppy was born in India\n",
      "I flip a coin and get heads, and I flip another coin and get tails\n",
      "None of these pairs of events affect each other 84\n",
      "\n",
      "Complementary events\n",
      "The complement of A is the opposite  or negation of A 18\n",
      "If A is an event,  Ā represents the\n",
      "complement of A 16\n",
      "For example, if A is the event where someone has cancer, Ā is the event\n",
      "where someone is cancer free 24\n",
      "\n",
      "To calculate the probability of, Ā, use the following formula:\n",
      "P(Ā) = 1 − P(A)\n",
      "For example, when you throw two dice, what is the probability that you rolled higher than\n",
      "a 3 49\n",
      "\n",
      "Let A represent rolling higher than a 3 10\n",
      "\n",
      "Ā represents rolling a 3 or less 10\n",
      "\n",
      "P(A) = 1 − P(Ā)\n",
      "P(A) = l - (P(2)+P(3))\n",
      "\n",
      "= 1 - (2/36 + 2/36)\n",
      "= 1 - (4/36)\n",
      "= 32/36 = 8 / 9\n",
      "= 64\n",
      "89\n",
      "For example, a start-up team has three investor meetings coming up 16\n",
      "We will have the\n",
      "following probabilities:\n",
      "A 60% chance of getting money from the first meeting\n",
      "A 15% chance of getting money from the second\n",
      "A 45% chance of getting money from the third\n",
      "What is the probability of them getting money from at least one meeting 58\n",
      "\n",
      "Let A be the team getting money from at least one investor, and, Ā, be the team not getting\n",
      "any money 27\n",
      "P(A)  can be calculated as follows:\n",
      "P(A) = 1 − P(Ā)\n",
      "To calculate P(Ā), we need to calculate the following:\n",
      "P(Ā) = P(no money from investor 1 AND no money from investor 2 AND no money from investor\n",
      "3)\n",
      "Let's assume that these events are independent (they don't talk to each other):\n",
      "P(Ā) = P(no money from investor 1) * P(no money from investor 2) * P(no money from investor 3) =\n",
      "0 115\n",
      "4 * 0 5\n",
      "85 * 0 5\n",
      "55 = 0 5\n",
      "187\n",
      "P(A) = 1 - 0 12\n",
      "187 = 0 5\n",
      "813 = 81%\n",
      "So, the start-up has an 81% chance of getting money from at least one meeting 25\n",
      "\n",
      "\n",
      "A bit deeper\n",
      "Without getting too deep into the machine learning  terminology, this test is what is known\n",
      "as a binary classifier , which means that it is trying to predict from only two options:\n",
      "having cancer or not having cancer 47\n",
      "When we are dealing with binary classifiers, we can \n",
      "draw  what is called confusion matrices , which are 2 x 2 matrices that house all the four\n",
      "possible outcomes of our experiment 38\n",
      "\n",
      "Let's try some different numbers 7\n",
      "Let's say 165 people walked in for the study 11\n",
      "So, our n\n",
      "(sample size) is 165 people 12\n",
      "All 165 people are given the test and asked whether they  have\n",
      "cancer (provided through various other means) 24\n",
      "The following confusion matrix  shows us\n",
      "the results of this experiment:\n",
      "The matrix shows that 50 people were predicted to have no cancer and did not have it, 100\n",
      "people were predicted to have cancer and actually did have it, and so on 51\n",
      "We have the\n",
      "following four classes, again, all with different names:\n",
      "The true positives  are the tests correctly predicting positive (cancer) == 100\n",
      "The true negatives  are the tests correctly predicting negative (no cancer) == 50\n",
      "The false positives  are the tests incorrectly predicting positive (cancer) == 10\n",
      "The false negatives  are the tests incorrectly predicting negative (no cancer) == 5\n",
      "The first two classes indicate where the test was correct, or true 100\n",
      "The last two classes\n",
      "indicate where the test was incorrect, or false 15\n",
      "\n",
      "False positives are sometimes called a Type I error , whereas false negatives are called a\n",
      "Type II error :\n",
      "\n",
      "\n",
      "Type I and Type II errors\n",
      "(Source: http://marginalrevolution 38\n",
      "com/marginalrevolution/2014/05/type-i-and-type-ii-errors-simpliﬁed 23\n",
      "html)\n",
      "We will get into this in the later chapters 11\n",
      "For now, we just need to understand why we use\n",
      "the set notation to denote probabilities for compound events 21\n",
      "This is because that's what\n",
      "they are 9\n",
      "When events A and B exist in the same universe, we can use intersections and\n",
      "unions to represent them happening either at the same time,  or to represent one happening\n",
      "versus the other 40\n",
      "\n",
      "We will go into this much more in later chapters, but it is good to introduce it now 20\n",
      "\n",
      "Summary\n",
      "In this chapter, we looked at the basics of probability and will continue to dive deeper into\n",
      "this field in the following chapter 28\n",
      "We approached most of our thinking as a Frequentist,\n",
      "and expressed the basics of experimentation and using probability to predict an outcome 25\n",
      "\n",
      "The next chapter will look at the Bayesian approach to probability and will also explore the\n",
      "use of probability to solve much more complex problems 27\n",
      "We will incorporate these basic\n",
      "probability principles in much more difficult scenarios 13\n",
      "\n",
      "\n",
      "\n",
      "Advanced Probability\n",
      "In the previous chapter,  we went over the basics of probability and how we can apply\n",
      "simple theorems to complex tasks 30\n",
      "To briefly summarize, probability is the mathematics of\n",
      "modeling events that may or may not occur 19\n",
      "We use formulas in order to describe  these\n",
      "events and even look at how multiple events can behave together 21\n",
      "In this chapter, we will\n",
      "explore more complicated theorems  of probability and how we can use them in a predictive\n",
      "capacity 28\n",
      "Advanced topics, such as Bayes' theorem  and random variables , give rise to\n",
      "common machine learning algorithms, such as the Naïve Bayes algorithm (also covered in\n",
      "this book) 40\n",
      "This chapter will focus on some of the more advanced topics  in probability\n",
      "theory, including the following topics:\n",
      "Exhaustive events\n",
      "Bayes' theorem\n",
      "Basic prediction rules\n",
      "Random variables\n",
      "We have one more definition to look at before we get started (the last one before the fun\n",
      "stuff, I promise) 64\n",
      "We have to look at collectively exhaustive events 8\n",
      "\n",
      "Collectively exhaustive events\n",
      "When given a set of two or more events, if at least  one of the events must occur, then such a\n",
      "set of events  is said to be collectively exhaustive 41\n",
      "Consider the following examples:\n",
      "Given a set of events {temperature < 60, temperature > 90} , these events\n",
      "are not collectively exhaustive because there is a third option that is not given in\n",
      "this set of events; the temperature could be between 60 and 90 56\n",
      "However, they\n",
      "are mutually exhaustive  because both  cannot happen at the same time 17\n",
      "\n",
      "In a dice roll, the set of events of rolling a {1, 2, 3, 4, 5, or 6}  are\n",
      "collectively exhaustive  because these are the only possible events, and at least\n",
      "one of them must happen 56\n",
      "\n",
      "\n",
      "Bayesian ideas revisited\n",
      "In the last chapter, we talked, very briefly, about Bayesian ways of thinking 23\n",
      "In short, when\n",
      "speaking about Bayes, you are speaking about the following three things and how they  all\n",
      "interact with each other:\n",
      "A prior distribution\n",
      "A posterior distribution\n",
      "A likelihood\n",
      "Basically, we are concerned with finding the posterior 51\n",
      "That's the thing we want to know 8\n",
      "\n",
      "Another way to phrase the Bayesian way of thinking is that data shapes and updates our\n",
      "belief 19\n",
      "We have a prior probability, or what we naively think about a hypothesis, and then\n",
      "we have a posterior probability, which is what we think about a hypothesis, given some\n",
      "data 38\n",
      "\n",
      "Bayes' theorem\n",
      "Bayes' theorem is the big result of Bayesian  inference 18\n",
      "Let's see how it even comes about 8\n",
      "\n",
      "Recall that we previously defined the following:\n",
      "P(A) = The probability that event A occurs\n",
      "P(A|B) = The probability that A occurs, given that B occurred\n",
      "P(A, B) = The probability that A and B occur\n",
      "P(A, B) = P(A) * P(B|A)\n",
      "That last bullet can be read as the probability that A and B occur is the probability that A occurs\n",
      "times the probability that B occurred, given that A already occurred 100\n",
      "\n",
      "It's from that last bullet point that Bayes' theorem takes its shape 16\n",
      "\n",
      "We know the following:\n",
      "P(A, B) = P(A) * P(B|A)\n",
      "P(B, A) = P(B) * P(A|B)\n",
      "P(A, B) = P(B, A)\n",
      "\n",
      "So, we can infer this:\n",
      "P(B) * P(A|B) = P(A) * P(B|A)\n",
      "Dividing both sides by P(B)  gives us Bayes' theorem, as shown:\n",
      "You can think of Bayes' theorem as follows:\n",
      "It is a way to get from P(A|B)  to P(B|A)  (if you only have one of them)\n",
      "It is a way to get P(A|B)  if you already know P(A)  (without knowing B)\n",
      "Let's try thinking about Bayes using the terms hypothesis  and data 171\n",
      "Suppose H = your\n",
      "hypothesis about the given data  and D = the data that you are given 22\n",
      "\n",
      "Bayes can be interpreted as trying to figure out P(H|D)  (the probability that our hypothesis is\n",
      "correct, given the data at hand ) 33\n",
      "\n",
      "Let's use our terminology from before:\n",
      "Let's take a look at that formula:\n",
      "P(H)  is the probability of the hypothesis before we observe the data, called the\n",
      "prior probability , or just prior\n",
      "P(H|D)  is what we want to compute, the probability of the hypothesis after we\n",
      "observe the data, called the posterior\n",
      "P(D|H)  is the probability of the data under the given hypothesis, called the\n",
      "likelihood\n",
      "P(D)  is the probability of the data under any hypothesis, called the normalizing\n",
      "constant\n",
      "This concept is not far off from the idea of machine learning and predictive analytics 131\n",
      "In\n",
      "many cases, when considering predictive analytics, we use the given data to predict an\n",
      "outcome 20\n",
      "Using the current terminology, H (our hypothesis) can be considered our outcome\n",
      "and P(H|D)  (the probability that our hypothesis is true, given our data) is another way of\n",
      "saying what is the chance that my hypothesis is correct, given the data in front of me 60\n",
      "\n",
      "\n",
      "Let's take a look at an example of how we can use Bayes' formula in the workplace 21\n",
      "\n",
      "Consider that you have two people in charge of writing blog posts for your company, Lucy\n",
      "and Avinash 23\n",
      "From past performance, you have liked 80% of Lucy's work and only 50% of\n",
      "Avinash's work 26\n",
      "A new blog post comes to your desk in the morning, but the author isn't\n",
      "mentioned 19\n",
      "You love the article 4\n",
      "A+ 2\n",
      "What is the probability that it came from Avinash 11\n",
      "\n",
      "Each blogger blogs at a very similar rate 9\n",
      "\n",
      "Before we freak out, let's do what  any experienced mathematician (and now you) would\n",
      "do 23\n",
      "Let's write out all of our information, as shown:\n",
      "H = hypothesis = the blog came from Avinash\n",
      "D = data = you loved the blog post\n",
      "P(H|D)  = the chance that it came from Avinash, given that you loved it\n",
      "P(D|H)  = the chance that you loved it, given that it came from Avinash\n",
      "P(H)  = the chance that an article came from Avinash\n",
      "P(D)  = the chance that you love an article\n",
      "Note that some of these variables make almost no sense without context 121\n",
      "P(D) , the\n",
      "probability that you would love any given article put on your desk, is a weird concept, but\n",
      "trust me, in the context of Bayes' formula, it will be relevant very soon 44\n",
      "\n",
      "Also, note that in the last two items, they assume nothing else 15\n",
      "P(D)  does not assume the\n",
      "origin of the blog post; think of P(D)  as if an article was plopped on your desk from some\n",
      "unknown source, what is the chance that you'd like it 47\n",
      "(again, I know it sounds weird out of\n",
      "context) 13\n",
      "\n",
      "So, we want to know P(H|D) 12\n",
      "Let's try to use Bayes' theorem, as shown here:\n",
      "But, do we know the numbers on the right-hand side of this equation 29\n",
      "I claim we do 4\n",
      "Let's\n",
      "see here:\n",
      "P(H)  is the probability that any given blog post comes from Avinash 23\n",
      "As bloggers\n",
      "write at a very similar rate, we can assume this is 0 17\n",
      "5 because we have a 50/50\n",
      "chance that it came from either blogger (note how I did not assume D, the data,\n",
      "for this) 34\n",
      "\n",
      "P(D|H)  is the probability that you love a post from Avinash, which we\n",
      "previously said was 50%, so, 0 34\n",
      "5 2\n",
      "\n",
      "\n",
      "P(D)  is interesting 7\n",
      "This is the chance that you love an article in general 11\n",
      "It means\n",
      "that we must take into account the scenario whether the post came from Lucy or\n",
      "Avinash 22\n",
      "Now, if the hypothesis forms a suite, then we can use our laws of\n",
      "probability, as mentioned in the previous chapter 25\n",
      "A suite is formed when a set of\n",
      "hypotheses is both collectively exhaustive and mutually exclusive 19\n",
      "In layman's\n",
      "terms, in a suite of events, exactly one and only one hypothesis can occur 21\n",
      "In our\n",
      "case, the two hypotheses are that the article came from Lucy, or that the article\n",
      "came from Avinash 26\n",
      "This is definitely a suite because of the following reasons:\n",
      "At least one of them wrote it\n",
      "At most one of them wrote it\n",
      "Therefore, exactly one  of them wrote it\n",
      "When we have a suite, we can use our multiplication and addition rules, as follows:\n",
      "D= (From, Avinash AND loved it) OR (From Lucy AND loved it)\n",
      "P(D)= P(Loved AND from Avinash) OR P(Loved AND from Lucy)\n",
      "P(D)= P(From Avinash) P(loved|from Avinash) +P(from Lucy) P(Loved|from Lucy)\n",
      "P(D)= 129\n",
      "5( 3\n",
      "5)+ 3\n",
      "5( 3\n",
      "8)= 3\n",
      "65\n",
      "Whew 5\n",
      "Way to go 3\n",
      "Now we can finish our equation, as shown:\n",
      "This means that there is a 38% chance that this article comes from Avinash 28\n",
      "What is\n",
      "interesting is that P(H) = 0 12\n",
      "5  and P(H|D) = 0 12\n",
      "38 2\n",
      "It means that without any data, the chance\n",
      "that a blog post came from Avinash was a coin flip, or 50/50 29\n",
      "Given some data (your\n",
      "thoughts on the article), we updated our beliefs about the hypothesis and it actually\n",
      "lowered the chance 27\n",
      "This is what Bayesian thinking is all about: updating our posterior\n",
      "beliefs about something from a prior assumption, given some new data about the subject 29\n",
      "\n",
      "\n",
      "More applications of Bayes' theorem\n",
      "Bayes' theorem shows up in a lot of applications, usually when we need to make fast\n",
      "decisions based on data and probability 36\n",
      "Most recommendation engines, such as Netflix's,\n",
      "use some elements of Bayesian updating 15\n",
      "And if you think through why that might be, it\n",
      "makes sense 14\n",
      "\n",
      "Let's suppose that in our simplistic world, Netflix only has 10 categories to choose from 19\n",
      "\n",
      "Now suppose that given no data, a user's chance of liking a comedy movie out of 10\n",
      "categories is 10% (just 1/10) 34\n",
      "\n",
      "Okay, now suppose that the user has given a few comedy movies 5/5 stars 19\n",
      "Now, when\n",
      "Netflix is wondering what the chance is that the user would like another comedy, the\n",
      "probability that they might like a comedy, P(H|D) , is going to be larger than a random\n",
      "guess of 10% 49\n",
      "\n",
      "Let's try some more examples of applying Bayes' theorem using more data 16\n",
      "This time, let's\n",
      "get a bit grittier 11\n",
      "\n",
      "Example – Titanic\n",
      "A very famous dataset involves looking at the survivors of the sinking of the Titanic in\n",
      "1912 24\n",
      "We will use an application of probability in order to figure out if there were any\n",
      "demographic features  that showed a relationship to passenger survival 28\n",
      "Mainly, we are\n",
      "curious to see if we can isolate any features of our dataset that can tell us more about the\n",
      "types of people who were likely to survive this disaster 37\n",
      "First, let's read in the data, as\n",
      "shown here:\n",
      "titanic =\n",
      "pd 19\n",
      "read_csv(https://raw 6\n",
      "githubusercontent 2\n",
      "com/sinanuozdemir/SF_DAT_15/maste\n",
      "r/data/titanic 20\n",
      "csv ')#read in a csv\n",
      "titanic = titanic[['Sex', 'Survived']]  #the Sex and Survived column\n",
      "titanic 33\n",
      "head()\n",
      "\n",
      "We get the following table:\n",
      "In the preceding table, each row represents a single passenger on the ship, and, for now, we\n",
      "are looking at two specific features: the sex of the individual and whether or not they\n",
      "survived the sinking 53\n",
      "For example, the first row represents a man who did not survive,\n",
      "while the fourth row (with index 3, remember how Python indexes lists) represents a\n",
      "female who did survive 37\n",
      "\n",
      "Let's start with some basics 7\n",
      "We'll start by calculating the probability that any given person\n",
      "on the ship survived, regardless of their gender 21\n",
      "To do this, we'll count the number of yeses\n",
      "in the Survived  column and divide this figure by the total number of rows, as shown here:\n",
      "num_rows = float(titanic 41\n",
      "shape[0]) # == 891 rows\n",
      "p_survived = (titanic 19\n",
      "Survived==\"yes\") 5\n",
      "sum() / num_rows # == 7\n",
      "38\n",
      "p_notsurvived = 1 - p_survived                          # == 19\n",
      "61\n",
      "Note that I only had to calculate P(Survived) , and I used the law of conjugate probabilities to\n",
      "calculate P(Died)  because those two events are complementary 39\n",
      "Now, let's calculate the\n",
      "probability that any single passenger is male or female:\n",
      "p_male = (titanic 24\n",
      "Sex==\"male\") 4\n",
      "sum() / num_rows     # == 8\n",
      "65\n",
      "p_female = 1 - p_male # == 13\n",
      "35\n",
      "Now, let's ask ourselves a question: did having a certain gender affect the survival rate 21\n",
      "For\n",
      "this, we can estimate P(Survived|Female)  or the chance that someone survived given that\n",
      "they were a female 28\n",
      "For this, we need to divide the number of women who survived by the\n",
      "total number of women, as shown here:\n",
      "\n",
      "\n",
      "Here's the code for that calculation:\n",
      "number_of_women = titanic[titanic 43\n",
      "Sex=='female'] 4\n",
      "shape[0] # == 314\n",
      "women_who_lived = titanic[(titanic 21\n",
      "Sex=='female') &\n",
      "(titanic 8\n",
      "Survived=='yes')] 5\n",
      "shape[0]                       # == 233\n",
      "p_survived_given_woman = women_who_lived / float(number_of_women)\n",
      "p_survived_given_woman            # == 40\n",
      "74\n",
      "That's a pretty big difference 9\n",
      "It seems that gender plays a big part in this dataset 11\n",
      "\n",
      "Example  – medical studies\n",
      "A classic use of Bayes' theorem is the interpretation of medical trials 21\n",
      "Routine testing for\n",
      "illegal drug use is increasingly common in workplaces and schools 14\n",
      "The companies that\n",
      "perform these tests maintain that the tests have a high sensitivity, which means that they\n",
      "are likely to produce a positive result if there are drugs in their system 35\n",
      "They claim that these\n",
      "tests are also highly specific, which means that they are likely to yield a negative result if\n",
      "there are no drugs 28\n",
      "\n",
      "On average, let's assume that the sensitivity of common drug tests is about 60% and the\n",
      "specificity is about 99% 29\n",
      "It means that if an employee is using drugs, the test has a 60%\n",
      "chance of being positive, while if an employee is not on drugs, the test has a 99% chance of\n",
      "being negative 44\n",
      "Now, suppose these tests are applied to a workforce where the actual rate\n",
      "of drug use is 5% 22\n",
      "\n",
      "The real question here is of the people who test positive, how many actually use drugs 18\n",
      "\n",
      "In Bayesian terms, we want to compute the probability of drug use, given a positive test:\n",
      "Let D = the event that drugs are in use\n",
      "Let E = the event that the test is positive\n",
      "Let N = the event that drugs are NOT  in use\n",
      "We are looking for P(D|E) 64\n",
      "\n",
      "By using Bayes' theorem, we can extrapolate it as follows:\n",
      "\n",
      "\n",
      "The prior, P(D)  is the probability of drug use before we see the outcome of the test, which is\n",
      "5% 43\n",
      "The likelihood, P(E|D) , is the probability of a positive test assuming drug use, which is\n",
      "the same thing as the sensitivity of the test 32\n",
      "The normalizing constant, P(E) , is a little bit\n",
      "trickier 17\n",
      "\n",
      "We have to consider two things: P(E and D)  as well as P(E and N) 22\n",
      "Basically, we must\n",
      "assume that the test is capable of being incorrect when the user is not using drugs 21\n",
      "Check\n",
      "out the following equations:\n",
      "So, our original equation becomes as follows:\n",
      "This means that of the people who test positive for drug use, about a quarter are innocent 34\n",
      "\n",
      "Random variables\n",
      "A random variable  uses real numerical values  to describe a probabilistic event 19\n",
      "In our\n",
      "previous work with variables (both in math and programming), we were used to the fact\n",
      "that a variable takes on a certain value 29\n",
      "For example, we might have a right-angled triangle\n",
      "in which we are given the variable h for the hypotenuse, and we must figure out the length\n",
      "of the hypotenuse 39\n",
      "We also might have the following, in Python:\n",
      "x = 5\n",
      "Both of these variables are equal to one value at a time 27\n",
      "In a random variable, we are subject\n",
      "to randomness, which means that our variables' values are, well, just that, variable 27\n",
      "They\n",
      "might take on multiple values depending on the environment 11\n",
      "\n",
      "\n",
      "A random variable still, as shown previously, holds a value 13\n",
      "The main distinction between\n",
      "variables as we have seen them and a random variable is the fact that a random variable's\n",
      "value may change depending on the situation 31\n",
      "\n",
      "However, if a random variable can have many values, how do we keep track of them all 20\n",
      "\n",
      "Each value that a random variable might take on is associated with a percentage 15\n",
      "For every\n",
      "value that a random variable might take on, there is a single probability that the variable\n",
      "will be this value 25\n",
      "\n",
      "With a random variable, we can also obtain our probability distribution of a random\n",
      "variable, which gives the variable's possible values and their probabilities 29\n",
      "\n",
      "Written out, we generally use single capital letters (mostly the specific letter X) to denote\n",
      "random variables 22\n",
      "For example, we might have the following:\n",
      "X: The outcome of a dice roll\n",
      "Y: The revenue earned by a company this year\n",
      "Z: The score of an applicant on an interview coding quiz (0-100%)\n",
      "Effectively, a random variable is a function that maps values from the sample space of an\n",
      "event (the set of all possible outcomes) to a probability value (between 0 and 1) 87\n",
      "Think\n",
      "about the event as being expressed as the following:\n",
      "It will assign a probability to each individual option 21\n",
      "There are two main types of random\n",
      "variables: discrete and continuous 13\n",
      "\n",
      "Discrete random variables\n",
      "A discrete random variable only  takes on a countable number  of possible values, such as\n",
      "the outcome of a dice roll, as shown here:\n",
      "\n",
      "\n",
      "Note how I use a capital X to define the random variable 49\n",
      "This is a common practice 5\n",
      "Also\n",
      "note how the random variable maps a probability to each individual outcome 14\n",
      "Random\n",
      "variables have  many properties, two of which are their expected value  and the variance 19\n",
      "We\n",
      "will use a probability mass function  (PMF ) to describe a discrete random variable 19\n",
      "\n",
      "They take on the following appearance:\n",
      "P(X = x) = PMF\n",
      "So, for a dice roll, P(X = 1) = 1/6  and P(X = 5) = 1/6 48\n",
      "\n",
      "Consider the following examples of discrete variables:\n",
      "The likely result of a survey question (for example, on a scale of 1-10)\n",
      "Whether the CEO will resign within the year (either true or false)\n",
      "The expected value of a random variable defines the mean value of a long run of repeated\n",
      "samples of the random variable 66\n",
      "This is sometimes called the mean of the variable 9\n",
      "\n",
      "For example, refer to the following Python code, which defines the random variable of a\n",
      "dice roll:\n",
      "import random\n",
      "def random_variable_of_dice_roll():\n",
      "    return random 35\n",
      "randint(1, 7) # a range of (1,7) # includes 1, 2, 3, 4,\n",
      "5, 6, but NOT 7\n",
      "This function will invoke a random variable and come out with a response 53\n",
      "Let's roll 100\n",
      "dice and average the result, as follows:\n",
      "trials = []\n",
      "num_trials = 100\n",
      "for trial in range(num_trials):\n",
      "trials 34\n",
      "append( random_variable_of_dice_roll() )\n",
      "print(sum(trials)/float(num_trials))  # == 3 23\n",
      "77\n",
      "So, taking 100 dice rolls and averaging them gives us a value of 3 20\n",
      "77 2\n",
      "Let's try this with a\n",
      "wide variety of trial numbers, as illustrated here:\n",
      "num_trials = range(100,10000, 10)\n",
      "avgs = []\n",
      "for num_trial in num_trials:\n",
      "    trials = []\n",
      "    for trial in range(1,num_trial):\n",
      "        trials 57\n",
      "append( random_variable_of_dice_roll() )\n",
      "    avgs 12\n",
      "append(sum(trials)/float(num_trial))\n",
      "\n",
      "plt 10\n",
      "plot(num_trials, avgs)\n",
      "plt 8\n",
      "xlabel('Number of Trials')\n",
      "plt 7\n",
      "ylabel(\"Average\")\n",
      "We get the following graph:\n",
      "The preceding graph represents the average dice roll as we look at more and more dice\n",
      "rolls 28\n",
      "We can see that the average dice roll is rapidly approaching 3 13\n",
      "5 2\n",
      "If we look towards\n",
      "the left of the graph, we see that if we only roll a die about 100 times, then we are not\n",
      "guaranteed to get an average dice roll of 3 42\n",
      "5 2\n",
      "However, if we roll 10,000 dice one after\n",
      "another, we see that we would very likely expect the average dice roll to be about 3 32\n",
      "5 2\n",
      "\n",
      "For a discrete random variable, we can also use a simple formula, shown as follows, to\n",
      "calculate the expected value:\n",
      "Expected value = E[X]= μx=∑\n",
      "Here, xi is the ith outcome and pi is the ith probability 51\n",
      "\n",
      "So, for our dice roll, we can find the exact expected value as follows:\n",
      "\n",
      "\n",
      "The preceding result shows us that for any given dice roll, we can \"expect\" a dice roll of 3 41\n",
      "5 2\n",
      "\n",
      "Now, obviously, that doesn't make sense because we can't get a 3 18\n",
      "5 on a dice roll, but it\n",
      "does make sense when put in the context of many dice rolls 22\n",
      "If you roll 10,000 dice, your\n",
      "average dice roll should approach 3 18\n",
      "5, as shown in the graph and code previously 11\n",
      "\n",
      "The average of the expected value of a random variable is generally not enough to grasp\n",
      "the full idea behind the variable 24\n",
      "For this reason, we introduce a new concept, called\n",
      "variance 14\n",
      "\n",
      "The variance of a random variable represents the spread of the variable 13\n",
      "It quantifies the\n",
      "variability of the expected value 11\n",
      "\n",
      "The formula for the variance of a discrete random variable is expressed as follows:\n",
      "xi and pi represent the same values as before and represents the expected value of the\n",
      "variable 34\n",
      "In this formula, I also mentioned the sigma of X 11\n",
      "Sigma, in this case, is the\n",
      "standard deviation, which is defined simply as the square root of the variance 23\n",
      "Let's look at\n",
      "a more complicated example of a discrete random variable 14\n",
      "\n",
      "Variance can be thought of like a give or take  metric 14\n",
      "If I say you can expect  to win $100 from\n",
      "a poker hand, you might be very happy 22\n",
      "If I append that statement with the additional\n",
      "detail that you might win $100, give or take $80, you now have a wide  range of\n",
      "expectations to deal with, which can be frustrating and might make a risk-averse player\n",
      "more wary of joining the game 57\n",
      "We can usually say that we have  an expected value, give or\n",
      "take the standard deviation 19\n",
      "\n",
      "Consider that your team measures the success  of a new product on a Likert scale , that is, as\n",
      "being in one of five categories, where a value of 0 represents a complete failure and 4\n",
      "represents a great success 50\n",
      "They estimate that a new project has the following chances of\n",
      "success based on user testing and the preliminary results of the performance of the product 27\n",
      "\n",
      "We first have to define our random variable 9\n",
      "\n",
      "Let the X random variable represent the success of our product 12\n",
      "X is indeed a discrete\n",
      "random variable because the X variable can only take on one of five options: 0, 1, 2, 3, or 4 36\n",
      "\n",
      "\n",
      "The following is the probability distribution of our random variable, X 13\n",
      "Note how we have\n",
      "a column for each potential outcome of X and, following each outcome, we have the\n",
      "probability that that particular outcome will be achieved:\n",
      "For example, the project has a 2% chance of failing completely and a 26% chance of being a\n",
      "great success 58\n",
      "We can calculate our expected value as follows:\n",
      "E[X] = 0(0 17\n",
      "02) + 1(0 8\n",
      "07) + 2(0 8\n",
      "25) + 3(0 8\n",
      "4) + 4(0 8\n",
      "26) = 2 6\n",
      "81\n",
      "This number means that the manager can expect  a success of about 2 18\n",
      "81 out of this project 6\n",
      "\n",
      "Now, by itself, that number is not very useful 12\n",
      "Perhaps, if given several products to choose\n",
      "from, an expected value might be a way to compare the potential successes of several\n",
      "products 27\n",
      "However, in this case, when we have but the one product to evaluate, we will\n",
      "need more 21\n",
      "\n",
      "Now, let's check the variance, as shown here:\n",
      "Variance= V[X]=σX2 = (xi −μX)2pi = (0 − 2 37\n",
      "81)2(0 6\n",
      "02) + (1 − 2 9\n",
      "81)2(0 6\n",
      "07)+  ",
      "(2 − 2 11\n",
      "81)2(0 6\n",
      "25) + (3 −\n",
      "2 9\n",
      "81)2(0 6\n",
      "4) + (4 − 2 9\n",
      "81)2(0 6\n",
      "26) = 4\n",
      "93\n",
      "Now that we have both the standard deviation and the expected value of the score of the\n",
      "project, let's try to summarize our results 30\n",
      "We could say that our project will have an\n",
      "expected score of 2 15\n",
      "81 plus or minus 0 7\n",
      "96, meaning that we can expect something between\n",
      "1 12\n",
      "85 and 3 5\n",
      "77 2\n",
      "\n",
      "So, one way we can address this project is that it is probably going to have a success rating\n",
      "of 2 25\n",
      "81, give or take about a point 9\n",
      "\n",
      "You might be thinking, wow, Sinan, so at best the project will be a 3 21\n",
      "8 and at worst it will be a\n",
      "1 11\n",
      "8 2\n",
      " 1\n",
      "Not quite 2\n",
      "\n",
      "It might be better than a 4 and it might also be worse than a 1 19\n",
      "8 2\n",
      "To take this one step\n",
      "further, let's calculate the following:\n",
      "P(X >= 3)\n",
      "\n",
      "First, take a minute and convince yourself that you can read that formula to yourself 37\n",
      "What\n",
      "am I asking when I am asking for P(X >= 3) 16\n",
      "Honestly, take a minute and figure it out 9\n",
      "\n",
      "P(X >= 3)  is the probability that our random variable will take on a value at least as big as 3 27\n",
      "\n",
      "In other words, what is the chance that our product will have a success rating of 3 or\n",
      "higher 23\n",
      "To calculate this, we can calculate the following:\n",
      "P(X >= 3) = P(X = 3) + P(X = 4) = 31\n",
      "66 = 66%\n",
      "This means that we have a 66% chance that our product will rate as either a 3 or a 4 30\n",
      "\n",
      "Another way to calculate this would be the conjugate way, as shown here:\n",
      "P(X >= 3) = 1 - P(X < 3)\n",
      "Again, take a moment to convince yourself that this formula holds up 46\n",
      "I am claiming that to\n",
      "find the probability that the product will be rated at least a 3 is the same as 1 minus the\n",
      "probability that the product will receive a rating below 3 40\n",
      "If this is true, then the two events\n",
      "(X >=3  and X < 3 ) must complement one another 24\n",
      "\n",
      "This is obviously true 5\n",
      "The product can be either of the following two options:\n",
      "Be rated 3 or above\n",
      "Be rated below a 3\n",
      "Let's check our math:\n",
      "P(X < 3) = P(X = 0) + P(X = 1) + P(X = 2)\n",
      "= 0 61\n",
      "02 + 0 5\n",
      "07 + 0 5\n",
      "25\n",
      "= 4\n",
      "0341 - P(X < 3)\n",
      "= 1 - 14\n",
      "34\n",
      "= 4\n",
      "66\n",
      "= P( x >= 3)\n",
      "It checks out 14\n",
      "\n",
      "\n",
      "Types of discrete random variables\n",
      "We can get a better idea of how random  variables work in practice by looking at specific\n",
      "types of random variables 30\n",
      "These specific types of random variables model different types\n",
      "of situations and end up revealing much simpler calculations for very complex event\n",
      "modeling 26\n",
      "\n",
      "Binomial random variables\n",
      "The first type of discrete random  variable we will look at is called a binomial random\n",
      "variable 26\n",
      "With a binomial random  variable, we look at a setting in which a single event\n",
      "happens over and over and we try to count the number of times the result is positive 38\n",
      "\n",
      "Before we can understand the random variable itself, we must look at the conditions in\n",
      "which it is even appropriate 23\n",
      "\n",
      "A binomial setting has the following four conditions:\n",
      "The possible outcomes are either success or failure\n",
      "The outcomes of trials cannot affect the outcome of another trial\n",
      "The number of trials was set (a fixed sample size)\n",
      "The chance of success of each trial must always be p\n",
      "A binomial random variable is a discrete random variable, X, that counts the number of\n",
      "successes in a binomial setting 82\n",
      "The parameters are n = the number of trials  and p = the chance\n",
      "of success of each trial 21\n",
      "\n",
      "Example: Fundraising meetings\n",
      "A start-up is taking 20 VC meetings to fund and count the number of offers they receive 26\n",
      "\n",
      "The PMF  for a binomial random  variable is as follows:\n",
      "\n",
      "\n",
      "Example: Restaurant openings\n",
      "A new restaurant in a town has a 20% chance of surviving its first year 38\n",
      "If 14 restaurants\n",
      "open this year, find the probability that exactly four restaurants survive their first year of\n",
      "being open to the public 27\n",
      "\n",
      "First, we should prove that this is a binomial setting:\n",
      "The possible outcomes are either success or failure (the restaurants either survive\n",
      "or not)\n",
      "The outcomes of trials cannot affect the outcome of another trial (assume that the\n",
      "opening of one restaurant doesn't affect another restaurant's opening and\n",
      "survival)\n",
      "The number of trials was set (14 restaurants opened)\n",
      "The chance of success of each trial must always be p (we assume that it is always\n",
      "20%)\n",
      "Here, we have our two parameters of n = 14  and p = 0 115\n",
      "2 2\n",
      "So, we can now plug these numbers\n",
      "into our binomial formula, as shown here:\n",
      "So, we have a 17% chance that exactly 4 of these restaurants will be open after a year 41\n",
      "\n",
      "Example: Blood types\n",
      "A couple has a 25% chance of a having a child with type O blood 23\n",
      "What is the chance that\n",
      "three of their five kids have type O blood 15\n",
      "\n",
      "Let X = the number of children with type O blood  with n = 5  and p = 0 24\n",
      "25 , as shown here:\n",
      "We can calculate this probability for the values of 0, 1, 2, 3, 4, and 5 to get a sense of the\n",
      "probability distribution:\n",
      "\n",
      "\n",
      "From here, we can calculate  an expected value  and the variance of this variable:\n",
      "So, this family can expect to have probably one or two kids with type O blood 79\n",
      "\n",
      "What if we want to know the probability that at least three of their kids have type O blood 20\n",
      "\n",
      "To know the probability that at least three of their kids have type O blood, we can use the\n",
      "following formula for discrete random variables:\n",
      "P(x>=3) =P(X=3)+P(X=4)+P(X=3) = 51\n",
      "00098+ 4\n",
      "01465+ 4\n",
      "08789=0 5\n",
      "103\n",
      "So, there is about a 10% chance that three of their kids have type O blood 22\n",
      "\n",
      "Shortcuts to binomial expected value and variance\n",
      "Binomial random variables have special calculations for the exact values\n",
      "of the expected values and variance 29\n",
      "If X is a binomial random variable,\n",
      "then we get the following:\n",
      "E(X) = np\n",
      "V(X) = np(1 − p)\n",
      "For our preceding example, we can use the following formulas to calculate\n",
      "an exact expected value and variance:\n",
      "E(X) = 56\n",
      "25(5) = 1 8\n",
      "25\n",
      "V(X) = 1 9\n",
      "25( 3\n",
      "75) = 0 6\n",
      "9375\n",
      "A binomial random variable is a discrete random variable that counts the number of\n",
      "successes in a binomial setting 27\n",
      "It is used in a wide variety of data-driven experiments, such\n",
      "as counting the number of people who will sign up for a website given a chance of\n",
      "conversion, or even, at a simple level, predicting stock price movements given a chance of\n",
      "decline (don't worry; we will be applying much more sophisticated models to predict the\n",
      "stock market later) 74\n",
      "\n",
      "Geometric random variables\n",
      "The second discrete random variable we will take a look at is called a geometric random\n",
      "variable 24\n",
      "It is actually quite similar to the binomial random variable in that we are\n",
      "concerned with a setting in which a single event is occurring over and over 32\n",
      "However, in\n",
      "the case of a geometric setting, the major difference is that we are not fixing the sample size 23\n",
      "\n",
      "\n",
      "We are not going into exactly 20 VC meetings as a start-up, nor are we having exactly five\n",
      "kids 24\n",
      "Instead, in a geometric setting, we are modeling the number of trials we will need to\n",
      "see before we obtain even a single success 27\n",
      "Specifically, a geometric setting has the\n",
      "following four conditions:\n",
      "The possible outcomes are either success or failure\n",
      "The outcomes of trials cannot affect the outcome of another trial\n",
      "The number of trials was not set\n",
      "The chance of success of each trial must always be p\n",
      "Note that these are the exact same conditions as a binomial variable, except the third\n",
      "condition 72\n",
      "\n",
      "A geometric random variable  is a discrete  random variable, X, that counts the number of\n",
      "trials needed to obtain one success 28\n",
      "The parameters are p = the chance of success of each trial\n",
      "and (1 − p) = the chance of failure of each trial 27\n",
      "\n",
      "To transform the previous binomial examples into geometric examples, we might do the\n",
      "following:\n",
      "Count the number of VC meetings that a start-up must take in order to get their\n",
      "first yes\n",
      "Count the number of coin flips needed in order to get a head (yes, I know it's\n",
      "boring, but it's a solid example 70\n",
      "\n",
      "The formula for the PMF is as follows:\n",
      "Both the binomial and geometric settings involve outcomes that are either successes or\n",
      "failures 28\n",
      "The big difference is that binomial random variables have a fixed number of trials,\n",
      "denoted as n 20\n",
      "Geometric random variables do not have a fixed number of trials 12\n",
      "Instead,\n",
      "geometric random variables model the number of samples needed in order to obtain the\n",
      "first successful trial, whatever success might mean in those experimental conditions 30\n",
      "\n",
      "Example: Weather\n",
      "There is a 34% chance that it will rain on any day in April 21\n",
      "Find the probability that the first\n",
      "day of rain in April will occur on April 4 18\n",
      "\n",
      "Let X = the number of days until it rains  (success) with p = 0 20\n",
      "34  and (1 − p) = 0 12\n",
      "66\n",
      "\n",
      "Lets' work it out: \n",
      "= 0 13\n",
      "054\n",
      "The probability that it will rain by the April 4 is as follows:\n",
      "P(X<=4) = P(1) + P(2) + P(3) + P(4) = 44\n",
      "34 + 3\n",
      "22 + 3\n",
      "14 +>1 = 6\n",
      "8\n",
      "So, there is an 80% chance that the first rain of the month will happen within the first four\n",
      "days 27\n",
      "\n",
      "Shortcuts to geometric expected value and variance\n",
      "Geometric random variables also have special calculations for the exact\n",
      "values of the expected values and variance 29\n",
      "If X is a geometric random\n",
      "variable, then we get the following:\n",
      "E(X) = 1/p\n",
      "Poisson random variable\n",
      "The third and last specific example of a discrete  random variable is a Poisson random\n",
      "variable 48\n",
      "\n",
      "To understand why we would need  this random variable, imagine that an event that we\n",
      "wish to model has a small probability of happening and that we wish to count the number\n",
      "of times that the event occurs in a certain time frame 48\n",
      "If we have an idea of the average\n",
      "number of occurrences, µ, over a specific period of time, given from past instances, then the\n",
      "Poisson random variable, denoted by X = Poi( µ), counts the total number of occurrences of\n",
      "the event during that given time period 61\n",
      "\n",
      "In other words, the Poisson distribution is a discrete probability distribution that counts the\n",
      "number of events that occur in a given interval of time 29\n",
      "\n",
      "Consider the following examples of Poisson random variables:\n",
      "Finding the probability of having a certain number of visitors on your site within\n",
      "an hour, knowing the past performance of the site\n",
      "Estimating the number of cars crashes at an intersection based on past police\n",
      "reports\n",
      "\n",
      "If we let X = the number of events in a given interval , and the average number of events per\n",
      "interval is the λ number, then the probability of observing x events in a given interval is\n",
      "given by the following formula:\n",
      "Here, e = Euler's constant (2 110\n",
      "718 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "Example: Call center\n",
      "The number of calls arriving at your call center follows a Poisson distribution at the rate of\n",
      "five calls per hour 29\n",
      "What is the probability that exactly six calls will come in between 10 and\n",
      "11 p 18\n",
      "m 1\n",
      " 1\n",
      "\n",
      "To set up this example, let's write out our given information 14\n",
      "Let X be the number of calls\n",
      "that arrive between 10 and 11 p 17\n",
      "m 1\n",
      "This is our Poisson random variable with the mean λ = 5 14\n",
      "\n",
      "The mean is 5 because we are using 5 as our previous expected value of the number of calls\n",
      "to come in at this time 29\n",
      "This number could have come from the previous work on\n",
      "estimating the number of calls that come in every hour or specifically that come in after 10\n",
      "p 32\n",
      "m 1\n",
      "The main idea is that we do have some idea of how many calls should be coming in,\n",
      "and then we use that information to create our Poisson random variable and use it to make\n",
      "predictions 39\n",
      "\n",
      "Continuing with our example, we have the following:\n",
      "P(X = 6) = 0 21\n",
      "146\n",
      "This means that there is about a 14 12\n",
      "6% chance that exactly six calls will come between 10\n",
      "and 11 p 18\n",
      "m 1\n",
      "\n",
      "Shortcuts to Poisson expected value and variance\n",
      "Poisson random variables also have special calculations for the exact\n",
      "values of the expected values and variance 31\n",
      "If X is a Poisson random\n",
      "variable with mean, then we get the following:\n",
      "E(X) = λ\n",
      "V(X) = λ\n",
      "This is actually interesting because both the expected value and the variance are the same\n",
      "number, and that number is simply the given parameter 56\n",
      "Now that we've seen three\n",
      "examples of discrete random variables, we must take a look at the other type of random\n",
      "variable, called the continuous random variable 32\n",
      "\n",
      "\n",
      "Continuous random variables\n",
      "Switching gears entirely, unlike a discrete  random variable, a continuous random variable\n",
      "can take on an infinite  number of possible values, not just a few countable ones 40\n",
      "We call the\n",
      "functions that describe the distribution density curves instead  of PMF 16\n",
      "\n",
      "Consider the following examples of continuous variables:\n",
      "The length of a sales representative's phone call (not the number of calls)\n",
      "The actual amount of oil in a drum marked 20 gallons (not the number of oil\n",
      "drums)\n",
      "If X is a continuous random variable, then there is a function, f(x), for any constants a and b:\n",
      "The preceding f(x) function is known  as the probability density function  (PDF ) 89\n",
      "The PDF is\n",
      "the continuous random variable version of the PMF for discrete random variables 17\n",
      "\n",
      "The most important continuous  distribution is the standard normal distribution 12\n",
      "You have,\n",
      "no doubt, either heard of the normal distribution or dealt with it 16\n",
      "The idea behind it is quite\n",
      "simple 8\n",
      "The PDF of this distribution is as follows:\n",
      "Here, μ is the mean of the variable and σ is the standard deviation 24\n",
      "This might look\n",
      "confusing, but let's graph it in Python with a mean of 0 and a standard deviation of 1, as\n",
      "shown here:\n",
      "import numpy as np\n",
      "import matplotlib 40\n",
      "pyplot as plt\n",
      "def normal_pdf(x, mu = 0, sigma = 1):\n",
      " return (1 22\n",
      "/np 2\n",
      "sqrt(2*3 5\n",
      "14 * sigma**2)) * np 9\n",
      "exp((-(x-mu)**2 / (2 11\n",
      "*\n",
      "sigma**2)))\n",
      "x_values = np 9\n",
      "linspace(-5,5,100)\n",
      "y_values = [normal_pdf(x) for x in x_values]\n",
      "plt 23\n",
      "plot(x_values, y_values)\n",
      "\n",
      "We get this graph:\n",
      "This gives rise to the all-too-familiar bell curve 23\n",
      "Note that the graph is symmetrical around\n",
      "the x = 0  line 16\n",
      "Let's try changing some of the parameters 8\n",
      "First, let's try with :\n",
      "\n",
      "\n",
      "Next, let's try with the value :\n",
      "Lastly, we will try with the values :\n",
      "\n",
      "\n",
      "In all the graphs, we have the standard bell shape  that we are all familiar with, but as we\n",
      "change our parameters, we see that the bell might get skinnier, thicker, or move from left to\n",
      "right 74\n",
      "\n",
      "In the following chapters, which focus on statistics, we will make much more use of the\n",
      "normal distribution as it applies to statistical thinking 28\n",
      "\n",
      "Summary\n",
      "Probability as a field works to explain our random and chaotic world 15\n",
      "Using the basic laws\n",
      "of probability, we can model real-life events that involve randomness 17\n",
      "We can use random\n",
      "variables to represent values that may take on several values, and we can use the\n",
      "probability mass or density functions to compare product lines or look at the test results 37\n",
      "\n",
      "We have seen some of the more complicated uses of probability in prediction 14\n",
      "Using\n",
      "random variables and Bayes' theorem are excellent ways to assign probabilities to real-life\n",
      "situations 22\n",
      "In later chapters, we will revisit Bayes' theorem and use it to create a very\n",
      "powerful and fast machine learning algorithm, called the Na ïve Bayes algorithm 36\n",
      "This\n",
      "algorithm captures the power of Bayesian thinking and applies it directly to the problem of\n",
      "predictive learning 21\n",
      "\n",
      "The next two chapters are focused on statistical thinking 10\n",
      "Like probability, these chapters\n",
      "will use mathematical formulas to model real-world events 15\n",
      "The main difference, however,\n",
      "will be the terminology we use to describe the world and the way we model different types\n",
      "of events 26\n",
      "In these upcoming chapters, we will attempt to model entire populations of data\n",
      "points based solely on a sample 21\n",
      "\n",
      "We will revisit many concepts in probability to make sense of statistical theorems as they\n",
      "are closely linked, and both are important mathematical concepts in the realm of data\n",
      "science 36\n",
      "\n",
      "\n",
      "\n",
      "Basic Statistics\n",
      "This chapter will focus on the statistical knowledge required by any aspiring data scientist 18\n",
      "\n",
      "We will explore ways of sampling and obtaining data without being affected by bias and\n",
      "then use measures of statistics to quantify and visualize our data 28\n",
      "Using the z-score and the\n",
      "empirical rule, we will see how we can standardize data for the purpose of both graphing\n",
      "and interpretability 31\n",
      "\n",
      "In this chapter, we will look at the following topics:\n",
      "How to obtain and sample data\n",
      "The measures of center, variance, and relative standing\n",
      "Normalization of data using the z-score\n",
      "The empirical rule\n",
      "What are statistics 46\n",
      "\n",
      "This might seem like an odd question to ask, but I am frequently surprised by the number\n",
      "of people who cannot answer this simple and yet powerful question: what are statistics 35\n",
      "\n",
      "Statistics are the numbers you always see on the news and in the paper 15\n",
      "Statistics are useful\n",
      "when trying to prove a point or trying to scare someone, but what are they 20\n",
      "\n",
      "To answer this question, we need to back up for a minute and talk about why we even\n",
      "measure them in the first place 27\n",
      "The goal of this field is to try to explain and model the\n",
      "world around us 17\n",
      "To do that, we have to take a look at the population 13\n",
      "\n",
      "We can define a population  as the entire pool of subjects of an experiment or a model 19\n",
      "\n",
      "\n",
      "Essentially, your population is who you care about 11\n",
      "Who are you trying to talk about 7\n",
      "If\n",
      "you are trying to test whether smoking leads to heart disease, your population would be\n",
      "the smokers of the world 24\n",
      "If you are trying to study teenage drinking problems, your\n",
      "population would be all teenagers 17\n",
      "\n",
      "Now, consider that you want to ask a question about your population 14\n",
      "For example, if your\n",
      "population is all of your employees (assume that you have over 1,000 employees), perhaps\n",
      "you want  to know what percentage of them use illicit drugs 38\n",
      "The question is called a\n",
      "parameter 7\n",
      "\n",
      "We can define a parameter as a numerical measurement describing a characteristic of a\n",
      "population 17\n",
      "\n",
      "For example, if you ask all 1,000 employees and 100 of them are using drugs, the rate of\n",
      "drug use is 10% 32\n",
      "The parameter here is 10% 7\n",
      "\n",
      "However, let's get real; you probably can't ask every single employee whether they are\n",
      "using drugs 22\n",
      "What if you have over 10,000 employees 10\n",
      "It would be very difficult to track\n",
      "everyone down in order to get your answer 16\n",
      "When this happens, it's impossible to figure\n",
      "out this parameter 13\n",
      "In this case, we can estimate  the parameter 10\n",
      "\n",
      "First, we will take a sample  of the population 12\n",
      "\n",
      "We can define a sample of a population as a subset (not necessarily random) of the\n",
      "population 21\n",
      "\n",
      "So, we perhaps ask 200 of the 1,000 employees you have 17\n",
      "Of these 200, suppose 26 use\n",
      "drugs, making the drug use rate 13% 21\n",
      "Here, 13% is not a parameter because we didn't get a\n",
      "chance to ask everyone 21\n",
      "This 13% is an estimate of a parameter 10\n",
      "Do you know what that's\n",
      "called 8\n",
      "\n",
      "That's right, a statistic 7\n",
      "\n",
      "We can define a statistic as a numerical measurement describing a characteristic of a sample\n",
      "of a population 20\n",
      "\n",
      "A statistic is just an estimation of a parameter 10\n",
      "It is a number that attempts to describe an\n",
      "entire population by describing a subset of that population 20\n",
      "This is necessary because you\n",
      "can never hope to give a survey to every single teenager or to every single smoker in the\n",
      "world 26\n",
      "That's what the field of statistics is all about: taking samples of populations and\n",
      "running tests on these samples 22\n",
      "\n",
      "So, the next time you are given a statistic, just remember that number only represents a\n",
      "sample of that population, not the entire pool of subjects 31\n",
      "\n",
      "\n",
      "How do we obtain and sample data 8\n",
      "\n",
      "If statistics is about taking samples  of populations, it must be very important to know how\n",
      "we obtain these samples, and you'd be correct 30\n",
      "Let's focus on just a few of the many ways  of\n",
      "obtaining and sampling data 20\n",
      "\n",
      "Obtaining data\n",
      "There are two main ways of collecting data for our analysis: observational  and\n",
      "experimentation 24\n",
      "Both these ways  have their pros and cons, of course 12\n",
      "They each produce\n",
      "different types of behavior and, therefore, warrant different types of analysis 17\n",
      "\n",
      "Observational\n",
      "We might obtain data through observational  means, which consists of measuring specific\n",
      "characteristics but not attempting to modify the subjects being studied 30\n",
      "For example, you\n",
      "have a tracking software on your website that observes users' behavior on the website, such\n",
      "as length of time spent on certain pages and the rate of clicking  on ads, all the while not\n",
      "affecting the user's experience, then that would be an observational study 60\n",
      "\n",
      "This is one of the most common ways to get data because it's just plain easy 18\n",
      "All you have\n",
      "to do is observe and collect data 11\n",
      "Observational studies are also limited in the types of data\n",
      "you may collect 15\n",
      "This is because the observer (you) is not in control of the environment 15\n",
      "You\n",
      "may only watch and collect natural behavior 9\n",
      "If you are looking to induce a certain type of\n",
      "behavior, an observational study would not be useful 20\n",
      "\n",
      "Experimental\n",
      "An experiment  consists of a treatment and the observation of its effect on the subjects 19\n",
      "\n",
      "Subjects in an experiment are called experimental units 9\n",
      "This is usually how most scientific\n",
      "labs collect data 10\n",
      "They will put people into two or more groups (usually just two) and call\n",
      "them the control and the experimental group 24\n",
      "\n",
      "The control group is exposed to a certain environment and then observed 13\n",
      "The\n",
      "experimental group is then exposed to a different environment and then observed 14\n",
      "The\n",
      "experimenter then aggregates data from both the groups and makes a decision about which\n",
      "environment was more favorable (favorable is a quality that the experimenter gets to\n",
      "decide) 41\n",
      "\n",
      "\n",
      "In a marketing example, consider that we expose half of our users to a certain landing page\n",
      "with certain images and a certain style (website A), and we measure whether or not they\n",
      "sign up for the service 44\n",
      "Then, we expose the other half to a different landing page, different\n",
      "images, and different styles (website B) and again measure whether or not they sign up 33\n",
      "We\n",
      "can then decide which of the two sites performed better and should be used going further 18\n",
      "\n",
      "This, specifically, is called an A/B test 11\n",
      "Let's see an example in Python 7\n",
      "Let's suppose we run\n",
      "the preceding test and obtain the following results as a list of lists:\n",
      "results = [ ['A', 1], ['B', 1], ['A', 0], ['A', 0] 47\n",
      " 1\n",
      " 1\n",
      "]\n",
      "Here, each object in the list result represents a subject (person) 15\n",
      "Each person then has the\n",
      "following two attributes:\n",
      "Which website they were exposed to, represented by a single character\n",
      "Whether or not they converted ( 0 for no and 1 for yes)\n",
      "We can then aggregate and come up with the following results table:\n",
      "users_exposed_to_A = []\n",
      "users_exposed_to_B = []\n",
      "# create two lists to hold the results of each individual website\n",
      "Once we create these two lists that will eventually hold each individual conversion Boolean\n",
      "(0 or 1), we will iterate all of our results of the test and add them to the appropriate list, as\n",
      "shown:\n",
      "for website, converted in results: # iterate through the results\n",
      "  # will look something like website == 'A' and converted == 0\n",
      "  if website == 'A':\n",
      "    users_exposed_to_A 165\n",
      "append(converted)\n",
      "  elif website == 'B':\n",
      "    users_exposed_to_B 17\n",
      "append(converted)\n",
      "Now, each list contains a series of 1s and 0s 19\n",
      "\n",
      "Remember that a 1 represents a user actually converting to the site after\n",
      "seeing that web page, and a 0 represents a user seeing the page and\n",
      "leaving before signing up/converting 40\n",
      "\n",
      "To get the total number of people exposed to website A, we can use the len()  feature in\n",
      "Python, as illustrated:\n",
      "len(users_exposed_to_A) == 188 #number of people exposed to website A\n",
      "len(users_exposed_to_B) == 158 #number of people exposed to website B\n",
      "\n",
      "To count the number of people who converted, we can use the sum()  of the list, as shown:\n",
      "sum(users_exposed_to_A) == 54 # people converted from website A\n",
      "sum(users_exposed_to_B) == 48 # people converted from website B\n",
      "If we subtract the length of the lists and the sum of the list, we are left with the number of\n",
      "people who did not convert for each site, as illustrated:\n",
      "len(users_exposed_to_A) - sum(users_exposed_to_A) == 134 # did not convert\n",
      "from website A\n",
      "len(users_exposed_to_B) - sum(users_exposed_to_B) == 110 # did not convert\n",
      "from website B\n",
      "We can aggregate and summarize our results in the following table, which represents our\n",
      "experiment of website conversion testing:\n",
      "Did not sign up Signed up\n",
      "Website A 134 54\n",
      "Website B 110 48\n",
      "The results of our A/B test\n",
      "We can quickly drum up some descriptive statistics 271\n",
      "We can say that the website conversion\n",
      "rates for the two websites are as follows:\n",
      "Conversion for website A : 154 /(154+34) = 30\n",
      "288\n",
      "Conversion for website B : 48/(110+48)= 15\n",
      "3\n",
      "Not much difference, but different nonetheless 10\n",
      "Even though B has the higher conversion\n",
      "rate, can we really say that version B significantly converts better 20\n",
      "Not yet 2\n",
      "To test the\n",
      "statistical significance  of such a result, a hypothesis test should be used 19\n",
      "These tests will be\n",
      "covered in depth in the next chapter, where we will revisit this exact same example and\n",
      "finish it using a proper statistical test 30\n",
      "\n",
      "Sampling data\n",
      "Remember how statistics are the result of measuring a sample of a population 17\n",
      "Well, we\n",
      "should talk about two very common ways to decide who gets the honor of being in the\n",
      "sample that we measure 26\n",
      "We will discuss the main type of sampling, called random\n",
      "sampling, which is the most common way to decide our sample sizes and our sample\n",
      "members 30\n",
      "\n",
      "\n",
      "Probability sampling\n",
      "Probability sampling is a way  of sampling from a population, in which every person has a\n",
      "known probability of being chosen but that number might  be a different probability than\n",
      "another user 41\n",
      "The simplest (and probably the most common) probability sampling method\n",
      "is a random sampling 17\n",
      "\n",
      "Random sampling\n",
      "Suppose that we are running an A/B test and we need to figure out who will be in group A\n",
      "and who  will be in group B 35\n",
      "There are the following three suggestions from your data team:\n",
      "Separate users based on location : Users on the West coast are placed in group A,\n",
      "while users on the East coast are placed in group B\n",
      "Separate users based on the time of day they visit the site : Users who visit\n",
      "between 7 p 63\n",
      "m 1\n",
      "and 4 a 4\n",
      "m 1\n",
      "are group A, while the rest are placed in group B\n",
      "Make it completely random : Every new user has a 50/50 chance of being placed\n",
      "in either group\n",
      "The first two are valid options for choosing samples and are fairly simple to implement, but\n",
      "they both have one fundamental flaw: they are both at risk of introducing a sampling bias 71\n",
      "\n",
      "A sampling bias  occurs when the way  the sample is obtained systemically favors some\n",
      "outcome over the target outcome 24\n",
      "\n",
      "It is not difficult to see why choosing option 1 or option 2 might introduce bias 19\n",
      "If we chose\n",
      "our groups based on where they live or what time they log in, we are priming our\n",
      "experiment incorrectly and, now, we have much less control over the results 38\n",
      "\n",
      "Specifically, we are at risk of introducing a confounding factor  into our analysis, which is bad\n",
      "news 24\n",
      "\n",
      "A confounding factor  is a variable that we are not directly measuring but connects the\n",
      "variables that are being measured 24\n",
      "\n",
      "Basically, a confounding factor is like the missing element in our analysis that is invisible\n",
      "but affects our results 23\n",
      "\n",
      "In this case, option 1 is not taking into account the potential confounding factor of\n",
      "geographical taste 23\n",
      "For example, if website A is unappealing, in general, to West coast users,\n",
      "it will affect your results drastically 25\n",
      "\n",
      "\n",
      "Similarly, option 2 might introduce a temporal (time-based) confounding factor 17\n",
      "What if\n",
      "website B is better viewed in a night-time environment (which was reserved for A), and\n",
      "users are reacting negatively to the style purely because of what time it is 36\n",
      "These are both\n",
      "factors that we want to avoid, so, we should go with option 3, which is a random sample 27\n",
      "\n",
      "While sampling bias can cause confounding, it is a different concept than\n",
      "confounding 18\n",
      "Options 1 and 2 were both sampling biases because we\n",
      "chose the samples incorrectly and were also examples of confounding\n",
      "factors because there was a third variable in each case that affected our\n",
      "decision 42\n",
      "\n",
      "A random sample is chosen such that every single member of a population has an equal\n",
      "chance of being chosen as any other member 27\n",
      "\n",
      "This is probably one of the easiest and most convenient ways to decide who will be a part\n",
      "of your sample 23\n",
      "Everyone has the exact same chance of being in any particular group 12\n",
      "\n",
      "Random sampling is an effective way of reducing the impact of confounding factors 15\n",
      "\n",
      "Unequal probability sampling\n",
      "Recall that I previously said that a probability sampling might have different probabilities\n",
      "for different potential sample members 26\n",
      "But what if this actually introduced problems 7\n",
      "\n",
      "Suppose we are interested in measuring the happiness level of our employees 14\n",
      "We already\n",
      "know that we can't ask every single member of staff because that would be silly and\n",
      "exhausting 24\n",
      "So, we need to take a sample 8\n",
      "Our data team suggests random sampling, and at\n",
      "first everyone high-fives because they feel very smart and statistical 22\n",
      "But then someone asks\n",
      "a seemingly harmless question: does anyone know the percentage of men/women who\n",
      "work here 23\n",
      "\n",
      "The high fives stop and the room goes silent 11\n",
      "\n",
      "This question is extremely important because sex is likely to be a confounding factor 16\n",
      "The\n",
      "team looks into it and discovers a split of 75% men and 25% women in the company 23\n",
      "\n",
      "This means that if we introduce a random sample, our sample will likely have a similar\n",
      "split and thus favor the results for men and not women 30\n",
      "To combat this, we can favor\n",
      "including more women than men in our survey in order to make the split of our sample less\n",
      "favored for men 31\n",
      "\n",
      "\n",
      "At first glance, introducing a favoring system in our random sampling seems like a bad\n",
      "idea; however, alleviating unequal sampling and, therefore, working to remove systematic\n",
      "bias among gender, race, disability, and so on is much more pertinent 51\n",
      "A simple random\n",
      "sample, where everyone has the same chance as everyone else, is very likely to drown out\n",
      "the voices and opinions of minority population members 31\n",
      "Therefore, it can be okay to\n",
      "introduce such a favoring system in your sampling techniques 19\n",
      "\n",
      "How do we measure statistics 6\n",
      "\n",
      "Once we have our sample, it's time to quantify  our results 15\n",
      "Suppose we wish to generalize\n",
      "the happiness of our employees or we want to figure out whether salaries in the company\n",
      "are very different from person to person 30\n",
      "\n",
      "These are some common ways of measuring our results 10\n",
      "\n",
      "Measures of center\n",
      "Measures of the center are how we define  the middle, or center, of a dataset 25\n",
      "We do this\n",
      "because sometimes we wish to make generalizations about data values 15\n",
      "For example,\n",
      "perhaps we're curious about what the average rainfall in Seattle is or what the median\n",
      "height of European males is 25\n",
      "It's a way to generalize a large set of data so that it's easier to\n",
      "convey to someone 22\n",
      "\n",
      "A measure of center is a value in the middle  of a dataset 15\n",
      "\n",
      "However, this can mean different things to different people 11\n",
      "Who's to say where the middle\n",
      "of a dataset is 12\n",
      "There are so many different ways of defining the center of data 12\n",
      "Let's take a\n",
      "look at a few 9\n",
      "\n",
      "The arithmetic mean  of a dataset is found  by adding up all of the values and then dividing\n",
      "it by the number of data values 29\n",
      "\n",
      "This is likely the most common way to define the center of data, but can be flawed 19\n",
      "Suppose\n",
      "we wish to find the mean of the following numbers:\n",
      "import numpy as np\n",
      "np 19\n",
      "mean([11, 15, 17, 14]) == 14 16\n",
      "25\n",
      "\n",
      "Simple enough; our average is 14 11\n",
      "25  and all of our values are fairly close to it 13\n",
      "But what if\n",
      "we introduce a new value: 31 12\n",
      "\n",
      "np 2\n",
      "mean([11, 15, 17, 14, 31]) == 17 19\n",
      "6\n",
      "This greatly affects the mean because the arithmetic mean is sensitive to outliers 16\n",
      "The new\n",
      "value, 31, is almost twice as large as the rest of the numbers and therefore skews  the mean 26\n",
      "\n",
      "Another, and sometimes better, measure of center is the median 13\n",
      "\n",
      "The median  is the number found  in the middle of the dataset when it is sorted in order, as\n",
      "shown:\n",
      "np 27\n",
      "median([11, 15, 17, 14]) == 14 16\n",
      "5\n",
      "np 4\n",
      "median([11, 15, 17, 14, 31]) == 15\n",
      "Note how the introduction of 31 using the median did not affect the median of the dataset\n",
      "greatly 41\n",
      "This is because the median is less sensitive to outliers 10\n",
      "\n",
      "When working with datasets with many outliers, it is sometimes more useful to use the\n",
      "median of the dataset, while if your data does not have many outliers and the data points\n",
      "are mostly close to one another, then the mean is likely a better option 52\n",
      "\n",
      "But how can we tell if the data is spread out 12\n",
      "Well, we will have to introduce a new type of\n",
      "statistic 14\n",
      "\n",
      "Measures of variation\n",
      "Measures of the center are used  to quantify the middle of the data, but now we will explore\n",
      "ways of measuring how to spread out  the data we collect is 41\n",
      "This is a useful way to identify if\n",
      "our data has many outliers lurking inside 16\n",
      "Let's start with an example 6\n",
      "\n",
      "Consider that we take a random sample of 24 of our friends on Facebook and wrote down\n",
      "how many friends that they had on Facebook 28\n",
      "Here's the list:\n",
      "friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\n",
      "455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\n",
      "np 85\n",
      "mean(friends) == 789 7\n",
      "1\n",
      "\n",
      "The average of this list is just over 789 13\n",
      "So, we could say that according to this sample, the\n",
      "average Facebook friend has 789 friends 20\n",
      "But what about the person who only has 89\n",
      "friends or the person who has over 1,600 friends 23\n",
      "In fact, not a lot of these numbers are\n",
      "really that close to 789 17\n",
      "\n",
      "Well, how about we use the median 9\n",
      "The median generally is not as affected by outliers:\n",
      "np 11\n",
      "median(friends) == 769 7\n",
      "5\n",
      "The median is 769 8\n",
      "5 , which is fairly close to the mean 10\n",
      "Hmm, good thought, but still, it\n",
      "doesn't really account for how drastically different a lot of these data points are to one\n",
      "another 29\n",
      "This is what statisticians call measuring the variation of data 11\n",
      "Let's start by\n",
      "introducing the most basic measure of variation: the range 16\n",
      "The range is simply the\n",
      "maximum value minus the minimum value, as illustrated:\n",
      "np 17\n",
      "max(friends) - np 6\n",
      "min(friends) == 1684\n",
      "The range tells us how far away the two most extreme values are 22\n",
      "Now, typically the range\n",
      "isn't widely used but it does have its use in the application 20\n",
      "Sometimes, we wish to just\n",
      "know how spread apart the outliers are 14\n",
      "This is most useful in scientific measurements or\n",
      "safety measurements 12\n",
      "\n",
      "Suppose a car company wants to measure how long it takes for an airbag to deploy 19\n",
      "\n",
      "Knowing the average of that time is nice, but they also really want to know how spread\n",
      "apart the slowest time is versus the fastest time 31\n",
      "This literally could be the difference\n",
      "between life and death 11\n",
      "\n",
      "Shifting back to the Facebook example, 1,684 is our range, but I'm not quite sure it's saying\n",
      "too much about our data 32\n",
      "Now, let's take a look at the most commonly used measure  of\n",
      "variation, the standard deviation 21\n",
      "\n",
      "I'm sure many of you have heard this term thrown around a lot and it might even incite a\n",
      "degree of fear, but what does it really mean 33\n",
      "In essence, standard deviation, denoted by s\n",
      "when we are working with a sample of a population, measures how much data values\n",
      "deviate from the arithmetic mean 34\n",
      "\n",
      "It's basically a way to see how spread out the data is 14\n",
      "There is a general formula to calculate\n",
      "the standard deviation, which is as follows:\n",
      "\n",
      "\n",
      "Let's look at each of the elements in this formula in turn:\n",
      "s is our sample's standard deviation\n",
      "x is each individual data point\n",
      "is the mean of the data\n",
      "n is the number of data points\n",
      "Before you freak out, let's break it down 71\n",
      "For each value in the sample, we will take that\n",
      "value, subtract the arithmetic mean from it, square the difference, and, once we've added\n",
      "up every single point this way, we will divide the entire thing by n, the number of points in\n",
      "the sample 56\n",
      "Finally, we take a square root of everything 9\n",
      "\n",
      "Without going into an in-depth analysis of the formula, think about it this way: it's basically\n",
      "derived from the distance formula 27\n",
      "Essentially, what the standard deviation is calculating is\n",
      "a sort of average distance of how far the data values are from the arithmetic mean 26\n",
      "\n",
      "If you take a closer look at the formula, you will see that it actually makes sense:\n",
      "By taking x-, you are finding the literal difference between the value and the\n",
      "mean of the sample 40\n",
      "\n",
      "By squaring the result, , we are putting a greater penalty on outliers\n",
      "because squaring a large error only makes it much larger 28\n",
      "\n",
      "By dividing by the number of items in the sample, we are taking (literally) the\n",
      "average squared distance between each point and the mean 30\n",
      "\n",
      "By taking the square root of the answer, we are putting the number in terms that\n",
      "we can understand 22\n",
      "For example, by squaring the number of friends minus the\n",
      "mean, we changed our units to friends squared, which makes no sense 27\n",
      "Taking\n",
      "the square root puts our units back to just \"friends 13\n",
      "\n",
      "Let's go back to our Facebook example for a visualization and further explanation of this 17\n",
      "\n",
      "Let's begin to calculate the standard deviation 9\n",
      "So, we'll start calculating a few of them 10\n",
      "\n",
      "Recall that the arithmetic mean of the data was just about 789, so, we'll use 789 as the mean 26\n",
      "\n",
      "We start by taking the difference between each data value and the mean, squaring it,\n",
      "adding them all up, dividing it by one less than the number of values, and then taking its\n",
      "square root 42\n",
      "This would look as follows:\n",
      " \n",
      "\n",
      "\n",
      "On the other hand, we can take the Python approach and do all this programmatically\n",
      "(which is usually preferred):\n",
      "np 31\n",
      "std(friends) # == 425 8\n",
      "2\n",
      "What the number 425 represents is the spread of data 14\n",
      "You could say that 425 is a kind of\n",
      "average distance the data  values are from the mean 21\n",
      "What this means, in simple words, is\n",
      "that this data is pretty spread out 17\n",
      "\n",
      "So, our standard deviation is about 425 10\n",
      "This means that the number of friends that these\n",
      "people have on Facebook doesn't seem to be close to a single number and that's quite\n",
      "evident when we plot the data in a bar graph, and also graph the mean as well as the\n",
      "visualizations of the standard deviation 58\n",
      "In the following plot, every person will be\n",
      "represented by a single bar in the bar chart, and the height of the bars represent the number\n",
      "of friends that the individuals have:\n",
      "import matplotlib 39\n",
      "pyplot as plt\n",
      "friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\n",
      "455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\n",
      "y_pos = range(len(friends))\n",
      "plt 92\n",
      "bar(y_pos, friends)\n",
      "plt 7\n",
      "plot((0, 25), (789, 789), 'b-')\n",
      "plt 18\n",
      "plot((0, 25), (789+425, 789+425), 'g-')\n",
      "plt 22\n",
      "plot((0, 25), (789-425, 789-425), 'r-')\n",
      "Here's the chart that we get:\n",
      "\n",
      "\n",
      "The blue line in the center is drawn at the mean (789), the red line near the bottom is drawn\n",
      "at the mean minus the standard deviation (789-425 = 364), and finally the green line\n",
      "towards the top is drawn at the mean plus the standard deviation (789+425 = 1,214) 96\n",
      "\n",
      "Note how most  of the data lives between the green and the red lines while the outliers live\n",
      "outside the lines 24\n",
      "There are three people who have friend counts below the red line and\n",
      "three people who have a friend count above the green line 25\n",
      "\n",
      "It's important to mention that the units for standard deviation are, in fact, the same units as\n",
      "the data's units 26\n",
      "So, in this example, we would say that the standard deviation is 425 friends\n",
      "on Facebook 20\n",
      "\n",
      "Another measure of variation is the variance, as described in the previous\n",
      "chapter 16\n",
      "The variance is simply the standard deviation squared 8\n",
      "\n",
      "So, now we know that the standard deviation and variance is good for checking how\n",
      "spread out our data is, and that we can use it along with the mean to create a kind of range\n",
      "that a lot of our data lies in 49\n",
      "But what if we want to compare the spread of two different\n",
      "datasets, maybe even with completely different units 21\n",
      "That's where the coefficient of\n",
      "variation comes into play 11\n",
      "\n",
      "Definition\n",
      "The coefficient of variation  is defined as the ratio of the data's standard deviation to its\n",
      "mean 23\n",
      "\n",
      "This ratio (which, by the way, is only helpful if we're working in the ratio level of\n",
      "measurement, where the division is allowed  and is meaningful) is a way to standardize the \n",
      "standard  deviation, which makes it easier to compare across datasets 55\n",
      "We use this measure\n",
      "frequently when attempting to compare means, and it spreads across populations that exist\n",
      "at different scales 24\n",
      "\n",
      "\n",
      "Example – employee salaries\n",
      "If we look at the mean and standard  deviation of employees' salaries in the same company\n",
      "but among different departments, we see that, at first glance, it may be tough to compare\n",
      "variations:\n",
      "This is especially true when the mean salary of one department is $25,000 , while another\n",
      "department has a mean salary in the six-figure area 78\n",
      "\n",
      "However, if we look at the last column, which is our coefficient of variation, it becomes\n",
      "clearer that the people in the executive department may be getting paid more but they are\n",
      "also getting wildly different salaries 44\n",
      "This is probably because the CEO is earning way more\n",
      "than an office manager, who is still in the executive department, which makes the data very\n",
      "spread out 32\n",
      "\n",
      "On the other hand, everyone in the mailroom, while not making as much money, is making\n",
      "just about the same as everyone else in the mailroom, which is why their coefficient of\n",
      "variation is only 8% 47\n",
      "\n",
      "With measures of variation, we can begin to answer big questions, such as how to spread\n",
      "out this data is or how we can come up with a good range that most of the data falls into 41\n",
      "\n",
      "Measures of relative standing\n",
      "We can combine both  the measures of centers and variations to create measures of relative\n",
      "standing 25\n",
      "Measures of variation  measure where particular data values are positioned,\n",
      "relative to the entire dataset 17\n",
      "\n",
      "\n",
      "Let's begin by learning a very  important value  in statistics, the z-score 18\n",
      "The z-score  is a way\n",
      "of telling us how far away a single data value is from the mean 22\n",
      "The z-score of an x data\n",
      "value is as follows:\n",
      "                                                                         z = \n",
      "Let's break down this formula:\n",
      "X is the data point\n",
      "the mean\n",
      "s is the standard deviation\n",
      "Remember that the standard deviation was (sort of) an average distance that the data is\n",
      "from the mean and now the z-score is an individualized value for each particular data\n",
      "point 76\n",
      "We can find the z-score of a data value by subtracting it from the mean and dividing\n",
      "it by the standard deviation 25\n",
      "The output will be the standardized distance a value is from a\n",
      "mean 14\n",
      "We use the z-score all over statistics 8\n",
      "It is a very effective way of normalizing data\n",
      "that exists on very different scales, and also to put data in the context of its mean 29\n",
      "\n",
      "Let's take our previous data on the number of friends on Facebook and standardize the data\n",
      "to the z-score 24\n",
      "For each data point, we will find its z-score by applying the preceding\n",
      "formula 17\n",
      "We will take each individual, subtract the average friends from the value, and\n",
      "divide that by the standard deviation, as shown:\n",
      "z_scores = []\n",
      "m = np 33\n",
      "mean(friends)  # average friends on Facebook\n",
      "s = np 14\n",
      "std(friends)   # standard deviation friends on Facebook\n",
      "for friend in friends:\n",
      "    z = (friend - m)/s  # z-score\n",
      "    z_scores 34\n",
      "append(z)  # make a list of the scores for plotting\n",
      "Now, let's plot these z-scores on a bar chart 27\n",
      "The following chart shows the same\n",
      "individuals from our previous example using friends on Facebook, but instead of the bar\n",
      "height revealing the raw number of friends, now each bar is the z-score of the number of\n",
      "friends they have on Facebook 49\n",
      "If we graph the z-scores, we'll notice a few things:\n",
      "plt 16\n",
      "bar(y_pos, z_scores)\n",
      "\n",
      "We get this chart:\n",
      "We can see that we have negative values (meaning that the data point is below the mean) 31\n",
      "\n",
      "The bars' lengths no longer represent the raw number of friends, but the degree to which\n",
      "that friend count differs from the mean 27\n",
      "\n",
      "This chart makes it very easy to pick out the individuals with much lower and higher\n",
      "friends on an average 22\n",
      "For example, the individual at index 0 has fewer friends on average\n",
      "(they had 109 friends where the average was 789) 28\n",
      "\n",
      "What if we want to graph the standard deviations 10\n",
      "Recall that we earlier graphed three\n",
      "horizontal lines: one at the mean, one at the mean plus the standard deviation (x+s) , and\n",
      "one at the mean minus the standard deviation (x-s) 43\n",
      "\n",
      "If we plug these values into the formula for the z-score, we will get the following:\n",
      "            \n",
      "\n",
      "\n",
      "This is no coincidence 26\n",
      "When we standardize the data using the z-score, our standard\n",
      "deviations become the metric of choice 21\n",
      "Let's plot a new graph with the standard deviations\n",
      "added:\n",
      "plt 14\n",
      "bar(y_pos, z_scores)\n",
      "plt 8\n",
      "plot((0, 25), (1, 1), 'g-')\n",
      "plt 18\n",
      "plot((0, 25), (0, 0), 'b-')\n",
      "plt 18\n",
      "plot((0, 25), (-1, -1), 'r-')\n",
      "The preceding code is adding the following three lines:\n",
      "A blue line at y = 0  that represents zero standard deviations away from the mean\n",
      "(which is on the x-axis)\n",
      "A green line that represents one standard deviation above the mean\n",
      "A red line that represents one standard deviation below the mean\n",
      "Let's look at the graph we get:\n",
      "The colors of the lines match  up with the lines drawn in the earlier graph of the raw friend\n",
      "count 108\n",
      "If you look carefully, the same people still fall outside of the green and the red lines 18\n",
      "\n",
      "Namely, the same three people still fall below the red (lower) line, and the same three\n",
      "people fall above the green (upper) line 32\n",
      "\n",
      "\n",
      "Z-scores are an effective way to standardize  data 13\n",
      "This means that we can put the entire set\n",
      "on the same scale 14\n",
      "For example, if we also measure each person's general happiness scale\n",
      "(which is between 0 and 1), we might have a dataset similar to the following dataset:\n",
      "friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\n",
      "455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\n",
      "happiness = [ 117\n",
      "8, 3\n",
      "6, 3\n",
      "3, 3\n",
      "6, 3\n",
      "6, 3\n",
      "4, 3\n",
      "8, 3\n",
      "5, 3\n",
      "4, 3\n",
      "3, 3\n",
      "3, 3\n",
      "6, 3\n",
      "2, 3\n",
      "8, 1, 6\n",
      "6,\n",
      " 3\n",
      "2, 3\n",
      "7, 3\n",
      "5, 3\n",
      "3, 3\n",
      "1, 0, 6\n",
      "3, 1]\n",
      "import pandas as pd\n",
      "df = pd 14\n",
      "DataFrame({'friends':friends, 'happiness':happiness})\n",
      "df 14\n",
      "head()\n",
      "We get this table:\n",
      "These data points are on two different dimensions, each with a very different scale 22\n",
      "The\n",
      "friend count can be in the thousands while our happiness score is stuck between 0 and 1 21\n",
      "\n",
      "To remedy this (and for some statistical/machine learning modeling, this concept will\n",
      "become essential), we can simply standardize the dataset using a prebuilt standardization\n",
      "package in scikit-learn, as follows:\n",
      "from sklearn import preprocessing\n",
      "df_scaled = pd 55\n",
      "DataFrame(preprocessing 3\n",
      "scale(df), columns =\n",
      "['friends_scaled', 'happiness_scaled'])\n",
      "df_scaled 16\n",
      "head()\n",
      "\n",
      "This code will scale both the friends and happiness columns simultaneously, thus revealing\n",
      "the z-score for each column 23\n",
      "It is important to note that by doing this, the preprocessing\n",
      "module in sklearn  is doing the following things separately for each column:\n",
      "Finding the mean of the column\n",
      "Finding the standard deviation of the column\n",
      "Applying the z-score function to each element in the column\n",
      "The result is two columns, as shown, that exist on the same scale as each other even if they\n",
      "were not previously:\n",
      "Now, we can plot friends and happiness on the same scale and the graph will at least be\n",
      "readable:\n",
      "df_scaled 106\n",
      "plot(kind='scatter', x = 'friends_scaled', y =\n",
      "'happiness_scaled')\n",
      "\n",
      "The preceding code gives us this graph:\n",
      "Now, our data is standardized to the z-score and this scatter plot is fairly easily\n",
      "interpretable 45\n",
      "In later chapters, this idea  of standardization will not only make our data\n",
      "more interpretable, but it will also be essential in our model optimization 31\n",
      "Many machine\n",
      "learning algorithms will require us to have standardized columns as they are reliant on the\n",
      "notion of scale 23\n",
      "\n",
      "The insightful part – correlations in data\n",
      "Throughout this book, we will discuss the difference between having data and having\n",
      "actionable insights about  your data 31\n",
      "Having data is only one step to a successful data\n",
      "science operation 13\n",
      "Being able to obtain, clean, and plot data helps to tell the story that the\n",
      "data has to offer but cannot reveal the moral 27\n",
      "In order to take this entire example one step\n",
      "further, we will look at the relationship between having friends on Facebook and\n",
      "happiness 28\n",
      "\n",
      "\n",
      "In subsequent chapters, we will look at a specific machine learning algorithm that attempts\n",
      "to find relationships between quantitative features, called linear regression, but we do not\n",
      "have to wait until then to begin to form hypotheses 43\n",
      "We have a sample of people, a measure\n",
      "of their online social presence, and their reported happiness 20\n",
      "The question of the day here is\n",
      "this: can we find a relationship between the number of friends on Facebook and overall\n",
      "happiness 27\n",
      "\n",
      "Now, obviously, this is a big question and should be treated respectfully 15\n",
      "Experiments to\n",
      "answer this question should be conducted in a laboratory setting, but we can begin to form\n",
      "a hypothesis about this question 27\n",
      "Given the nature of our data, we really only have the\n",
      "following three options for a hypothesis:\n",
      "There is a positive association between the number of online friends and\n",
      "happiness (as one goes up, so does the other)\n",
      "There is a negative association between them (as the number of friends goes up,\n",
      "your happiness goes down)\n",
      "There is no association between the variables (as one changes, the other doesn't\n",
      "really change that much)\n",
      "Can we use basic statistics to form a hypothesis about this question 101\n",
      "I say we can 4\n",
      "But first,\n",
      "we must introduce a concept called correlation 10\n",
      "\n",
      "Correlation coefficients  are a quantitative measure  that describes the strength of\n",
      "association/relationship between two variables 22\n",
      "\n",
      "The correlation between the two sets of data tells us about how they move together 16\n",
      "Would\n",
      "changing one help us predict the other 9\n",
      "This concept is not only interesting in this case, but\n",
      "it is one of the core assumptions that many machine learning models make on data 27\n",
      "For\n",
      "many prediction algorithms to work, they rely on the fact that there is some sort of\n",
      "relationship between the variables that we are looking at 29\n",
      "The learning algorithms then\n",
      "exploit this relationship in order to make accurate predictions 15\n",
      "\n",
      "A few things to note about a correlation coefficient are as follows:\n",
      "It will lie between -1 and 1\n",
      "The greater the absolute value (closer to -1 or 1), the stronger the relationship\n",
      "between the variables:\n",
      "The strongest correlation is a -1 or a 1\n",
      "The weakest correlation is a 0\n",
      "\n",
      "A positive correlation means that as one variable increases, the other one tends to\n",
      "increase as well\n",
      "A negative correlation means that as one variable increases, the other one tends\n",
      "to decrease\n",
      "We can use pandas  to quickly show us correlation coefficients between every feature and\n",
      "every other feature in the DataFrame, as illustrated:\n",
      "# correlation between variables\n",
      "df 138\n",
      "corr()\n",
      "We get this table:\n",
      "This table shows the correlation between friends  and happiness 17\n",
      "Note the first two things:\n",
      "The diagonal of the matrix is filled with positive is 16\n",
      "This is because they represent\n",
      "the correlation between the variable and itself, which, of course, forms a perfect\n",
      "line, making the correlation perfectly positive 30\n",
      "\n",
      "The matrix is symmetrical across the diagonal 9\n",
      "This is true for any correlation\n",
      "matrix made in pandas 11\n",
      "\n",
      "There are a few caveats to trusting the correlation coefficient 12\n",
      "One is that, in general, a\n",
      "correlation will attempt to measure a linear  relationship between variables 21\n",
      "This means that\n",
      "if there is no visible correlation revealed by this measure, it does not mean that there is no\n",
      "relationship between the variables, only that there is no line of best fit that goes through the\n",
      "lines easily 45\n",
      "There might be a non-linear  relationship that defines the two variables 13\n",
      "\n",
      "It is important to realize that causation is not implied by correlation 14\n",
      "Just because there is a\n",
      "weak negative correlation between these two variables does not necessarily mean that your\n",
      "overall happiness decreases as the number of friends you keep on Facebook go up 34\n",
      "This\n",
      "causation must be tested further and, in later chapters, we will attempt to do just that 22\n",
      "\n",
      "To sum up, we can use correlation to make hypotheses about the relationship between\n",
      "variables, but we will need to use more sophisticated statistical methods and machine\n",
      "learning algorithms to solidify these assumptions and hypotheses 41\n",
      "\n",
      "\n",
      "The empirical rule\n",
      "Recall that a normal distribution is defined  as having a specific probability distribution that\n",
      "resembles a bell curve 27\n",
      "In statistics, we love it when our data behaves normally 11\n",
      "For\n",
      "example, we may have data that resembles a normal distribution, like so:\n",
      "The empirical rule  states that we can expect a certain amount of data to live between sets of\n",
      "standard deviations 39\n",
      "Specifically, the empirical rule states the following for data that is\n",
      "distributed normally:\n",
      "About 68% of the data falls within 1 standard deviation\n",
      "About 95% of the data falls within 2 standard deviations\n",
      "About 99 47\n",
      "7% of the data falls within 3 standard deviations\n",
      "For example, let's see if our Facebook friends' data holds up to this 29\n",
      "Let's use our DataFrame\n",
      "to find the percentage of people that fall within 1, 2, and 3 standard deviations of the mean,\n",
      "as shown:\n",
      "# finding the percentage of people within one standard deviation of the\n",
      "mean\n",
      "within_1_std = df_scaled[(df_scaled['friends_scaled'] <= 1) &\n",
      "(df_scaled['friends_scaled'] >= -1)] 77\n",
      "shape[0]\n",
      "within_1_std / float(df_scaled 12\n",
      "shape[0])\n",
      "# 0 7\n",
      "75\n",
      "# finding the percentage of people within two standard deviations of the\n",
      "mean\n",
      "within_2_std = df_scaled[(df_scaled['friends_scaled'] <= 2) &\n",
      "(df_scaled['friends_scaled'] >= -2)] 47\n",
      "shape[0]\n",
      "within_2_std / float(df_scaled 12\n",
      "shape[0])\n",
      "\n",
      "# 0 7\n",
      "916\n",
      "# finding the percentage of people within three standard deviations of the\n",
      "mean\n",
      "within_3_std = df_scaled[(df_scaled['friends_scaled'] <= 3) &\n",
      "(df_scaled['friends_scaled'] >= -3)] 47\n",
      "shape[0]\n",
      "within_3_std / float(df_scaled 12\n",
      "shape[0])\n",
      "# 1 7\n",
      "0\n",
      "We can see that our data does seem to follow the empirical rule 16\n",
      "About 75% of the people\n",
      "are within a single standard deviation of the mean 17\n",
      "About 92% of the people are within two\n",
      "standard deviations, and all of them are within three standard deviations 23\n",
      "\n",
      "Example: Exam scores\n",
      "Let's say that we're measuring the scores of an exam and the scores generally have a bell-\n",
      "shaped normal distribution 30\n",
      "The average of the exam was 84% and the standard deviation\n",
      "was 6% 18\n",
      "We can say the following, with approximate certainty:\n",
      "About 68% of the class scored between 78% and 90% because 78 is 6 units below\n",
      "84, and 90 is 6 units above 84\n",
      "If we were asked what percentage of the class scored between 72 % and 96%, we\n",
      "would notice that 72 is 2 standard deviations below the mean, and 96 is 2\n",
      "standard deviations above the mean, so the empirical rule tells us that about 95%\n",
      "of the class scored in that range\n",
      "However, not all data is normally  distributed, so we can't always use the empirical rule 133\n",
      "We\n",
      "have another theorem that helps us analyze any kind of distribution 13\n",
      "In the next chapter,\n",
      "we will go into depth about when we can assume the normal distribution 18\n",
      "This is because\n",
      "many statistical tests and hypotheses require the underlying data to come from a normally\n",
      "distributed population 21\n",
      "\n",
      "Previously, when we standardized our data to the z-score, we did not\n",
      "require the normal distribution assumption 22\n",
      "\n",
      "\n",
      "Summary\n",
      "In this chapter, we covered much of the basic statistics required by most data scientists 19\n",
      "\n",
      "Everything from how we obtain/sample data to how to standardize data according to the z-\n",
      "score and applications of the empirical rule was covered 28\n",
      "We also reviewed how to take\n",
      "samples for data analysis 11\n",
      "In addition, we reviewed various statistical measures, such as the\n",
      "mean and standard deviation, that help describe data 22\n",
      "\n",
      "In the next chapter, we will look at much more advanced applications of statistics 16\n",
      "One\n",
      "thing that we will consider is how to use hypothesis tests on data that we can assume to be\n",
      "normal 23\n",
      "As we use these tests, we will also quantify our errors and identify the best\n",
      "practices to solve these errors 23\n",
      "\n",
      "\n",
      "\n",
      "Advanced Statistics\n",
      "We are concerned with making inferences about entire populations based on certain\n",
      "samples of data 21\n",
      "We will be using hypothesis tests along with different estimation tests in\n",
      "order to gain a better understanding of populations, given samples of data 26\n",
      "\n",
      "The key topics that we will cover in this chapter are as follows:\n",
      "Point estimates\n",
      "Confidence intervals\n",
      "The central limit theorem\n",
      "Hypothesis testing\n",
      "Point estimates\n",
      "Recall that, in the previous chapter, we mentioned how difficult it is to obtain a population\n",
      "parameter; so, we had to use sample data to calculate a statistic that was an estimate of a\n",
      "parameter 77\n",
      "When we make these estimates, we call them point estimates 11\n",
      "\n",
      "A point estimate  is an estimate  of a population parameter based on sample data 17\n",
      "\n",
      "We use point estimates to estimate population means, variances, and other statistics 16\n",
      "To\n",
      "obtain these estimates, we simply apply the function that we wish to measure for our\n",
      "population to a sample of the data 27\n",
      "For example, suppose there is a company of 9,000\n",
      "employees and we are interested in ascertaining the average length of breaks taken by\n",
      "employees in a single day 36\n",
      "As we probably cannot ask every single person, we will take a\n",
      "sample of the 9,000 people and take a mean of the sample 29\n",
      "This sample mean will be our\n",
      "point estimate 9\n",
      "\n",
      "\n",
      "The following code is broken into three parts:\n",
      "We will use the probability distribution, known as the Poisson distribution , to\n",
      "randomly generate 9,000 answers to the question: for how many minutes in a day\n",
      "do you usually take breaks 51\n",
      "This will represent our population 5\n",
      "Remember, from\n",
      "Chapter 6 , Advanced Probability , that the Poisson random variable  is used when\n",
      "we know the average value of an event and wish to model a distribution around\n",
      "it 39\n",
      "\n",
      "Note that this average value is not usually known 10\n",
      "I am calculating it to\n",
      "show the difference between our parameter and our statistic 15\n",
      "I also set a\n",
      "random seed in order to encourage reproducibility (this allows us to get\n",
      "the same random numbers each time) 28\n",
      "\n",
      "We will take a sample of 100 employees (using the Python random sample method) and\n",
      "find a point estimate of a mean (called a sample mean) 33\n",
      "\n",
      "Note that this is just over 1% of our population 13\n",
      "\n",
      "Compare our sample mean (the mean of the sample of 100 employees) to our population\n",
      "mean 21\n",
      "\n",
      "Let's take a look at the following code:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "from scipy 28\n",
      "stats import poisson\n",
      "np 6\n",
      "random 1\n",
      "seed(1234)\n",
      "long_breaks = stats 10\n",
      "poisson 2\n",
      "rvs(loc=10, mu=60, size=3000)\n",
      "# represents 3000 people who take about a 60 minute break\n",
      "The long_breaks  variable represents 3,000  answers to the question, \" how many minutes\n",
      "on an average do you take breaks for 60\n",
      ",\"  and these answers will be on the longer side 11\n",
      "Let's\n",
      "see a visualization of this distribution, shown as follows:\n",
      "pd 15\n",
      "Series(long_breaks) 5\n",
      "hist()\n",
      "\n",
      "\n",
      "We can see that our average of 60 minutes is to the left of the distribution 19\n",
      "Also, because we\n",
      "only sampled 3,000  people, our bars are at their highest at around 700–800 people 27\n",
      "\n",
      "Now, let's model 6,000  people who take, on an average, about 15 minutes' worth of breaks 27\n",
      "\n",
      "Let's again use the Poisson distribution to simulate 6,000  people, as shown:\n",
      "short_breaks = stats 26\n",
      "poisson 2\n",
      "rvs(loc=10, mu=15, size=6000)\n",
      "# represents 6000 people who take about a 15 minute break\n",
      "pd 31\n",
      "Series(short_breaks) 5\n",
      "hist()\n",
      "\n",
      "\n",
      "Okay, so, we have a distribution for the people who take longer breaks and a distribution\n",
      "for the people who take shorter breaks 28\n",
      "Again, note how our average break length of 15\n",
      "minutes falls to the left-hand side of the distribution, and note that the tallest bar is for\n",
      "about 1,600  people:\n",
      "breaks = np 44\n",
      "concatenate((long_breaks, short_breaks))\n",
      "# put the two arrays together to get our \"population\" of 9000 people\n",
      "The breaks  variable is the amalgamation of all the 9,000  employees, both long and short\n",
      "break takers 54\n",
      "Let's see the entire distribution of people in a single visualization:\n",
      "pd 14\n",
      "Series(breaks) 5\n",
      "hist()\n",
      "We can see we have two humps 10\n",
      "On the left, we have our larger hump of people who take\n",
      "about a 15-minute break, and on the right, we have a smaller hump of people who take\n",
      "longer breaks 41\n",
      "Later on, we will investigate this graph further 9\n",
      "\n",
      "We can find the total average break length by running the following code:\n",
      "breaks 17\n",
      "mean()\n",
      "# 39 5\n",
      "99 minutes is our parameter 6\n",
      "\n",
      "\n",
      "Our average company break length is about 40 minutes 11\n",
      "Remember that our population is\n",
      "the entire company's employee size of 9,000 people, and our parameter is 40 minutes 26\n",
      "In the\n",
      "real world, our goal would be to estimate the population parameter because we would not\n",
      "have the resources to ask every single employee in a survey their average break length for\n",
      "many reasons 39\n",
      "Instead, we will use a point estimate 8\n",
      "\n",
      "So, to make our point, we want to simulate a world where we ask 100 random people about\n",
      "the length of their breaks 28\n",
      "To do this, let's take a random sample of 100 employees out of\n",
      "the 9,000 employees we simulated, as shown:\n",
      "sample_breaks = np 34\n",
      "random 1\n",
      "choice(a = breaks, size=100)\n",
      "# taking a sample of 100 employees\n",
      "Now, let's take the mean of the sample and subtract it from the population mean and see\n",
      "how far off we were:\n",
      "breaks 46\n",
      "mean() - sample_breaks 6\n",
      "mean()\n",
      "# difference between means is 4 9\n",
      "09 minutes, not bad 6\n",
      "\n",
      "This is extremely interesting because, with only about 1% of our population (100 out of\n",
      "9,000), we were able to get within 4 minutes of our population parameter and get a very\n",
      "accurate estimate of our population mean 50\n",
      "Not bad 2\n",
      "\n",
      "Here, we calculated a point estimate for the mean, but we can also do this for proportion\n",
      "parameters 22\n",
      "By proportion, I am referring to a ratio of two quantitative values 13\n",
      "\n",
      "Let's suppose that in a company of 10,000 people, our employees are 20% white, 10% black,\n",
      "10% Hispanic, 30% Asian, and 30% identify as other 44\n",
      "We will take a sample of 1,000\n",
      "employees and see whether their race proportions are similar:\n",
      "employee_races = ([\"white\"]*2000) + ([\"black\"]*1000) +\\\n",
      "                   ([\"hispanic\"]*1000) + ([\"asian\"]*3000) +\\\n",
      "                   ([\"other\"]*3000)\n",
      "The employee_races  represents our employee population 84\n",
      "For example, in our company\n",
      "of 10,000 people, 2,000 people are white (20%) and 3,000 people are Asian (30%) 35\n",
      "\n",
      "Let's take a random sample of 1,000 people, as shown:\n",
      "import random\n",
      "demo_sample = random 24\n",
      "sample(employee_races, 1000)   # Sample 1000 values\n",
      "\n",
      "for race in set(demo_sample):\n",
      "    print( race + \" proportion estimate:\" )\n",
      "    print( demo_sample 40\n",
      "count(race)/1000 6\n",
      ")\n",
      "The output obtained would be as follows:\n",
      "hispanic proportion estimate:\n",
      "0 15\n",
      "103\n",
      "white proportion estimate:\n",
      "0 8\n",
      "192\n",
      "other proportion estimate:\n",
      "0 8\n",
      "288\n",
      "black proportion estimate:\n",
      "0 8\n",
      "1\n",
      "asian proportion estimate:\n",
      "0 9\n",
      "317\n",
      "We can see that the race proportion estimates are very close to the underlying population's\n",
      "proportions 23\n",
      "For example, we got 10 7\n",
      "3% for Hispanic in our sample and the population\n",
      "proportion for Hispanic was 10% 20\n",
      "\n",
      "Sampling distributions\n",
      "In Chapter 7 , Basic Statistics , we mentioned how much we love it when data follows the\n",
      "normal distribution 26\n",
      "One of the reasons for this is that many statistical tests (including the\n",
      "ones we will use in this chapter) rely on data that follows a normal pattern, and for the\n",
      "most part, a lot of real-world data is not normal (surprised 52\n",
      " 1\n",
      "Take our employee  break\n",
      "data, for example —you might think I was just being fancy creating data using the Poisson\n",
      "distribution, but I had a reason for this 35\n",
      "I specifically wanted non-normal data, as shown:\n",
      "pd 11\n",
      "DataFrame(breaks) 5\n",
      "hist(bins=50,range=(5,100))\n",
      "\n",
      "\n",
      "As you can see, our data is definitely not following a normal distribution; it appears to be\n",
      "bi-modal , which means that there are two peaks of break times, at around 25 and 70\n",
      "minutes 55\n",
      "As our data is not normal, many of the most popular statistics tests may not apply;\n",
      "however, if we follow the given procedure, we can create normal data 32\n",
      "Think I'm crazy 4\n",
      "\n",
      "Well, see for yourself 6\n",
      "\n",
      "First off, we will need to utilize what is known as a sampling distribution , which is a\n",
      "distribution of point estimates of several samples of the same size 32\n",
      "Our procedure for\n",
      "creating a sampling distribution will be the following:\n",
      "Take 500 different samples of the break times of size 100 each1 28\n",
      "\n",
      "Take a histogram of these 500 different point estimates (revealing their2 16\n",
      "\n",
      "distribution)\n",
      "The number of elements in the sample (100) was arbitrary but large enough to be a\n",
      "representative sample of the population 28\n",
      "The number of samples I took (500) was also\n",
      "arbitrary, but large enough to ensure that our data would converge to a normal\n",
      "distribution:\n",
      "point_estimates = []\n",
      "for x in range(500):         # Generate 500 samples\n",
      "   sample = np 54\n",
      "random 1\n",
      "choice(a= breaks, size=100)\n",
      "\n",
      "#take a sample of 100 points\n",
      "point_estimates 21\n",
      "append( sample 3\n",
      "mean() )\n",
      "# add the sample mean to our list of point estimates\n",
      "pd 16\n",
      "DataFrame(point_estimates) 5\n",
      "hist()\n",
      "# look at the distribution of our sample means\n",
      "Behold 14\n",
      "The sampling distribution of the sample mean appears to be normal even though\n",
      "we took data from an underlying bimodal population distribution 25\n",
      "It is important to note\n",
      "that the bars in this histogram represent the average break length of 500 samples of\n",
      "employees, where each sample has 100 people in it 34\n",
      "In other words, a sampling distribution\n",
      "is a distribution of several point estimates 15\n",
      "\n",
      "Our data converged to a normal distribution because of something called the central limit\n",
      "theorem , which states that the sampling distribution (the distribution of point estimates)\n",
      "will approach a normal distribution as we increase the number of samples taken 44\n",
      "\n",
      "\n",
      "What's more, as we take more and more samples, the mean of the sampling distribution\n",
      "will approach the true population mean, as shown:\n",
      "breaks 32\n",
      "mean() - np 4\n",
      "array(point_estimates) 5\n",
      "mean()\n",
      "# 3\n",
      "047 minutes difference\n",
      "This is actually a very exciting result because it means that we can get even closer than a\n",
      "single point estimate by taking multiple point estimates and utilizing the central limit\n",
      "theorem 39\n",
      "\n",
      "In general, as we increase the number of samples taken, our estimate will\n",
      "get closer to the parameter (actual value) 26\n",
      "\n",
      "Confidence intervals\n",
      "While point estimates are okay, estimates of a population parameter and sampling\n",
      "distributions are even better 24\n",
      "There are the following two main issues with these\n",
      "approaches:\n",
      "Single point estimates are very prone to error (due to sampling bias among other\n",
      "things)\n",
      "Taking multiple samples of a certain size for sampling distributions might not be\n",
      "feasible, and may sometimes be even more infeasible than actually finding the\n",
      "population parameter\n",
      "For these reasons and more, we may turn to a concept known as the confidence interval to\n",
      "find statistics 86\n",
      "\n",
      "A confidence interval  is a range of values based on a point estimate that contains the true\n",
      "population parameter at some confidence level 26\n",
      "\n",
      "Confidence  is an important concept in advanced statistics 11\n",
      "Its meaning is sometimes\n",
      "misconstrued 9\n",
      "Informally, a confidence level does not represent a probability of being correct ;\n",
      "instead, it represents the frequency that the obtained answer will be accurate 28\n",
      "For example,\n",
      "if you want to have a 95% chance of capturing the true population parameter using only a\n",
      "single point estimate, you have to set your confidence level to 95% 38\n",
      "\n",
      "\n",
      "Higher confidence levels result in wider (larger) confidence intervals in\n",
      "order to be more sure 20\n",
      "\n",
      "Calculating a confidence interval involves finding a point estimate and then incorporating a\n",
      "margin of error to create a range 23\n",
      "The margin of error  is a value that represents our\n",
      "certainty that our point estimate is accurate and is based on our desired confidence level,\n",
      "the variance of the data, and how big your sample is 41\n",
      "There are many ways to calculate\n",
      "confidence intervals; for the purpose of brevity and simplicity, we will look at a single way\n",
      "of taking the confidence interval of a population mean 37\n",
      "For this confidence interval, we\n",
      "need the following:\n",
      "A point estimate 14\n",
      "For this, we will take our sample mean of break lengths from\n",
      "our previous example 17\n",
      "\n",
      "An estimate of the population standard deviation, which represents the variance\n",
      "in the data 17\n",
      "This is calculated by taking the sample standard deviation (the\n",
      "standard deviation of the sample data) and dividing that number by the square\n",
      "root of the population size 32\n",
      "\n",
      "The degrees of freedom (which is the -1 sample size) 14\n",
      "\n",
      "Obtaining these numbers might seem arbitrary but, trust me, there is a reason for all of\n",
      "them 23\n",
      "However, again for simplicity, I will use prebuilt Python modules, as shown, to\n",
      "calculate our confidence interval and then demonstrate its value:\n",
      "import math\n",
      "sample_size = 100\n",
      "# the size of the sample we wish to take\n",
      "sample = np 52\n",
      "random 1\n",
      "choice(a= breaks, size = sample_size)\n",
      "# a sample of sample_size taken from the 9,000 breaks population from\n",
      "before\n",
      "sample_mean = sample 33\n",
      "mean()\n",
      "# the sample mean of the break lengths sample\n",
      "\n",
      "sample_stdev = sample 17\n",
      "std()\n",
      "# sample standard deviation\n",
      "sigma = sample_stdev/math 13\n",
      "sqrt(sample_size)\n",
      "# population standard deviation estimate\n",
      "stats 11\n",
      "t 1\n",
      "interval(alpha = 0 5\n",
      "95,              # Confidence level 95%\n",
      "                 df= sample_size - 1,       # Degrees of freedom\n",
      "                 loc = sample_mean,         # Sample mean\n",
      "                 scale = sigma)             # Standard deviation\n",
      "# (36 49\n",
      "36, 45 5\n",
      "44)\n",
      "To reiterate, this range of values (from 36 15\n",
      "36  to 45 6\n",
      "44 ) represents a confidence interval for\n",
      "the average break time with 95% confidence 18\n",
      "\n",
      "We already know that our population parameter is 39 11\n",
      "99 , and note that the interval\n",
      "includes the population mean of 39 16\n",
      "99 2\n",
      "\n",
      "I mentioned earlier that the confidence  level is not a percentage of accuracy of our interval\n",
      "but the percent chance that the interval will even contain the population parameter at all 34\n",
      "\n",
      "To better understand the confidence level, let's take 10,000 confidence intervals and see\n",
      "how often our population means falls in the interval 29\n",
      "First, let's make a function, as\n",
      "illustrated, that makes a single confidence interval from our breaks data:\n",
      "# function to make confidence interval\n",
      "def makeConfidenceInterval():\n",
      "    sample_size = 100\n",
      "    sample = np 48\n",
      "random 1\n",
      "choice(a= breaks, size = sample_size)\n",
      "    sample_mean = sample 15\n",
      "mean()\n",
      "\n",
      "    # sample mean\n",
      "    sample_stdev = sample 13\n",
      "std()\n",
      "    # sample standard deviation\n",
      "    sigma = sample_stdev/math 15\n",
      "sqrt(sample_size)\n",
      "    # population Standard deviation estimate\n",
      "    return stats 14\n",
      "t 1\n",
      "interval(alpha = 0 5\n",
      "95, df= sample_size - 1, loc =\n",
      "sample_mean, scale = sigma)\n",
      "Now that we have a function that will create a single confidence interval, let's create a\n",
      "procedure that will test the probability that a single confidence interval will contain the true\n",
      "population parameter, 39 60\n",
      "99 :\n",
      "Take 10,000 confidence intervals of the sample mean 14\n",
      "1 2\n",
      "\n",
      "Count the number of times that the population parameter falls into our2 14\n",
      "\n",
      "confidence intervals 3\n",
      "\n",
      "Output the ratio of the number of times the parameter fell into the interval by3 17\n",
      "\n",
      "10,000:\n",
      "times_in_interval = 0 11\n",
      "\n",
      "for i in range(10000):\n",
      "    interval = makeConfidenceInterval()\n",
      "    if 39 21\n",
      "99 >= interval[0] and 39 10\n",
      "99 <= interval[1]:\n",
      "    # if 39 12\n",
      "99 falls in the interval\n",
      "        times_in_interval += 1\n",
      "print(times_in_interval / 10000)\n",
      "# 0 27\n",
      "9455\n",
      "\n",
      "Success 5\n",
      "We see that about 95% of our confidence intervals contained our actual population\n",
      "mean 17\n",
      "Estimating population parameters through point estimates and confidence intervals\n",
      "is a relatively simple and powerful form of statistical inference 21\n",
      "\n",
      "Let's also take a quick look at how  the size of confidence intervals changes as we change\n",
      "our confidence level 24\n",
      "Let's calculate confidence intervals for multiple confidence levels and\n",
      "look at how large the intervals are by looking at the difference between the two numbers 27\n",
      "\n",
      "Our hypothesis will be that as we make our confidence level larger, we will likely see larger\n",
      "confidence intervals to be surer that we catch the true population parameter:\n",
      "for confidence in ( 38\n",
      "5, 3\n",
      "8, 3\n",
      "85, 3\n",
      "9, 3\n",
      "95, 3\n",
      "99):\n",
      "    confidence_interval = stats 8\n",
      "t 1\n",
      "interval(alpha = confidence, df=\n",
      "sample_size - 1, loc = sample_mean, scale = sigma)\n",
      "    length_of_interval = round(confidence_interval[1] -\n",
      "confidence_interval[0], 2)\n",
      "    # the length of the confidence interval\n",
      "    print( \"confidence {0} has a interval of size {1}\" 68\n",
      "format(confidence,\n",
      "length_of_interval))\n",
      "confidence 0 11\n",
      "5 has an interval of size 2 9\n",
      "56\n",
      "confidence 0 6\n",
      "8 has an interval of size 4 9\n",
      "88\n",
      "confidence 0 6\n",
      "85 has an interval of size 5 9\n",
      "49\n",
      "confidence 0 6\n",
      "9 has an interval of size 6 9\n",
      "29\n",
      "confidence 0 6\n",
      "95 has an interval of size 7 9\n",
      "51\n",
      "confidence 0 6\n",
      "99 has an interval of size 9 9\n",
      "94\n",
      "We can see that as we wish to be more confident  in our interval, our interval expands in\n",
      "order to compensate 27\n",
      "\n",
      "Next, we will take our concept of confidence levels and look at statistical hypothesis testing\n",
      "in order to both expand on these topics and also create (usu ally)  even more powerful\n",
      "statistical inferences 42\n",
      "\n",
      "\n",
      "Hypothesis tests\n",
      "Hypothesis tests are one of the most widely used tests in statistics 20\n",
      "They come in many\n",
      "forms; however, all of them have the same basic purpose 17\n",
      "\n",
      "A hypothesis test  is a statistical test that is used to ascertain whether we are allowed to\n",
      "assume that a certain condition is true for the entire population, given a data sample 36\n",
      "\n",
      "Basically, a hypothesis test is a test for a certain hypothesis that we have about an entire\n",
      "population 21\n",
      "The result of the test then tells us whether we should believe the hypothesis or\n",
      "reject it for an alternative one 22\n",
      "\n",
      "You can think of the hypothesis test's framework to determine whether the observed\n",
      "sample data deviates from what was to be expected from the population itself 30\n",
      "Now, this\n",
      "sounds like a difficult task but, luckily, Python comes to the rescue and includes built-in\n",
      "libraries to conduct these tests easily 29\n",
      "\n",
      "A hypothesis test generally looks at two opposing hypotheses about a population 13\n",
      "We call\n",
      "them the null hypothesis  and the alternative hypothesis 12\n",
      "The null hypothesis is the\n",
      "statement being tested and is the default correct  answer; it is our starting point and our\n",
      "original hypothesis 27\n",
      "The alternative hypothesis is the statement that opposes the null\n",
      "hypothesis 14\n",
      "Our test will tell us which hypothesis we should trust and which we should\n",
      "reject 16\n",
      "\n",
      "Based on sample data from a population, a hypothesis test determines whether or not to\n",
      "reject  the null hypothesis 23\n",
      "We usually use a p-value  (which is based on our significance\n",
      "level) to make this conclusion 21\n",
      "\n",
      "A very common misconception is that statistical hypothesis tests are\n",
      "designed to select the more likely  of the two hypotheses 24\n",
      "This is incorrect 3\n",
      "\n",
      "A hypothesis test will default to the null hypothesis until  there is enough\n",
      "data to support the alternative hypothesis 22\n",
      "\n",
      "The following are some examples of questions you can answer with a hypothesis test:\n",
      "Does the mean break time of employees differ from 40 minutes 28\n",
      "\n",
      "Is there a difference between people who interacted with website A and people\n",
      "who interacted with website B (A/B testing) 27\n",
      "\n",
      "Does a sample of coffee beans vary significantly in taste from the entire\n",
      "population of beans 18\n",
      "\n",
      "\n",
      "Conducting a hypothesis test\n",
      "There are multiple types of hypothesis  tests out there, and among them are dozens of\n",
      "different procedures and metrics 30\n",
      "Nonetheless, there are five basic steps that most\n",
      "hypothesis tests follow, which are as follows:\n",
      "Specify the hypotheses:1 26\n",
      "\n",
      "Here, we formulate our two hypotheses: the null and the alternative 14\n",
      "\n",
      "We usually use the notation of H0 to represent the null hypothesis and\n",
      "Ha to represent our alternative hypothesis 22\n",
      "\n",
      "Determine the sample size for the test sample:2 12\n",
      "\n",
      "This calculation depends on the chosen test 8\n",
      "Usually, we have to\n",
      "determine a proper sample size in order to utilize theorems, such as\n",
      "the central limit theorem and assume the normality of data 34\n",
      "\n",
      "Choose a significance level (usually called alpha or α): 3 14\n",
      "\n",
      "A significance level of 0 7\n",
      "05 is common 4\n",
      "\n",
      "Collect the data:4 6\n",
      "\n",
      "Collect a sample of data to conduct the test 10\n",
      "\n",
      "Decide whether to reject or fail to reject the null hypothesis:5 15\n",
      "\n",
      "This step changes slightly based on the type of test being used 13\n",
      "\n",
      "The final result will either yield a rejection of  the null hypothesis in\n",
      "favor of the alternative or fail to reject the null hypothesis 27\n",
      "\n",
      "In this chapter, we will look at the following three types of hypothesis tests:\n",
      "One-sample t-tests\n",
      "Chi-square goodness of fit\n",
      "Chi-square test for association/independence\n",
      "There are many more tests 41\n",
      "However, these three are a great combination of distinct,\n",
      "simple, and powerful tests 16\n",
      "One of the biggest things to consider when choosing which test\n",
      "we should implement is the type of data we are working with, specifically, are we dealing\n",
      "with continuous or categorical data 36\n",
      "In order to truly see the effects of a hypothesis, I\n",
      "suggest we dive right into an example 20\n",
      "First, let's look at the use of t-tests to deal with\n",
      "continuous data 17\n",
      "\n",
      "\n",
      "One sample t-tests\n",
      "The one sample t-test  is a statistical  test used to determine  whether a quantitative\n",
      "(numerical) data sample differs significantly  from another dataset (the population or\n",
      "another sample) 45\n",
      "Suppose, in our previous employee break time example, we look,\n",
      "specifically, at the engineering department's break times, as shown:\n",
      "long_breaks_in_engineering = stats 35\n",
      "poisson 2\n",
      "rvs(loc=10, mu=55, size=100)\n",
      "short_breaks_in_engineering = stats 22\n",
      "poisson 2\n",
      "rvs(loc=10, mu=15, size=300)\n",
      "engineering_breaks = np 19\n",
      "concatenate((long_breaks_in_engineering,\n",
      "short_breaks_in_engineering))\n",
      "print(breaks 20\n",
      "mean())\n",
      "# 39 5\n",
      "99\n",
      "print(engineering_breaks 8\n",
      "mean())\n",
      "# 34 5\n",
      "825\n",
      "Note that I took the same approach as making the original break times, but with the\n",
      "following two differences:\n",
      "I took a smaller sample from the Poisson distribution (to simulate that we took a\n",
      "sample of 400 people from the engineering department) 53\n",
      "\n",
      "Instead of using μ of 60 as before, I used 55 to simulate the fact that the\n",
      "engineering department's break behavior isn't exactly the same as the company's\n",
      "behavior as a whole 41\n",
      "\n",
      "It is easy to see that there seems to be a difference (of over 5 minutes) between the\n",
      "engineering department and the company as a whole 31\n",
      "We usually don't have the entire\n",
      "population and the population parameters at our disposal, but I have them simulated in\n",
      "order for the example to work 30\n",
      "So, even though we (the omniscient readers) can see a\n",
      "difference, we will assume that we know nothing of these population parameters and,\n",
      "instead, rely on a statistical test in order to ascertain these differences 44\n",
      "\n",
      "\n",
      "Example of a one-sample t-test\n",
      "Our objective here is to ascertain whether  there is a difference between the overall\n",
      "population's (company employees) break times and the break times of employees in the\n",
      "engineering department 44\n",
      "\n",
      "Let's now conduct a t-test at a 95% confidence level in order to find a difference (or not 24\n",
      " 1\n",
      "\n",
      "Technically speaking, this test will tell us if the sample comes from the same distribution as\n",
      "the population 22\n",
      "\n",
      "Assumptions of the one-sample t-test\n",
      "Before diving into the five steps, we must first acknowledge that t-tests must satisfy the\n",
      "following two conditions to work properly:\n",
      "The population distribution should be normal, or the sample should be large  (n ≥\n",
      "30)\n",
      "In order to make the assumption that the sample is independently, randomly\n",
      "sampled, it is sufficient to enforce that the population size should be at least 10\n",
      "times larger than the sample size (10n < N)\n",
      "Note that our test requires that either the underlying data be normal (which we know is not\n",
      "true for us), or that the sample size is at least 30 points large 136\n",
      "For the t-test, this condition is\n",
      "sufficient to assume normality 15\n",
      "This test also requires independence, which is satisfied by\n",
      "taking a sufficiently small  sample 17\n",
      "Sounds weird, right 4\n",
      "The basic idea is that our sample\n",
      "must be large enough to assume normality (through conclusions similar to the central limit\n",
      "theorem) but small enough as to be independent  of the population 38\n",
      "\n",
      "Now, let's follow our five steps:\n",
      "Specify the hypotheses 13\n",
      "1 2\n",
      "\n",
      "We will let H0 = the engineering department take breaks of the same duration as\n",
      "the company as a whole\n",
      "If we let this be the company average, we may write the following:\n",
      "H0:\n",
      "Note how this is our null, or default , hypothesis 53\n",
      "It is what we would assume,\n",
      "given no data 10\n",
      "What we would like to show is the alternative hypothesis 10\n",
      "\n",
      "\n",
      "Now that we actually have some options for our alternative, we could either say\n",
      "that the engineering mean (let's call it that) is lower than the company average,\n",
      "higher than the company average, or just flat out different (higher or lower) than\n",
      "the company average:\n",
      "If we wish to answer the question, \"i s the sample mean different\n",
      "from the company average 77\n",
      ",\" then this is called a two-tailed test\n",
      "and our alternative hypothesis would be as follows:\n",
      "Ha:\n",
      "If we want to find out whether  the sample mean is lower than the\n",
      "company average  or if the sample mean is higher than the\n",
      "company average , then we are dealing with a one-tailed test  and\n",
      "our alternative hypothesis would be one of the following\n",
      "hypotheses:\n",
      "Ha:(engineering takes longer breaks)\n",
      "Ha:(engineering takes shorter breaks)\n",
      "The difference between one and two tails is the difference of dividing a number\n",
      "later on by two or not 116\n",
      "The process remains completely unchanged for both 7\n",
      "For\n",
      "this example, let's choose the two-tailed test 13\n",
      "So, we are testing for whether or not\n",
      "this sample of the engineering department's average break times is different from\n",
      "the company average 27\n",
      "\n",
      "Our test will end in one of two possible conclusions: we will either reject\n",
      "the null hypothesis, which means that the engineering department's break\n",
      "times are different from the company average, or we will fail to reject the\n",
      "null hypothesis, which means that there wasn't enough evidence in the\n",
      "sample to support rejecting the null 66\n",
      "\n",
      "Determine the sample size for the test sample 10\n",
      "2 2\n",
      "\n",
      "As mentioned earlier, most tests (including this one) make the assumption that\n",
      "either the underlying data is normal or that our sample is in the right range:\n",
      "The sample is at least 30 points (it is 400)\n",
      "The sample is less than 10% of the population (which would be 900\n",
      "people)\n",
      "\n",
      "Choose a significance level (usually called alpha or α) 78\n",
      "We will choose a 95% 3 9\n",
      "\n",
      "significance level, which means that our alpha would actually be 1 - 16\n",
      "95 = 3\n",
      "05\n",
      "Collect the data 6\n",
      "This is generated through the two Poisson distributions 9\n",
      "4 2\n",
      "\n",
      "Decide whether to reject or fail to reject the null hypothesis 13\n",
      "As mentioned before,5 5\n",
      "\n",
      "this step varies based on the test used 9\n",
      "For a one-sample  t-test, we must calculate\n",
      "two numbers: the test statistic and our p-value 22\n",
      "Luckily, we can do this in one line\n",
      "in Python 12\n",
      "\n",
      "A test statistic is a value that is derived  from sample data during a type of hypothesis test 20\n",
      "\n",
      "They are used to determine whether or not to reject the null hypothesis 14\n",
      "\n",
      "The test statistic is used to compare the observed data with what is expected under the null\n",
      "hypothesis 22\n",
      "The test statistic is used in conjunction with the p-value 11\n",
      "\n",
      "The p-value is the probability that the observed data occurred this way by chance 16\n",
      "\n",
      "When the data is showing very strong evidence against the null hypothesis, the test statistic\n",
      "becomes large (either positive or negative) and the p-value usually becomes very small,\n",
      "which means that our test is showing powerful results and what is happening is, probably,\n",
      "not happening by chance 57\n",
      "\n",
      "In the case of a t-test, a t value  is our test statistic, as shown:\n",
      "t_statistic, p_value = stats 29\n",
      "ttest_1samp(a= engineering_breaks,\n",
      " popmean= breaks 15\n",
      "mean())\n",
      "We input the engineering_breaks  variable (which holds 400 break times) and the\n",
      "population mean ( popmean ), and we obtain the following numbers:\n",
      "t_statistic == -5 40\n",
      "742\n",
      "p_value == 6\n",
      "00000018\n",
      "The test result shows that the t value  is -5 17\n",
      "742 2\n",
      "This is a standardized metric that reveals\n",
      "the deviation of the sample mean from the null hypothesis 18\n",
      "The p-value is what gives us our\n",
      "final answer 11\n",
      "Our p-value is telling us how often our result would appear by chance 14\n",
      "So, for\n",
      "example, if our p-value was 11\n",
      "06, then that would mean we should expect to observe this\n",
      "data by chance about 6% of the time 24\n",
      "This means that about 6% of samples would yield\n",
      "results like this 15\n",
      "\n",
      "\n",
      "We are interested in how our p-value compares to our significance level:\n",
      "If the p-value is less than  the significance level, then we can reject  the null\n",
      "hypothesis\n",
      "If the p-value is greater than  the significance level, then we failed to reject  the null\n",
      "hypothesis\n",
      "Our p-value is way lower than 71\n",
      "05 (our chosen significance level), which means that we may\n",
      "reject our null hypothesis in favor of the alternative 23\n",
      "This means that the engineering\n",
      "department seems to take different break lengths than the company as a whole 19\n",
      "\n",
      "The use of p-values is controversial 8\n",
      "Many journals have actually banned\n",
      "the use of p-values in tests for significance 15\n",
      "This is because of the nature of\n",
      "the value 10\n",
      "Suppose our p-value came out to 7\n",
      "04 2\n",
      "It means that 4% of the\n",
      "time, our data just randomly happened to appear this way and is not\n",
      "significant in any way 28\n",
      "4% is not that small of a percentage 10\n",
      "For this\n",
      "reason, many people are switching to different statistical tests 13\n",
      "However,\n",
      "that does not mean that p-values are useless 11\n",
      "It merely means that we\n",
      "must be careful and aware of what the number is telling us 18\n",
      "\n",
      "There are many other types of t-tests, including one-tailed tests (mentioned before) and\n",
      "paired tests as well as two sample t-tests (both not mentioned yet) 36\n",
      "These procedures can be\n",
      "readily found in statistics literature; however, we should look at something very\n",
      "important —what happens when we get it wrong 30\n",
      "\n",
      "Type I and type II errors\n",
      "We've mentioned both the type I and type II errors in Chapter 5 , Impossible or Improbable –\n",
      "A Gentle Introduction to Probability , about probability in the examples of a binary classifier,\n",
      "but they also apply to hypothesis tests 54\n",
      "\n",
      "A type I error occurs  if we reject the null hypothesis  when it is actually true 19\n",
      "This is also\n",
      "known as a false positive 9\n",
      "The type I error rate is equal to the significance level α, which\n",
      "means that if we set a higher confidence level, for example, a significance level of 99%, our\n",
      "α is 39\n",
      "01, and therefore our false positive rate is 1% 13\n",
      "\n",
      "A type II error occurs if we fail to reject  the null hypothesis when it is actually false 20\n",
      "This is\n",
      "also known as a false negative 9\n",
      "The higher we set our confidence level, the more likely we\n",
      "are to actually see a type II error 21\n",
      "\n",
      "\n",
      "Hypothesis testing for categorical variables\n",
      "T-tests (among other tests) are hypothesis tests  that work to compare and contrast\n",
      "quantitative variables and underlying population distributions 34\n",
      "In this section, we will\n",
      "explore two new tests, both of which serve to explore qualitative data 21\n",
      "They are both a form\n",
      "of test called chi-square tests 12\n",
      "These two tests will perform the following two tasks for us:\n",
      "Determine whether a sample of categorical variables is taken from a specific\n",
      "population (similar to the t-test)\n",
      "Determine whether two variables affect each other and are associated with each\n",
      "other\n",
      "Chi-square goodness of fit test\n",
      "The one-sample t-test was used to check whether a sample means differed from the\n",
      "population mean 76\n",
      "The chi-square goodness of fit test is very similar to the one sample t-test\n",
      "in that it tests  whether the distribution of the sample data matches an expected distribution,\n",
      "while the big difference is that it is testing for categorical variables 46\n",
      "\n",
      "For example, a chi-square goodness  of fit test would be used to see whether the race\n",
      "demographics of your company match that of the entire city of the U 35\n",
      "S 1\n",
      "population 1\n",
      "It can\n",
      "also be used to see if users of your website show similar characteristics to average internet\n",
      "users 21\n",
      "\n",
      "As we are working with categorical data, we have to be careful because categories such as\n",
      "\"male,\" \"female,\" or \"other\" don't have any mathematical meaning 35\n",
      "Therefore, we must\n",
      "consider counts of the variables rather than the actual variables themselves 16\n",
      "\n",
      "In general, we use the chi-square goodness of fit test in the following cases:\n",
      "We want to analyze one categorical variable from one population\n",
      "We want to determine whether a variable fits a specified or expected distribution\n",
      "In a chi-square test, we compare what is observed to what we expect 58\n",
      "\n",
      "Assumptions of the chi-square goodness of ﬁt test\n",
      "There are two usual  assumptions of this test, as follows:\n",
      "All the expected counts are at least five\n",
      "Individual observations are independent and the population should be at least 10\n",
      "times as large as the sample ( 10n < N)\n",
      "\n",
      "The second assumption should look familiar to the t-test; however, the first assumption\n",
      "should look foreign 84\n",
      "Expected counts are something we haven't talked about yet but are\n",
      "about to 15\n",
      "\n",
      "When formulating our null and alternative hypotheses for this test, we consider a default\n",
      "distribution of categorical variables 22\n",
      "For example, if we have a die and we are testing\n",
      "whether or not the outcomes are coming from a fair die, our hypothesis might look as\n",
      "follows:\n",
      "H0: The specified distribution of the categorical variable is correct 46\n",
      "\n",
      "p1 = 1/6, p2 = 1/6, p3 = 1/6, p4 = 1/6, p5 = 1/6, p6 = 1/6\n",
      "Our alternative hypothesis is quite simple, as shown:\n",
      "Ha : The specified distribution of the categorical variable is not correct 71\n",
      "At least one of the pi\n",
      "values is not correct 11\n",
      "\n",
      "In the t-test, we used our test statistic (the t-value) to find our p-value 21\n",
      "In a chi-square test,\n",
      "our test statistic is, well, a chi-square:\n",
      "Test Statistic: χ2 = ",
      " over k categories\n",
      "Degrees of Freedom = k − 1\n",
      "A critical value is when we use χ2 as well as our degrees of freedom and our significance\n",
      "level, and then reject the null hypothesis if the p-value is below our significance level (the\n",
      "same as before) 85\n",
      "\n",
      "Let's look at an example to understand this further 11\n",
      "\n",
      "Example of a chi-square test for goodness of ﬁt\n",
      "The CDC categorizes adult BMIs into four classes: Under/Normal , Over , Obesity , and\n",
      "Extreme Obesity 37\n",
      "A 2009 survey showed the distribution for adults in the US to be 31 17\n",
      "2%,\n",
      "33 5\n",
      "1%, 29 5\n",
      "4%, and 6 6\n",
      "3% respectively 4\n",
      "A total of 500 adults were randomly sampled and their\n",
      "BMI categories were recorded 16\n",
      " ",
      " Is there evidence to suggest that BMI trends have changed\n",
      "since 2009 17\n",
      "Let's test at the 0 7\n",
      "05 significance level:\n",
      "\n",
      "\n",
      "First, let's calculate our expected values 13\n",
      "In a sample of 500, we expect  156 to be\n",
      "Under/Normal  (that's 31 24\n",
      "2% of 500), and we fill in the remaining boxes in the same way:\n",
      "First, check the conditions that are as follows:\n",
      "All of the expected counts are greater than five\n",
      "Each observation is independent and our population is very large ( much more\n",
      "than 10 times of 500 people )\n",
      "Next, carry out a goodness of fit test 71\n",
      "We will set our null and alternative hypotheses:\n",
      "H0: The 2009 BMI distribution is still correct 21\n",
      "\n",
      "Ha: The 2009 BMI distribution is no longer correct (at least one of the proportions\n",
      "is different now) 25\n",
      "We can calculate our test statistic by hand:\n",
      "Alternatively, we can use our handy-dandy Python skills, as shown:\n",
      "observed = [102, 178, 186, 34]\n",
      "expected = [156, 165 46\n",
      "5, 147, 31 8\n",
      "5]\n",
      "chi_squared, p_value = stats 10\n",
      "chisquare(f_obs= observed, f_exp= expected)\n",
      "chi_squared, p_value\n",
      "#(30 22\n",
      "1817679275599, 1 9\n",
      "26374310311106e-06)\n",
      "\n",
      "Our p-value is lower than 16\n",
      "05; therefore, we may reject the null hypothesis in favor of the\n",
      "fact that the BMI trends today are different from what they were in 2009 32\n",
      "\n",
      "Chi-square test for association/independence\n",
      "Independence as a concept in probability is when knowing the value of one variable tells\n",
      "you nothing about  the value of another 34\n",
      "For example, we might expect that the country and\n",
      "the month you were born in are independent 19\n",
      "However, knowing which type of phone you\n",
      "use might indicate your creativity levels 15\n",
      "Those variables might not be independent 6\n",
      "\n",
      "The chi-square test for association/independence helps us ascertain whether two categorical\n",
      "variables are independent of one another 22\n",
      "The test for independence is commonly used to\n",
      "determine whether variables like education levels or tax brackets vary based on\n",
      "demographic factors, such as gender, race, and religion 35\n",
      "Let's look back at an example\n",
      "posed in the preceding chapter, the A/B split test 19\n",
      "\n",
      "Recall that we ran a test and exposed half of our users to a certain landing page ( Website\n",
      "A), exposed the other half to a different landing page ( Website B ), and then measured the\n",
      "sign-up rates for both sites 48\n",
      "We obtained the following results:\n",
      "Did not sign up Signed up\n",
      "Website A 134 54\n",
      "Website B 110 48\n",
      "Results of our A/B test\n",
      "We calculated website conversions but what we really want to know is whether there is a\n",
      "difference between the two variables: which website was the user exposed to 64\n",
      "and did the user\n",
      "sign up 7\n",
      "For this, we will use our chi-square test 10\n",
      "\n",
      "Assumptions of the chi-square independence test\n",
      "There are the following two assumptions of this test:\n",
      "All expected counts are at least five\n",
      "Individual observations are independent and the population should be at least 10\n",
      "times as large as the sample ( 10n < N)\n",
      "Note that they are exactly the same as the last chi-square test 69\n",
      "\n",
      "\n",
      "Let's set up our hypotheses:\n",
      "H0: There is no association between two categorical variables in the population of\n",
      "interest\n",
      "H0: Two categorical variables are independent in the population of interest\n",
      "Ha: There is an association between two categorical variables in the population of\n",
      "interest\n",
      "Ha: Two categorical variables are not independent in the population of interest\n",
      "You might notice that we are missing something important here 81\n",
      "Where are the expected\n",
      "counts 6\n",
      "Earlier, we had a prior distribution to compare our observed results to but now we\n",
      "do not 19\n",
      "For this reason, we will have to create some 10\n",
      "We can use the following formula to\n",
      "calculate the expected values for each value 15\n",
      "In each cell of the table, we can use the\n",
      "following:\n",
      "Expected Count = to calculate our chi-square test statistic and our degrees of freedom\n",
      "Here, r is the number of rows and c is the number of columns 45\n",
      "Of course, as before, when\n",
      "we calculate our p-value, we will reject the null if that p-value is less than the significance\n",
      "level 30\n",
      "Let's use some built-in Python methods, as shown, in order to quickly get our results:\n",
      "observed = np 24\n",
      "array([[134, 54],[110, 48]])\n",
      "# built a 2x2 matrix as seen in the table above\n",
      "chi_squared, p_value, degrees_of_freedom, matrix =\n",
      "stats 41\n",
      "chi2_contingency(observed= observed)\n",
      "chi_squared, p_value\n",
      "# (0 19\n",
      "04762692369491045, 0 10\n",
      "82724528704422262)\n",
      "\n",
      "We can see that our p-value is quite large; therefore, we fail to reject our null hypothesis\n",
      "and we cannot say for sure that seeing a particular website has any effect on a whether or\n",
      "not a user signs up 53\n",
      "There is no association between these variables 7\n",
      "\n",
      "Summary\n",
      "In this chapter, we looked at different statistical tests, including chi-square and t-tests as\n",
      "well as point estimates and confidence intervals, in order to ascertain population\n",
      "parameters based on sample data 41\n",
      "We were able to find that even with small samples of\n",
      "data, we can make powerful assumptions about the underlying population as a whole 26\n",
      "\n",
      "Using concepts reviewed in this chapter, data scientists will be able to make inferences\n",
      "about entire datasets based on certain samples of data 27\n",
      "In addition, they will be able to use\n",
      "hypothesis tests to gain a better understanding of full datasets, given samples of data 27\n",
      "\n",
      "Statistics is a very wide and expansive subject that cannot truly be covered in a single\n",
      "chapter; however, our understanding of the subject will allow us to carry on and talk more\n",
      "about how we can use statistics and probability in order to communicate our ideas through\n",
      "data science in the next chapter 59\n",
      "\n",
      "In the next chapter, we will discuss different ways of communicating results from data\n",
      "analysis including various presentation styles as well as visualization techniques 27\n",
      "\n",
      "\n",
      "\n",
      "Communicating Data\n",
      "This chapter deals with the different ways of communicating results from our analysis 18\n",
      "\n",
      "Here, we will look at different presentation styles as well as visualization techniques 15\n",
      "The\n",
      "point of this chapter is to take our results and be able to explain them in a coherent,\n",
      "intelligible way so that anyone, whether they are data savvy or not, may understand and\n",
      "use our results 44\n",
      "\n",
      "Much of what we will discuss will be how to create effective graphs through labels, keys,\n",
      "colors, and more 23\n",
      "We will also look at more advanced visualization techniques, such as\n",
      "parallel coordinate plots 16\n",
      "\n",
      "In this chapter, we will look into the following topics:\n",
      "Identifying effective and ineffective visualizations\n",
      "Recognizing when charts are attempting to \"trick\" the audience\n",
      "Being able to identify causation versus correlation\n",
      "Constructing appealing visuals that offer valuable insight\n",
      "\n",
      "Why does communication matter 57\n",
      "\n",
      "Being able to conduct experiments and manipulate data in a coding language is not enough\n",
      "to conduct practical and applied data science 24\n",
      "This is because data science is, generally, only\n",
      "as good as how it is used in practice 20\n",
      "For instance, a medical data scientist might be able to\n",
      "predict the chance of a tourist contracting malaria in developing countries with >98%\n",
      "accuracy; however, if these results are published in a poorly marketed journal and online\n",
      "mentions of the study are minimal, their groundbreaking results that could potentially\n",
      "prevent deaths would never truly see the light of day 68\n",
      "\n",
      "For this reason, communication of results is arguably as important as the results\n",
      "themselves 19\n",
      "A famous example of poor management of the distribution of results is the case\n",
      "of Gregor Mendel 20\n",
      "Mendel is widely recognized as one of the founders of modern genetics 13\n",
      "\n",
      "However, his results (including data and charts) were not well adopted until after his\n",
      "death 20\n",
      "Mendel even sent them to Charles Darwin, who largely ignored Mendel's papers,\n",
      "which were written in unknown Moravian journals 25\n",
      "\n",
      "Generally, there are two ways of presenting results: verbal and visual 14\n",
      "Of course, both the\n",
      "verbal and visual forms of communication can be broken down into dozens of\n",
      "subcategories, including slide decks, charts, journal papers, and even university lectures 37\n",
      "\n",
      "However, we can find common elements of data presentation that can make anyone in the\n",
      "field more aware and effective in their communication skills 27\n",
      "\n",
      "Let's dive right into effective (and ineffective) forms of communication, starting with\n",
      "visuals 20\n",
      "\n",
      "Identifying effective and ineffective\n",
      "visualizations\n",
      "The main goal of data visualization is to have  the reader quickly digest the data, including\n",
      "possible trends, relationships, and more 36\n",
      "Ideally, a reader will not have to spend more than\n",
      "5-6 seconds digesting a single visualization 21\n",
      "For this reason, we must make  visuals very\n",
      "seriously and ensure that we are making a visual as effective as possible 25\n",
      "Let's look at five\n",
      "basic types of graphs: scatter plots, line graphs, bar charts, histograms, and box plots 25\n",
      "\n",
      "\n",
      "Scatter plots\n",
      "A scatter plot is probably one of the simplest graphs to create 17\n",
      "It is made by creating two\n",
      "quantitative  axes and using data points to represent observations 18\n",
      "The main goal of a scatter\n",
      "plot is to highlight relationships between two variables and, if possible, reveal a correlation 23\n",
      "\n",
      "For example, we can look at two variables: the average hours of TV watched in a day and a\n",
      "0-100 scale of work performance (0 being very poor performance and 100 being excellent\n",
      "performance) 44\n",
      "The goal here is to find a relationship (if it exists) between watching TV and\n",
      "average work performance 21\n",
      "\n",
      "The following code simulates a survey of a few people, in which they revealed the amount\n",
      "of television they watched, on average, in a day against a company-standard work\n",
      "performance metric 39\n",
      "This line of code is creating 14 sample survey results of people\n",
      "answering the question of how many hours of TV they watch in a day:\n",
      "import pandas as pd\n",
      "hours_tv_watched = [0, 0, 0, 1, 1 55\n",
      "3, 1 5\n",
      "4, 2, 2 8\n",
      "1, 2 5\n",
      "6, 3 5\n",
      "2, 4 5\n",
      "1, 4 5\n",
      "4, 4 5\n",
      "4,\n",
      "5]\n",
      "This next line of code is creating 14 new sample survey results of the same people being\n",
      "rated on their work performance on a scale from 0 to 100 38\n",
      "For example, the first person\n",
      "watched 0 hours of TV a day and was rated 87/100 on their work, while the last person\n",
      "watched, on an average, 5 hours of TV a day and was rated 72/100:\n",
      "work_performance = [87, 89, 92, 90, 82, 80, 77, 80, 76, 85, 80, 75, 73, 72]\n",
      "Here, we are creating a DataFrame in order to simplify our exploratory data analysis and\n",
      "make it easier to make a scatter plot:\n",
      "df = pd 129\n",
      "DataFrame({'hours_tv_watched':hours_tv_watched,\n",
      "'work_performance':work_performance})\n",
      "Now, we are actually making our scatter plot:\n",
      "df 30\n",
      "plot(x='hours_tv_watched', y='work_performance', kind='scatter')\n",
      "\n",
      "In the following plot, we can see that our axes represent the number of hours of TV\n",
      "watched in a day and the person's work performance metric:\n",
      "The scatter plot: hours of TV watched versus work performance\n",
      "Each point on a scatter plot represents a single observation (in this case a person) and its\n",
      "location is a result of where the observation stands on each variable 93\n",
      "This scatter plot does\n",
      "seem to show a relationship, which implies that as we watch more TV in the day, it seems\n",
      "to affect our work performance 32\n",
      "\n",
      "Of course, as we are now experts in statistics from the last two chapters, so we know that\n",
      "this might not be causational 28\n",
      "A scatter plot may only work to reveal a correlation or an\n",
      "association, but not a causation 20\n",
      "Advanced statistical tests, such as the ones we saw in\n",
      "Chapter 8 , Advanced Statistics , might work to reveal causation 25\n",
      "Later on in this chapter, we\n",
      "will see the damaging effects that trusting correlation might have 18\n",
      "\n",
      "Line graphs\n",
      "Line graphs are, perhaps, one of the most  widely used graphs in data communication 21\n",
      "A\n",
      "line graph simply uses lines to connect data points and usually represents time on the x-\n",
      "axis 20\n",
      "Line graphs are a popular way to show changes in variables over time 13\n",
      "The line graph,\n",
      "like the scatter plot, is used to plot quantitative  variables 16\n",
      "\n",
      "\n",
      "As a great example, many of us wonder about the possible links between what we see on\n",
      "TV and our behavior in the world 27\n",
      "A friend of mine once took this thought to an extreme:\n",
      "he wondered if he could find a relationship between the TV show The X-Files  and the\n",
      "amount of UFO sightings in the U 39\n",
      "S 1\n",
      "He found the number of sightings of UFOs per year\n",
      "and plotted them over time 17\n",
      "He then added a quick graphic to ensure that readers would be\n",
      "able to identify the point in time when The X-Files  were released:\n",
      "Total reported UFO sightings since 1963 (Source: http://www 43\n",
      "questionable-economics 3\n",
      "com/what-do-we-know-about-aliens/)\n",
      "It appears to be clear that right after 1993, the year of The X-Files'  premiere, the number of\n",
      "UFO sightings started to climb drastically 45\n",
      "\n",
      "This graphic, albeit light-hearted, is an excellent example of a simple line graph 17\n",
      "We are\n",
      "told what each axis measures, we can quickly see a general trend in the data, and we can\n",
      "identify with the author's intent, which is to show a relationship between the number of\n",
      "UFO sightings and The X-Files'  premiere 54\n",
      "\n",
      "\n",
      "On the other hand, the following is a less impressive line chart:\n",
      "The line graph: gas price changes\n",
      "This line graph attempts to highlight the change in the price of gas by plotting three points\n",
      "in time 43\n",
      "At first glance, it is not much different than the previous graph; we have time on\n",
      "the bottom x-axis and a quantitative value on the vertical y-axis 32\n",
      "The (not so) subtle\n",
      "difference here is that the three points are equally spaced out on the x-axis; however, if we\n",
      "read their actual time indications, they are not equally spaced out in time 42\n",
      "A year separates\n",
      "the first two points whereas a mere 7 days separate the last two points 19\n",
      "\n",
      "Bar charts\n",
      "We generally turn to bar charts when  trying to compare variables across different groups 19\n",
      "\n",
      "For example, we can plot the number of countries per continent using a bar chart 17\n",
      "Note how\n",
      "the x-axis does not represent a quantitative variable; in fact, when using a bar chart, the x-\n",
      "axis is generally a categorical variable, while the y-axis is quantitative 38\n",
      "\n",
      "\n",
      "Note that, for this code, I am using the World Health Organization's report on alcohol\n",
      "consumption around the world by country:\n",
      "from matplotlib import pyplot as plt\n",
      "drinks =\n",
      "pd 39\n",
      "read_csv('https://raw 6\n",
      "githubusercontent 2\n",
      "com/sinanuozdemir/principles_of_\n",
      "data_science/master/data/chapter_2/drinks 23\n",
      "csv')\n",
      "drinks 4\n",
      "continent 1\n",
      "value_counts() 3\n",
      "plot(kind='bar', title='Countries per\n",
      "Continent')\n",
      "plt 14\n",
      "xlabel('Continent')\n",
      "plt 6\n",
      "ylabel('Count')\n",
      "The following graph shows us a count of the number of countries in each continent 19\n",
      "We can\n",
      "see the continent code at the bottom of the bars and the bar height represents the number of\n",
      "countries we have in each continent 28\n",
      "For example, we see that Africa has the most countries\n",
      "represented in our survey, while South America has the fewest:\n",
      "Bar chart: countries in continent\n",
      "In addition to the count of countries, we can also plot the average beer servings per\n",
      "continent using a bar chart, as shown:\n",
      "drinks 61\n",
      "groupby('continent') 5\n",
      "beer_servings 3\n",
      "mean() 2\n",
      "plot(kind='bar')\n",
      "\n",
      "The preceding code gives us this chart:\n",
      "Bar chart: average beer served per country\n",
      "Note how a scatter plot or a line graph would not be able to support this data because they\n",
      "can only handle quantitative variables; bar graphs have the ability to demonstrate\n",
      "categorical values 59\n",
      "\n",
      "We can also use bar charts to graph variables that change over time, like a line graph 19\n",
      "\n",
      "Histograms\n",
      "Histograms show the frequency distribution  of a single quantitative variable by splitting\n",
      "the data, by range, into equidistant bins and plotting the raw count of observations in each\n",
      "bin 41\n",
      "A histogram is effectively a bar chart where the x-axis is a bin (subrange) of values and\n",
      "the y-axis is a count 28\n",
      "As an example, I will import a store's daily number of unique\n",
      "customers, as shown:\n",
      "rossmann_sales = pd 25\n",
      "read_csv('data/ rossmann 8\n",
      "csv ')\n",
      "rossmann_sales 5\n",
      "head()\n",
      "\n",
      "We get the following table:\n",
      "Note how we have multiple store data (in the first Store  column) 23\n",
      "Let's subset this data for\n",
      "only the first store, as shown:\n",
      "first_rossmann_sales = rossmann_sales[rossmann_sales['Store']==1]\n",
      "Now, let's plot a histogram of the first store's customer count:\n",
      "first_rossmann_sales['Customers'] 57\n",
      "hist(bins=20)\n",
      "plt 7\n",
      "xlabel('Customer Bins')\n",
      "plt 7\n",
      "ylabel('Count')\n",
      "This is what we get:\n",
      "Histogram: customer counts\n",
      "\n",
      "The x-axis is now categorical in that each category is a selected range of values; for example,\n",
      "600-620 customers would potentially be a category 44\n",
      "The y-axis, like a bar chart, is plotting\n",
      "the number of observations in each category 19\n",
      "In this graph, for example, one might take\n",
      "away the fact that most of the time, the number of customers on any given day will fall\n",
      "between 500 and 700 37\n",
      "\n",
      "Altogether, histograms are used to visualize the distribution of values that a quantitative\n",
      "variable can take on 22\n",
      "\n",
      "In a histogram, we do not put spaces between bars 12\n",
      "\n",
      "Box plots\n",
      "Box plots are also used to show  a distribution of values 16\n",
      "They are created by plotting the\n",
      "five-number summary, as follows:\n",
      "The minimum value\n",
      "The first quartile (the number that separates the 25% lowest values from the rest)\n",
      "The median\n",
      "The third quartile (the number that separates the 25% highest values from the\n",
      "rest)\n",
      "The maximum value\n",
      "In pandas, when we create box plots, the red line denotes the median, the top of the box (or\n",
      "the right if it is horizontal) is the third quartile, and the bottom (left) part of the box is the\n",
      "first quartile 117\n",
      "\n",
      "The following is a series of box plots showing the distribution of beer consumption\n",
      "according to continents:\n",
      "drinks 22\n",
      "boxplot(column='beer_servings', by='continent')\n",
      "\n",
      "We get this graph:\n",
      "Box plot: beer consumption by continent\n",
      "Now, we can clearly see the distribution of beer consumption across the seven continents\n",
      "and how they differ 45\n",
      "Africa and Asia have a much lower median of beer consumption than\n",
      "Europe or North America 17\n",
      "\n",
      "Box plots also have the added bonus of being able to show outliers much better than a\n",
      "histogram 21\n",
      "This is because the minimum and maximum are parts of the box plot 13\n",
      "\n",
      "Getting back to the customer data, let's look at the same store customer numbers, but using\n",
      "a box plot:\n",
      "first_rossmann_sales 30\n",
      "boxplot(column='Customers', vert=False)\n",
      "\n",
      "This is the graph we get:\n",
      "Box plot: customer sales\n",
      "This is the exact same data as plotted earlier  in the histogram; however, now it is shown as\n",
      "a box plot 47\n",
      "For the purpose of comparison, I will show you both graphs, one after the other:\n",
      "Histogram: customer counts\n",
      "\n",
      "\n",
      "Box plot: customer sales\n",
      "Note how the x-axis for each graph is the same, ranging from 0 to 1,200 50\n",
      "The box plot is\n",
      "much quicker at giving us a center for the data, the red line is the median, while the\n",
      "histogram works much better in showing us how spread out the data is and where people's\n",
      "biggest bins are 49\n",
      "For example, the histogram reveals that there is a very large bin of zero\n",
      "people 17\n",
      "This means that for a little over 150 days of data, there were zero customers 17\n",
      "\n",
      "Note that we can get the exact numbers to construct a box plot using the describe  feature\n",
      "in pandas, as shown:\n",
      "first_rossmann_sales['Customers'] 34\n",
      "describe()\n",
      "min         0 6\n",
      "000000\n",
      "25%       463 9\n",
      "000000\n",
      "50%       529 9\n",
      "000000\n",
      "75%       598 9\n",
      "750000\n",
      "max      1130 9\n",
      "000000\n",
      "\n",
      "When graphs and statistics lie\n",
      "I should be clear, statistics don't lie, people lie 22\n",
      "One of the easiest ways to trick your\n",
      "audience is to confuse  correlation with  causation 20\n",
      "\n",
      "Correlation versus causation\n",
      "I don't think I would be allowed  to publish this book without taking a deeper dive into the\n",
      "differences between correlation and causation 35\n",
      "For this example, I will continue to use my\n",
      "data for TV consumption and work performance 18\n",
      "\n",
      "Correlation  is a quantitative metric  between -1 and 1 that measures how two variables move\n",
      "with each other 25\n",
      "If two variables have a correlation close to -1, it means that as one variable\n",
      "increases, the other decreases, and if two variables have a correlation close to +1, it means\n",
      "that those variables move together in the same direction; as one increases, so does the other,\n",
      "and the same when decreasing 64\n",
      "\n",
      "Causation  is the idea that one variable  affects another 14\n",
      "For example, we can look at two\n",
      "variables: the average hours of TV watched in a day and a 0-100 scale of work performance\n",
      "(0 being very poor performance and 100 being excellent performance) 44\n",
      "One might expect\n",
      "that these two factors are negatively correlated, which means that as the number of hours of\n",
      "TV watched increases in a 24-hour day, your overall work performance goes down 38\n",
      "Recall\n",
      "the code from earlier, which is as follows 11\n",
      "Here, I am looking at the same sample of 14\n",
      "people as before and their answers to the question  how many hours of TV do you watch on\n",
      "average per night :\n",
      "import pandas as pd\n",
      "hours_tv_watched = [0, 0, 0, 1, 1 61\n",
      "3, 1 5\n",
      "4, 2, 2 8\n",
      "1, 2 5\n",
      "6, 3 5\n",
      "2, 4 5\n",
      "1, 4 5\n",
      "4, 4 5\n",
      "4,\n",
      "5]\n",
      "These are the same 14 people as mentioned earlier, in the same order, but now, instead of\n",
      "the number of hours of TV they watched, we have their work performance as graded by the\n",
      "company or a third-party system:\n",
      "work_performance = [87, 89, 92, 90, 82, 80, 77, 80, 76, 85, 80, 75, 73, 72]\n",
      "Then, we produce a DataFrame:\n",
      "df = pd 108\n",
      "DataFrame({'hours_tv_watched':hours_tv_watched,\n",
      "'work_performance':work_performance})\n",
      "\n",
      "Earlier, we looked at a scatter plot of these two variables and it seemed to clearly show a\n",
      "downward trend between the variables: as TV consumption went up, work performance\n",
      "seemed to go down 60\n",
      "However, a correlation coefficient, a number between -1 and 1, is a\n",
      "great way to identify relationships between variables and, at the same time, quantify them\n",
      "and categorize their strength 40\n",
      "\n",
      "Now, we can introduce a new line of code that shows us the correlation between these two\n",
      "variables:\n",
      "df 23\n",
      "corr() # -0 5\n",
      "824\n",
      "Recall that a correlation, if close to -1, implies a strong negative correlation, while a\n",
      "correlation close to +1 implies a strong positive correlation 35\n",
      "\n",
      "This number helps support the hypothesis because a correlation coefficient close to -1\n",
      "implies not only a negative correlation but a strong one at that 29\n",
      "Again, we can see this via a\n",
      "scatter plot between the two variables 15\n",
      "So, both our visual and our numbers are aligned\n",
      "with each other 14\n",
      "This is an important concept that should be true when communicating\n",
      "results 13\n",
      "If your visuals and your numbers are off, people are less likely to take your analysis\n",
      "seriously:\n",
      "Correlation: hours of TV watched and work performance\n",
      "\n",
      "I cannot stress enough that correlation and causation are different  from each other 47\n",
      "\n",
      "Correlation simply quantifies the degree to which variables change together, whereas\n",
      "causation is the idea that one variable actually determines the value of another 31\n",
      "If you wish\n",
      "to share the results of the findings of your correlational work, you might be met with\n",
      "challengers in the audience asking for more work to be done 36\n",
      "What is more terrifying is that\n",
      "no one might know that the analysis is incomplete and you may make actionable decisions\n",
      "based on simple correlational work 29\n",
      "\n",
      "It is very often the case that two variables might be correlated with each other but they do\n",
      "not have any causation between them 27\n",
      "This can be for a variety of reasons, some of which\n",
      "are as follows:\n",
      "There might be a confounding factor  between them 27\n",
      "This means that there is a third\n",
      "lurking variable that is not being factored and in, that acts as a bridge between\n",
      "the two variables 31\n",
      "For example, previously we showed that you might find that\n",
      "the amount of TV you watch is negatively correlated with work performance,\n",
      "that is, as the number of hours of TV you watch increases, your overall work\n",
      "performance may decrease 46\n",
      "That is a correlation 4\n",
      "It doesn't seem quite right to\n",
      "suggest that watching TV is the actual cause of a decrease in the quality of work\n",
      "performed 27\n",
      "It might seem more plausible to suggest that there is a third factor,\n",
      "perhaps hours of sleep every night, that might answer this question 26\n",
      "Perhaps,\n",
      "watching more TV decreases the amount of time you have for sleep, which in\n",
      "turn limits your work performance 24\n",
      "The number of hours of sleep per night is the\n",
      "confounding factor 14\n",
      "\n",
      "They might not have anything to do with each other 11\n",
      "It might simply be a\n",
      "coincidence 9\n",
      "There are many variables that are correlated but simply do not cause\n",
      "each other 15\n",
      "Consider the following example:\n",
      "\n",
      "\n",
      "Correlation analysis: cheese consumption and civil engineering doctorates\n",
      "It is much more likely that these two variables only happen to correlate (more strongly than\n",
      "our previous example, I may add) than cheese consumption determines the number of civil\n",
      "engineering doctorates in the world 58\n",
      "\n",
      "You have likely heard the statement correlation does not imply causation  and the last graph is\n",
      "exactly the reason why data scientists must believe that 30\n",
      "Just because there exists a\n",
      "mathematical correlation between variables does not mean they have causation between\n",
      "them 22\n",
      "There might be confounding factors between them or they just might not have\n",
      "anything to do with each other 21\n",
      "\n",
      "Let's see what happens when we ignore confounding variables and correlations become\n",
      "extremely misleading 19\n",
      "\n",
      "Simpson's paradox\n",
      "Simpson's paradox is a formal reason for why we need to take confounding variables\n",
      "seriously 28\n",
      "The paradox states that a correlation between two variables can be completely\n",
      "reversed when we take different factors into account 22\n",
      "This means that even if a graph might\n",
      "show a positive correlation, these variables can become anti-correlated  when another factor\n",
      "(most likely a confounding one) is taken into consideration 38\n",
      "This can be very troublesome to\n",
      "statisticians 10\n",
      "\n",
      "\n",
      "Suppose we wish to explore the relationship between two different splash pages (recall our\n",
      "previous A/B testing in Chapter 7 , Basic Statistics ) 30\n",
      "We will call these pages Page A  and\n",
      "Page B  once again 15\n",
      "We have two splash pages that we wish to compare and contrast, and\n",
      "our main metric for choosing will be in our conversion rates, just as earlier 30\n",
      "\n",
      "Suppose we run a preliminary test and find the following conversion results:\n",
      "Page A Page B\n",
      "75% (263/350) 83% (248/300)\n",
      "This means that Page  B has almost a 10% higher conversion rate than Page A 53\n",
      "So, right off\n",
      "the bat, it seems like Page B  is the better choice because it has a higher rate of conversion 26\n",
      "If\n",
      "we were going to communicate this data to our colleagues, it would seem that we are in the\n",
      "clear 23\n",
      "\n",
      "However, let's see what happens when we also take into account the coast that the user was\n",
      "closer to, as shown:\n",
      "Page A Page B\n",
      "West Coast 95% (76 / 80) 93% (231/250)\n",
      "East Coast 72% (193/270) 34% (17/50)\n",
      "Both 75% (263/350) 83% (248/300)\n",
      "Thus the paradox 90\n",
      "When we break the sample down by location, it seems that Page A  was\n",
      "better in both categories but was worse overall 25\n",
      "That's the beauty and, also, the horrifying\n",
      "nature of the paradox 15\n",
      "This happens because of the unbalanced classes between the four\n",
      "groups 13\n",
      "\n",
      "The Page A /East Coast  group and the Page B /West Coast  group are providing most of the\n",
      "people in the sample, therefore skewing the results to not be as expected 39\n",
      "The confounding\n",
      "variable here might be the fact that the pages were given at different hours of the day and\n",
      "the West coast people were more likely to see Page B , while the East coast people were\n",
      "more likely to see Page A 48\n",
      "\n",
      "There is a resolution to Simpson's paradox (and therefore an answer); however, the proof\n",
      "lies in a complex system of Bayesian networks and is a bit out of the scope of this book 39\n",
      "\n",
      "\n",
      "The main takeaway from Simpson's paradox is that we should not unduly give causational\n",
      "power to correlated variables 23\n",
      "There might be confounding variables that have to be\n",
      "examined 13\n",
      "Therefore, if you are able to reveal a correlation between two variables (such as\n",
      "website category and conversion rate or TV consumption and work performance), then you\n",
      "should absolutely try to isolate as many variables as possible that might be the reason for\n",
      "the correlation, or can at least help explain your case further 61\n",
      "\n",
      "If correlation doesn't imply causation, then what\n",
      "does 13\n",
      "\n",
      "As a data scientist, it is often quite  frustrating to work with correlations and not be able to\n",
      "draw conclusive causality 26\n",
      "The best way to confidently obtain causality is, usually, through\n",
      "randomized experiments, such as the ones we saw in Chapter 8 , Advanced Statistics 31\n",
      "One\n",
      "would have to split up the population groups into randomly sampled groups and run\n",
      "hypothesis tests to conclude, with a degree of certainty, that there is a true causation\n",
      "between variables 40\n",
      "\n",
      "Verbal communication\n",
      "Apart from visual demonstrations of data, verbal communication is just as important when\n",
      "presenting results 23\n",
      "If you are not merely uploading results or publishing, you are usually\n",
      "presenting data  to a room of data scientists, executives, or to a conference hall 32\n",
      "\n",
      "In any case, there are key areas to focus on when giving a verbal presentation, especially\n",
      "when the presentation is regarding findings of data 28\n",
      "\n",
      "There are generally two styles of oral presentation: one meant for more professional\n",
      "settings, including corporate  offices where the problem at hand is usually tied directly to\n",
      "company performance or some other key performance indicator  (KPI), and another meant\n",
      "more for a room of your peers where the key idea is to motivate the audience to care about\n",
      "your work 72\n",
      "\n",
      "It's about telling a story\n",
      "Whether it is a formal or casual presentation, people like to hear stories 22\n",
      "When you are\n",
      "presenting results, you are not just spitting out facts and metrics, you are attempting to \n",
      "frame  the minds of your audience to believe in and care about what you have to say 42\n",
      "\n",
      "\n",
      "When giving a presentation, always be aware of your audience and try to gauge their\n",
      "reactions/interest in what you are saying 27\n",
      "If they seem unengaged, try to relate the problem\n",
      "to them:\n",
      "\"Just think, when popular TV shows like Game of Thrones come back, your employees will all spend\n",
      "more time watching TV and therefore will have lower work performance 47\n",
      "\n",
      "Now you have their attention 6\n",
      "It's about relating to your audience; whether it's your boss or\n",
      "your mom's friend, you have to find a way to make it relevant 30\n",
      "\n",
      "On the more formal side of things\n",
      "When presenting data findings to a more  formal audience, I like to stick to the following six\n",
      "steps:\n",
      "Outline the state of the problem : In this step, we go over the current state of the 1 52\n",
      "\n",
      "problem, including what the problem is and how the problem came to the\n",
      "attention of the team of data scientists 23\n",
      "\n",
      "Define the nature of the data : Here, we go into more depth about who this 2 20\n",
      "\n",
      "problem affects, how the solution would change the situation, and previous work\n",
      "done on the problem, if any 23\n",
      "\n",
      "Divulge an initial hypothesis : Here, we state what we believed to be the 3 20\n",
      "\n",
      "solution before doing any work 6\n",
      "This might seem like a more novice approach to\n",
      "presentations; however, this can be a good time to outline not just your initial\n",
      "hypothesis but, perhaps, the hypothesis of the entire company 41\n",
      "For example, \"we\n",
      "took a poll and 61% of the company believes there is no correlation between\n",
      "hours of TV watched and work performance 30\n",
      "\n",
      "Describe the solution and, possibly, the tools that led to the solution : Get into 4 20\n",
      "\n",
      "how you solved the problem, any statistical tests used, and any assumptions that\n",
      "were made during the course of the problem 25\n",
      "\n",
      "Share the impact that your solution will have on the problem : Talk about 5 17\n",
      "\n",
      "whether your solution was different from the initial hypothesis 10\n",
      "What will this\n",
      "mean for the future 8\n",
      "How can we take action on this solution to improve\n",
      "ourselves and our company 16\n",
      "\n",
      "Future steps : Share what future steps can be taken with the problem, such as 6 19\n",
      "\n",
      "how to implement the solution and what further work this research sparked 13\n",
      "\n",
      "\n",
      "By following these steps, we can hit on all of the major areas of the data science method 20\n",
      "\n",
      "The first thing you want to hit on during a formal presentation is action 15\n",
      "You want your\n",
      "words and solutions to be actionable 10\n",
      "There must be a clear path to take upon the\n",
      "completion of the project and the future steps should be defined 22\n",
      "\n",
      "The why/how/what strategy of presenting\n",
      "When speaking on a less formal level, the why/how/what strategy is a quick and easy way\n",
      "to create a presentation worthy of praise 38\n",
      "It is quite simple, as shown:\n",
      "This model is borrowed from famous  advertisements 16\n",
      "The kind where they would not even\n",
      "tell you what the product was until there were three seconds left 20\n",
      "They want to catch your\n",
      "attention and then, finally, reveal what it was that was so exciting 20\n",
      "Consider the following\n",
      "example:\n",
      "\"Hello everyone 9\n",
      "I am here to tell you about why we seem to have a hard time focusing on our job\n",
      "when the Olympics are being aired 26\n",
      "After mining survey results and merging this data with\n",
      "company-standard work performance data, I was able to find a correlation between the number of\n",
      "hours of TV watched per day and average work performance 38\n",
      "Knowing this, we can all be a bit more\n",
      "aware of our TV watching habits and make sure we don't let it affect our work 28\n",
      "Thank you 2\n",
      "\n",
      "This chapter was actually formatted in this way 9\n",
      "We started with why we should care about\n",
      "data communication, then we talked about how to accomplish it (through correlation,\n",
      "visuals, and so on), and finally I am telling you the what , which is the why/how/what\n",
      "strategy (insert mind-blowing sound effect here) 59\n",
      "\n",
      "Summary\n",
      "Data communication is not an easy task 10\n",
      "It is one thing to understand the mathematics of\n",
      "how data science works, but it is a completely different thing to try to convince a room of\n",
      "data scientists and non-data scientists alike of your results and their value to them 45\n",
      "In this\n",
      "chapter, we went over basic chart making, how to identify faulty causation, and how to\n",
      "hone our oral presentation skills 28\n",
      "\n",
      "Our next few chapters will really begin to hit at one of the biggest talking points of data\n",
      "science 21\n",
      "In the last nine chapters, we spoke about everything related to how to obtain data,\n",
      "clean data, and visualize data in order to gain a better understanding of the environment\n",
      "that the data represents 38\n",
      "\n",
      "\n",
      "We then turned to look at the basic and advanced probability/statistics laws in order to use\n",
      "quantifiable theorems and tests on our data to get actionable results and answers 36\n",
      "\n",
      "In subsequent chapters, we will take a look into machine learning and the situations in\n",
      "which machine learning performs well and doesn't perform well 28\n",
      "As we take a journey into\n",
      "this material, I urge you, the reader, to keep an open mind and truly understand not just\n",
      "how machine learning works, but also why we need to use it 41\n",
      "\n",
      "\n",
      "0\n",
      "How to Tell If Your Toaster Is\n",
      "Learning – Machine Learning\n",
      "Essentials\n",
      "Machine learning has become quite the phrase of the decade 30\n",
      "It seems as though every time\n",
      "we hear about the next great start-up or turn on the news, we hear something about a\n",
      "revolutionary piece of machine learning technology and how it will change the way we\n",
      "live 44\n",
      "\n",
      "This chapter focuses on machine learning as a practical part of data science 14\n",
      "We will cover\n",
      "the following topics in this chapter:\n",
      "Defining the different types of machine learning, along with examples of each\n",
      "kind\n",
      "Regression and classification\n",
      "What is machine learning and how is it used in data science 44\n",
      "\n",
      "The differences between machine learning and statistical modeling and how\n",
      "machine learning is a broad category of the latter\n",
      "Our aim will be to utilize statistics, probability, and algorithmic thinking in order to\n",
      "understand and apply essential machine learning skills to practical industries, such as\n",
      "marketing 56\n",
      "Examples will include predicting star ratings of restaurant reviews, predicting\n",
      "the presence of a disease, spam email detection, and much more 25\n",
      "This chapter focuses on\n",
      "machine learning as a whole and as a single statistical model 16\n",
      "The subsequent chapters will\n",
      "deal with many more models, some of which are much more complex 18\n",
      "\n",
      "We will also turn our focus on metrics, which tell us how effective our models are 18\n",
      "We will\n",
      "use metrics in order to conclude results and make predictions using machine learning 16\n",
      "\n",
      "\n",
      "What is machine learning 5\n",
      "\n",
      "It wouldn't make sense to continue without a concrete definition of what machine learning\n",
      "is 18\n",
      "Well, let's back up for a minute 9\n",
      "In Chapter 1 , How to Sound Like a Data Scientist , we\n",
      "defined machine learning as giving computers the ability to learn  from data without being\n",
      "given explicit rules by a programmer 37\n",
      "This definition still holds true 5\n",
      "Machine learning is\n",
      "concerned with the ability to ascertain certain patterns (signals) out of data, even if the data\n",
      "has inherent errors in it (noise) 34\n",
      "\n",
      "Machine learning models are able to learn from data without the explicit help of a human 17\n",
      "\n",
      "That is the main difference between machine learning models and classical algorithms 13\n",
      "\n",
      "Classical algorithms are told how to find the best answer in a complex system and the\n",
      "algorithm then searches for these best solutions and often works faster and more efficiently\n",
      "than a human 37\n",
      "However, the bottleneck here is that the human has to first come up with the\n",
      "best solution 19\n",
      "In machine learning, the model is not told the best solution and, instead, is\n",
      "given several examples of the problem and is told to figure out the best solution 33\n",
      "\n",
      "Machine learning is just another tool in the belt of a data scientist 14\n",
      "It is on the same level as\n",
      "statistical tests (chi-square or t-tests) or uses base probability/statistics to estimate\n",
      "population parameters 29\n",
      "Machine learning is often regarded as the only thing data scientists\n",
      "know how to do, and this is simply untrue 22\n",
      "A true data scientist is able to recognize when\n",
      "machine learning is applicable and, more importantly, when it is not 23\n",
      "\n",
      "Machine learning is a game of correlations and relationships 10\n",
      "Most machine learning\n",
      "algorithms in existence are concerned with finding and/or exploiting relationships between\n",
      "datasets (often represented as columns in a pandas DataFrame) 29\n",
      "Once machine learning\n",
      "algorithms can pinpoint certain correlations, the model can either use these relationships to\n",
      "predict future observations or generalize the data to reveal interesting patterns 31\n",
      "\n",
      "Perhaps a great way to explain machine learning is to offer an example of a problem\n",
      "coupled with two possible solutions: one using a machine learning algorithm and the other\n",
      "utilizing a non-machine learning algorithm 42\n",
      "\n",
      "Example – facial recognition\n",
      "This problem is very well documented 12\n",
      "Given a picture of a face, who does it belong to 12\n",
      "\n",
      "However, I argue that there  is a more important question that must be asked even before\n",
      "this 21\n",
      "Suppose you wish to implement a home security system that recognizes who is\n",
      "entering your house 18\n",
      "Most likely, during the day, your house will be empty most of the\n",
      "time and the facial recognition will kick in only if there is a person in the shot 33\n",
      "This is\n",
      "exactly the question I propose we try and solve —given a photo, is there a face in it 24\n",
      "\n",
      "\n",
      "Given this problem, I propose the following two solutions:\n",
      "A non-machine learning algorithm that will define a face as having a roundish\n",
      "structure, two eyes, hair, nose, and so on 40\n",
      "The algorithm then looks for these\n",
      "hard-coded features in the photo and returns whether or not it was able to find\n",
      "any of these features 28\n",
      "\n",
      "A machine learning algorithm that will work a bit differently 11\n",
      "The model will\n",
      "only be given several pictures of faces and non-faces that are labeled as such 20\n",
      "\n",
      "From the examples (called training sets), it would figure out its own definition of\n",
      "a face 20\n",
      "\n",
      "The machine learning version of the solution is never told what a face is, it is merely given\n",
      "several examples, some with faces, and some without 32\n",
      "It is then up to the machine learning\n",
      "model to figure out the difference between the two 18\n",
      "Once it figures out the difference\n",
      "between the two, it uses this information to take in a picture and predict whether or not\n",
      "there is a face in the new picture 34\n",
      "For example, to train the model, we will give it the\n",
      "following three photographs:\n",
      "Images for training machine learning model\n",
      "\n",
      "The model will then figure out the difference between the pictures labeled as Face  and the\n",
      "pictures labeled as No Face  and be able to use that difference to find faces in future photos 62\n",
      "\n",
      "Machine learning isn't perfect\n",
      "There are many caveats of machine  learning 16\n",
      "Many are specific to different models being\n",
      "implemented, but there are some assumptions that are universal for any machine learning\n",
      "model:\n",
      "The data used, for the most part, is preprocessed and cleaned using the methods\n",
      "outlined in the earlier chapters 48\n",
      "Almost no machine learning model will tolerate\n",
      "dirty data with missing values or categorical values 16\n",
      "Use dummy variables and\n",
      "filling/dropping techniques to handle these discrepancies 14\n",
      "\n",
      "Each row of a cleaned dataset represents a single observation of the environment\n",
      "we are trying to model 20\n",
      "\n",
      "If our goal is to find relationships between variables, then there is an assumption\n",
      "that there is some kind of relationship between these variables 27\n",
      "\n",
      "This assumption is particularly important 6\n",
      "Many machine learning models take\n",
      "this assumption very seriously 10\n",
      "These models are not able to communicate that\n",
      "there might not be a relationship 15\n",
      "\n",
      "Machine learning models are generally considered semi-automatic, which means\n",
      "that intelligent decisions by humans are still needed 21\n",
      "\n",
      "The machine is very smart but has a hard time putting things into context 15\n",
      "The\n",
      "output of most models is a series of numbers and metrics attempting to quantify\n",
      "how well the model did 22\n",
      "It is up to a human to put these metrics into perspective\n",
      "and communicate the results to an audience 20\n",
      "\n",
      "Most machine learning models are sensitive to noisy data 10\n",
      "This means that the\n",
      "models get confused when you include data that doesn't make sense 17\n",
      "For\n",
      "example, if you are attempting to find relationships between economic data\n",
      "around the world and one of your columns is puppy adoption rates in the capital\n",
      "city, that information is likely not to be relevant and will confuse the model 46\n",
      "\n",
      "These assumptions will come up again and again when dealing with machine learning 14\n",
      "\n",
      "They are all too important and are often ignored by novice data scientists 14\n",
      "\n",
      "\n",
      "How does machine learning work 6\n",
      "\n",
      "Each flavor of machine  learning and each individual model works in very different ways,\n",
      "exploiting different parts of mathematics and data science 26\n",
      "However, in general, machine\n",
      "learning works by taking in data, finding relationships within the data, and giving as\n",
      "output what the model learned, as illustrated in the following diagram:\n",
      "An overview of machine learning models\n",
      "As we explore the different types of machine learning models, we will see how they\n",
      "manipulate data differently and come up with different outputs for different applications 75\n",
      "\n",
      "Types of machine learning\n",
      "There are many ways to segment machine  learning and dive deeper 18\n",
      "In Chapter 1 , How to\n",
      "Sound Like a Data Scientist , I mentioned statistical and probabilistic models 21\n",
      "These models\n",
      "utilize statistics and probability, which we've seen in the previous chapters, in order to find\n",
      "relationships between data and make predictions 29\n",
      "In this chapter, we will implement both\n",
      "types of models 12\n",
      "In the following chapter, we will see machine learning outside the rigid\n",
      "mathematical world of statistics/probability 23\n",
      "You can segment machine learning models by\n",
      "different characteristics, including the following:\n",
      "The types of data/organic structures they utilize (tree/graph/neural network)\n",
      "The field of mathematics they are most related to (statistical/probabilistic)\n",
      "The level of computation required to train (deep learning)\n",
      "\n",
      "For the purpose of education, I will offer my own breakdown of machine learning models 75\n",
      "\n",
      "Branching off from  the top level of machine learning, there are the following three subsets:\n",
      "Supervised learning\n",
      "Unsupervised learning\n",
      "Reinforcement learning\n",
      "Supervised learning\n",
      "Simply put, supervised learning finds  associations between features of a dataset and a\n",
      "target variable 56\n",
      "For example, supervised learning models might try to find the association\n",
      "between a person's health features (heart rate, obesity level, and so on) and that person's\n",
      "risk of having a heart attack (the target variable) 46\n",
      "\n",
      "These associations allow supervised models to make predictions based on past examples 13\n",
      "\n",
      "This is often the first thing that comes to people's minds when they hear the phrase,\n",
      "machine learning, but it in no way does it encompass the realm of machine learning 35\n",
      "\n",
      "Supervised machine learning models are often called predictive analytics models , named\n",
      "for their ability to predict the future based on the past 26\n",
      "\n",
      "Supervised machine learning requires  a certain type of data called labeled data 15\n",
      "This means\n",
      "that we must teach our model by giving it historical examples that are labeled with the\n",
      "correct answer 22\n",
      "Recall the facial recognition example 5\n",
      "That is a supervised learning model\n",
      "because we are training our model with the previous pictures labeled as either face or not\n",
      "face, and then asking the model to predict whether or not a new picture has a face in it 44\n",
      "\n",
      "Specifically, supervised learning works using parts of the data to predict another part 16\n",
      "First,\n",
      "we must separate data into two parts, as follows:\n",
      "The predictors, which are the columns that will be used to make our prediction 28\n",
      "\n",
      "These are sometimes called features, input values, variables, and independent\n",
      "variables 16\n",
      "\n",
      "The response, which is the column that we wish to predict 13\n",
      "This is sometimes\n",
      "called outcome, label, target, and dependent variable 14\n",
      "\n",
      "Supervised learning attempts to find a relationship between the predictors and the\n",
      "response in order to make a prediction 22\n",
      "The idea is that, in the future, a data observation\n",
      "will present itself and we will only know the predictors 23\n",
      "The model will then have to use\n",
      "the predictors to make an accurate prediction of the response value 19\n",
      "\n",
      "\n",
      "Example – heart attack prediction\n",
      "Suppose we wish to predict whether  someone will have a heart attack within a year 24\n",
      "To\n",
      "predict this, we are given that person's cholesterol level, blood pressure, height, their\n",
      "smoking habits, and perhaps more 28\n",
      "From this data, we must ascertain the likelihood of a\n",
      "heart attack 14\n",
      "Suppose, to make this prediction, we look at previous patients and their\n",
      "medical history 17\n",
      "As these are previous patients, we know not only their predictors\n",
      "(cholesterol, blood pressure, and so on), but we also know if they actually had a heart\n",
      "attack (because it already happened 42\n",
      " 1\n",
      "\n",
      "This is a supervised machine learning problem because we are doing the following:\n",
      "We are making a prediction about someone\n",
      "We are using historical training data to find relationships between medical\n",
      "variables and heart attacks:\n",
      "An overview of supervised models\n",
      "The hope here is that a patient will walk in tomorrow and our model will be able to identify\n",
      "whether or not the patient is at risk for a heart attack based on her/his conditions (just like a\n",
      "doctor would 91\n",
      " 1\n",
      "\n",
      "\n",
      "As the model sees more and more labeled data, it adjusts itself in order to match the correct\n",
      "labels given to us 25\n",
      "We can use different metrics (explained later in this chapter) to pinpoint\n",
      "exactly how well our supervised machine learning model is doing and how it can better\n",
      "adjust itself 34\n",
      "\n",
      "One of the biggest drawbacks of supervised  machine learning is that we need this labeled\n",
      "data, which can be very difficult to get a hold of 30\n",
      "Suppose we wish to predict heart attacks;\n",
      "we might need thousands of patients along with all of their medical information and years'\n",
      "worth of follow-up records for each person, which could be a nightmare to obtain 40\n",
      "\n",
      "In short, supervised models use historical labeled data in order to make predictions about\n",
      "the future 19\n",
      "Some possible applications for supervised learning include the following:\n",
      "Stock price predictions\n",
      "Weather predictions\n",
      "Crime predictions\n",
      "Note how each of the preceding  examples uses the word prediction , which makes sense\n",
      "seeing how I emphasized supervised learning's ability to make predictions about the\n",
      "future 52\n",
      "Predictions, however, are not where the story ends 11\n",
      "\n",
      "Here is a visualization of how supervised models use labeled data to fit themselves and\n",
      "prepare themselves to make predictions:\n",
      "Note how the supervised model learns from a bunch of training data and then, when it is\n",
      "ready, it looks at unseen cases and outputs a prediction 53\n",
      "\n",
      "\n",
      "It's not only about predictions\n",
      "Supervised learning exploits  the relationship between the predictors and the response to\n",
      "make predictions, but sometimes, it is enough just knowing that there even is a\n",
      "relationship 41\n",
      "Suppose we are using a supervised machine learning model to predict\n",
      "whether or not a customer will purchase a given item 22\n",
      "A possible dataset might look as\n",
      "follows:\n",
      "Person ID Age Gender Employed 16\n",
      "Bought the product 3\n",
      "\n",
      "1 63 F N Y\n",
      "2 24 M Y N\n",
      "Note that, in this case, our predictors are Age, Gender, and Employed 32\n",
      ", while our response\n",
      "is Bought the product 9\n",
      "This is because we want to see if, given someone's age, gender, and\n",
      "employment status, they will buy the product 26\n",
      "\n",
      "Assume that a model is trained on this data and can make accurate predictions about\n",
      "whether or not someone will buy something 25\n",
      "That, in and of itself, is exciting, but there's\n",
      "something else that is arguably even more exciting 22\n",
      "The fact that we could make accurate\n",
      "predictions implies that there is a relationship between these variables, which means that to\n",
      "know if someone will buy your product, you only need to know their age, gender, and\n",
      "employment status 46\n",
      "This might contradict the previous market research, indicating that\n",
      "much more must be known about a potential customer to make such a prediction 25\n",
      "\n",
      "This speaks to supervised machine learning's ability to understand which predictors affect\n",
      "the response and how 19\n",
      "For example, are women more likely to buy the product, w hich age\n",
      "groups are prone to decline the product, is there a combination of age and gender that is a\n",
      "better predictor than any one column on its own 46\n",
      "As someone's age increases, do their\n",
      "chances of buying the product go up, down, or stay the same 24\n",
      "\n",
      "It is also possible that all of the columns are not necessary 13\n",
      "A possible output of a machine\n",
      "learning might suggest that only certain columns are necessary to make the prediction and\n",
      "that the other columns are only noise (they do not correlate to the response and therefore\n",
      "confuse the model) 45\n",
      "\n",
      "\n",
      "Types of supervised learning\n",
      "There are, in general, two types  of supervised learning models: regression  and\n",
      "classification 25\n",
      "The difference between the two is quite  simple and lies in the response\n",
      "variable 16\n",
      "\n",
      "Regression\n",
      "Regression models attempt to predict  a continuous response 12\n",
      "This means that the response\n",
      "can take on a range of infinite values 14\n",
      "Consider the following examples:\n",
      "Dollar amounts\n",
      "Salary\n",
      "Budget\n",
      "Temperature\n",
      "Time\n",
      "Generally recorded in seconds or minutes\n",
      "Classiﬁcation\n",
      "Classification  attempts to predict  a categorical response, which means that the response\n",
      "only has a finite amount of choices 53\n",
      "Here are some  examples:\n",
      "Cancer grade (1, 2, 3, 4, 5)\n",
      "True/false questions, such as the following examples:\n",
      "\"Will this person have a heart attack within a year 45\n",
      "\n",
      "\"Will you get this job 7\n",
      "\n",
      "Given a photo of a face, who does this face belong to 14\n",
      "(facial recognition)\n",
      "Predict the year someone was born:\n",
      "Note that there are many possible answers (over 100) but still\n",
      "finitely many more\n",
      "Example – regression\n",
      "The following graphs show a relationship between three categorical variables (age, year\n",
      "they were born, and education level) and a person's wage:\n",
      "\n",
      "\n",
      "Regression examples (source: https://lagunita 75\n",
      "stanford 2\n",
      "edu/c4x/HumanitiesScience/StatLearning/asset/introduction 15\n",
      "pdf)\n",
      "Note that, even though each predictor is categorical, this example is regressive because the\n",
      "y axis, our dependent variable, our response, is continuous 32\n",
      "\n",
      "Our earlier heart attack example is classification because the response was: will this person\n",
      "have a heart attack within a year 24\n",
      "This has only two possible answers: Yes or No 10\n",
      "\n",
      "Data is in the eyes of the beholder\n",
      "Sometimes, it can be tricky to decide  whether or not you should use classification or\n",
      "regression 31\n",
      "Consider that we are interested in the weather outside 9\n",
      "We could ask the\n",
      "question, how hot is it outside 12\n",
      "In this case, your answer is on a continuous scale, and some\n",
      "possible answers are 60 20\n",
      "7 degrees or 98 degrees 7\n",
      "However, as an exercise, go and ask 10\n",
      "people what the temperature is outside 18\n",
      "I guarantee you that someone (if not most people)\n",
      "will not answer in some exact degrees but will bucket their answer and say something like\n",
      "it's in the 60s 35\n",
      "\n",
      "We might wish to consider this problem as a classification problem, where the response\n",
      "variable is no longer in exact degrees but is in a bucket 29\n",
      "There would only be a finite\n",
      "number of buckets in theory, making the model perhaps learn the differences between 60s\n",
      "and 70s a bit better 32\n",
      "\n",
      "\n",
      "Unsupervised learning\n",
      "The second type of machine learning  does not deal  with predictions but has a much more\n",
      "open objective 27\n",
      "Unsupervised learning takes in a set of predictors and utilizes relationships\n",
      "between the predictors in order to accomplish tasks such as the following:\n",
      "It reduces the dimension of the data by condensing variables together 39\n",
      "An\n",
      "example of this would be file compression 9\n",
      "Compression works by utilizing\n",
      "patterns in the data and representing the data in a smaller format 17\n",
      "\n",
      "It finds groups of observations that behave similarly and groups them together 13\n",
      "\n",
      "The first element on this list is called dimension reduction  and the second is called\n",
      "clustering 20\n",
      "Both of these are examples of unsupervised learning because they do not\n",
      "attempt to find a relationship between predictors and a specific response and therefore are\n",
      "not used to make predictions of any kind 38\n",
      "Unsupervised models, instead, are utilized to\n",
      "find organizations and representations of the data that were previously unknown 22\n",
      "\n",
      "The following screenshot is a representation of a cluster analysis:\n",
      "Example of cluster analysis\n",
      "\n",
      "The model will recognize that each uniquely colored cluster of observations is similar to\n",
      "another but different from the other clusters 39\n",
      "\n",
      "A big advantage for unsupervised learning is that it does not require labeled data, which\n",
      "means that it is much easier to get data that complies with unsupervised learning models 38\n",
      "\n",
      "Of course, a drawback to this is that we lose all predictive power because the response\n",
      "variable holds the information to make predictions and, without it, our model will be\n",
      "hopeless in making any sort of predictions 44\n",
      "\n",
      "A big drawback is that it is difficult  to see how well  we are doing 18\n",
      "In a regression or\n",
      "classification problem, we can easily tell how well our models are predicting by comparing\n",
      "our models' answers to the actual answers 29\n",
      "For example, if our supervised model predicts\n",
      "rain and it is sunny outside, the model was incorrect 20\n",
      "If our supervised model predicts the\n",
      "price will go up by 1 dollar and it goes up by 99 cents, our model was very close 29\n",
      "In\n",
      "supervised modeling, this concept is foreign because we have no answer to compare our\n",
      "models to 21\n",
      "Unsupervised models are merely suggesting differences and similarities, which\n",
      "then require a human's interpretation:\n",
      "An overview of unsupervised models\n",
      "In short, the main goal of unsupervised models is to find similarities and differences\n",
      "between data observations 49\n",
      "We will discuss unsupervised models in depth in later chapters 12\n",
      "\n",
      "\n",
      "Reinforcement learning\n",
      "In reinforcement learning, algorithms get to choose  an action in an environment and then\n",
      "are rewarded (positively or negatively) for choosing this action 35\n",
      "The algorithm then  adjusts\n",
      "itself and modifies its strategy in order to accomplish some goal, which is usually to get\n",
      "more rewards 27\n",
      "\n",
      "This type of machine  learning is very popular in AI-assisted gameplay as agents  (the AI) are\n",
      "allowed to explore a virtual world and collect rewards and learn the best navigation\n",
      "techniques 41\n",
      "This model is also popular in robotics, especially in the field of self-automated\n",
      "machinery, including cars:\n",
      "Self-driving cars ( image source: https://www 34\n",
      "quora 2\n",
      "com/How-do-Googles-self-driving-cars-work)\n",
      "\n",
      "Self-driving cars read in sensor input, act accordingly, and are then rewarded for taking a\n",
      "certain action 36\n",
      "The car then adjusts its behavior to collect more rewards 10\n",
      "It can be thought\n",
      "that reinforcement is similar to supervised learning in that the agent is learning  from its \n",
      "past actions to make better moves in the future; however, the main difference lies in the\n",
      "reward 42\n",
      "The reward does not have to be tied in any way to a correct  or incorrect  decision 19\n",
      "\n",
      "The reward simply encourages (or discourages) different actions 12\n",
      "\n",
      "Reinforcement learning is the least explored of the three types of machine learning and\n",
      "therefore is not explored in great length in this text 29\n",
      "The remainder of this chapter will focus\n",
      "on supervised and unsupervised learning 15\n",
      "\n",
      "Overview of the types of machine learning\n",
      "Of the three types of machine  learning —supervised, unsupervised, and reinforcement\n",
      "learning —we can imagine the world of machine learning as something like this:\n",
      "Each of the three types of machine learning has its benefits and its drawbacks, as listed:\n",
      "Supervised machine learning : This exploits  relationships between predictors \n",
      "and response variables to make predictions of future data observations 83\n",
      "The pros\n",
      "are as follows:\n",
      "It can make future predictions\n",
      "It can quantify relationships between predictors and response\n",
      "variables\n",
      "It can show us how variables affect each other and how much\n",
      "\n",
      "The cons are as follows:\n",
      "It requires labeled data (which can be difficult to get)\n",
      "Unsupervised machine learning : This finds  similarities and differences between\n",
      "data points 71\n",
      "The pros are as follows:\n",
      "It can find groups of data  points that behave similarly that a\n",
      "human would never have noted 25\n",
      "\n",
      "It can be a preprocessing step for supervised learning 10\n",
      "\n",
      "Think of clustering a bunch of data points and then using these\n",
      "clusters as the response 18\n",
      "\n",
      "It can use unlabeled data, which is much easier to find 14\n",
      "\n",
      "The cons are as follows:\n",
      "It has zero predictive power\n",
      "It can be hard to determine if we are on the right track\n",
      "It relies much more on human interpretation\n",
      "Reinforcement learning : This is reward-based learning that encourages agents to\n",
      "take particular actions  in their environments 57\n",
      "The pros are as follows:\n",
      "Very complicated rewards systems create very complicated AI\n",
      "systems\n",
      "It can learn in almost any environment, including our own Earth\n",
      "The cons are as follows:\n",
      "The agent is erratic at first and makes many terrible choices before\n",
      "realizing that these choices have negative rewards\n",
      "For example, a car might crash into a wall and not know that that is\n",
      "not okay until the environment negatively rewards it\n",
      "It can take a while before the agent avoids decisions altogether\n",
      "The agent might play it safe and only choose one action and be\n",
      "\"too afraid\" to try anything else for fear of being punished\n",
      "\n",
      "How does statistical modeling fit into all of\n",
      "this 134\n",
      "\n",
      "Up until now, I have been using the term  machine learning, but you may ask how statistical\n",
      "modeling plays a role in all of this 31\n",
      "\n",
      "This is still a debated topic in the field of data science 13\n",
      "I believe that statistical modeling is\n",
      "another term for machine learning models that heavily relies on using mathematical rules\n",
      "borrowed from probability and statistics to create relationships between data variables\n",
      "(often in a predictive sense) 41\n",
      "\n",
      "The remainder of this chapter will focus mostly on one statistical/probabilistic\n",
      "model —linear regression 21\n",
      "\n",
      "Linear regression\n",
      "Finally 5\n",
      "We will explore our first true machine learning model 9\n",
      "Linear regression is a form\n",
      "of regression, which means that it is a machine learning model that attempts to find a\n",
      "relationship between predictors and a response variable and that response variable is, you\n",
      "guessed it, continuous 44\n",
      "This notion is synonymous with making a line of best fit 11\n",
      "\n",
      "In the case of linear regression, we will attempt to find a linear relationship between our\n",
      "predictors and our response variable 25\n",
      "Formally, we wish to solve a formula of the following\n",
      "format:\n",
      "Let's look at the constituents of this formula:\n",
      "y is our response variable\n",
      "xi is our ith variable ( ith column or ith predictor)\n",
      "B0 is the intercept\n",
      "Bi is the coefficient for the xi term\n",
      "\n",
      "Let's take a look at some data before we go in depth 71\n",
      "This dataset is publically available and\n",
      "attempts to predict  the number of bikes needed on a particular day for a bike sharing\n",
      "program:\n",
      "# read the data and set the datetime as the index\n",
      "# taken from Kaggle: https://www 50\n",
      "kaggle 3\n",
      "com/c/bike-sharing-demand/data\n",
      "import pandas as pd\n",
      "import matplotlib 15\n",
      "pyplot as plt\n",
      "%matplotlib inline\n",
      "url =\n",
      "'https://raw 13\n",
      "githubusercontent 2\n",
      "com/justmarkham/DAT8/master/data/bikeshare 14\n",
      "c\n",
      "sv'\n",
      "bikes = pd 8\n",
      "read_csv(url)\n",
      "bikes 6\n",
      "head()\n",
      "Following is the output:\n",
      "We can see that every row represents a single hour of bike usage 20\n",
      "In this case, we are\n",
      "interested in predicting count , which represents the total number of bikes rented in the\n",
      "period of that hour 27\n",
      "\n",
      "Let's, for example, look at a scatter plot between temperature (the temp  column) and\n",
      "count , as shown:\n",
      "bikes 29\n",
      "plot(kind='scatter', x='temp', y='count', alpha=0 16\n",
      "2)\n",
      "\n",
      "We get the following graph as output:\n",
      "And now, let's use a module, called seaborn , to draw ourselves a line of best fit, as follows:\n",
      "import seaborn as sns #using seaborn to get a line of best fit\n",
      "sns 51\n",
      "lmplot(x='temp', y='count', data=bikes, aspect=1 17\n",
      "5,\n",
      "scatter_kws={'alpha':0 10\n",
      "2})\n",
      "Following is the output:\n",
      "\n",
      "\n",
      "This line in the graph attempts to visualize and quantify the relationship between temp  and\n",
      "count 26\n",
      "To make a prediction, we simply find a given temperature and then see where the\n",
      "line would predict the count 22\n",
      "For example, if the temperature is 20 degrees (Celsius, mind\n",
      "you), then our line would predict that about 200 bikes will be rented 31\n",
      "If the temperature is\n",
      "above 40 degrees, then more than 400 bikes will be needed 19\n",
      "\n",
      "It appears that as temp  goes up, our count  also goes up 16\n",
      "Let's see if our correlation value,\n",
      "which quantifies a linear relationship between variables, also matches this notion:\n",
      "bikes[['count', 'temp']] 30\n",
      "corr()\n",
      "# 0 5\n",
      "3944\n",
      "There is a (weak) positive correlation between the two variables 16\n",
      "Now, let's go back to the\n",
      "form of the linear regression:\n",
      "Our model will attempt to draw a perfect  line between all of the dots in the preceding\n",
      "graph but, of course, we can clearly see that there is no perfect line between these dots 53\n",
      "The\n",
      "model will then find the best fit  line possible 12\n",
      "How 1\n",
      "We can draw infinite lines between the\n",
      "data points, but what makes a line the best 18\n",
      "\n",
      "Consider the following diagram:\n",
      "In our model, we are given the x and the y and the model learns  the beta coefficients, also\n",
      "known as model coefficients :\n",
      "The black dots are the observed values of x and y 45\n",
      "\n",
      "The blue line is our line of best fit 10\n",
      "\n",
      "The red lines between the dots and the line are called the residuals; they are the\n",
      "distances between the observed values and the line 28\n",
      "They are how wrong the line\n",
      "is 8\n",
      "\n",
      "\n",
      "Each data point has a residual or a distance to the line of best fit 16\n",
      "The sum of squared\n",
      "residuals  is the summation of each residual squared 17\n",
      "The best fit line has the smallest sum\n",
      "of squared residual value 13\n",
      "Let's build this line in Python:\n",
      "# create X and y\n",
      "feature_cols = ['temp'] # a list of the predictors\n",
      "X = bikes[feature_cols] # subsetting our data to only the predictors\n",
      "y = bikes['count'] # our response variable\n",
      "Note how we made an X and a y variable 65\n",
      "These represent our predictors and our response\n",
      "variable 9\n",
      "Then, we will import our machine learning module, scikit-learn , as shown:\n",
      "# import scikit-learn, our machine learning module\n",
      "from sklearn 32\n",
      "linear_model import LinearRegression\n",
      "Finally, we will fit our model to the predictors and the response variable, as follows:\n",
      "linreg = LinearRegression() #instantiate a new model\n",
      "linreg 39\n",
      "fit(X, y) #fit the model to our data\n",
      "# print the coefficients\n",
      "print(linreg 22\n",
      "intercept_)\n",
      "print(linreg 6\n",
      "coef_)\n",
      "6 3\n",
      "04621295962  # our Beta_0\n",
      "[ 9 15\n",
      "17054048]     # our beta parameters\n",
      "Let's interpret this:\n",
      "B0 (6 20\n",
      "04)  is the value of y when X = 0\n",
      "It is the estimation of bikes that will be rented when the temperature is 0 degrees\n",
      "Celsius\n",
      "So, at 0 degrees, six bikes are predicted to be in use (it's cold 55\n",
      "\n",
      "Sometimes, it might not make sense  to interpret the intercept at all because there might not\n",
      "be a concept of zero of something 27\n",
      "Recall the levels of data 5\n",
      "Not all levels have this notion 6\n",
      "\n",
      "Temperature exists at a level that has the inherent notion of no bikes ; so, we are safe 20\n",
      "Be\n",
      "careful in the future though and verify results: \n",
      "B1 (9 17\n",
      "17)  is our temperature coefficient 8\n",
      "\n",
      "It is the change in y divided by the change in x1 14\n",
      "\n",
      "It represents how x and y move together 9\n",
      "\n",
      "\n",
      "A change in 1 degree Celsius is associated with an increase of about nine bikes\n",
      "rented 20\n",
      "\n",
      "The sign of this coefficient is important 8\n",
      "If it were negative, that would imply that\n",
      "a rise in temperature is associated with a drop in rentals 21\n",
      "\n",
      "Consider the following representation of the beta coefficients in a linear regression:\n",
      "It is important to reiterate that these  are all statements of correlation and not a statement of\n",
      "causation 37\n",
      "We have no means of stating whether or not the rental increase is caused by the\n",
      "change in temperature, it is just that there appears to be movement together 31\n",
      "\n",
      "Using scikit-learn  to make predictions is easy:\n",
      "linreg 15\n",
      "predict(20)\n",
      "# 189 7\n",
      "4570\n",
      "This means that 190 bikes will likely be rented if the temperature is 20 degrees 21\n",
      "\n",
      "Adding more predictors\n",
      "Adding more predictors to the model  is as simple as telling the linear regression model in\n",
      "scikit-learn  about them 30\n",
      "\n",
      "Before we do, we should look at the data dictionary provided to us to make more sense out\n",
      "of these predictors:\n",
      "season : 1 = spring, 2 = summer, 3 = fall, and 4 = winter\n",
      "holiday : Whether the day is considered a holiday\n",
      "workingday : Whether the day is a weekend or holiday\n",
      "\n",
      "weather :\n",
      "Clear , Few clouds , Partly cloudy\n",
      "Mist + Cloudy , Mist + Broken clouds , Mist + Few\n",
      "clouds , Mist\n",
      "Light Snow , Light Rain + Thunderstorm + Scattered\n",
      "clouds , Light Rain + Scattered clouds\n",
      "Heavy Rain  + Ice Pallets  + Thunderstorm  + Mist , Snow +\n",
      "Fog\n",
      "temp : Temperature in Celsius\n",
      "atemp : Feels like temperature in Celsius\n",
      "humidity : Relative humidity\n",
      "windspeed : Wind speed\n",
      "casual : Number of non-registered user rentals initiated\n",
      "registered : Number of registered user rentals initiated\n",
      "count : Number of total rentals\n",
      "Now let's actually create our linear  regression model 210\n",
      "As before, we will first create a list\n",
      "holding the features we wish to look at, create our features and our response datasets ( X\n",
      "and y), and then fit our linear regression 38\n",
      "Once we fit our regression model, we will take a\n",
      "look at the model's coefficients in order to see how our features are interacting with our\n",
      "response:\n",
      "# create a list of features\n",
      "feature_cols = ['temp', 'season', 'weather', 'humidity']\n",
      "# create X and y\n",
      "X = bikes[feature_cols]\n",
      "y = bikes['count']\n",
      "# instantiate and fit\n",
      "linreg = LinearRegression()\n",
      "linreg 86\n",
      "fit(X, y)\n",
      "# pair the feature names with the coefficients\n",
      "result = zip(feature_cols, linreg 22\n",
      "coef_)\n",
      "resultSet = set(result)\n",
      "print(resultSet)\n",
      "\n",
      "This gives us the following output:\n",
      "[('temp', 7 23\n",
      "8648249924774403),\n",
      " ('season', 22 13\n",
      "538757532466754),\n",
      " ('weather', 6 12\n",
      "6703020359238048),\n",
      " ('humidity', -3 13\n",
      "1188733823964974)]\n",
      "And this is what that means:\n",
      "Holding all other predictors constant, a 1 unit increase in temperature is\n",
      "associated with a rental increase of 7 39\n",
      "86 bikes\n",
      "Holding all other predictors constant, a 1 unit increase in season is associated\n",
      "with a rental increase of 22 28\n",
      "5 bikes\n",
      "Holding all other predictors constant, a 1 unit increase in weather is associated\n",
      "with a rental increase of 6 28\n",
      "67 bikes\n",
      "Holding all other predictors constant, a 1 unit increase in humidity is associated\n",
      "with a rental decrease of 3 28\n",
      "12 bikes\n",
      "This is interesting 7\n",
      "Note that, as weather  goes up (meaning that the weather is getting\n",
      "closer to overcast), the bike demand goes up, as is the case when the season variables\n",
      "increase (meaning that we are approaching winter) 46\n",
      "This is not what I was expecting at all 9\n",
      "\n",
      "Let's take a look at the individual scatter  plots between each predictor and the response, as\n",
      "illustrated:\n",
      "feature_cols = ['temp', 'season', 'weather', 'humidity']\n",
      "# multiple scatter plots\n",
      "sns 45\n",
      "pairplot(bikes, x_vars=feature_cols, y_vars='count', kind='reg')\n",
      "We get the following output:\n",
      "\n",
      "\n",
      "Note how the weather line is trending downwards, which is the opposite of what the last\n",
      "linear model was suggesting 48\n",
      "Now, we have to worry about which of these predictors are\n",
      "actually helping us make the prediction, and which ones are just noise 26\n",
      "To do so, we're\n",
      "going to need some more advanced metrics 14\n",
      "\n",
      "Regression metrics\n",
      "There are three main metrics  when using regression  machine learning models 17\n",
      "They are as\n",
      "follows:\n",
      "The mean absolute error\n",
      "The mean squared error\n",
      "The root mean squared error\n",
      "Each metric attempts to describe and quantify the effectiveness of a regression model by\n",
      "comparing a list of predictions to a list of correct answers 50\n",
      "Each of the following mentioned\n",
      "metrics is slightly different from the rest and tells a different story:\n",
      "Let's look at the coefficients:\n",
      "n is the number of observations\n",
      "yi is the actual value\n",
      "ŷ is the predicted value\n",
      "\n",
      "Let's take a look in Python:\n",
      "# example true and predicted response values\n",
      "true = [9, 6, 7, 6]\n",
      "pred = [8, 7, 7, 12]\n",
      "# note that each value in the last represents a single prediction for a\n",
      "model\n",
      "# So we are comparing four predictions to four actual answers\n",
      "# calculate these metrics by hand 125\n",
      "\n",
      "from sklearn import metrics\n",
      "import numpy as np\n",
      "print('MAE:', metrics 17\n",
      "mean_absolute_error(true, pred))\n",
      "print('MSE:', metrics 13\n",
      "mean_squared_error(true, pred))\n",
      "print('RMSE:', np 13\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(true, pred)))\n",
      "Following is the output:\n",
      "MAE: 2 17\n",
      "0\n",
      "MSE: 9 8\n",
      "5\n",
      "RMSE: 3 8\n",
      "08220700148\n",
      "The breakdown  of these  numbers is as follows:\n",
      "MAE is probably the easiest to understand, because it's just the average error 33\n",
      "It\n",
      "denotes, on an average, how wrong the model is 14\n",
      "\n",
      "MSE is more effective than MAE, because MSE punishes larger errors, which tends\n",
      "to be much more useful in the real world 29\n",
      "\n",
      "RMSE  is even more popular than MSE, because it is much more interpretable 18\n",
      "\n",
      "RMSE  is usually the preferred metric for regression, but no matter which one you choose,\n",
      "they are all loss functions and therefore are something to be minimized 32\n",
      "Let's use RMSE  to\n",
      "ascertain which columns are helping and which are hurting 18\n",
      "\n",
      "Let's start with only using temperature 8\n",
      "Note that our procedure will be as follows:\n",
      "Create our X and our y variables 1 18\n",
      "\n",
      "Fit a linear regression model2 7\n",
      "\n",
      "Use the model to make a list of predictions based on X 3 15\n",
      "\n",
      "Calculate RMSE  between the predictions and the actual values 4 14\n",
      "\n",
      "\n",
      "Let's take a look at the code:\n",
      "from sklearn import metrics\n",
      "# import metrics from scikit-learn\n",
      "feature_cols = ['temp']\n",
      "# create X and y\n",
      "X = bikes[feature_cols]\n",
      "linreg = LinearRegression()\n",
      "linreg 51\n",
      "fit(X, y)\n",
      "y_pred = linreg 10\n",
      "predict(X)\n",
      "np 4\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y, y_pred)) # RMSE\n",
      "# Can be interpreted loosely as an average error\n",
      "#166 24\n",
      "45\n",
      "Now, let's try it using temperature and humidity, as shown:\n",
      "feature_cols = ['temp', 'humidity']\n",
      "# create X and y\n",
      "X = bikes[feature_cols]\n",
      "linreg = LinearRegression()\n",
      "linreg 47\n",
      "fit(X, y)\n",
      "y_pred = linreg 10\n",
      "predict(X)\n",
      "np 4\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y, y_pred)) # RMSE\n",
      "# 157 15\n",
      "79\n",
      "It got better 6\n",
      "Let's try using even more predictors, as illustrated:\n",
      "feature_cols = ['temp', 'humidity', 'season', 'holiday', 'workingday',\n",
      "'windspeed', 'atemp']\n",
      "# create X and y\n",
      "X = bikes[feature_cols]\n",
      "linreg = LinearRegression()\n",
      "linreg 60\n",
      "fit(X, y)\n",
      "y_pred = linreg 10\n",
      "predict(X)\n",
      "np 4\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y, y_pred)) # RMSE\n",
      "# 155 15\n",
      "75\n",
      "Even better 5\n",
      "At first, this seems like a major triumph, but there is actually a hidden danger\n",
      "here 19\n",
      "Note that we are training the line to fit to X and y and then asking it to predict X again 21\n",
      "\n",
      "This is actually a huge mistake in machine learning because it can lead to overfitting , which\n",
      "means that our model is merely memorizing  the data and regurgitating it back to us 40\n",
      "\n",
      "\n",
      "Imagine that you are a student, and you walk into the first day of class and the teacher says\n",
      "that the final exam is very difficult in this class 32\n",
      "In order to prepare you, she gives you\n",
      "practice test after practice test after practice test 18\n",
      "The day of the final exam arrives and you \n",
      "are shocked to find out that every question on the exam is exactly  the same as in the\n",
      "practice test 32\n",
      "Luckily, you did them so many times that you remember the answer and get a\n",
      "100% in the exam 22\n",
      "\n",
      "The same thing applies here, more or less 10\n",
      "By fitting and predicting on the same data, the\n",
      "model is memorizing the data and getting better at it 22\n",
      "A great way to combat this\n",
      "overfitting problem is to use the train/test approach to fit machine learning models, which\n",
      "works as illustrated:\n",
      "Essentially, we will take the following steps:\n",
      "Split up the dataset into two parts: a training and a test set 1 56\n",
      "\n",
      "Fit our model on the training set and then test it on the test set, just like in school,2 23\n",
      "\n",
      "where the teacher would teach from one set of notes and then test us on different\n",
      "(but similar) questions\n",
      "Once our model is good enough (based on our metrics), we turn our model's3 42\n",
      "\n",
      "attention toward the entire dataset\n",
      "Our model awaits new data previously unseen by anyone4 17\n",
      "\n",
      "The goal here is to minimize the out-of-sample errors of our model, which are the errors\n",
      "our model has on data that it has never seen before 32\n",
      "This is important because the main idea\n",
      "(usually) of a supervised model is to predict outcomes for new data 22\n",
      "If our model is unable\n",
      "to generalize from our training data and use that to predict unseen cases, then our model\n",
      "isn't very good 29\n",
      "\n",
      "\n",
      "The preceding diagram outlines a simple way of ensuring that our model can effectively\n",
      "ingest the training data and use it to predict data points that the model itself has never seen 35\n",
      "\n",
      "Of course, as data scientists, we know that the test set also has answers attached to them,\n",
      "but the model doesn't know that 28\n",
      "\n",
      "All of this might sound complicated, but luckily, the scikit-learn  package has a built-in\n",
      "method to do this, as shown:\n",
      "from sklearn 33\n",
      "cross_validation import train_test_split\n",
      "# function that splits data into training and testing sets\n",
      "feature_cols = ['temp']\n",
      "X = bikes[feature_cols]\n",
      "y = bikes['count']\n",
      "# setting our overall data X, and y\n",
      "# Note that in this example, we are attempting to find an association\n",
      "between the temperature of the day and the number of bike rentals 74\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y) # split the data\n",
      "into training and testing sets\n",
      "# X_train and y_train will be used to train the model\n",
      "# X_test and y_test will be used to test the model\n",
      "# Remember that all four of these variables are just subsets of the overall\n",
      "X and y 77\n",
      "\n",
      "linreg = LinearRegression()\n",
      "# instantiate the model\n",
      "linreg 14\n",
      "fit(X_train, y_train)\n",
      "# fit the model to our training set\n",
      "y_pred = linreg 21\n",
      "predict(X_test)\n",
      "# predict our testing set\n",
      "np 11\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y_test, y_pred)) # RMSE\n",
      "# Calculate our metric: 166 20\n",
      "91\n",
      "We will spend more time on the reasoning behind this train/test split in Chapter 12 , Beyond\n",
      "the Essentials , and look into an even more helpful method, but the main reason we must go\n",
      "through this extra work is because we do not want to fall into a trap where our model is\n",
      "simply regurgitating our dataset back to us and will not be able to handle unseen data\n",
      "points 84\n",
      "\n",
      "In other words, our train/test split is ensuring that the metrics we are looking at are more\n",
      "honest estimates of our sample performance 28\n",
      "\n",
      "\n",
      "Now, let's try again with more predictors, as follows:\n",
      "feature_cols = ['temp', 'workingday']\n",
      "X = bikes[feature_cols]\n",
      "y = bikes['count']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "# Pick a new random training and test set\n",
      "linreg = LinearRegression()\n",
      "linreg 74\n",
      "fit(X_train, y_train)\n",
      "y_pred = linreg 12\n",
      "predict(X_test)\n",
      "# fit and predict\n",
      "np 10\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y_test, y_pred))\n",
      "# 166 12\n",
      "95\n",
      "Now our model actually got worse with that addition 12\n",
      "This implies that workingday  might\n",
      "not be very predictive of our response, the bike rental count 20\n",
      "\n",
      "Now, all of this is good and well, but how well is our model really doing at predicting 21\n",
      "We\n",
      "have our root mean squared error of around 167 bikes, but is that good 18\n",
      "One way to\n",
      "discover this is to evaluate the null model 12\n",
      "\n",
      "The null model in supervised  machine learning  represents effectively guessing the expected\n",
      "outcome over and over, and seeing how you did 26\n",
      "For example, in regression, if we only\n",
      "ever guess the average number of hourly bike rentals, then how well would that model do 27\n",
      "\n",
      "First, let's get the average hourly bike rental, as shown:\n",
      "average_bike_rental = bikes['count'] 25\n",
      "mean()\n",
      "average_bike_rental\n",
      "# 191 11\n",
      "57\n",
      "This means that, overall, in this dataset, regardless of weather, time, day of the week,\n",
      "humidity, and everything else, the average number of bikes that go out every hour is about\n",
      "192 44\n",
      "\n",
      "Let's make a fake prediction list, wherein every single guess is 191 16\n",
      "57 2\n",
      "Let's make this guess\n",
      "for every single hour, as follows:\n",
      "num_rows = bikes 18\n",
      "shape[0]\n",
      "num_rows\n",
      "# 10886\n",
      "null_model_predictions = [average_bike_rental]*num_rows\n",
      "null_model_predictions\n",
      "\n",
      "The output is as follows:\n",
      "[191 38\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 7\n",
      " 1\n",
      " 1\n",
      "\n",
      " 191 3\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824,\n",
      " 191 9\n",
      "57413191254824]\n",
      "So, now we have 10886  values, all of them are the average hourly bike rental number 28\n",
      "\n",
      "Now, let's see what RMSE  would be if our model only ever guessed the expected value of\n",
      "the average hourly bike rental count:\n",
      "np 31\n",
      "sqrt(metrics 2\n",
      "mean_squared_error(y, null_model_predictions))\n",
      "The output is as follows:\n",
      "181 16\n",
      "13613\n",
      "Simply guessing, it looks like our RMSE  would be 181 bikes 19\n",
      "So, even with one or two\n",
      "features, we can beat it 14\n",
      "Beating the null model is a kind of baseline in machine learning 13\n",
      "If\n",
      "you think about it, why go through any effort at all if your machine learning  is not even \n",
      "better  than just guessing 28\n",
      "\n",
      "We've spent a great deal of time on linear regression, but I'd like to now take some time to\n",
      "look at our next machine learning model, which is actually, somewhat, a cousin of linear\n",
      "regression 45\n",
      "They are based on very similar ideas but have one major difference —while\n",
      "linear regression is a regression model and can only be used to make predictions of\n",
      "continuous numbers, our next machine learning model will be a classification model, which\n",
      "means that it will attempt to make associations between features and a categorical response 61\n",
      "\n",
      "Logistic regression\n",
      "Our first classification model is called logistic  regression 14\n",
      "I can already hear the questions\n",
      "you have in your head: what makes is logistic 17\n",
      "Why is it called regression if you claim that\n",
      "this is a classification algorithm 15\n",
      "All in good time, my friend 7\n",
      "\n",
      "\n",
      "Logistic regression is a generalization of the linear regression model adapted to fit\n",
      "classification problems 19\n",
      "In linear regression, we use a set of quantitative feature variables to\n",
      "predict a continuous response variable 19\n",
      "In logistic regression, we use a set of quantitative\n",
      "feature variables to predict the probabilities  of class membership 21\n",
      "These probabilities can\n",
      "then be mapped to class labels, hence predicting a class for each observation 18\n",
      "\n",
      "When performing linear regression, we use the following function to make our line of best\n",
      "fit:\n",
      "Here, y is our response variable (the thing we wish to predict), our beta represents our\n",
      "model parameters and x represents our input variable (a single one in this case, but it can\n",
      "take in more, as we have seen) 69\n",
      "\n",
      "Briefly, let's assume that one of the response options in our classification problem is class 1 21\n",
      "\n",
      "When performing logistic regression, we use the following form:\n",
      "Probability of y=1, given x\n",
      "Here, y represents the conditional probability that our response variable belongs to class 1,\n",
      "given our data, x 43\n",
      "Now, you might be wondering what on earth is that monstrosity of a\n",
      "function on the right-hand side, and where did the e variable come from 33\n",
      "Well, that\n",
      "monstrosity is called the logistic function and it is actually wonderful 18\n",
      "And that variable, e,\n",
      "is no variable at all 11\n",
      "Let's back up a tick 6\n",
      "\n",
      "The variable e is a special number, like π 11\n",
      "It is, approximately, 2 7\n",
      "718, and is called Euler's\n",
      "number 10\n",
      "It is used frequently in modeling environments with natural  growth and decay 13\n",
      "\n",
      "For example, scientists use e to model the population growth of bacteria and buffalo alike 17\n",
      "\n",
      "Euler's number is used to model the radioactive decay of chemicals and to calculate\n",
      "continuous compound interest 21\n",
      "Today, we will use e for a very special purpose, for machine\n",
      "learning 16\n",
      "\n",
      "Why can't we just make a linear regression directly to the probability of the data point\n",
      "belonging to a certain class, like this:\n",
      "\n",
      "\n",
      "We can't do that for a few reasons, but I will point out a big one 48\n",
      "Linear regression, because\n",
      "it attempts to relate to a continuous response variable, assumes that our y is continuous 21\n",
      "In\n",
      "our case, y would represent the probability of an event occurring 14\n",
      "Even though our\n",
      "probability is, in fact, a continuous range, it is just that —a range between 0 and 1 27\n",
      "A line\n",
      "would extrapolate beyond 0 and 1 and  be able  to predict a probability of -4 or 1,542 29\n",
      "We can't\n",
      "have that 6\n",
      "Our graph must  be bound neatly between 0 and 1 on the y axis, just like a real\n",
      "probability 24\n",
      "\n",
      "Another reason is a bit more philosophical 8\n",
      "Using a linear regression, we are making a\n",
      "serious assumption 12\n",
      "Our big assumption here is that there is a linear relationship between\n",
      "probability and our features 17\n",
      "In general, if we think about the probability of an event, we\n",
      "tend to think of smooth curves representing them, not a single boring line 30\n",
      "So, we need\n",
      "something a bit more appropriate 10\n",
      "For this, let's go back and revisit basic probability for a\n",
      "minute 15\n",
      "\n",
      "Probability, odds, and log odds\n",
      "We are familiar with the basic concept of probability  in that the probability of an event\n",
      "occurring can be simply modeled as the number of ways the event can occur divided by all\n",
      "of the possible  outcomes 51\n",
      "For example, if, out of 3,000  people who walked into a store, 1,000\n",
      "actually bought something, then we could say that the probability of a single person buying\n",
      "an item is as shown here:\n",
      "Pr(buy) = 1000/3000 = 1/3 = 33 67\n",
      "3%\n",
      "However, we also have  a related concept, called odds 15\n",
      "The odds of an outcome occurring is\n",
      "the ratio of the number of ways that the outcome occurs divided by every other possible\n",
      "outcome instead of all possible outcomes 31\n",
      "In the same example, the odds of a person buying\n",
      "something would be as follows:\n",
      "Odds(buy) = 1000/3000 = 1/3 = 33 38\n",
      "3%\n",
      "This means that, for every customer you convert, you will not convert two customers 19\n",
      "These\n",
      "concepts are so related, there is even a formula to get from one to the other 20\n",
      "We have that:\n",
      "\n",
      "\n",
      "Let's check this with our example, as illustrated:\n",
      "It checks out 18\n",
      "\n",
      "Let's use Python to make a table of probabilities and associated odds, as shown:\n",
      "# create a table of probability versus odds\n",
      "table = pd 30\n",
      "DataFrame({'probability':[0 5\n",
      "1, 0 5\n",
      "2, 0 5\n",
      "25, 0 5\n",
      "5, 0 5\n",
      "6, 0 5\n",
      "8, 0 5\n",
      "9]})\n",
      "table['odds'] = table 10\n",
      "probability/(1 - table 5\n",
      "probability)\n",
      "table\n",
      "Following is the output:\n",
      "So, we see that, as our probabilities increase, so do our odds, but at a much faster rate 31\n",
      "In\n",
      "fact, as the probability  of an event occurring nears 1, our odds will shoot off into infinity 24\n",
      "\n",
      "Earlier, we said that we couldn't simply regress to probability  because our line would\n",
      "shoot off into positive and negative infinities, predicting improper probabilities, but what if\n",
      "we regress to odds 41\n",
      "Well, odds  go off to positive infinity, but alas, they will merely\n",
      "approach 0 on the bottom, but never go below 0 31\n",
      "Therefore, we cannot simply regress to\n",
      "probability  or odds 12\n",
      "It looks like we've hit rock bottom folks 9\n",
      "\n",
      "\n",
      "However, wait, natural numbers and logarithms come to the rescue 14\n",
      "Think of logarithms as\n",
      "follows:\n",
      "Basically, logarithms and exponents are one and the same 21\n",
      "We are just so used to writing \n",
      "exponents  in the first way that we forget there is another way to write them 25\n",
      "How about\n",
      "another example 5\n",
      "If we take the logarithm of a number, we are asking the question, what\n",
      "exponent would we need to put on this number to make it the given number 34\n",
      "\n",
      "Note that np 4\n",
      "log  automatically does  all logarithms in base e, which is what we want:\n",
      "np 19\n",
      "log(10) # == 2 8\n",
      "3025\n",
      "# meaning that e ^ 2 11\n",
      "302 == 10\n",
      "# to prove that\n",
      "2 12\n",
      "71828**2 5\n",
      "3025850929940459 # == 9 11\n",
      "9999\n",
      "# e ^ log(10) == 10\n",
      "Let's go ahead and add the logarithm of odds , or logodds , to our table , as follows:\n",
      "# add log-odds to the table\n",
      "table['logodds'] = np 57\n",
      "log(table 2\n",
      "odds)\n",
      "table\n",
      "Following is the output:\n",
      "\n",
      "\n",
      "So, now every row has the probability  of a single event occurring, the odds  of that event\n",
      "occurring, and now the logodds  of that event occurring 45\n",
      "Let's go ahead and ensure that our\n",
      "numbers are on the up and up 16\n",
      "Let's choose a probability of 6\n",
      "25, as illustrated:\n",
      "prob = 8\n",
      "25\n",
      "odds = prob / (1 - prob)\n",
      "odds\n",
      "# 0 19\n",
      "33333333\n",
      "logodds = np 10\n",
      "log(odds)\n",
      "logodds\n",
      "# -1 11\n",
      "09861228\n",
      "It checks out 8\n",
      "Wait, look 3\n",
      "Our logodds  variable seems to go down below zero and, in fact,\n",
      "logodds  is not bounded above, nor is it bounded below, which means that it is a great\n",
      "candidate for a response variable for linear regression 48\n",
      "In fact, this is where our story of\n",
      "logistic regression really begins 15\n",
      "\n",
      "The math of logistic regression\n",
      "The long and short of it is that logistic  regression is a linear regression between our feature,\n",
      "x, and the log-odds of our data belonging to a certain class that we will call true for the sake\n",
      "of generalization 54\n",
      "\n",
      "If p represents the probability of a data point belonging to a particular class, then logistic\n",
      "regression can be written as follows:\n",
      "If we rearrange our variables and solve this for p, we would get the logistic function, which\n",
      "takes on an S shape, where y is bounded by [0,1] :\n",
      "\n",
      "\n",
      "\n",
      "The preceding graph represents the logistic function's ability to map our continuous input,\n",
      "x, to a smooth probability curve that begins at the left, near probability 0, and as we\n",
      "increase x, our probability of belonging to a certain class rises naturally  and smoothly up to\n",
      "probability 1 124\n",
      "Let's explain that in other words:\n",
      "Logistic regression gives an output of the probabilities of a specific class being\n",
      "true\n",
      "Those probabilities can be converted into class predictions\n",
      "The logistic function has some nice properties, as follows:\n",
      "It takes on an S shape\n",
      "Output is bounded by 0 and 1, as a probability should be\n",
      "\n",
      "In order to interpret the output value of a logistic function, we must understand the\n",
      "difference between probability and odds 90\n",
      "The odds of an event are given by the ratio of the\n",
      "probability of the event by its complement, as shown:\n",
      "In linear regression, the β1 parameter represents the change in the response variable for a\n",
      "unit change in x 46\n",
      "In logistic  regression, β1 represents the change in the log-odds for a unit\n",
      "change in x 23\n",
      "This means gives us the change in the odds for a unit change in x 15\n",
      "\n",
      "Consider that we are interested in mobile purchase behavior 10\n",
      "Let y be a class label denoting\n",
      "purchase/no purchase, and let x denote whether the phone was an iPhone 23\n",
      "Also, suppose\n",
      "that we perform a logistic regression, and we get β= 0 18\n",
      "693 2\n",
      "In this case, the odds ratio is\n",
      "np 10\n",
      "exp(0 3\n",
      "693) = 2 , which means that the likelihood of purchase is twice as high if the phone\n",
      "is an iPhone 25\n",
      "\n",
      "Our examples have mostly been binary classification, meaning that we are\n",
      "only predicting one of two outcomes, but logistic regression can handle\n",
      "predicting multiple options in our categorical response using a one-\n",
      "versus-all approach, meaning that it will fit a probability curve for each\n",
      "categorical response 58\n",
      "\n",
      "Back to our bikes briefly to see scikit-learn's logistic regression in action 17\n",
      "I will begin by\n",
      "making a new response variable that is categorical 13\n",
      "To make things simple, I made a\n",
      "column called above_average , which is true if the hourly bike rental count is above\n",
      "average and false otherwise:\n",
      "# Make a categorical response\n",
      "bikes['above_average'] = bikes['count'] >= average_bike_rental\n",
      "As mentioned before, we should look at our null model 66\n",
      "In regression, our null model\n",
      "always predicts the average response, but in classification, our null model always predicts\n",
      "the most common outcome 27\n",
      "In this case, we can use a pandas value count to see that 14\n",
      "About\n",
      "60% of the time, the bike rental count is not above average:\n",
      "bikes['above_average'] 23\n",
      "value_counts(normalize=True)\n",
      "\n",
      "Now, let's actually use logistic regression to try and predict whether or not the hourly bike\n",
      "rental count will be above average, as shown:\n",
      "from sklearn 38\n",
      "linear_model import LogisticRegression\n",
      "feature_cols = ['temp']\n",
      "# using only temperature\n",
      "X = bikes[feature_cols]\n",
      "y = bikes['above_average']\n",
      "# make our overall X and y variables, this time our y is\n",
      "# out binary response variable, above_average\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "# make our train test split\n",
      "logreg = LogisticRegression()\n",
      "# instantiate our model\n",
      "logreg 94\n",
      "fit(X_train, y_train)\n",
      "# fit our model to our training set\n",
      "logreg 18\n",
      "score(X_test, y_test)\n",
      "# score it on our test set to get a better sense of out of sample\n",
      "performance\n",
      "# 0 29\n",
      "65650257\n",
      "It seems that, by only using temperature, we can beat the null model of guessing false all of\n",
      "the time 28\n",
      "This is our first step in making our model the best it can be 14\n",
      "\n",
      "Between linear and logistic  regression, I'd say we already have a great tool belt of machine\n",
      "learning forming, but I have a question —it seems that both of these algorithms are only\n",
      "able to take in quantitative columns as features, but what if I have a categorical feature that\n",
      "I think has an association to my response 67\n",
      "\n",
      "Dummy variables\n",
      "Dummy variables are used when we are hoping to convert a categorical feature into a\n",
      "quantitative one 23\n",
      "Remember that we have two types of categorical features: nominal and\n",
      "ordinal 14\n",
      "Ordinal features have natural order among them, while nominal data does not 14\n",
      "\n",
      "\n",
      "Encoding qualitative (nominal) data using separate columns is called making dummy\n",
      "variables and it works by turning each unique category of a nominal column into its own\n",
      "column that is either true or false 40\n",
      "\n",
      "For example, if we had a column  for someone's college major and we wished to plug that\n",
      "information into a linear or logistic regression, we couldn't because they only take in\n",
      "numbers 40\n",
      "So, for each row, we had new columns that represent the single nominal column 16\n",
      "\n",
      "In this case, we have four unique majors: computer science, engineering, business, and\n",
      "literature 22\n",
      "We end up with three new columns (we omit computer science as it is not\n",
      "necessary):\n",
      "Note that the first row has a 0 in all of the columns, which means that this person did not\n",
      "major in engineering, did not major in business, and did not major in literature 58\n",
      "The second\n",
      "person has a single 1 in the Engineering  column as that is the major they studied 21\n",
      "\n",
      "In our bikes example, let's define a new column, called when_is_it , which is going to be\n",
      "one of the following four options:\n",
      "Morning\n",
      "Afternoon\n",
      "Rush_hour\n",
      "Off_hours\n",
      "To do this, our approach will be to make a new column that is simply the hour of the day,\n",
      "use that column to determine when in the day it is, and explore whether or not we think\n",
      "that column might help us predict the above_daily  column:\n",
      "bikes['hour'] = bikes['datetime'] 107\n",
      "apply(lambda x:int(x[11]+x[12]))\n",
      "# make a column that is just the hour of the day\n",
      "bikes['hour'] 30\n",
      "head()\n",
      "0 1 2 3\n",
      "\n",
      "Great, now let's define a function that turns these hours into strings 24\n",
      "For this example, let's\n",
      "define the hours between 5 and 11 as morning, between 11 a 23\n",
      "m 1\n",
      "and 4 p 4\n",
      "m 1\n",
      "as being\n",
      "afternoon, 4 and 6 as being rush hour, and everything else as being off hours:\n",
      "# this function takes in an integer hour\n",
      "# and outputs one of our four options\n",
      "def when_is_it(hour):\n",
      "    if hour >= 5 and hour < 11:\n",
      "        return \"morning\"\n",
      "    elif hour >= 11 and hour < 16:\n",
      "        return \"afternoon\"\n",
      "    elif hour >= 16 and hour < 18:\n",
      "        return \"rush_hour\"\n",
      "    else:\n",
      "        return \"off_hours\"\n",
      "Let's apply this function  to our new hour  column and make our brand new column,\n",
      "when_is_it :\n",
      "bikes['when_is_it'] = bikes['hour'] 146\n",
      "apply(when_is_it)\n",
      "bikes[['when_is_it', 'above_average']] 17\n",
      "head()\n",
      "Following is the table:\n",
      "Let's try to use only this new column to determine whether or not the hourly bike rental\n",
      "count will be above average 31\n",
      "Before we do, let's do the basics of exploratory data analysis\n",
      "and make a graph to see if we can visualize a difference between the four times of the day 34\n",
      "\n",
      "Our graph will be a bar chart with one bar per time of the day 16\n",
      "Each bar will represent the\n",
      "percentage of times that this time of the day had a greater than normal bike rental:\n",
      "bikes 25\n",
      "groupby('when_is_it') 7\n",
      "above_average 2\n",
      "mean() 2\n",
      "plot(kind='bar')\n",
      "\n",
      "Following is the output:\n",
      "We can see that there is a pretty big difference 20\n",
      "For example, when it is off hours, the chance\n",
      "of having more than average bike rentals is about 25%, whereas during rush hour, the\n",
      "chance of being above average is over 80% 42\n",
      "Okay, this is exciting, but let's use some built-in\n",
      "pandas tools to extract dummy columns, as follows:\n",
      "when_dummies = pd 30\n",
      "get_dummies(bikes['when_is_it'], prefix='when__')\n",
      "when_dummies 18\n",
      "head()\n",
      "\n",
      "Following is the output:\n",
      "when_dummies = when_dummies 14\n",
      "iloc[:, 1:]\n",
      "# remove the first column\n",
      "when_dummies 15\n",
      "head()\n",
      "Following is the output:\n",
      "Great 8\n",
      "Now we have a DataFrame full of numbers that we can plug in to our logistic\n",
      "regression:\n",
      "X = when_dummies\n",
      "# our new X is our dummy variables\n",
      "y = bikes 38\n",
      "above_average\n",
      "logreg = LogisticRegression()\n",
      "# instantiate our model\n",
      "logreg 16\n",
      "fit(X_train, y_train)\n",
      "# fit our model to our training set\n",
      "logreg 18\n",
      "score(X_test, y_test)\n",
      "# score it on our test set to get a better sense of out of sample\n",
      "\n",
      "performance\n",
      "# 0 29\n",
      "685157\n",
      "This is even better than just using the temperature 13\n",
      "What if we tacked temperature and\n",
      "humidity onto that 11\n",
      "So, now we are using the temperature, humidity, and our time of day\n",
      "dummy variables to predict whether or not we will have higher than average bike rentals:\n",
      "new_bike = pd 38\n",
      "concat([bikes[['temp', 'humidity']], when_dummies], axis=1) #\n",
      "combine temperature, humidity, and the dummy variables\n",
      "X = new_bike # our new X is our dummy variables\n",
      "y = bikes 46\n",
      "above_average\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "logreg = LogisticRegression() # instantiate our model\n",
      "logreg 35\n",
      "fit(X_train, y_train) # fit our model to our training set\n",
      "logreg 18\n",
      "score(X_test, y_test) # score it on our test set to get a better\n",
      "sense of out of sample performance\n",
      "# 0 29\n",
      "75165\n",
      "Wow 5\n",
      "Okay, let's quit while we're ahead 9\n",
      "\n",
      "Summary\n",
      "In this chapter, we looked at machine learning and its different subcategories 17\n",
      "We explored\n",
      "supervised, unsupervised, and reinforcement learning strategies and looked at situations\n",
      "where each one would come in handy 26\n",
      "\n",
      "Looking into linear regression, we were able to find relationships between predictors and a\n",
      "continuous response variable 20\n",
      "Through the train/test split, we were able to help avoid\n",
      "overfitting our machine learning models and get a more generalized prediction 26\n",
      "We were\n",
      "able to use metrics, such as the root mean squared error, to evaluate our models as well 22\n",
      "\n",
      "\n",
      "By extending our notion of linear regression into logistic regression, we were able to then\n",
      "find association between the same predictors, but now to categorical responses 30\n",
      "By\n",
      "introducing dummy variables into the mix, we were able to add categorical features to our\n",
      "models and improve our performance even further 27\n",
      "\n",
      "In the next few chapters, we will be taking a much deeper dive into many more machine\n",
      "learning models and, along the way, we will learn new metrics, new validation techniques,\n",
      "and more importantly, new ways of applying our data science to the world 52\n",
      "\n",
      "\n",
      "1\n",
      "Predictions Don't Grow on\n",
      "Trees – or Do They 15\n",
      "\n",
      "Our goal in this chapter is to see and apply concepts learned from previous chapters in\n",
      "order to construct and use modern learning algorithms in order to glean insights and make\n",
      "predictions on real datasets 38\n",
      "While we explore the following algorithms, we should always\n",
      "remember that we are constantly keeping our metrics in mind 21\n",
      "\n",
      "In this chapter , we will be looking at the following machine learning algorithms:\n",
      "Decision trees\n",
      "Naive Bayes classification\n",
      "k-means clustering  \n",
      "The first two are examples of supervised learning, while the final algorithm is an example\n",
      "of unsupervised learning 52\n",
      "\n",
      "Let's get to it 6\n",
      "\n",
      "Naive  Bayes classification\n",
      "Let's get right into it 14\n",
      "Let's begin with Naive  Bayes classification 10\n",
      "This machine learning\n",
      "model relies heavily on results from previous chapters, specifically with Bayes' theorem:\n",
      "\n",
      "\n",
      "Let's look a little closer at the specific features of this formula:\n",
      "P(H)  is the probability of the hypothesis before we observe  the data, called the\n",
      "prior probability , or just prior\n",
      "P(H|D)  is what we want to compute, the probability of the hypothesis after we\n",
      "observe the data, called the posterior\n",
      "P(D|H)  is the probability of the data under the given  hypothesis, called the\n",
      "likelihood\n",
      "P(D)  is the probability of the data under  any hypothesis, called the normalizing\n",
      "constant\n",
      "Naive Bayes classification is a classification model, and therefore a supervised model 150\n",
      "\n",
      "Given this, what kind of data do we need 11\n",
      "\n",
      "Labeled data\n",
      "Unlabeled data\n",
      "(Insert Jeopardy music here )\n",
      "If you answered labeled data , then you're well on your way to becoming a data scientist 36\n",
      "\n",
      "Suppose we have a dataset with n features, ( x1, x2, …, xn) and a class label, C 28\n",
      "For\n",
      "example, let's take some data involving spam text classification 13\n",
      "Our data would consist of\n",
      "rows of individual text samples and columns of both our features and our class labels 21\n",
      "Our\n",
      "features would be words and phrases that are contained within the text samples and our\n",
      "class labels are simply spam  or not spam 27\n",
      "In this scenario, I will replace the not spam\n",
      "class with an easier-to-say word, ham:\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "df =\n",
      "pd 33\n",
      "read_table('https://raw 6\n",
      "githubusercontent 2\n",
      "com/sinanuozdemir/sfdat22/mast\n",
      "er/data/sms 18\n",
      "tsv',\n",
      "                   sep='\\t', header=None, names=['label', 'msg'])\n",
      "df\n",
      "\n",
      "Here is a sample of text data in a row column format:\n",
      "Let's do some preliminary statistics to see what we are dealing with 46\n",
      "Let's see the difference\n",
      "in the number of ham and spam messages at our disposal:\n",
      "df 19\n",
      "label 1\n",
      "value_counts() 3\n",
      "plot(kind=\"bar\")\n",
      "This gives us a bar chart, as follows:\n",
      "\n",
      "\n",
      "So, we have way more ham messages than we do spam 27\n",
      "Because this is a classification\n",
      "problem, it will be very useful to know our null accuracy rate , which is the percentage\n",
      "chance of predicting a single row correctly if we keep guessing the most common class,\n",
      "ham:\n",
      "df 45\n",
      "label 1\n",
      "value_counts() / df 5\n",
      "shape[0]\n",
      "ham     0 8\n",
      "865937\n",
      "spam    0 8\n",
      "134063\n",
      "So if we blindly guessed ham, we would be correct about 87% of the time, but we can do\n",
      "better than that 31\n",
      "If we have a set of classes, C, and features, xi, then we can use Bayes'\n",
      "theorem to predict the probability that a single row belongs to class C, using the following\n",
      "formula:\n",
      "Let's look at this formula in a little more detail:\n",
      "P(class C | {xi}) : The posterior probability is the probability that the row belongs\n",
      "to class C  given the features {xi} 83\n",
      "\n",
      "P({xi} | class C) : This is the likelihood that we would observe these features given\n",
      "that the row was in class C 29\n",
      "\n",
      "P(class C) : This is the prior probability 11\n",
      "It is the probability that the data point\n",
      "belongs to class C  before we see any data 19\n",
      "\n",
      "P({xi}) : This is our normalization constant 11\n",
      "\n",
      "For example, imagine we have an email with three words: send cash now 16\n",
      "We'll use\n",
      "Naive Bayes to classify the email as either being spam or ham:\n",
      "We are concerned with the difference of these two numbers 29\n",
      "We can use the following\n",
      "criteria to classify any single text sample:\n",
      "If P(spam | send cash now)  is larger than P(ham | send cash now) ,\n",
      "then we will classify the text as spam\n",
      "\n",
      "If P(ham | send cash now)  is larger than P(spam | send cash now) , then\n",
      "we will label the text ham\n",
      "Because both equations have P (send money now)  in the denominator, we can ignore them 95\n",
      "\n",
      "So, now we are concerned with the following:\n",
      "Let's figure out the numbers  in this equation:\n",
      "P(spam) = 0 29\n",
      "134063\n",
      "P(ham) = 0 11\n",
      "865937\n",
      "P(send cash now | spam)\n",
      "P(send cash now | ham)\n",
      "The final two likelihoods might seem like they would not be so difficult to calculate 34\n",
      "All we\n",
      "have to do is count the numbers of spam messages that include the send money\n",
      "now phrase and divide that by the total number of spam messages:\n",
      "df 33\n",
      "msg = df 3\n",
      "msg 1\n",
      "apply(lambda x:x 4\n",
      "lower())\n",
      "# make all strings lower case so we can search easier\n",
      "df[df 16\n",
      "msg 1\n",
      "str 1\n",
      "contains('send cash now')] 6\n",
      "shape\n",
      "(0, 2)\n",
      "Oh no 10\n",
      "There are none 3\n",
      "There are literally zero texts with the exact phrase send cash now 12\n",
      "\n",
      "The hidden problem here is that this phrase is very specific and we can't assume that we\n",
      "will have enough data in the world to have seen this exact phrase many times before 36\n",
      "\n",
      "Instead, we can make a naïve assumption  in our Bayes' theorem 17\n",
      "If we assume that the\n",
      "features (words) are conditionally independent (meaning that no word affects the existence\n",
      "of another word), then we can rewrite the formula:\n",
      "spams = df[df 39\n",
      "label == 'spam']\n",
      "for word in ['send', 'cash', 'now']:\n",
      "    print( word, spams[spams 27\n",
      "msg 1\n",
      "str 1\n",
      "contains(word)] 3\n",
      "shape[0] /\n",
      "float(spams 8\n",
      "shape[0]))\n",
      "\n",
      "P(send|spam) = 0 12\n",
      "096\n",
      "P(cash |spam) = 0 12\n",
      "091\n",
      "P(now|spam) = 0 11\n",
      "280\n",
      "With this, we can calculate the following:\n",
      "P(send cash now| spam) ∗ P(spam)= ( 26\n",
      "096 ∗ 4\n",
      "091∗ 4\n",
      "280) ∗ 5\n",
      "134 = 0 5\n",
      "00032\n",
      "Repeating the same procedure for ham gives us the following:\n",
      "P(send|ham) = 0 24\n",
      "03\n",
      "P(cash|ham) = 0 12\n",
      "003\n",
      "P(now|ham) = 0 11\n",
      "109\n",
      "The fact that these numbers are both very low is not as important as the fact that the spam\n",
      "probability is much larger than the ham calculation 31\n",
      "If we calculate 0 5\n",
      "00032 / 0 6\n",
      "0000084 =\n",
      "38 6\n",
      "1 we see that the send cash now  probability for spam is 38 times higher than for spam 21\n",
      "\n",
      "Doing this means that we can classify send cash now  as spam 14\n",
      "Simple, right 3\n",
      "\n",
      "Let's use Python to implement a Naive Bayes classifier without having to do all of these\n",
      "calculations ourselves 24\n",
      "\n",
      "First, let's revisit the count vectorizer in scikit-learn, which turns text into numerical data\n",
      "for us 25\n",
      "Let's assume that we will train on three documents (sentences):\n",
      "# simple count vectorizer example\n",
      "from sklearn 23\n",
      "feature_extraction 2\n",
      "text import CountVectorizer\n",
      "# start with a simple example\n",
      "train_simple = ['call you tonight',\n",
      "                'Call me a cab',\n",
      "                'please call me 32\n",
      " 1\n",
      " 1\n",
      "PLEASE 44 3\n",
      "\n",
      "# learn the 'vocabulary' of the training data\n",
      "vect = CountVectorizer()\n",
      "train_simple_dtm = vect 24\n",
      "fit_transform(train_simple)\n",
      "pd 6\n",
      "DataFrame(train_simple_dtm 5\n",
      "toarray(), columns=vect 6\n",
      "get_feature_names())\n",
      "\n",
      "\n",
      "Note that each row represents one of the three documents (sentences), each column\n",
      "represents one of the words present in the documents, and each cell contains the number of\n",
      "times each word appears in each document 47\n",
      "\n",
      "We can then use the count vectorizer to transform new incoming test documents to\n",
      "conform with our training set (the three sentences):\n",
      "# transform testing data into a document-term matrix (using existing\n",
      "vocabulary, notice don't is missing)\n",
      "test_simple = [\"please don't call me\"]\n",
      "test_simple_dtm = vect 66\n",
      "transform(test_simple)\n",
      "test_simple_dtm 8\n",
      "toarray()\n",
      "pd 4\n",
      "DataFrame(test_simple_dtm 5\n",
      "toarray(), columns=vect 6\n",
      "get_feature_names())\n",
      "Note how, in our test sentence, we had a new word, namely don't 21\n",
      "When we vectorized it,\n",
      "because we hadn't seen that word previously in our training data, the vectorizer simply\n",
      "ignored it 26\n",
      "This is important and incentivizes data scientists to obtain as much data as\n",
      "possible for their training sets 20\n",
      "\n",
      "Now, let's do this for our actual data:\n",
      "# split into training and testing sets\n",
      "from sklearn 22\n",
      "cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(df 23\n",
      "msg, df 3\n",
      "label,\n",
      "random_state=1)\n",
      "# instantiate the vectorizer\n",
      "vect = CountVectorizer()\n",
      "# learn vocabulary and create document-term matrix in a single step\n",
      "train_dtm = vect 36\n",
      "fit_transform(X_train)\n",
      "train_dtm\n",
      "The following is the output: \n",
      "<4179x7456 sparse matrix of type '<class 'numpy 30\n",
      "int64'>'\n",
      " with 55209 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "Note that the format is in a sparse matrix, meaning the matrix is so large and full of zeroes 37\n",
      "\n",
      "There is a special format to deal with objects such as this 13\n",
      "Take a look  at the number of\n",
      "columns 10\n",
      "\n",
      "There are 7,456 words 8\n",
      "\n",
      "This means that in our training set, there are 7,456 unique words to look at 20\n",
      "We can now\n",
      "transform our test data to conform to our vocabulary:\n",
      "# transform testing data into a document-term matrix\n",
      "test_dtm = vect 29\n",
      "transform(X_test)\n",
      "test_dtm\n",
      "The output is as follows:\n",
      "<1393x7456 sparse matrix of type '<class 'numpy 28\n",
      "int64'>'\n",
      "        with 17604 stored elements in Compressed Sparse Row format>\n",
      "Note that we have the same exact number of columns because it is conforming to our test\n",
      "set to be exactly the same vocabulary as before 46\n",
      "No more, no less 5\n",
      "\n",
      "Now let's build a Naive Bayes model (similar to the linear regression process):\n",
      "## MODEL BUILDING WITH NAIVE BAYES\n",
      "# train a Naive Bayes model using train_dtm\n",
      "from sklearn 45\n",
      "naive_bayes import MultinomialNB\n",
      "# import our model\n",
      "nb = MultinomialNB()\n",
      "# instantiate our model\n",
      "nb 28\n",
      "fit(train_dtm, y_train)\n",
      "# fit it to our training set\n",
      "Now the nb variable holds our fitted model 24\n",
      "The training phase of the model involves\n",
      "computing the likelihood function, which is the conditional probability of each feature\n",
      "given each class:\n",
      "# make predictions on test data using test_dtm\n",
      "preds = nb 42\n",
      "predict(test_dtm)\n",
      "preds\n",
      "The output is as follows:\n",
      "array(['ham', 'ham', 'ham', 24\n",
      " 1\n",
      " 1\n",
      ", 'ham', 'spam', 'ham'],\n",
      "      dtype='|S4')\n",
      "\n",
      "The prediction phase of the model involves computing the posterior probability of each\n",
      "class given the observed features, and choosing the class with the highest probability 45\n",
      "\n",
      "We will use sklearn's built-in accuracy and confusion matrix to look at how well our Naive\n",
      "Bayes models are performing:\n",
      "# compare predictions to true labels\n",
      "from sklearn import metrics\n",
      "print metrics 41\n",
      "accuracy_score(y_test, preds)\n",
      "print metrics 9\n",
      "confusion_matrix(y_test, preds)\n",
      "The output is as follows:\n",
      "accuracy == 0 17\n",
      "988513998564\n",
      "confusion matrix ==\n",
      "[[1203    5]\n",
      " [  11  174]]\n",
      "First off, our accuracy is great 32\n",
      "Compared to our null accuracy, which was 87%, 99% is a\n",
      "fantastic improvement 20\n",
      "\n",
      "Now to our confusion matrix 6\n",
      "From before, we know that each row represents actual values\n",
      "while columns represent predicted values, so the top left value, 1,203, represents our true\n",
      "negatives 35\n",
      "But what is negative and positive 6\n",
      "We gave the model the spam  and ham strings\n",
      "as our classes, not positive and negative 19\n",
      "\n",
      "We can use the following:\n",
      "nb 8\n",
      "classes_\n",
      "The output is as follows:\n",
      "array(['ham', 'spam'])\n",
      "We can then line up the indices so that 1,203 refers to true ham predictions and 174 refers to\n",
      "true spam  predictions 43\n",
      "\n",
      "There were also five false spam classifications , meaning that five messages were predicted as\n",
      "spam , but were actually ham, as well as 11 false ham classifications 32\n",
      "\n",
      "In summary, Naive Bayes classification uses  Bayes' theorem in order to fit posterior\n",
      "probabilities of classes so that data points are correctly labeled as belonging to the proper\n",
      "class 39\n",
      "\n",
      "\n",
      "Decision trees\n",
      "Decision trees are supervised models  that can either perform regression or classification 17\n",
      "\n",
      "Let's take a look at some major league baseball player data from 1986-1987 20\n",
      "Each dot\n",
      "represents a single player in the league:\n",
      "Years ( x-axis) : Number of years played in the major leagues\n",
      "Hits ( y-axis) : Number of hits the player had in the previous year\n",
      "Salary (color) : Low salary is blue/green, high salary is red/yellow\n",
      "The preceding data is our training data 70\n",
      "The idea is to build a model that predicts the salary\n",
      "of future players based on Years  and Hits 21\n",
      "A decision tree aims to make splits  on our data in\n",
      "order to segment the data points that act similarly to each other, but differently to the\n",
      "others 32\n",
      "The tree makes multiples of these splits in order to make the most accurate\n",
      "prediction possible 17\n",
      "Let's see a tree built for the preceding data:\n",
      "\n",
      "\n",
      "Let's read this from top to bottom:\n",
      "The first split is Years < 4 28\n",
      "5 : when a splitting rule is true , you follow the left\n",
      "branch 16\n",
      "When a splitting rule is false , you follow the right branch 12\n",
      "So for a new\n",
      "player, if they have been playing for less than 4 17\n",
      "5 years, we will go down the left\n",
      "branch 12\n",
      "\n",
      "For players in the left branch, the mean salary is $166,000, hence you label it with\n",
      "that value (salary has been divided by 1,000 and log-transformed to 5 42\n",
      "11 for ease\n",
      "of computation) 8\n",
      "\n",
      "For players in the right branch, there is a further split on Hits < 117 18\n",
      "5 , dividing\n",
      "players into two more salary regions: $403,000 (transformed to 6 22\n",
      "00) and $846,000\n",
      "(transformed to 6 14\n",
      "74) 3\n",
      "\n",
      "This tree doesn't just give us predictions; it also provides some more information about our\n",
      "data:\n",
      "It seems that the number of years in the league is the most important factor in\n",
      "determining salary, with a smaller number of years correlating to a lower salary 55\n",
      "\n",
      "If a player has not been playing for long (< 4 13\n",
      "5 years), the number of hits they\n",
      "have is not an important factor when it comes to their salary 22\n",
      "\n",
      "\n",
      "For players with 5+ years under their  belt, hits are an important factor for their\n",
      "salary determination 23\n",
      "\n",
      "Our tree only made up to two decisions before spitting out an answer (two is\n",
      "called our depth of the tree) 26\n",
      "\n",
      "How does a computer build a regression tree 9\n",
      "\n",
      "Modern decision  tree algorithms tend to use a recursive  binary splitting approach:\n",
      "The process begins at the top of the tree 25\n",
      "1 2\n",
      "\n",
      "For every feature, it will examine every possible split and choose the feature and2 17\n",
      "\n",
      "split such that the resulting tree has the lowest possible Mean Squared Error\n",
      "(MSE ) 19\n",
      "The algorithm  makes that split 6\n",
      "\n",
      "It will then examine the two resulting regions and again make a single split (in3 18\n",
      "\n",
      "one of the regions) to minimize the MSE 10\n",
      "\n",
      "It will keep repeating Step 3  until a stopping criterion is met: 4 18\n",
      "\n",
      "Maximum tree depth (maximum number of splits required to arrive at\n",
      "a leaf)\n",
      "Minimum number of observations in a leaf (final) node\n",
      "For classification trees, the algorithm is very similar with the biggest difference being the\n",
      "metric we optimize over 49\n",
      "Because MSE only exists for regression problems, we cannot use\n",
      "it 13\n",
      "However, instead of accuracy, classification trees optimize over either the Gini index  or\n",
      "entropy 19\n",
      "\n",
      "How does a computer fit a classification tree 9\n",
      "\n",
      "Similarly to a regression tree, a classification  tree is built by optimizing over  a metric (in\n",
      "this case, the Gini index) and choosing the best split to make this optimization 39\n",
      "More\n",
      "formally, at each node, the tree will take the following steps:\n",
      "Calculate the purity of the data1 24\n",
      "\n",
      "Select a candidate split2 6\n",
      "\n",
      "Calculate the purity of the data after the split3 11\n",
      "\n",
      "Repeat for all variables4 6\n",
      "\n",
      "Choose the variable with the greatest increase in purity5 11\n",
      "\n",
      "Repeat for each split until some stop criteria is met6 12\n",
      "\n",
      "\n",
      "Let's say that we are predicting the likelihood of death aboard a luxury cruise ship given\n",
      "demographic features 22\n",
      "Suppose we start with 25 people, 10 of whom survived, and 15 of\n",
      "whom died:\n",
      "Before split All\n",
      "Survived 10\n",
      "Died 15\n",
      "We first calculate the Gini index before doing anything:\n",
      "In this example, overall classes are survived  and died, illustrated in the following formula:\n",
      "This means that the purity of the dataset is 0 77\n",
      "48 2\n",
      "\n",
      "Now let's consider a potential split on gender 10\n",
      "We first calculate the Gini index for each\n",
      "gender:\n",
      "\n",
      "\n",
      "The following formula calculates Gini index for male and female as follows:\n",
      "Once we have the Gini index for each gender, we then calculate the overall Gini index for\n",
      "the split on gender, as follows:\n",
      "So, the gini coefficient for splitting  on gender is 0 69\n",
      "27 2\n",
      "We then follow this procedure  for\n",
      "three potential splits:\n",
      "Gender (male or female)\n",
      "Number of siblings on board (0 or 1+)\n",
      "Class (first and second versus third)\n",
      "In this example, we would choose the gender to split on as it is the lowest Gini index 58\n",
      "\n",
      "\n",
      "The following table briefly summarizes the differences between classification and\n",
      "regression decision trees:\n",
      "Regression trees Classification trees\n",
      "Predict a quantitative response Predict a qualitative response\n",
      "Prediction is the average value in each\n",
      "leafPrediction is the most common label in each leaf\n",
      "Splits are chosen to minimize MSESplits are chosen to minimize Gini index\n",
      "(usually)\n",
      "Let's use scikit-learn's built-in decision tree function in order to build a decision tree:\n",
      "# read in the data\n",
      "titanic = pd 102\n",
      "read_csv('short_titanic 7\n",
      "csv')\n",
      "# encode female as 0 and male as 1\n",
      "titanic['Sex'] = titanic 23\n",
      "Sex 1\n",
      "map({'female':0, 'male':1})\n",
      "# fill in the missing values for age with the median age\n",
      "titanic 27\n",
      "Age 1\n",
      "fillna(titanic 5\n",
      "Age 1\n",
      "median(), inplace=True)\n",
      "# create a DataFrame of dummy variables for Embarked\n",
      "embarked_dummies = pd 22\n",
      "get_dummies(titanic 6\n",
      "Embarked, prefix='Embarked')\n",
      "embarked_dummies 12\n",
      "drop(embarked_dummies 6\n",
      "columns[0], axis=1, inplace=True)\n",
      "# concatenate the original DataFrame and the dummy DataFrame\n",
      "titanic = pd 26\n",
      "concat([titanic, embarked_dummies], axis=1)\n",
      "# define X and y\n",
      "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
      "X = titanic[feature_cols]\n",
      "y = titanic 55\n",
      "Survived\n",
      "X 4\n",
      "head()\n",
      "\n",
      "\n",
      "Note that we are going to use class, sex, age, and dummy variables for city embarked as our\n",
      "features:\n",
      "# fit a classification tree with max_depth=3 on all data from sklearn 41\n",
      "tree\n",
      "import DecisionTreeClassifier treeclf = DecisionTreeClassifier(max_depth=3,\n",
      "random_state=1) treeclf 24\n",
      "fit(X, y)\n",
      "max_depth  is a limit to the depth of our tree 17\n",
      "It means that, for any data point, our tree is\n",
      "only able to ask up to three questions and make up to three  splits 28\n",
      "We can output our tree\n",
      "into a visual  format and we will obtain the following:\n",
      "We can notice a few things:\n",
      "Sex is the first split, meaning that sex is the most important determining factor of\n",
      "whether or not a person survived the crash\n",
      "Embarked_Q  was never used in any split\n",
      "For either classification or regression trees, we can also do something very interesting with\n",
      "decision trees, which is that we can output a number that represents each feature's\n",
      "importance in the prediction of our data points:\n",
      "# compute the feature importances\n",
      "pd 113\n",
      "DataFrame({'feature':feature_cols,\n",
      "'importance':treeclf 13\n",
      "feature_importances_})\n",
      "\n",
      "\n",
      "The importance scores are an average  Gini index difference for each variable, with higher \n",
      "values  corresponding to higher importance  to the prediction 33\n",
      "We can use this information to\n",
      "select fewer features in the future 13\n",
      "For example, both of the embarked variables are very\n",
      "low in comparison to the rest of the features, so we may be able to say that they are not\n",
      "important in our prediction of life or death 41\n",
      "\n",
      "Unsupervised learning\n",
      "It's time to see some examples of unsupervised  learning, given that we spend a majority of\n",
      "this book on supervised learning models 34\n",
      "\n",
      "When to use unsupervised learning\n",
      "There are many times when unsupervised learning  can be appropriate 22\n",
      "Some very common\n",
      "examples include the following:\n",
      "There is no clear response  variable 16\n",
      "There is nothing that we are explicitly trying\n",
      "to predict or correlate to other variables 16\n",
      "\n",
      "To extract structure from data where no apparent structure/patterns exist (can be\n",
      "a supervised learning problem) 23\n",
      "\n",
      "When an unsupervised concept called feature extraction is used 12\n",
      "Feature\n",
      "extraction is the process of creating new features from existing ones 14\n",
      "These new\n",
      "features can be even stronger than the original features 12\n",
      "\n",
      "\n",
      "The first tends to be the most common reason that data scientists choose to use\n",
      "unsupervised learning 21\n",
      "This case arises frequently when we are working with data and we\n",
      "are not explicitly trying to predict any of the columns and we merely wish to find patterns\n",
      "of similar (and dissimilar) groups of points 41\n",
      "The second option comes into play even if we\n",
      "are explicitly attempting to use a supervised model to predict a response variable 23\n",
      "\n",
      "Sometimes, simple EDA might not produce any clear patterns in the data in the few\n",
      "dimensions that humans can imagine, whereas a machine might pick up on data points\n",
      "behaving similarly to each other in greater dimensions 44\n",
      "\n",
      "The third common reason to use unsupervised learning is to extract new features from\n",
      "features that already exist 22\n",
      "This process (lovingly called feature extraction ) might produce\n",
      "features that can be used in a future supervised model or that can be used for presentation\n",
      "purposes (marketing or otherwise) 38\n",
      "\n",
      "k-means clustering\n",
      "k-means clustering is our first example  of an unsupervised machine learning model 23\n",
      "\n",
      "Remember this means that we are not making predictions; we are trying instead to extract\n",
      "structure from seemingly unstructured data 24\n",
      "\n",
      "Clustering is a family of unsupervised machine learning models that attempt to group data\n",
      "points into clusters  with centroids 25\n",
      "\n",
      "Definition\n",
      "Cluster : This is a group of data points that behave similarly 15\n",
      "\n",
      "Centroid : This is the center  of a cluster 12\n",
      "It can be thought of as an average\n",
      "point in the cluster 13\n",
      "\n",
      "The preceding definition can be quite vague, but it becomes specific when narrowed down\n",
      "to specific domains 20\n",
      "For example, online shoppers who behave similarly might shop for\n",
      "similar things or at similar shops, whereas similar software companies might make\n",
      "comparable software at comparable prices 32\n",
      "\n",
      "\n",
      "Here is a visualization of clusters of points:\n",
      "In the preceding diagram, our human brains can very easily see the difference between the\n",
      "four clusters 29\n",
      "We can see that the red cluster is at the bottom-left of the graph while the\n",
      "green cluster lives in the bottom-right portion of the graph 29\n",
      "This means that the red data\n",
      "points are similar to each other, but not similar to data points in the other clusters 24\n",
      "\n",
      "We can also see the centroids of each cluster as the square in each color 16\n",
      "Note that the\n",
      "centroid is not an actual data point, but is merely an abstraction of a cluster that represents\n",
      "the center of the cluster 28\n",
      "\n",
      "The concept of similarity  is central to the definition of a cluster, and therefore to cluster\n",
      "analysis 21\n",
      " ",
      "In general, greater similarity between points leads to better clustering 14\n",
      "In most\n",
      "cases, we turn data into points in n-dimensional space and use the distance between these\n",
      "points as a form of similarity 27\n",
      "The centroid of the cluster then is usually the average of each\n",
      "dimension (column) for each data point in each cluster 24\n",
      "So, for example, the centroid of the\n",
      "red cluster is the result of taking the average value of each column of each red data point 28\n",
      "\n",
      "\n",
      "The purpose of cluster analysis is to enhance our understanding of a dataset by dividing\n",
      "the data into groups 21\n",
      "Clustering provides a layer of abstraction from individual data points 11\n",
      "\n",
      "The goal is to extract and enhance the natural structure of the data 14\n",
      "There are many kinds of\n",
      "classification procedures 8\n",
      "For our class, we will be focusing on k-means clustering, which is\n",
      "one of the most popular clustering algorithms 24\n",
      "\n",
      "k-means is an iterative method that partitions  a dataset into k clusters 16\n",
      "It works in four\n",
      "steps:\n",
      "Choose k initial centroids (note that k is an input) 1 21\n",
      "\n",
      "For each point, assign the point to the nearest centroid2 13\n",
      "\n",
      "Recalculate the centroid positions3 7\n",
      "\n",
      "Repeat Step 2  and Step 3  until stopping criteria is met 4 18\n",
      "\n",
      "Illustrative example – data points\n",
      "Imagine that we have the following  data points in a two-dimensional space:\n",
      "Each dot is colored gray to assume no prior grouping before applying the k-means\n",
      "algorithm 41\n",
      "The goal here is to eventually color in each dot and create groupings (clusters),\n",
      "as illustrated in the following plot:\n",
      "\n",
      "\n",
      "Here, Step 1  has been applied 33\n",
      "We have (randomly) chosen three centroids (red, blue, and\n",
      "yellow) 18\n",
      "\n",
      "Most k-means algorithms place random initial centroids, but there exist\n",
      "other pre-compute methods to place initial centroids 24\n",
      "For now, random is\n",
      "fine 7\n",
      "\n",
      "\n",
      "\n",
      "The first part of Step 2  has been applied 12\n",
      "For each data point, we found the most similar\n",
      "centroid (closest):\n",
      "The second part of Step 2  has been applied here 27\n",
      "We have colored in each data point in\n",
      "accordance with its most similar centroid:\n",
      "This is Step 3  and the crux of k-means 32\n",
      "Note that we have physically moved the centroids to\n",
      "be the actual center of each cluster 17\n",
      "We have, for each color, computed the average  point\n",
      "and made that point the new centroid 20\n",
      "For example, suppose the three red data points had\n",
      "the coordinates: ( 1, 3 ), (2, 5 ), and (3, 4 ) 34\n",
      "The center ( red cross ) would be calculated as\n",
      "follows:\n",
      "# centroid calculation\n",
      "import numpy as np\n",
      "red_point1 = np 28\n",
      "array([1, 3])\n",
      "red_point2 = np 12\n",
      "array([2, 5])\n",
      "red_point3 = np 12\n",
      "array([3, 4])\n",
      "\n",
      "red_center = (red_point1 + red_point2 + red_point3) / 3 26\n",
      "\n",
      "red_center\n",
      "# array([ 2 9\n",
      ",  4 4\n",
      "\n",
      "That is, the ( 2, 4 ) point would be the coordinates of the preceding red cross 22\n",
      "\n",
      "None of the actual data points will ever move 10\n",
      "They cannot 2\n",
      "The only\n",
      "entities that move are the centroids, which are not actual data points 16\n",
      "\n",
      "We continue with our algorithm by repeating Step 2 11\n",
      "Here is the first part where we find the\n",
      "closest center for each point 15\n",
      "Note a big change: the point that is circled in the following\n",
      "figure used to be a yellow point, but has changed to be a red cluster point because the\n",
      "yellow cluster moved closer to its yellow constituents:\n",
      "\n",
      "\n",
      "It might help to think of points as being planets in space with\n",
      "gravitational pull 61\n",
      "Each centroid is pulled by the planets' gravity 9\n",
      "\n",
      "Here is the second part of Step 2  again 12\n",
      "We have assigned each point to the color of the\n",
      "closest cluster:\n",
      "Here, we recalculate once more the centroids for each cluster ( Step 3 ) 31\n",
      "Note that the blue\n",
      "center did not move at all, while the yellow and red centers both moved 20\n",
      "\n",
      "Because we have reached a stopping  criterion (clusters do not move if we repeat Step 2  and\n",
      "Step 3 ), we finalize our algorithm and we have our three clusters, which is the final result of\n",
      "the k-means algorithm:\n",
      "\n",
      "\n",
      "Illustrative example – beer 56\n",
      "\n",
      "Enough data  science, beer 7\n",
      "\n",
      "OK, OK, settle down 7\n",
      "It's a long book; let's grab a beer 11\n",
      "On that note, did you know there\n",
      "are many types of beer 14\n",
      "I wonder if we could possibly group beers into different categories\n",
      "based on different quantitative features … Let's try 21\n",
      "See the following:\n",
      "# import the beer dataset\n",
      "url = ' 13\n",
      " 1\n",
      "/data/beer 4\n",
      "txt'\n",
      "beer = pd 5\n",
      "read_csv(url, sep=' ')\n",
      "print beer 9\n",
      "shape\n",
      "The output is as follows:\n",
      "(20, 5)\n",
      "beer 15\n",
      "head()\n",
      "\n",
      "\n",
      "Here we have 20 beers with five columns: name , calories , sodium , alcohol , and cost 22\n",
      "In\n",
      "clustering (like almost all machine learning models), we like quantitative features, so we\n",
      "will ignore the name of the beer in our clustering:\n",
      "# define X\n",
      "X = beer 38\n",
      "drop('name', axis=1)\n",
      "Now we will perform k-means using scikit-learn:\n",
      "# K-means with 3 clusters\n",
      "from sklearn 32\n",
      "cluster import KMeans\n",
      "km = KMeans(n_clusters=3, random_state=1)\n",
      "km 20\n",
      "fit(X)\n",
      "n_clusters  is our k 9\n",
      "It is our input number of clusters 7\n",
      "random_state  as\n",
      "always produces reproducible results for educational purposes 13\n",
      "Using\n",
      "three clusters for now is random 8\n",
      "\n",
      "Our k-means algorithm has run the algorithm on our data points and come up with three\n",
      "clusters:\n",
      "# save the cluster labels and sort by cluster\n",
      "beer['cluster'] = km 38\n",
      "labels_\n",
      "We can take a look at the center of each cluster by using a groupby  and mean  statement:\n",
      "# calculate the mean of each feature for each cluster\n",
      "beer 36\n",
      "groupby('cluster') 5\n",
      "mean()\n",
      "On human inspection, we can see that cluster 0 has, on average, a higher calorie, sodium,\n",
      "and alcohol content and costs more 30\n",
      "These might be considered heavier beers 6\n",
      "Cluster 2 has\n",
      "on average a very low alcohol content and very few calories 16\n",
      "These are probably light beers 5\n",
      "\n",
      "Cluster 1 is somewhere in the middle 9\n",
      "\n",
      "\n",
      "Let's use Python to make  a graph to see this in more detail:\n",
      "import matplotlib 19\n",
      "pyplot as plt\n",
      "%matplotlib inline\n",
      "# save the DataFrame of cluster centers\n",
      "centers = beer 19\n",
      "groupby('cluster') 5\n",
      "mean()\n",
      "# create a \"colors\" array for plotting\n",
      "colors = np 15\n",
      "array(['red', 'green', 'blue', 'yellow'])\n",
      "# scatter plot of calories versus alcohol, colored by cluster (0=red,\n",
      "1=green, 2=blue)\n",
      "plt 39\n",
      "scatter(beer 3\n",
      "calories, beer 3\n",
      "alcohol, c=colors[list(beer 8\n",
      "cluster)],\n",
      "s=50)\n",
      "# cluster centers, marked by \"+\"\n",
      "plt 15\n",
      "scatter(centers 3\n",
      "calories, centers 3\n",
      "alcohol, linewidths=3, marker='+',\n",
      "s=300, c='black')\n",
      "# add labels\n",
      "plt 23\n",
      "xlabel('calories')\n",
      "plt 6\n",
      "ylabel('alcohol')\n",
      "A big part of unsupervised learning is human inspection 16\n",
      "Clustering has\n",
      "no context of the problem domain and can only tell us the clusters it found\n",
      "it cannot tell us what the clusters mean 28\n",
      "\n",
      "\n",
      "\n",
      "Choosing an optimal number for K and\n",
      "cluster validation\n",
      "A big part of k-means clustering is knowing the optimal number of clusters 27\n",
      "If we knew this\n",
      "number ahead of time, then that might defeat the purpose of even using  unsupervised\n",
      "learning 25\n",
      "So we need a way to evaluate the output of our cluster analysis 13\n",
      "\n",
      "The problem here is that, because we are not performing any kind of prediction, we cannot\n",
      "gauge how right  the algorithm is at predictions 30\n",
      "Metrics such as accuracy and RMSE go\n",
      "right out of the window 14\n",
      "\n",
      "The Silhouette Coefficient\n",
      "The Silhouette Coefficient  is a common metric  for evaluating clustering performance in\n",
      "situations when the true cluster assignments are not known 35\n",
      "\n",
      "A Silhouette Coefficient is calculated for each observation as follows:\n",
      " \n",
      "Let's look a little closer at the specific features of this formula:\n",
      "a: Mean distance to all other points in its cluster\n",
      "b: Mean distance to all other points in the next nearest cluster\n",
      "It ranges from -1 (worst) to 1 (best) 70\n",
      "A global score  is calculated by taking the mean score\n",
      "for all observations 15\n",
      "In general, a Silhouette Coefficient of 1 is preferred, while a score of -1\n",
      "is not preferable:\n",
      "# calculate Silhouette Coefficient for K=3\n",
      "from sklearn import metrics\n",
      "metrics 42\n",
      "silhouette_score(X, km 5\n",
      "labels_)\n",
      "The output is as follows:\n",
      "0 9\n",
      "67317750464557957\n",
      "\n",
      "Let's try calculating the coefficient for multiple values of K to find the best value:\n",
      "# center and scale the data\n",
      "from sklearn 34\n",
      "preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "X_scaled = scaler 15\n",
      "fit_transform(X)\n",
      "# calculate SC for K=2 through K=19\n",
      "k_range = range(2, 20)\n",
      "scores = []\n",
      "for k in k_range:\n",
      "    km = KMeans(n_clusters=k, random_state=1)\n",
      "    km 51\n",
      "fit(X_scaled)\n",
      "    scores 6\n",
      "append(metrics 2\n",
      "silhouette_score(X, km 5\n",
      "labels_))\n",
      "# plot the results\n",
      "plt 9\n",
      "plot(k_range, scores)\n",
      "plt 7\n",
      "xlabel('Number of clusters')\n",
      "plt 7\n",
      "ylabel('Silhouette Coefficient')\n",
      "plt 8\n",
      "grid(True)\n",
      "So, it looks like our optimal number of beer clusters is 4 17\n",
      "This means that our k-means\n",
      "algorithm has determined that there seem to be four distinct types of beer 21\n",
      "\n",
      "\n",
      "k-means is a popular algorithm because of its computational  efficiency and simple and\n",
      "intuitive nature 21\n",
      "k-means, however, highly scale dependent and is not suitable for data with\n",
      "widely varying shapes and densities 23\n",
      "There are ways to combat this issue by scaling data\n",
      "using scikit-learn's standard scalar:\n",
      "# center and scale the data\n",
      "from sklearn 29\n",
      "preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "X_scaled = scaler 15\n",
      "fit_transform(X)\n",
      "# K-means with 3 clusters on scaled data\n",
      "km = KMeans(n_clusters=3, random_state=1)\n",
      "km 31\n",
      "fit(X_scaled)\n",
      "Easy 5\n",
      "\n",
      "Now let's take a look at the third reason to use unsupervised methods that falls under the\n",
      "third option in our reasons to use unsupervised methods: feature extraction 36\n",
      "\n",
      "Feature extraction and principal component\n",
      "analysis\n",
      "Sometimes we have an overwhelming  number of columns and likely  not enough rows to\n",
      "handle the great quantity of columns 32\n",
      "\n",
      "A great example of this is when we were looking at the send cash now  example in our\n",
      "Nave Bayes example 26\n",
      "We had literally 0 instances of texts with that exact phrase, so instead,\n",
      "we turned to a naïve assumption that allowed us to extrapolate a probability for both of our\n",
      "categories 37\n",
      "\n",
      "The reason we had this problem in the first place is because of something called the curse of\n",
      "dimensionality 22\n",
      "The curse of dimensionality basically says that as we introduce and\n",
      "consider new feature columns, we need almost exponentially more rows (data points) in\n",
      "order to fill in the empty spaces that we create 40\n",
      "\n",
      "\n",
      "Consider an example where we attempt to use a learning model that utilizes the distance\n",
      "between points on a corpus of text that has 4,086 pieces of text and that the whole thing has\n",
      "been Countvectorized 44\n",
      "Let's assume that these texts between them have 18,884 words:\n",
      "X 16\n",
      "shape\n",
      "The output is as follows:\n",
      "(20, 4)\n",
      "Now, let's do an experiment 21\n",
      "I will first consider a single word as the only dimension of our\n",
      "text 15\n",
      "Then, I will count how many of pieces of text are within 1 unit of each other 19\n",
      "For\n",
      "example, if two sentences both contain that word, they would be 0 units away\n",
      "and, similarly, if neither of them contain the word, they would be 0 units away from one\n",
      "another:\n",
      "d = 1\n",
      "# Let's look for points within 1 unit of one another\n",
      "X_first_word = X 68\n",
      "iloc[:,:1]\n",
      "# Only looking at the first column, but ALL of the rows\n",
      "from sklearn 21\n",
      "neighbors import NearestNeighbors\n",
      "# this module will calculate for us distances between each point\n",
      "neigh = NearestNeighbors(n_neighbors=4086)\n",
      "neigh 32\n",
      "fit(X_first_word)\n",
      "# tell the module to calculate each distance between each point\n",
      "Note that we have 16,695,396 ( 4086*4086 ) distances to scan over 39\n",
      "\n",
      "A = neigh 4\n",
      "kneighbors_graph(X_first_word, mode='distance') 11\n",
      "todense()\n",
      "# This matrix holds all distances (over 16 million of them)\n",
      "num_points_within_d = (A < d) 27\n",
      "sum()\n",
      "# Count the number of pairs of points within 1 unit of distance\n",
      "num_points_within_d\n",
      "16258504\n",
      "\n",
      "So, 16 30\n",
      "2 million pairs of texts are within a single unit of distance 13\n",
      "Now, let's try again with\n",
      "the first two words:\n",
      "X_first_two_words = X 19\n",
      "iloc[:,:2]\n",
      "neigh = NearestNeighbors(n_neighbors=4086)\n",
      "neigh 19\n",
      "fit(X_first_two_words)\n",
      "A = neigh 9\n",
      "kneighbors_graph(X_first_two_words, mode='distance') 12\n",
      "todense()\n",
      "num_points_within_d = (A < d) 13\n",
      "sum()\n",
      "num_points_within_d\n",
      "16161970\n",
      "Great 12\n",
      "By adding this new column, we lost about 100,000 pairs of points that were within a\n",
      "single unit of distance 25\n",
      "This is because we are adding space in between  them for every \n",
      "dimension  that we add 19\n",
      "Let's take this test a step further and calculate this number for the\n",
      "first 100 words and then plot the results:\n",
      "d = 1\n",
      "# Scan for points within one unit\n",
      "num_columns = range(1, 100)\n",
      "# Looking at the first 100 columns\n",
      "points = []\n",
      "# We will be collecting the number of points within 1 unit for a graph\n",
      "neigh = NearestNeighbors(n_neighbors=X 86\n",
      "shape[0])\n",
      "for subset in num_columns:\n",
      "    X_subset = X 15\n",
      "iloc[:,:subset]\n",
      "  # look at the first column, then first two columns, then first three\n",
      "columns, etc\n",
      "    neigh 28\n",
      "fit(X_subset)\n",
      "    A = neigh 8\n",
      "kneighbors_graph(X_subset, mode='distance') 10\n",
      "todense()\n",
      "    num_points_within_d = (A < d) 14\n",
      "sum()\n",
      "# calculate the number of points within 1 unit\n",
      "    points 15\n",
      "append(num_points_within_d)\n",
      "Now, let's plot the number of points within 1 unit versus the number of dimensions we\n",
      "looked at:\n",
      "\n",
      "\n",
      "We can see clearly that the number of points within a single unit of one another goes down\n",
      "dramatically as we introduce more and more columns 59\n",
      "And this is only the first 100\n",
      "columns 10\n",
      "Let's see how many points are within a single unit by the time we consider all\n",
      "18,000+ words:\n",
      "neigh = NearestNeighbors(n_neighbors=4086)\n",
      "neigh 38\n",
      "fit(X)\n",
      "A = neigh 6\n",
      "kneighbors_graph(X, mode='distance') 9\n",
      "todense()\n",
      "num_points_within_d = (A < d) 13\n",
      "sum()\n",
      "num_points_within_d\n",
      "4090\n",
      "By the end, only 4,000 sentences are within a unit of one another 27\n",
      "All of this space that we\n",
      "add in by considering new columns makes it harder for the finite amount of points we have\n",
      "to stay happily within range of each other 33\n",
      "We would have to add in more points in order\n",
      "to fill in this gap 16\n",
      "And that, my friends, is why we should consider using dimension\n",
      "reduction 16\n",
      "\n",
      "\n",
      "The curse of dimensionality is solved by either adding more data points (which is not\n",
      "always possible) or implementing dimension reduction 26\n",
      "Dimension reduction is simply the\n",
      "act of reducing the number of columns in our dataset and not the number of rows 22\n",
      "There\n",
      "are two ways of implementing dimension reduction:\n",
      "Feature selection : This is the act of subsetting our column features and only\n",
      "using the best features\n",
      "Feature extraction : This is the act of mathematically transforming our feature set\n",
      "into a new extracted coordinate system\n",
      "We are familiar with feature selection as the process of saying the \" Embarked_Q \" is not\n",
      "helping my decision tree; let's get rid of it and see how it performs 91\n",
      "It is literally when we (or the\n",
      "machine) make the decision to ignore certain columns 18\n",
      "\n",
      "Feature extraction is a bit trickier …\n",
      "In feature extraction, we are using usually fairly complicated mathematical formulas in\n",
      "order to obtain new super columns  that are usually better than any single original column 40\n",
      "\n",
      "Our primary model for doing so is called Principal Component Analysis  (PCA ) 16\n",
      "PCA will\n",
      "extract a set number of super columns in order  to represent our original  data with much\n",
      "fewer columns 25\n",
      "Let's take a concrete example 6\n",
      "Previously, I mentioned some text with 4,086\n",
      "rows and over 18,000 columns 20\n",
      "That dataset is actually a set of Yelp online reviews:\n",
      "url = ' 14\n",
      " 1\n",
      "/data/yelp 4\n",
      "csv'\n",
      "yelp = pd 6\n",
      "read_csv(url, encoding='unicode-escape')\n",
      "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
      "yelp_best_worst = yelp[(yelp 39\n",
      "stars==5) | (yelp 8\n",
      "stars==1)]\n",
      "# define X and y\n",
      "X = yelp_best_worst 17\n",
      "text\n",
      "y = yelp_best_worst 9\n",
      "stars == 5\n",
      "Our goal is to predict whether or not a person gave a 5- or 1-star review  based on the words\n",
      "they used in the review 36\n",
      "Let's set a base line with logistic regression and see how well we\n",
      "can predict this binary category:\n",
      "from sklearn 23\n",
      "linear_model import LogisticRegression\n",
      "lr = LogisticRegression()\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
      "# Make our training and testing sets\n",
      "vect = CountVectorizer(stop_words='english')\n",
      "# Count the number of words but remove stop words like a, an, the, you, etc\n",
      "\n",
      "X_train_dtm = vect 79\n",
      "fit_transform(X_train)\n",
      "X_test_dtm = vect 11\n",
      "transform(X_test)\n",
      "# transform our text into document term matrices\n",
      "lr 14\n",
      "fit(X_train_dtm, y_train)\n",
      "# fit to our training set\n",
      "lr 17\n",
      "score(X_test_dtm, y_test)\n",
      "# score on our testing set\n",
      "The output is as follows:\n",
      "0 23\n",
      "91193737\n",
      "So, by utilizing all of the words in our corpus, our model seems to have over a 91%\n",
      "accuracy 28\n",
      "Not bad 2\n",
      "\n",
      "Let's try only using the top 100 used words:\n",
      "vect = CountVectorizer(stop_words='english', max_features=100)\n",
      "# Only use the 100 most used words\n",
      "X_train_dtm = vect 43\n",
      "fit_transform(X_train)\n",
      "X_test_dtm = vect 11\n",
      "transform(X_test)\n",
      "print( X_test_dtm 10\n",
      "shape) # (1022, 100)\n",
      "lr 11\n",
      "fit(X_train_dtm, y_train)\n",
      "lr 10\n",
      "score(X_test_dtm, y_test)\n",
      "The output is as follows:\n",
      "0 16\n",
      "8816\n",
      "Note how our training and testing matrices have 100 columns 15\n",
      "This is because I told our\n",
      "vectorizer to only look at the top 100 words 18\n",
      "See also that our performance took a hit and is\n",
      "now down to 88% accuracy 18\n",
      "This makes sense because we are ignoring over 4,700 words in\n",
      "our corpus 17\n",
      "\n",
      "Now, let's take a different approach 9\n",
      "Let's import a PCA module and tell it to make us 100\n",
      "new super columns and see how that performs:\n",
      "from sklearn import decomposition\n",
      "# We will be creating 100 super columns\n",
      "vect = CountVectorizer(stop_words='english')\n",
      "# Don't ignore any words\n",
      "pca = decomposition 59\n",
      "PCA(n_components=100)\n",
      "# instantate a pca object\n",
      "\n",
      "X_train_dtm = vect 19\n",
      "fit_transform(X_train) 5\n",
      "todense()\n",
      "# A dense matrix is required to pass into PCA, does not affect the overall\n",
      "message\n",
      "X_train_dtm = pca 28\n",
      "fit_transform(X_train_dtm)\n",
      "X_test_dtm = vect 13\n",
      "transform(X_test) 4\n",
      "todense()\n",
      "X_test_dtm = pca 9\n",
      "transform(X_test_dtm)\n",
      "print( X_test_dtm 12\n",
      "shape) # (1022, 100)\n",
      "lr 11\n",
      "fit(X_train_dtm, y_train)\n",
      "lr 10\n",
      "score(X_test_dtm, y_test)\n",
      "The output is as follows:\n",
      " 15\n",
      "89628\n",
      "Not only do our matrices still have 100 columns, but these columns are no longer words in\n",
      "our corpus 26\n",
      "They are complex transformations of columns and are 100 new columns 12\n",
      "Also\n",
      "note that using 100 of these new columns gives us a better predictive performance than\n",
      "using the 100 top words 25\n",
      "\n",
      "Feature extraction is a great way to use mathematical formulas to extract brand new\n",
      "columns that generally perform better than just selecting the best ones beforehand 28\n",
      "\n",
      "But how do we visualize these new super columns 10\n",
      "Well, I can think of no better way than\n",
      "to look at an example using image analysis 19\n",
      "Specifically, let's make facial recognition\n",
      "software 9\n",
      "OK 1\n",
      "OK 1\n",
      "Let's begin by importing some faces given to us by scikit-learn:\n",
      "from sklearn 18\n",
      "datasets import fetch_lfw_people\n",
      "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0 24\n",
      "4)\n",
      "# introspect the images arrays to find the shapes (for plotting)\n",
      "n_samples, h, w = lfw_people 27\n",
      "images 1\n",
      "shape\n",
      "# for machine learning we use the 2 data directly (as relative pixel #\n",
      "positions info is ignored by this model)\n",
      "X = lfw_people 31\n",
      "data\n",
      "y = lfw_people 7\n",
      "target\n",
      "n_features = X 6\n",
      "shape[1]\n",
      "X 5\n",
      "shape (1288, 1850)\n",
      "\n",
      "We have gathered 1,288 images of people's faces, and each one has 1,850 features (pixels)\n",
      "that identify  that person 39\n",
      "Here's an example:\n",
      "plt 6\n",
      "imshow(X[0] 5\n",
      "reshape((h, w)), cmap=plt 8\n",
      "cm 1\n",
      "gray)\n",
      "lfw_people 4\n",
      "target_names[y[0]] 'Hugo Chavez'\n",
      "plt 12\n",
      "imshow(X[100] 5\n",
      "reshape((h, w)), cmap=plt 8\n",
      "cm 1\n",
      "gray)\n",
      "lfw_people 4\n",
      "target_names[y[100]] 'George W Bush'\n",
      "\n",
      "\n",
      "Great 12\n",
      "To get a glimpse at the type of dataset we are looking at, let's look at a few overall\n",
      "metrics:\n",
      "# the label to predict is the id of the person\n",
      "target_names = lfw_people 42\n",
      "target_names\n",
      "n_classes = target_names 8\n",
      "shape[0]\n",
      "print(\"Total dataset size:\")\n",
      "print(\"n_samples: %d\" % n_samples)\n",
      "print(\"n_features: %d\" % n_features)\n",
      "print(\"n_classes: %d\" % n_classes)\n",
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n",
      "So, we have 1,288 images, 1,850 features, and 7 classes (people) to choose from 96\n",
      "Our goal is\n",
      "to make a classifier that will assign the person's face a name based on the 1,850 pixels given\n",
      "to us 29\n",
      "\n",
      "Let's take a base line and see how logistic regression performs on our data without doing\n",
      "anything:\n",
      "from sklearn 23\n",
      "linear_model import LogisticRegression\n",
      "from sklearn 8\n",
      "metrics import accuracy_score\n",
      "from time import time # for timing our work\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "X, y, test_size=0 39\n",
      "25, random_state=1)\n",
      "# get our training and test set\n",
      "t0 = time() # get the time now\n",
      "logreg = LogisticRegression()\n",
      "logreg 35\n",
      "fit(X_train, y_train)\n",
      "# Predicting people's names on the test set\n",
      "y_pred = logreg 23\n",
      "predict(X_test)\n",
      "print( accuracy_score(y_pred, y_test), \"Accuracy\")\n",
      "print( (time() - t0), \"seconds\" )\n",
      "The output is as follows:\n",
      "0 37\n",
      "810559006211 Accuracy\n",
      "6 8\n",
      "31762504578 seconds\n",
      "So, within 6 12\n",
      "3 seconds, we were able to get an 81% on our test set 17\n",
      "Not too bad 3\n",
      "\n",
      "\n",
      "Now let's try this with our super faces :\n",
      "# split into a training and testing set\n",
      "from sklearn 22\n",
      "cross_validation import train_test_split\n",
      "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled #\n",
      "dataset): unsupervised feature extraction / dimensionality reduction\n",
      "n_components = 75\n",
      "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
      "% (n_components, X_train 67\n",
      "shape[0]))\n",
      "pca = decomposition 8\n",
      "PCA(n_components=n_components,\n",
      "whiten=True) 10\n",
      "fit(X_train)\n",
      "# This whiten parameter speeds up the computation of our extracted columns\n",
      "# Projecting the input data on the eigenfaces orthonormal basis\n",
      "X_train_pca = pca 39\n",
      "transform(X_train)\n",
      "X_test_pca = pca 10\n",
      "transform(X_test)\n",
      "The preceding code is collecting 75 extracted columns from our 1,850 unprocessed columns 22\n",
      "\n",
      "These are our super faces 6\n",
      "Now, let's plug in our newly extracted columns into our logistic\n",
      "regression and compare:\n",
      "t0 = time()\n",
      "# Predicting people's names on the test set WITH PCA\n",
      "logreg 39\n",
      "fit(X_train_pca, y_train)\n",
      "y_pred = logreg 14\n",
      "predict(X_test_pca)\n",
      "print accuracy_score(y_pred, y_test), \"Accuracy\"\n",
      "print (time() - t0), \"seconds\"\n",
      "0 30\n",
      "82298136646 Accuracy\n",
      "0 8\n",
      "194181919098 seconds\n",
      "Wow 8\n",
      "Not only was this entire calculation about 30 times faster than the unprocessed\n",
      "images, but the predictive performance also got better 25\n",
      "This shows us that PCA and feature\n",
      "extraction, in general, can help us all around when performing machine learning on\n",
      "complex datasets with many columns 30\n",
      "By searching for these patterns in the dataset and\n",
      "extracting new feature  columns, we can speed up and enhance  our learning algorithms 27\n",
      "\n",
      "Let's look at one more interesting thing 9\n",
      "I mentioned before that one of the purposes of this\n",
      "example was to examine and visualize our eigenfaces , as they are called: our super columns 29\n",
      "\n",
      "I will not disappoint 5\n",
      "Let's write some code that will show us our super columns as they\n",
      "would look to us humans:\n",
      "def plot_gallery(images, titles, n_row=3, n_col=4):\n",
      "\"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
      "plt 49\n",
      "figure(figsize=(1 4\n",
      "8 * n_col, 2 8\n",
      "4 * n_row))\n",
      "plt 7\n",
      "subplots_adjust(bottom=0, left= 9\n",
      "01, right= 5\n",
      "99, top= 5\n",
      "90, hspace= 6\n",
      "35)\n",
      "for i in range(n_row * n_col):\n",
      "\n",
      "plt 14\n",
      "subplot(n_row, n_col, i + 1)\n",
      "plt 13\n",
      "imshow(images[i], cmap=plt 6\n",
      "cm 1\n",
      "gray)\n",
      "plt 3\n",
      "title(titles[i], size=12)\n",
      "# plot the gallery of the most significative eigenfaces\n",
      "eigenfaces = pca 26\n",
      "components_ 2\n",
      "reshape((n_components, h, w))\n",
      "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces 30\n",
      "shape[0])]\n",
      "plot_gallery(eigenfaces, eigenface_titles)\n",
      "plt 15\n",
      "show()\n",
      "\n",
      "\n",
      "Wow 3\n",
      "A haunting and yet beautiful representation of what the data believes to be the most\n",
      "importance features of a face 22\n",
      "As we move from the top-left (first super column) to the\n",
      "bottom, it is actually somewhat easy to see what the image is trying to tell us 32\n",
      "The first\n",
      "super column looks like a very general face structure with eyes and nose and a mouth 19\n",
      "It is\n",
      "almost saying \"I represent the basic qualities of a face that all faces must have 19\n",
      "Our second\n",
      "super column directly to its right seems to be telling us about shadows in the image 19\n",
      "The\n",
      "next one might be telling us that skin tone plays a role in detecting who this is, which might\n",
      "be why the third face is much darker than the first two 35\n",
      "\n",
      "Using feature extraction unsupervised learning methods such as PCA can give us a very\n",
      "deep look into our data and reveal to us what the data believes to be the most important\n",
      "features, not just what we believe them to be 47\n",
      "Feature extraction is a great preprocessing\n",
      "tool that can speed up our future learning methods, make them more powerful, and give us\n",
      "more insight into how the data believes it should be viewed 37\n",
      "To sum up this section, we\n",
      "will list the pros and cons 14\n",
      "\n",
      "Here are the pros of using feature extraction:\n",
      "Our models become much faster\n",
      "Our predictive performance can become better\n",
      "It can give us insight into the extracted features (eigenfaces)\n",
      "And here are the cons of using feature extraction:\n",
      "We lose some of the interpretability  of our features as they  are new\n",
      "mathematically-derived columns, not our old ones\n",
      "We can lose predictive performance because we are losing information as we\n",
      "extract fewer columns\n",
      "\n",
      "Summary\n",
      "Between decision trees, Na ïve Bayes classification, feature extraction, and k-means\n",
      "clustering, we have seen that machine learning goes way beyond the simplicity of linear\n",
      "and logistic regression and can solve many types of complicated problems 142\n",
      "\n",
      "We also saw examples of both supervised and unsupervised learning and, in doing so,\n",
      "became familiar with many types of data science related problems 30\n",
      "\n",
      "In the next chapter, we will be looking at even more complicated learning algorithms,\n",
      "including artificial neural networks and ensembling techniques 25\n",
      "We will also see and\n",
      "understand more complicated concepts in data science, including the bias-variance\n",
      "tradeoff, as well as the concept of overfitting 33\n",
      "\n",
      "\n",
      "2\n",
      "Beyond the Essentials\n",
      "In this chapter,  we will be discussing some of the more complicated parts of data science\n",
      "that can put some people off 32\n",
      "The reason for this is that data science is not all fun and\n",
      "machine learning 16\n",
      "Sometimes, we have to discuss and consider theoretical and\n",
      "mathematical paradigms and evaluate our procedures 21\n",
      "\n",
      "This chapter will explore many of these procedures step by step so that we completely and\n",
      "totally understand the topics 23\n",
      "We will be discussing topics such as the following:\n",
      "Cross-validation\n",
      "The bias/variance tradeoff\n",
      "Overfitting and underfitting\n",
      "Ensembling techniques\n",
      "Random forests\n",
      "Neural networks\n",
      "These are only some of the topics to be covered 49\n",
      "At no point do I want you to be confused 10\n",
      "I\n",
      "will attempt to explain each procedure/algorithm with the utmost care and with many\n",
      "examples and visuals 21\n",
      "\n",
      "\n",
      "The bias/variance tradeoff\n",
      "We have discussed the concept of bias and variance  briefly in the previous chapters 23\n",
      "When\n",
      "we are discussing these two concepts, we are generally speaking of supervised learning\n",
      "algorithms 19\n",
      "We are specifically talking about deriving errors from our predictive models\n",
      "due to bias and variance 17\n",
      "\n",
      "Errors due to bias\n",
      "When speaking of errors due to bias, we are speaking  of the difference between the\n",
      "expected prediction of our model and the actual (correct) value, which we are trying to\n",
      "predict 44\n",
      "Bias, in effect, measures how far, in general, our model's predictions are from the\n",
      "correct value 22\n",
      "\n",
      "Think about bias as simply being the difference between a predicted value and the actual\n",
      "value 18\n",
      "For example, consider that our model, represented as F(x), predicts the value of 29 as\n",
      "follows:\n",
      "Here, the value of 29 should have been predicted as 79:\n",
      "If a machine learning model tends to be very accurate in its prediction (regression or\n",
      "classification), then it is considered a low bias model, whereas if the model is more often\n",
      "than not wrong, it is considered to be a high bias model 89\n",
      "\n",
      "Bias is a measure you can use to judge models on the basis of their accuracy  or just how\n",
      "correct the model is on average 28\n",
      "\n",
      "Error due to variance\n",
      "An error due to variance is dependent  on the variability of a model's prediction for a given\n",
      "data point 28\n",
      "Imagine that you repeat the machine learning model building process over and\n",
      "over 14\n",
      "The variance is measured by looking at how much the predictions for a fixed point\n",
      "vary between different end results 21\n",
      "\n",
      "\n",
      "To imagine variance in your head, think about a population of data points 15\n",
      "If you were to\n",
      "take randomized samples over and over, how drastically would your machine learning\n",
      "model change or fit differently each time 26\n",
      "If the model does not change much between\n",
      "samples, the model would be considered a low variance model 20\n",
      "If your model changes\n",
      "drastically between samples, then that model would be considered a high variance model 20\n",
      "\n",
      "Variance is a great measure with which to judge our model on the basis of generalizability 20\n",
      "\n",
      "If our model has a low variance, we can expect it to behave in a certain way when using it\n",
      "in the wild and predict values without human supervision 32\n",
      "\n",
      "Our goal is to optimize both bias and variance 10\n",
      "Ideally, we are looking for the lowest\n",
      "possible variance and bias 13\n",
      "\n",
      "I find that this can be best explained using an example 12\n",
      "\n",
      "Example – comparing body and brain weight of\n",
      "mammals\n",
      "Imagine that we are considering a relationship between  the brain weight of mammals and\n",
      "their corresponding body weights 34\n",
      "A hypothesis might read that there is a positive\n",
      "correlation between the two (as one goes up, so does the other) 26\n",
      "But how strong is this\n",
      "relationship 7\n",
      "Is it even linear 4\n",
      "Perhaps, as the brain weight increases, there is a logarithmic\n",
      "or quadratic increase in body weight 20\n",
      "\n",
      "Let's use Python to explore this:\n",
      "# # Exploring the Bias-Variance Tradeoff\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "%matplotlib inline\n",
      "\n",
      "I will be using a module, called seaborn , to visualize data points as a scatter plot and also\n",
      "to graph linear (and higher polynomial) regression models:\n",
      "# ## Brain and body weight\n",
      "'''\n",
      "This is a [dataset]) of the average\n",
      "weight of the body and the brain for\n",
      "62 mammal species 102\n",
      "Let's read it into pandas and\n",
      "take a quick look:\n",
      "'''\n",
      "df =\n",
      "pd 17\n",
      "read_table('http://people 6\n",
      "sc 1\n",
      "fsu 2\n",
      "edu/~jburkardt/datasets/regression/x01 13\n",
      "\n",
      "txt', sep='\\s+', skiprows=33, names=['id','brain','body'], index_col='id')\n",
      "df 26\n",
      "head()\n",
      "We are going to take a small subset of the samples  to exaggerate the visual representations\n",
      "of bias and variance, as follows:\n",
      "# We're going to focus on a smaller subset in which the body weight is less\n",
      "than 200:\n",
      "df = df[df 55\n",
      "body < 200]\n",
      "df 6\n",
      "shape\n",
      "The output is as follows:\n",
      "(51, 2)\n",
      "\n",
      "We're actually going to pretend that there are only 51 mammal species in existence 31\n",
      "In other\n",
      "words, we are pretending that this is the entire dataset of brain and body weights for every \n",
      "known  mammal species:\n",
      "# Let's create a scatterplot\n",
      "sns 37\n",
      "lmplot(x='body', y='brain', data=df, ci=None, fit_reg=False)\n",
      "Scatter plot of mammalian brain and body weights\n",
      "There appears to be a relationship between brain and body weight for mammals 44\n",
      "So far, we\n",
      "might assume that it is a positive correlation 13\n",
      "\n",
      "\n",
      "Now, let's throw a linear regression  into the mix 13\n",
      "Let's use seaborn  to make and plot a\n",
      "first-degree polynomial (linear) regression:\n",
      "sns 20\n",
      "lmplot(x='body', y='brain', data=df, ci=None)\n",
      "The same scatter plot as before with a linear regression visualization put in\n",
      "Now, let's pretend that a new mammal species is discovered 43\n",
      "We measure the body weight\n",
      "of every member of this species that we can find and calculate an average body weight of\n",
      "100 25\n",
      "We want to predict the average brain weight of this species (rather than measuring it\n",
      "directly) 20\n",
      "Using this line, we might predict a brain weight of about 45 14\n",
      "\n",
      "\n",
      "Something you might note is that this line isn't that close to the data points in the graph, so\n",
      "maybe it isn't the best model to use 32\n",
      "You might argue that the bias is too high 9\n",
      "And I would\n",
      "agree 5\n",
      "Linear regression models tend to have a high bias, but linear regression also has\n",
      "something up its sleeve: it has a very low variance 27\n",
      "However, what does that really mean 7\n",
      "\n",
      "Let's say that we take our entire population of mammals  and randomly split them into two\n",
      "samples, as follows:\n",
      "# set a random seed for reproducibility np 35\n",
      "random 1\n",
      "seed(12345) # randomly\n",
      "assign every row to either sample 1 or sample 2 df['sample'] =\n",
      "np 26\n",
      "random 1\n",
      "randint(1, 3, len(df)) df 11\n",
      "head()\n",
      "We include a new sample column:\n",
      "# Compare the two samples, they are fairly different 19\n",
      "\n",
      "df 2\n",
      "groupby('sample')[['brain', 'body']] 11\n",
      "mean()\n",
      "\n",
      "\n",
      "We can now tell seaborn  to create two plots, in which  the left plot only uses the data from\n",
      "sample 1 and the right plot only uses the data from sample 2:\n",
      "# col='sample' subsets the data by sample and creates two\n",
      "# separate plots\n",
      "sns 60\n",
      "lmplot(x='body', y='brain', data=df, ci=None, col='sample')\n",
      "Side-by-side scatter plots of linear regressions for samples 1 and 2\n",
      "They barely look different, right 43\n",
      "If you look closely, you will note that not a single data\n",
      "point is shared between the samples and yet the line looks almost identical 27\n",
      "To further\n",
      "show this point, let's put both the lines of best fit in the same graph and use colors to\n",
      "separate the samples, as illustrated:\n",
      "# hue='sample' subsets the data by sample and creates a # single plot\n",
      "sns 51\n",
      "lmplot(x='body', y='brain', data=df, ci=None, hue='sample')\n",
      "\n",
      "\n",
      "Presentation of two lines of best ﬁt in the same graph\n",
      "The line looks pretty similar between the two plots, despite the fact that they used separate\n",
      "samples of data 55\n",
      "In both cases, we would predict a brain weight of about 45 14\n",
      "\n",
      "The fact that even though the linear regression was given to completely distinct datasets\n",
      "pulled from the same population, it produced a very similar line, suggests that the model is\n",
      "of low variance 39\n",
      "\n",
      "\n",
      "What if we increased our model's complexity and allowed it to learn more 15\n",
      "Instead of\n",
      "fitting a line, let's let seaborn  fit a fourth-degree polynomial (a quartic polynomial) 24\n",
      "By\n",
      "adding to the degree  of the polynomial, the graph will be able to make twists and turns in\n",
      "order to fit our data better, as shown:\n",
      "# What would a low bias, high variance model look like 45\n",
      "Let's try\n",
      "polynomial regression, with an fourth order polynomial:\n",
      "sns 15\n",
      "lmplot(x='body', y='brain', data=df, ci=None, col='sample', order=4)\n",
      "Using a quartic polynomial for regression purposes\n",
      "Note how, for two distinct samples from the same population, the quartic polynomial looks\n",
      "vastly different 55\n",
      "This is a sign of high variance 7\n",
      "\n",
      "This model is low bias because it matches our data well 12\n",
      "However, it has high variance\n",
      "because the models are wildly different depending upon which points happen to be in the\n",
      "sample 24\n",
      "(For a body weight of 100, the brain weight prediction would either be 40 or 0,\n",
      "depending upon which data happened to be in the sample 32\n",
      "\n",
      "\n",
      "Our polynomial is also unaware of the general relationship of the data 13\n",
      "It seems obvious\n",
      "that there is a positive correlation between the brain and body weight of mammals 18\n",
      "\n",
      "However, in our quartic  polynomials, this relationship is nowhere to be found and is\n",
      "unreliable 24\n",
      "In our first sample (the graph on the left), the polynomial ends up shooting\n",
      "downwards, while in the second graph, the graph is going upwards towards the end 34\n",
      "Our\n",
      "model is unpredictable and can behave wildly differently, depending on the given training\n",
      "set 18\n",
      "\n",
      "It is our job, as data scientists, to find the middle ground 15\n",
      "\n",
      "Perhaps we can create a model that has less bias than the linear model, and less variance\n",
      "than the fourth-order polynomial:\n",
      "# Let's try a second order polynomial instead:\n",
      "sns 37\n",
      "lmplot(x='body', y='brain', data=df, ci=None, col='sample', order=2)\n",
      "Scatter plot using a quadratic polynomial as our estimator\n",
      "This plot seems to have a good balance of bias and variance 47\n",
      "\n",
      "\n",
      "Two extreme cases of bias/variance tradeoff\n",
      "What we just saw were two extreme cases  of model fitting: one was underfitting and the\n",
      "other was overfitting 37\n",
      "\n",
      "Underfitting\n",
      "Underfitting occurs when our models  make little to no attempt  to fit our data 23\n",
      "Models that\n",
      "are high bias and low variance are prone to underfitting 15\n",
      "In the case of the mammal\n",
      "brain/body weight example, the linear regression is underfitting our data 22\n",
      "While we have a\n",
      "general shape of the relationship, we are left with a high bias 18\n",
      "\n",
      "If your learning algorithm shows high bias and/or is underfitting, the following suggestions\n",
      "may help:\n",
      "Use more features : Try including new features into the model if it helps with our\n",
      "predictive power 42\n",
      "\n",
      "Try a more complicated model : Adding complexity to your model can help\n",
      "improve bias 18\n",
      "An overly complicated model will hurt too 7\n",
      "\n",
      "Overfitting\n",
      "Overfitting is the result of the model trying too hard to fit into the training set, resulting in a\n",
      "lower bias but a much higher variance 35\n",
      "Models that are low bias and high variance are \n",
      "prone  to overfitting 17\n",
      "In the case of the mammal brain/body weight example, the fourth-\n",
      "degree polynomial (quartic) regression is overfitting our data 28\n",
      "\n",
      "If your learning algorithm  shows high variance and/or is overfitting, the following\n",
      "suggestions may help:\n",
      "Use fewer features : Using fewer features can decrease our variance and prevent\n",
      "overfitting\n",
      "Fit on more training samples : Using more training data points in our cross-\n",
      "validation can reduce the effect of overfitting, and improve our high variance\n",
      "estimator\n",
      "\n",
      "How bias/variance play into error functions\n",
      "Error functions (which measure how incorrect  our models are) can be thought of as\n",
      "functions of bias, variance, and irreducible error 113\n",
      "Mathematically put, the error of\n",
      "predicting a dataset using  our supervised learning model might look as follows:\n",
      "Here, Bias2 is our bias term squared (which arises when simplifying the mentioned\n",
      "statement from more complicated equations), and Variance  is a measurement of how much\n",
      "our model fitting varies between randomized samples 66\n",
      "\n",
      "Simply put, both bias and variance contribute to errors 11\n",
      "As we increase our model\n",
      "complexity (for example, go from a linear regression to an eighth-degree polynomial\n",
      "regression or grow our decision trees deeper), we find that Bias2 decreases, Variance\n",
      "increases, and the total error of the model forms a parabolic shape, as illustrated:\n",
      "Relationship of bias and variance\n",
      "Our goal, as data scientists, is to find the sweet spot  that optimizes our model complexity 87\n",
      "It\n",
      "is easy to overfit our data 9\n",
      "To combat overfitting in practice, we should always use cross-\n",
      "validation (splitting up datasets iteratively and retraining models and averaging metrics) to\n",
      "get the best predictor of an error 39\n",
      "\n",
      "\n",
      "To illustrate this point, I will introduce (quickly) a new supervised algorithm and\n",
      "demonstrate the bias/variance tradeoff visually 29\n",
      "\n",
      "We will be using the K-Nearest Neighbors  (KNN ) algorithm, which is a supervised\n",
      "learning algorithm that uses a lookalike paradigm, which means  that it makes predictions\n",
      "based on similar data points seen in the past 51\n",
      "\n",
      "KNN has a complexity input, K, which represents  how many similar  data points to\n",
      "compare to 23\n",
      "If K = 3 , then, for a given input, we look to the nearest three data points and use\n",
      "them for our prediction 28\n",
      "In this case, K represents our model's complexity:\n",
      "from sklearn 13\n",
      "neighbors import KNeighborsClassifier\n",
      "# read in the iris data\n",
      "from sklearn 15\n",
      "datasets import load_iris\n",
      "iris = load_iris()\n",
      "X, y = iris 17\n",
      "data, iris 3\n",
      "target\n",
      "So, we have our X and our y 11\n",
      "A great way to overfit a model is to train and predict the exact\n",
      "same data:\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn 32\n",
      "fit(X, y)\n",
      "knn 7\n",
      "score(X, y)\n",
      "The output is as follows:\n",
      "1 12\n",
      "0\n",
      "Wow, 100% accuracy 9\n",
      " 1\n",
      "This is too good to be true 7\n",
      "\n",
      "By training and predicting the same data, we are essentially telling our data to purely\n",
      "memorize the training set and spit it back to us (this is called our training error) 38\n",
      "This is the\n",
      "reason we introduced our training and test sets in Chapter 10 , How to Tell If Your Toaster Is\n",
      "Learning – Machine Learning Essentials 31\n",
      "\n",
      "K folds cross-validation\n",
      "K folds cross-validation is a much better estimator  of our model's performance, even more\n",
      "so than our train-test split 31\n",
      "Here's how it works:\n",
      "We will take a finite number of equal slices of our data (usually 3, 5, or 10) 30\n",
      "1 2\n",
      "\n",
      "Assume that this number is called k 9\n",
      "\n",
      "For each \"fold\" of the cross-validation, we will treat k-1 of the sections as the 2 24\n",
      "\n",
      "training set, and the remaining section as our test set 12\n",
      "\n",
      "\n",
      "For the remaining folds, a different arrangement of k-1 sections is considered for 3 19\n",
      "\n",
      "our training set and a different section is our training set 12\n",
      "\n",
      "We compute a set metric for each fold of the cross-validation 13\n",
      "4 2\n",
      "\n",
      "We average our scores at the end 8\n",
      "5 2\n",
      "\n",
      "Cross-validation is effectively using multiple train-test splits being done on the same\n",
      "dataset 17\n",
      "This is done for a few reasons, but mainly because cross-validation is the most \n",
      "honest  estimate of our model's out-of-sample  (OOS ) error 34\n",
      "\n",
      "To explain this visually, let's look at our mammal brain and body weight example for a\n",
      "second 22\n",
      "The following code manually creates a five-fold cross-validation, wherein five\n",
      "different training and test sets are made from the same population:\n",
      "import matplotlib 28\n",
      "pyplot as plt\n",
      "from sklearn 6\n",
      "cross_validation import KFold\n",
      "df =\n",
      "pd 9\n",
      "read_table('http://people 6\n",
      "sc 1\n",
      "fsu 2\n",
      "edu/~jburkardt/datasets/regression/x01 13\n",
      "\n",
      "txt', sep='\\s+', skiprows=33, names=['id','brain','body'])\n",
      "df = df[df 24\n",
      "brain < 300][df 6\n",
      "body < 500]\n",
      "# limit points for visibility\n",
      "nfolds = 5\n",
      "fig, axes = plt 22\n",
      "subplots(1, nfolds, figsize=(14,4))\n",
      "for i, fold in enumerate(KFold(len(df), n_folds=nfolds,\n",
      "                              shuffle=True)):\n",
      "    training, validation = fold\n",
      "    x, y = df 49\n",
      "iloc[training]['body'], df 8\n",
      "iloc[training]['brain']\n",
      "    axes[i] 11\n",
      "plot(x, y, 'ro')\n",
      "    x, y = df 14\n",
      "iloc[validation]['body'], df 8\n",
      "iloc[validation]['brain']\n",
      "    axes[i] 11\n",
      "plot(x, y, 'bo')\n",
      "plt 9\n",
      "tight_layout()\n",
      "Five-fold cross-validation: red = training sets, blue = test sets\n",
      "\n",
      "Here, each graph shows the exact same population of mammals, but the dots are colored\n",
      "red if they belong to the training set of that fold and blue if they belong to the testing set 56\n",
      "By\n",
      "doing this, we are obtaining five different instances of the same machine learning model in\n",
      "order to see if the performance remains consistent across the folds 30\n",
      "\n",
      "If you stare at the dots long enough, you will note that each dot appears in a training set\n",
      "exactly four times ( k - 1), while the same dot appears in a test set exactly once and only\n",
      "once 47\n",
      "\n",
      "Some features of k-fold cross-validation include  the following:\n",
      "It is a more accurate estimate of the OOS prediction error than a single train-test\n",
      "split because it is taking several independent train-test splits and averaging the\n",
      "results together 47\n",
      "\n",
      "It is a more efficient use of data than single train-test splits because the entire\n",
      "dataset is being used for multiple train-test splits instead of just one 31\n",
      "\n",
      "Each record in our dataset is used for both training and testing 13\n",
      "\n",
      "This method presents a clear tradeoff between efficiency and computational\n",
      "expense 14\n",
      "A 10-fold CV is 10x more expensive computationally than a single\n",
      "train/test split 20\n",
      "\n",
      "This method can be used for parameter tuning and model selection 12\n",
      "\n",
      "Basically, whenever we wish to test a model on a set of data, whether we just completed\n",
      "tuning some parameters or feature engineering, a k-fold cross-validation is an excellent\n",
      "way to estimate the performance on our model 46\n",
      "\n",
      "Of course, sklearn  comes with an easy-to-use cross-validation module, called\n",
      "cross_val_score , which automatically splits up our dataset for us, runs the model on\n",
      "each fold, and gives us a neat and tidy output of results:\n",
      "# Using a training set and test set is so important\n",
      "# Just as important is cross validation 69\n",
      "Remember cross validation\n",
      "# is using several different train test splits and\n",
      "# averaging your results 18\n",
      "\n",
      "## CROSS-VALIDATION\n",
      "# check CV score for K=1\n",
      "from sklearn 18\n",
      "cross_validation import cross_val_score, train_test_split\n",
      "tree = KNeighborsClassifier(n_neighbors=1)\n",
      "scores = cross_val_score(tree, X, y, cv=5, scoring='accuracy')\n",
      "scores 41\n",
      "mean()\n",
      "0 3\n",
      "95999999999\n",
      "\n",
      "This is a much more reasonable accuracy than our previous score of 1 20\n",
      "Remember that we\n",
      "are not getting 100% accuracy anymore because we have a distinct training and test set 21\n",
      "The\n",
      "data points that KNN has never seen are the test points and it, therefore, cannot match\n",
      "them exactly to themselves 26\n",
      "\n",
      "Let's try cross-validating KNN with K=5 (increasing our model's complexity), as shown:\n",
      "# check CV score for K=5\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
      "scores\n",
      "np 67\n",
      "mean(scores)\n",
      "0 4\n",
      "97333333\n",
      "Even better 7\n",
      "So, now we have to find the best K 10\n",
      "The best K is the one that maximizes  our\n",
      "accuracy 13\n",
      "Let's try a few:\n",
      "# search for an optimal value of K\n",
      "k_range = range(1, 30, 2) # [1, 3, 5, 7, 41\n",
      " 1\n",
      " 1\n",
      ", 27, 29]\n",
      "errors = []\n",
      "for k in k_range:\n",
      "    knn = KNeighborsClassifier(n_neighbors=k)\n",
      "   # instantiate a KNN with k neighbors\n",
      "   scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
      " # get our five accuracy scores\n",
      "    accuracy = np 69\n",
      "mean(scores)\n",
      "   # average them together\n",
      "    error = 1 - accuracy\n",
      "   # get our error, which is 1 minus the accuracy\n",
      "    errors 33\n",
      "append(error)\n",
      "   # keep track of a list of errors\n",
      "\n",
      "We now have an error value (1 - accuracy) for each value of K (1, 3, 5, 7, 9 43\n",
      " 1\n",
      ", 1\n",
      " 1\n",
      ", 29):\n",
      "# plot the K values (x-axis) versus the 5-fold CV score (y-axis)\n",
      "plt 25\n",
      "figure()\n",
      "plt 3\n",
      "plot(k_range, errors)\n",
      "plt 7\n",
      "xlabel('K')\n",
      "plt 5\n",
      "ylabel('Error')\n",
      "Graph of errors of the KNN model against KNN's complexity, represented by the value of K\n",
      "Compare this graph to the previous graph of model complexity  and bias/variance 40\n",
      "Toward\n",
      "the left, our graph has a higher bias and is underfitting 17\n",
      "As we increased our model's\n",
      "complexity, the error term began to go down, but after a while, our model became overly\n",
      "complex, and the high variance kicked in, making our error term go back up 44\n",
      "\n",
      "It seems that the optimal value of K is between 6 and 10 16\n",
      "\n",
      "\n",
      "Grid searching\n",
      "sklearn  also has, up its sleeve, another useful  tool called grid searching 21\n",
      "A grid search will\n",
      "by brute force try many different model parameters and give us the best one based on a\n",
      "metric of our choosing 27\n",
      "For example, we can choose to optimize KNN for accuracy in the\n",
      "following manner:\n",
      "from sklearn 20\n",
      "grid_search import GridSearchCV\n",
      "from sklearn 9\n",
      "neighbors import KNeighborsClassifier\n",
      "# import our grid search module\n",
      "knn = KNeighborsClassifier(n_jobs=-1)\n",
      "# instantiate a blank slate KNN, no neighbors\n",
      "k_range = list(range(1, 31, 2))\n",
      "print(k_range)\n",
      "#k_range = range(1, 30)\n",
      "param_grid = dict(n_neighbors=k_range)\n",
      "# param_grid = {\"n_ neighbors\": [1, 3, 5, 91\n",
      " 1\n",
      " 1\n",
      "}\n",
      "print(param_grid)\n",
      "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
      "grid 25\n",
      "fit(X, y)\n",
      "\n",
      "In the grid 8\n",
      "fit()  line of code, what is happening is that, for each combination of features\n",
      "in this case, we have 15 different possibilities for K, so we are cross-validating each one five\n",
      "times 43\n",
      "This means that by the end of this code, we will have 15 * 5 = 75  different KNN\n",
      "models 27\n",
      "You can see how, when applying  this technique to more complex models, we could\n",
      "run into difficulties with time:\n",
      "# check the results of the grid search\n",
      "grid 34\n",
      "grid_scores_\n",
      "grid_mean_scores = [result[1] for result in grid 16\n",
      "grid_scores_]\n",
      "# this is a list of the average accuracies for each parameter\n",
      "# combination\n",
      "plt 22\n",
      "figure()\n",
      "plt 3\n",
      "ylim([0 3\n",
      "9, 1])\n",
      "plt 7\n",
      "xlabel('Tuning Parameter: N nearest neighbors')\n",
      "plt 11\n",
      "ylabel('Classification Accuracy')\n",
      "plt 6\n",
      "plot(k_range, grid_mean_scores)\n",
      "plt 9\n",
      "plot(grid 2\n",
      "best_params_['n_neighbors'], grid 8\n",
      "best_score_, 'ro',\n",
      "markersize=12, markeredgewidth=1 17\n",
      "5,\n",
      "         markerfacecolor='None', markeredgecolor='r')\n",
      "Classiﬁcation Accuracy versus Tuning Parameters in N nearest neighbors\n",
      "Note that the preceding graph is basically the same as the one we achieved previously with\n",
      "our for loop, but much easier 55\n",
      "\n",
      "\n",
      "We see that seven neighbors (circled in the preceding graph) seem to have the best\n",
      "accuracy 21\n",
      "However, we can also, very easily, get our best parameters and our best model, as\n",
      "shown:\n",
      "grid 23\n",
      "best_params_\n",
      "# {'n_neighbors': 7}\n",
      "grid 12\n",
      "best_score_\n",
      "# 0 6\n",
      "9799999999\n",
      "grid 7\n",
      "best_estimator_\n",
      "# actually returns the unfit model with the best parameters\n",
      "# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
      "           weights='uniform')\n",
      "I'll take this one step further 65\n",
      "Maybe you've noted that KNN has other parameters as well,\n",
      "such as algorithm , p, and weights 21\n",
      "A quick look at the scikit-learn documentation reveals\n",
      "that we have some options for each of these, which are as follows:\n",
      "p is an integer and represents the type of distance we wish to use 41\n",
      "By default, we\n",
      "use p=2, which is our standard distance formula 16\n",
      "\n",
      "weights  is, by default, uniform , but can also be distance , which weighs points\n",
      "by their distance, meaning that close neighbors have a greater impact on the\n",
      "prediction 36\n",
      "\n",
      "algorithm  is how the model finds the nearest neighbors 11\n",
      "We can try\n",
      "ball_tree , kd_tree , or brute 12\n",
      "The default is auto , which tries to use the best\n",
      "one automatically:\n",
      "knn = KNeighborsClassifier()\n",
      "k_range = range(1, 30)\n",
      "algorithm_options = ['kd_tree', 'ball_tree', 'auto', 'brute']\n",
      "p_range = range(1, 8)\n",
      "weight_range = ['uniform', 'distance']\n",
      "param_grid = dict(n_neighbors=k_range, weights=weight_range,\n",
      "algorithm=algorithm_options, p=p_range)\n",
      "# trying many more options\n",
      "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
      "grid 118\n",
      "fit(X, y)\n",
      "\n",
      "The preceding code takes about a minute to run on my laptop  because it is trying many, 1,\n",
      "648, different combinations of parameters and cross-validating each one five times 41\n",
      "All in\n",
      "all, to get the best answer, it is fitting 8,400 different KNN models:\n",
      "grid 24\n",
      "best_score_\n",
      "0 4\n",
      "98666666\n",
      "grid 6\n",
      "best_params_\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 6, 'p': 3, 'weights': 'uniform'}\n",
      "Grid searching is a simple (but inefficient) way of parameter tuning our models to get the\n",
      "best possible outcome 51\n",
      "It should be noted that to get the best possible outcome, data\n",
      "scientists should use feature manipulation (both reduction and engineering) to obtain better\n",
      "results in practice as well 35\n",
      "It should not merely be up to the model to achieve the best\n",
      "performance 15\n",
      "\n",
      "Visualizing training error versus cross-validation\n",
      "error\n",
      "I think it is important once again to go over and compare the cross-validation error and the\n",
      "training error 32\n",
      "This time, let's put them both on the same graph to compare  how they both \n",
      "change  as we vary the model  complexity 28\n",
      "\n",
      "I will use the mammal dataset once more to show the cross-validation error and the\n",
      "training error (the error on predicting the training set) 30\n",
      "Recall that we are attempting to\n",
      "regress the body weight of a mammal to the brain weight of a mammal:\n",
      "# This function uses a numpy polynomial fit function to\n",
      "# calculate the RMSE of given X and y\n",
      "def rmse(x, y, coefs):\n",
      "    yfit = np 62\n",
      "polyval(coefs, x)\n",
      "    rmse = np 12\n",
      "sqrt(np 2\n",
      "mean((y - yfit) ** 2))\n",
      "    return rmse\n",
      "xtrain, xtest, ytrain, ytest = train_test_split(df['body'], df['brain'])\n",
      "train_err = []\n",
      "validation_err = []\n",
      "degrees = range(1, 8)\n",
      "for i, d in enumerate(degrees):\n",
      "    p = np 69\n",
      "polyfit(xtrain, ytrain, d)\n",
      "  # built in numpy polynomial fit function\n",
      "\n",
      "    train_err 22\n",
      "append(rmse(xtrain, ytrain, p))\n",
      "    validation_err 14\n",
      "append(rmse(xtest, ytest, p))\n",
      "fig, ax = plt 16\n",
      "subplots()\n",
      "# begin to make our graph\n",
      "ax 11\n",
      "plot(degrees, validation_err, lw=2, label = 'cross-validation error')\n",
      "ax 19\n",
      "plot(degrees, train_err, lw=2, label = 'training error')\n",
      "# Our two curves, one for training error, the other for cross validation\n",
      "ax 34\n",
      "legend(loc=0)\n",
      "ax 6\n",
      "set_xlabel('degree of polynomial')\n",
      "ax 8\n",
      "set_ylabel('RMSE')\n",
      "So, we see that as we increase our degree of fit, our training error goes down without a\n",
      "hitch, but we are now smart enough to know that as we increase the model complexity, our\n",
      "model is overfitting to our data and is merely regurgitating our data back to us, whereas\n",
      "our cross-validation error line is much more honest and begins to perform poorly after\n",
      "about degree 2 or 3 93\n",
      "\n",
      "\n",
      "Let's recap:\n",
      "Underfitting occurs when the cross-validation error and the training error are\n",
      "both high\n",
      "Overrfitting occurs when the cross-validation error is high, while the training\n",
      "error is low\n",
      "We have a good fit when the cross-validation error is low, and only slightly\n",
      "higher than the training error\n",
      "Both underfitting (high bias) and overfitting (high variance) will result in poor\n",
      "generalization of the data 93\n",
      "\n",
      "Here are some tips if you face high  bias or variance 13\n",
      "\n",
      "Try the following if your model  tends to have a high bias :\n",
      "Try adding more features to the training and test sets\n",
      "Either add to the complexity of your model  or try a more modern sophisticated\n",
      "model\n",
      "Try the following if your model tends to have a high variance :\n",
      "Try to include more training samples, which reduces the effect of overfitting\n",
      "In general, the bias/variance tradeoff is the struggle to minimize the bias and variance in\n",
      "our learning algorithms 96\n",
      "Many newer learning algorithms, invented in the past few\n",
      "decades, were made with the intention of having the best of both worlds 26\n",
      "\n",
      "Ensembling techniques\n",
      "Ensemble learning , or ensembling, is the process of combining  multiple predictive models\n",
      "to produce a supermodel that is more accurate than any individual model on its own:\n",
      "Regression : We will take the average of the predictions for each model\n",
      "Classification : Take a vote and use the most common prediction, or take the\n",
      "average of the predicted probabilities\n",
      "\n",
      "Imagine that we are working on a binary classification problem (predicting either 0 or 1):\n",
      "# ENSEMBLING\n",
      "import numpy as np\n",
      "# set a seed for reproducibility\n",
      "np 116\n",
      "random 1\n",
      "seed(12345)\n",
      "# generate 2000 random numbers (between 0 and 1) for each model,\n",
      "representing 2000 observations\n",
      "mod1 = np 35\n",
      "random 1\n",
      "rand(2000)\n",
      "mod2 = np 9\n",
      "random 1\n",
      "rand(2000)\n",
      "mod3 = np 9\n",
      "random 1\n",
      "rand(2000)\n",
      "mod4 = np 9\n",
      "random 1\n",
      "rand(2000)\n",
      "mod5 = np 9\n",
      "random 1\n",
      "rand(2000)\n",
      "Now, we simulate five different learning models, and each  has about a 70% accuracy, as\n",
      "follows:\n",
      "# each model independently predicts 1 (the \"correct response\") if random\n",
      "number was at least 0 52\n",
      "4\n",
      "preds1 = np 8\n",
      "where(mod1 > 0 6\n",
      "4, 1, 0)\n",
      "preds2 = np 14\n",
      "where(mod2 > 0 6\n",
      "4, 1, 0)\n",
      "preds3 = np 14\n",
      "where(mod3 > 0 6\n",
      "4, 1, 0)\n",
      "preds4 = np 14\n",
      "where(mod4 > 0 6\n",
      "4, 1, 0)\n",
      "preds5 = np 14\n",
      "where(mod5 > 0 6\n",
      "4, 1, 0)\n",
      "print(preds1 13\n",
      "mean())\n",
      "0 3\n",
      "596\n",
      "print (preds2 8\n",
      "mean())\n",
      "0 3\n",
      "6065\n",
      "print (preds3 9\n",
      "mean())\n",
      "0 3\n",
      "591\n",
      "print (preds4 8\n",
      "mean())\n",
      "0 3\n",
      "5965\n",
      "print( preds5 8\n",
      "mean())\n",
      "# 0 5\n",
      "611\n",
      "# Each model has an \"accuracy of around 60% on its own\n",
      "Now, let's apply my degrees in magic 28\n",
      "Er 1\n",
      " 1\n",
      " 1\n",
      "sorry, math:\n",
      "# average the predictions and then round to 0 or 1\n",
      "ensemble_preds = np 22\n",
      "round((preds1 + preds2 + preds3 + preds4 +\n",
      "preds5)/5 20\n",
      "0) 3\n",
      "astype(int)\n",
      "ensemble_preds 6\n",
      "mean()\n",
      "\n",
      "The output is as follows:\n",
      "0 9\n",
      "674\n",
      "As you add more models to a voting process, the probability of errors will decrease; this is\n",
      "known as Condorcet's jury theorem 31\n",
      "\n",
      "Crazy, right 5\n",
      "\n",
      "For ensembling to work well in practice, the models must  have the following\n",
      "characteristics:\n",
      "Accuracy : Each model must at least outperform the null model\n",
      "Independence : A model's prediction process is not affected by another model's\n",
      "prediction process\n",
      "If you have a bunch of individually OK models, the edge case mistakes made by one model\n",
      "are probably not going to be made by the other models, so the mistakes will be ignored\n",
      "when combining the models 95\n",
      "\n",
      "There are the following two basic methods for ensembling:\n",
      "Manually ensemble your individual models by writing a good deal of code\n",
      "Use a model that ensembles for you\n",
      "We're going to look at a model that ensembles for us 48\n",
      "To do this, let's take a look at decision\n",
      "trees again 14\n",
      "\n",
      "Decision trees tend to have low bias and high variance 11\n",
      "Given any dataset, the tree can keep\n",
      "asking questions (making decisions) until it is able to nitpick and distinguish between every\n",
      "single  example in the dataset 33\n",
      "It could keep asking question after question until there is only\n",
      "a single example in each leaf (terminal) node 22\n",
      "The tree is trying too hard, growing too deep,\n",
      "and just memorizing every single detail of our training set 22\n",
      "However, if we started over, the\n",
      "tree could potentially ask different questions and still grow very deep 20\n",
      "This means that\n",
      "there are many possible trees that could distinguish between all elements, which means\n",
      "the higher variance 22\n",
      "It is unable to generalize well 6\n",
      "\n",
      "In order to reduce the variance of a single tree, we can place a restriction on the number of\n",
      "questions asked in a tree (the max_depth  parameter) or we can create an ensemble version\n",
      "of decision trees, called random forests 49\n",
      "\n",
      "\n",
      "Random forests\n",
      "The primary weakness of decision trees is that different  splits in the training  data can lead\n",
      "to very different trees 27\n",
      "Bagging is a general purpose procedure to reduce the variance of a\n",
      "machine learning method but is particularly useful for decision trees 24\n",
      "\n",
      "Bagging is short for Bootstrap aggregation, which means  the aggregation of Bootstrap\n",
      "samples 18\n",
      "What is a Bootstrap sample 5\n",
      "\n",
      "A Bootstrap sample is a smaller  sample that is \" bootstrapped\" from a larger  sample 21\n",
      "\n",
      "Bootstrapping is a type of resampling where large numbers of smaller  samples of the same\n",
      "size are repeatedly drawn, with replacement, from a single original  sample:\n",
      "# set a seed for reproducibility\n",
      "np 46\n",
      "random 1\n",
      "seed(1)\n",
      "# create an array of 1 through 20\n",
      "nums = np 18\n",
      "arange(1, 21)\n",
      "print (nums)\n",
      "The output is as follows:\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "# sample that array 20 times with replacement\n",
      "np 79\n",
      "random 1\n",
      "choice(a=nums, size=20, replace=True)\n",
      "The preceding command will create a bootstrapped sample as follows:\n",
      " [ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]\n",
      "# This is our bootstrapped sample notice it has repeat variables 87\n",
      "\n",
      "So, how does bagging work for decision trees 11\n",
      "\n",
      "Grow B trees using Bootstrap samples from the training data 1 13\n",
      "\n",
      "Train each tree on its Bootstrap sample and make predictions2 12\n",
      "\n",
      "Combine the predictions:3 6\n",
      "\n",
      "Average the predictions for regression trees\n",
      "Take a vote for classification trees\n",
      "\n",
      "The following are a few things to note:\n",
      "Each Bootstrap sample should be the same size as the original training set\n",
      "B should be a large enough value that the error seems to have stabilized\n",
      "The trees are grown intentionally deep so that they have low bias/high variance\n",
      "The reason we grow the trees intentionally deep is that the bagging  inherently increases\n",
      "predictive accuracy by reducing the variance, similar to how cross-validation reduces the\n",
      "variance associated with estimating our out-of-sample error 111\n",
      "\n",
      "Random forests are a variation  of bagged trees 11\n",
      "\n",
      "However, when building each tree, each time we consider a split between the features, a\n",
      "random sample of m features is chosen as split candidates from the full set of p features 37\n",
      "\n",
      "The split is only allowed to be one of those m features:\n",
      "A new random sample of features is chosen for every single  tree at every single\n",
      "split\n",
      "For classification, m is typically chosen to be the square root of p\n",
      "For regression, m is typically chosen to be somewhere between p/3 and p\n",
      "What's the point 69\n",
      "\n",
      "Suppose there is one very strong feature in the dataset 12\n",
      "When using decision (or bagged)\n",
      "trees, most of the trees will use that feature as the top split, resulting in an ensemble of\n",
      "similar trees that are highly correlated with each other 38\n",
      "\n",
      "If our trees are highly correlated with each other, then averaging these quantities will not\n",
      "significantly reduce variance (which is the entire goal of ensembling) 32\n",
      "Also, by randomly\n",
      "leaving out candidate features from each split, random forests reduce the variance of the\n",
      "resulting model 25\n",
      "\n",
      "\n",
      "Random forests can be used in both classification and regression problems and can be\n",
      "easily used in scikit-learn 24\n",
      "Let's try to predict MLB salaries based on statistics about the\n",
      "player, as shown:\n",
      "# read in the data\n",
      "url = ' 27\n",
      " 1\n",
      "/data/hitters 4\n",
      "csv'\n",
      "hitters = pd 6\n",
      "read_csv(url)\n",
      "# remove rows with missing values\n",
      "hitters 13\n",
      "dropna(inplace=True)\n",
      "# encode categorical variables as integers\n",
      "hitters['League'] = pd 19\n",
      "factorize(hitters 4\n",
      "League)[0]\n",
      "hitters['Division'] = pd 11\n",
      "factorize(hitters 4\n",
      "Division)[0]\n",
      "hitters['NewLeague'] = pd 12\n",
      "factorize(hitters 4\n",
      "NewLeague)[0]\n",
      "# define features: exclude career statistics (which start with \"C\") and the\n",
      "response (Salary)\n",
      "feature_cols = [h for h in hitters 35\n",
      "columns if h[0] 6\n",
      "= 'C' and h 6\n",
      "=\n",
      "'Salary']\n",
      "# define X and y\n",
      "X = hitters[feature_cols]\n",
      "y = hitters 20\n",
      "Salary\n",
      "Let's try and predict the salary  first using a single  decision tree, as illustrated:\n",
      "from sklearn 23\n",
      "tree import DecisionTreeRegressor\n",
      "# list of values to try for max_depth\n",
      "max_depth_range = range(1, 21)\n",
      "# list to store the average RMSE for each value of max_depth\n",
      "RMSE_scores = []\n",
      "# use 10-fold cross-validation with each value of max_depth\n",
      "from sklearn 63\n",
      "cross_validation import cross_val_score\n",
      "for depth in max_depth_range:\n",
      "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
      "    MSE_scores = cross_val_score(treereg, X, y, cv=10,\n",
      "scoring='mean_squared_error')\n",
      "    RMSE_scores 61\n",
      "append(np 2\n",
      "mean(np 2\n",
      "sqrt(-MSE_scores)))\n",
      "# plot max_depth (x-axis) versus RMSE (y-axis)\n",
      "plt 22\n",
      "plot(max_depth_range, RMSE_scores)\n",
      "\n",
      "plt 10\n",
      "xlabel('max_depth')\n",
      "plt 6\n",
      "ylabel('RMSE (lower is better)')\n",
      "RMSE for decision tree models against the max depth of the tree (complexity)\n",
      "Let's do the same thing, but this time with a random forest:\n",
      "from sklearn 43\n",
      "ensemble import RandomForestRegressor\n",
      "# list of values to try for n_estimators\n",
      "estimator_range = range(10, 310, 10)\n",
      "# list to store the average RMSE for each value of n_estimators\n",
      "RMSE_scores = []\n",
      "# use 5-fold cross-validation with each value of n_estimators (WARNING:\n",
      "SLOW 67\n",
      "\n",
      "for estimator in estimator_range:\n",
      "    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",
      "    MSE_scores = cross_val_score(rfreg, X, y, cv=5,\n",
      "scoring='mean_squared_error')\n",
      "    RMSE_scores 54\n",
      "append(np 2\n",
      "mean(np 2\n",
      "sqrt(-MSE_scores)))\n",
      "\n",
      "# plot n_estimators (x-axis) versus RMSE (y-axis)\n",
      "plt 22\n",
      "plot(estimator_range, RMSE_scores)\n",
      "plt 10\n",
      "xlabel('n_estimators')\n",
      "plt 6\n",
      "ylabel('RMSE (lower is better)')\n",
      "RMSE for random forest models against the max depth of the tree (complexity)\n",
      "Note already the y-axis; our RMSE is much lower on an average 41\n",
      "See how we can obtain a\n",
      "major increase in predictive power using random forests 15\n",
      "\n",
      "\n",
      "In random forests, we still have the concept of important features, like we had in decision\n",
      "trees:\n",
      "# n_estimators=150 is sufficiently good\n",
      "rfreg = RandomForestRegressor(n_estimators=150, random_state=1)\n",
      "rfreg 48\n",
      "fit(X, y)\n",
      "# compute feature importances\n",
      "pd 12\n",
      "DataFrame({'feature':feature_cols,\n",
      "'importance':rfreg 13\n",
      "feature_importances_}) 5\n",
      "sort('importance', ascending =\n",
      "False)\n",
      "So, it looks like the number of years the player  has been in the league  is still the most\n",
      "important feature when deciding that player's salary 40\n",
      "\n",
      "\n",
      "Comparing random forests with decision trees\n",
      "It is important to realize that just using  random forests is not the solution  to your data\n",
      "science problems 31\n",
      "While random forests provide many advantages, many disadvantages\n",
      "also come with them 14\n",
      "\n",
      "The advantages of random forests are as follows:\n",
      "Their performance is competitive with the best-supervised learning methods\n",
      "They provide a more reliable estimate of feature importance\n",
      "They allow you to estimate out-of-sample errors without using train/test splits or\n",
      "cross-validation\n",
      "The disadvantages of random forests are as follows:\n",
      "They are less interpretable (cannot visualize an entire forest of decision trees)\n",
      "They are slower to train and predict (not great for production or real-time\n",
      "purposes)\n",
      "Neural networks\n",
      "Probably one of the most talked about machine  learning models, neural networks are\n",
      "computational networks built to model animals' nervous systems 124\n",
      "Before getting too deep\n",
      "into the structure, let's take a look at the big advantages of neural networks 21\n",
      "\n",
      "The key component of a neural network is that it is not only a complex structure, but it is\n",
      "also a complex and flexible structure 28\n",
      "This means the following two things:\n",
      "Neural networks are able to estimate any function shape (this is called being non-\n",
      "parametric)\n",
      "Neural networks can adapt and literally change their own internal structure\n",
      "based on their environment\n",
      "\n",
      "Basic structure\n",
      "Neural networks are made up of interconnected nodes ( perceptrons ) that each  take in input\n",
      "(quantitative value), and output other quantitative values 79\n",
      "Signals travel through  the\n",
      "network and eventually end up at a prediction node:\n",
      "Visualization of neural network interconnected nodes\n",
      "Another huge advantage of neural networks is that they can be used for supervised\n",
      "learning, unsupervised learning, and reinforcement learning problems 49\n",
      "The ability to be so\n",
      "flexible, predict many functional shapes, and adapt to their surroundings make neural\n",
      "networks highly preferable in select fields, as follows:\n",
      "Pattern recognition : This is probably the most  common application of neural\n",
      "networks 49\n",
      "Some examples are handwriting recognition and image processing\n",
      "(facial recognition) 13\n",
      "\n",
      "Entity movement : Examples for this include  self-driving cars, robotic animals,\n",
      "and drone movement 19\n",
      "\n",
      "Anomaly detection : As neural networks are good at recognizing  patterns, they\n",
      "can also be used to recognize when a data point does not fit a pattern 32\n",
      "Think of a\n",
      "neural network monitoring a stock price movement; after a while of learning the\n",
      "general pattern of a stock price, the network can alert you when something is\n",
      "unusual in the movement 41\n",
      "\n",
      "\n",
      "The simplest form of a neural network is a single perceptron 13\n",
      "A perceptron, visualized as\n",
      "follows, takes in some input and outputs a signal:\n",
      "This signal is obtained by combining  the input with several weights and then is put\n",
      "through some activation function 41\n",
      "In cases of simple binary outputs, we generally use the\n",
      "logistic function, as shown:\n",
      "To create a neural network, we need to connect multiple perceptrons to each other in a\n",
      "network fashion, as illustrated in the following graph 48\n",
      "\n",
      "\n",
      "A multilayer perceptron  (MLP ) is a finite acyclic  graph 19\n",
      "The nodes are neurons with\n",
      "logistic activation:\n",
      "As we train the model, we update the weights (which are random at first) of the model in\n",
      "order to get the best predictions possible 39\n",
      "If an observation goes through the model and is\n",
      "outputted as false when it should have been true, the logistic functions in the single\n",
      "perceptrons are changed slightly 35\n",
      "This is called back-propagation 6\n",
      "Neural networks are \n",
      "usually  trained in batches, which means that the network is given several training  data\n",
      "points at once several times, and each time, the back-propagation algorithm will trigger an\n",
      "internal weight change in the network 47\n",
      "\n",
      "It isn't hard to see that we can grow the network very deep and have many hidden layers,\n",
      "which are associated with the complexity of the neural network 31\n",
      "When we grow our neural\n",
      "networks very deep, we are dipping our toes into the idea of deep learning 22\n",
      "The main\n",
      "advantage of deep neural networks (networks with many layers) is that they can\n",
      "approximate almost any shape function and they can (theoretically) learn optimal\n",
      "combinations of features for us and use these combinations to obtain the best predictive\n",
      "power 55\n",
      "\n",
      "\n",
      "Let's see this in action 7\n",
      "I will be using a module called PyBrain to make my neural\n",
      "networks 16\n",
      "However, first let's take a look at a new dataset, which is a dataset of \n",
      "handwritten  digits 23\n",
      "We will first try to recognize digits using a random forest, as shown:\n",
      "from sklearn 17\n",
      "cross_validation import cross_val_score\n",
      "from sklearn import datasets\n",
      "import matplotlib 14\n",
      "pyplot as plt\n",
      "from sklearn 6\n",
      "ensemble import RandomForestClassifier\n",
      "%matplotlib inline\n",
      "digits = datasets 12\n",
      "load_digits()\n",
      "plt 4\n",
      "imshow(digits 2\n",
      "images[100], cmap=plt 6\n",
      "cm 1\n",
      "gray_r, interpolation='nearest')\n",
      "# a 4 digit\n",
      "X, y = digits 18\n",
      "data, digits 3\n",
      "target\n",
      "# 64 pixels per image\n",
      "X[0] 13\n",
      "shape\n",
      "# Try Random Forest\n",
      "rfclf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
      "cross_val_score(rfclf, X, y, cv=5, scoring='accuracy') 41\n",
      "mean()\n",
      "0 3\n",
      "9382782\n",
      "\n",
      "Pretty good 7\n",
      "An accuracy of 94% is nothing to laugh at, but can we do even better 18\n",
      "\n",
      "Warning 2\n",
      "The PyBrain syntax can be a bit tricky 9\n",
      "\n",
      "from pybrain 4\n",
      "datasets            import ClassificationDataSet\n",
      "from pybrain 9\n",
      "utilities           import percentError\n",
      "from pybrain 9\n",
      "tools 1\n",
      "shortcuts     import buildNetwork\n",
      "from pybrain 9\n",
      "supervised 1\n",
      "trainers import BackpropTrainer\n",
      "from pybrain 10\n",
      "structure 1\n",
      "modules   import SoftmaxLayer\n",
      "from numpy import ravel\n",
      "# pybrain has its own data sample class that we must add\n",
      "# our training and test set to\n",
      "ds = ClassificationDataSet(64, 1 , nb_classes=10)\n",
      "for k in xrange(len(X)):\n",
      "    ds 59\n",
      "addSample(ravel(X[k]),y[k])\n",
      "# their equivalent of train test split\n",
      "test_data, training_data = ds 25\n",
      "splitWithProportion( 0 7\n",
      "25 )\n",
      "# pybrain's version of dummy variables\n",
      "test_data 14\n",
      "_convertToOneOfMany( )\n",
      "training_data 9\n",
      "_convertToOneOfMany( )\n",
      "print test_data 10\n",
      "indim # number of pixels going in\n",
      "# 64\n",
      "print test_data 16\n",
      "outdim # number of possible options (10 digits)\n",
      "# 10\n",
      "# instantiate the model with 64 hidden layers (standard params)\n",
      "fnn = buildNetwork( training_data 36\n",
      "indim, 64, training_data 8\n",
      "outdim,\n",
      "outclass=SoftmaxLayer )\n",
      "trainer = BackpropTrainer( fnn, dataset=training_data, momentum=0 28\n",
      "1,\n",
      "learningrate=0 7\n",
      "01 , verbose=True, weightdecay=0 10\n",
      "01)\n",
      "# change the number of epochs to try to get better results 15\n",
      "\n",
      "trainer 2\n",
      "trainEpochs (10) # 10 batches\n",
      "\n",
      "print 'Percent Error on Test dataset: ' , \\\n",
      "        percentError( trainer 27\n",
      "testOnClassData (\n",
      "           dataset=test_data )\n",
      "           , test_data['class'] )\n",
      "The model will output a final error on a test set:\n",
      "Percent Error on Test dataset: 4 38\n",
      "67706013363\n",
      "accuracy = 1 - 11\n",
      "0467706013363\n",
      "accuracy\n",
      "0 10\n",
      "95322\n",
      "Already better 6\n",
      "Both the random forests and neural networks  do very well with this\n",
      "problem because both of them are non-parametric, which means that they do not rely on\n",
      "the underlying shape of the data to make predictions 42\n",
      "They are able to estimate any shape\n",
      "of function 10\n",
      "\n",
      "To predict the shape, we can use the following code:\n",
      "plt 14\n",
      "imshow(digits 2\n",
      "images[0], cmap=plt 6\n",
      "cm 1\n",
      "gray_r, interpolation='nearest')\n",
      "fnn 9\n",
      "activate(X[0])\n",
      "array([ 0 9\n",
      "92183643,  0 8\n",
      "00126609,  0 8\n",
      "00303146,  0 8\n",
      "00387049,  0 8\n",
      "01067609,\n",
      "        0 8\n",
      "00718017,  0 8\n",
      "00825521,  0 8\n",
      "00917995,  0 8\n",
      "00696929,  0 8\n",
      "02773482])\n",
      "\n",
      "\n",
      "The array represents a probability for every single digit, which means that there is a 92%\n",
      "chance that the digit in the preceding screenshot is a 0 (which it is) 42\n",
      "Note how the next\n",
      "highest probability is for a 9, which makes sense because 9 and 0 have similar shapes\n",
      "(ovular) 30\n",
      "\n",
      "Neural networks do have a major flaw 9\n",
      "If left alone, they have a very high variance 10\n",
      "To see\n",
      "this, let's run the exact same code as the preceding one and train the exact same type of\n",
      "neural network  on the exact same data, as illustrated:\n",
      "# Do it again and see the difference in error\n",
      "fnn = buildNetwork( training_data 56\n",
      "indim, 64, training_data 8\n",
      "outdim,\n",
      "outclass=SoftmaxLayer )\n",
      "trainer = BackpropTrainer( fnn, dataset=training_data, momentum=0 28\n",
      "1,\n",
      "learningrate=0 7\n",
      "01 , verbose=True, weightdecay=0 10\n",
      "01)\n",
      "# change the number of eopchs to try to get better results 17\n",
      "\n",
      "trainer 2\n",
      "trainEpochs (10)\n",
      "print ('Percent Error on Test dataset: ' , \\\n",
      "        percentError( trainer 22\n",
      "testOnClassData (\n",
      "           dataset=test_data )\n",
      "           , test_data['class'] ) )\n",
      "accuracy = 1 - 24\n",
      "0645879732739\n",
      "accuracy\n",
      "0 10\n",
      "93541\n",
      "See how just rerunning the model and instantiating different weights made the network\n",
      "turn out to be different than before 27\n",
      "This is a symptom of being a high variance model 10\n",
      "In\n",
      "addition, neural networks generally require many training samples in order to combat the\n",
      "high variances  of the model and also require a large amount of computation power to work\n",
      "well in production environments 41\n",
      "\n",
      "\n",
      "Summary\n",
      "This concludes our long journey into the principles of data science 14\n",
      "In the last 300-odd\n",
      "pages, we looked at different techniques in probability, statistics, and machine learning to\n",
      "answer the most difficult questions out there 32\n",
      "I would like to personally congratulate you\n",
      "on making it through this book 14\n",
      "I hope that it proved useful and inspired you to learn even\n",
      "more 14\n",
      "\n",
      "This isn't everything I need to know 9\n",
      "\n",
      "Nope 3\n",
      "There is only so much I can fit into a principles  level book 14\n",
      "There is still so much to\n",
      "learn 8\n",
      "\n",
      "Where can I learn more 6\n",
      "\n",
      "I recommend going to find open source data challenges ( https://www 14\n",
      "kaggle 3\n",
      "com/  is a\n",
      "good source) for this 11\n",
      "I'd also recommend seeking out, trying, and solving your own\n",
      "problems at home 17\n",
      "\n",
      "When do I get to call myself a data scientist 11\n",
      "\n",
      "When you begin cultivating actionable insights from datasets, both large and small, that\n",
      "companies and people can use, then you have the honor of calling yourself a true data\n",
      "scientist 37\n",
      "\n",
      "In next chapter, we will apply concepts learned in this book to real-life case studies,\n",
      "including building predictive models to predict the stock market 28\n",
      "We will also cover the\n",
      "emerging machine learning topic of TensorFlow 13\n",
      "\n",
      "\n",
      "3\n",
      "Case Studies\n",
      "In this chapter, we will take a look at a few case studies to help you develop a better\n",
      "understanding of the topics we've seen so far 37\n",
      "\n",
      "Case study 1 – Predicting stock prices\n",
      "based on social media\n",
      "Our first case study will be quite  exciting 25\n",
      "We will attempt to predict the price of the stock\n",
      "of a publicly traded company using only social media sentiment 21\n",
      "While this example  will\n",
      "not use any explicit statistical/machine learning algorithms, we will utilize exploratory data\n",
      "analysis  (EDA ) and use visuals in order to achieve our goal 37\n",
      "\n",
      "Text sentiment analysis\n",
      "When talking about sentiment, it should be clear  what is meant 18\n",
      "By sentiment, I am\n",
      "referring to a quantitative value (at the interval level) between -1 and 1 24\n",
      "If the sentiment\n",
      "score of a text piece is close to -1, it is said to have negative sentiment 22\n",
      "If the sentiment score\n",
      "is close to 1, then the text is said to have positive sentiment 20\n",
      "If the sentiment score is close to\n",
      "0, we say it has neutral sentiment 16\n",
      "We will use a Python module called TextBlob  to\n",
      "measure our text sentiment:\n",
      "from textblob import TextBlob\n",
      "# use the textblob module to make a function called stringToSentiment that\n",
      "returns a sentences sentiment\n",
      "def stringToSentiment(text):\n",
      "    return TextBlob(text) 59\n",
      "sentiment 1\n",
      "polarity\n",
      "\n",
      "Now, we can use this function, which calls the Textblob  module to score text out of the\n",
      "box:\n",
      "stringToSentiment('i hate you')\n",
      "# -0 38\n",
      "8\n",
      "stringToSentiment('i love you')\n",
      "# 0 15\n",
      "5\n",
      "stringToSentiment('i see you')\n",
      "# 0 15\n",
      "0\n",
      "Now, let's read  in our tweets  for our study :\n",
      "# read in tweets data into a dataframe\n",
      "from textblob import TextBlob\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "# these tweets are from last May and are about Apple (AAPL)\n",
      "tweets = pd 60\n",
      "read_csv(' 3\n",
      " 1\n",
      "/data/ so_many_tweets 6\n",
      "csv ')\n",
      "tweets 3\n",
      "head()\n",
      "Exploratory data analysis\n",
      "So we have four  columns, as follows:\n",
      "Text : Unstructured text at the nominal level\n",
      "Date : Datetime (we will think of datetime in a continuous way)\n",
      "Status : Status unique ID at the nominal level\n",
      "Retweet : Status ID of tweet showing that this tweet was a retweet at the nominal\n",
      "level\n",
      "\n",
      "So we have four columns, but how many rows 84\n",
      "Also, what does each row represent 7\n",
      "It\n",
      "seems that each  row represents a single tweet about the company:\n",
      "tweets 17\n",
      "shape\n",
      "The output is as follows:\n",
      "(52512, 4)\n",
      "So, we have four columns and 52512  tweets/rows at our disposal 32\n",
      "Oh boy … Our goal here\n",
      "is to eventually use the tweets' sentiments, so we will likely need a sentiment column in the\n",
      "DataFrame 28\n",
      "Using our fairly straightforward function from the previous example, let's add\n",
      "this column 16\n",
      "\n",
      "# create a new column in tweets called sentiment that maps\n",
      "stringToSentiment to the text column\n",
      "tweets['sentiment'] = tweets['Text'] 32\n",
      "apply(stringToSentiment)\n",
      "tweets 7\n",
      "head()\n",
      "The preceding code will apply the stringToSentiment  function to each and every\n",
      "element in the Text  column of the tweets  DataFrame :\n",
      "tweets 32\n",
      "head()\n",
      "So, now we have a sense for the sentiment score for each tweet in this dataset 19\n",
      "Let's simplify\n",
      "our problem and try to use an entire days' worth of tweets to predict whether or not the\n",
      "price of AAPL will increase within 24 hours 34\n",
      "If this is the case, we have another issue here 11\n",
      "\n",
      "The Date  column reveals that we have multiple tweets for each day 14\n",
      "Just look at the first\n",
      "five tweets; they are all on the same day 16\n",
      "We will resample this dataset in order to get a\n",
      "sense of the average  sentiment of the stock on Twitter every day 25\n",
      "\n",
      "\n",
      "We will do this in three steps:\n",
      "We will ensure that the Date  column is of the Python datetime  type 24\n",
      "1 2\n",
      "\n",
      "We will replace our DataFrame's index with the datetime  column (which allows 2 18\n",
      "\n",
      "us to use complex datetime  functions) 9\n",
      "\n",
      "We will resample the data so that each row, instead of representing a tweet, will3 20\n",
      "\n",
      "represent a single day with an aggregated sentiment score for each day:\n",
      "The index of the DataFrame is a special series used to identify rows in our\n",
      "structure 31\n",
      "By default, a DataFrame will use incremental integers to\n",
      "represent rows ( 0 for the first row, 1 for the second row, and so on) 32\n",
      "\n",
      "import pandas as pd\n",
      "tweets 7\n",
      "index = pd 3\n",
      "RangeIndex(start=0, stop=52512, step=1)\n",
      "# As a list, we can splice it\n",
      "list(tweets 28\n",
      "index)[:5]\n",
      "[0, 1, 2, 3, 4]\n",
      "Let's tackle this date  issue now 27\n",
      "We will ensure  that the Date  column is of the 4 14\n",
      "\n",
      "Python datetime  type:\n",
      "# cast the date column as a datetime\n",
      "tweets['Date'] = pd 21\n",
      "to_datetime(tweets 4\n",
      "Date)\n",
      "tweets['Date'] 6\n",
      "head()\n",
      "Date\n",
      "2015-05-24 03:46:08   2015-05-24 03:46:08\n",
      "2015-05-24 04:17:42   2015-05-24 04:17:42\n",
      "2015-05-24 04:13:22   2015-05-24 04:13:22\n",
      "2015-05-24 04:08:34   2015-05-24 04:08:34\n",
      "2015-05-24 04:04:42   2015-05-24 04:04:42\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "We will replace our DataFrame's index with the datetime  column (which allows 5 167\n",
      "\n",
      "us to use complex datetime  functions ):\n",
      "tweets 10\n",
      "index = tweets 3\n",
      "Date\n",
      "tweets 3\n",
      "index\n",
      "Index([u'2015-05-24 03:46:08', u'2015-05-24 04:17:42',\n",
      "u'2015-05-24 04:13:22',\n",
      "       u'2015-05-24 04:08:34', u'2015-05-24 04:04:42',\n",
      "u'2015-05-24 04:00:01',\n",
      "       u'2015-05-24 03:54:07', u'2015-05-24 04:25:29',\n",
      "u'2015-05-24 04:24:47',\n",
      "       u'2015-05-24 04:06:42',\n",
      " 157\n",
      " 1\n",
      " 1\n",
      "\n",
      "       u'2015-05-02 16:30:02', u'2015-05-02 16:29:35',\n",
      "u'2015-05-02 16:28:26',\n",
      "       u'2015-05-02 16:27:53', u'2015-05-02 16:27:02',\n",
      "u'2015-05-02 16:26:39',\n",
      "       u'2015-05-02 16:25:00', u'2015-05-02 16:23:39',\n",
      "u'2015-05-02 16:23:38',\n",
      "       u'2015-05-02 16:23:21'],\n",
      "      dtype='object', name=u'Date', length=52512)\n",
      "tweets 171\n",
      "head()\n",
      "\n",
      "\n",
      "Note that the black index on the left used to be numbers, but now is the\n",
      "exact datetime  that the tweet was sent 28\n",
      "\n",
      "Resample the data so that each row, instead of representing a tweet, will6 18\n",
      "\n",
      "represent a single day with an aggregated sentiment score for each day:\n",
      "# create a dataframe called daily_tweets which resamples tweets\n",
      "by D, averaging the columns\n",
      "daily_tweets = tweets[['sentiment']] 41\n",
      "resample('D', how='mean')\n",
      "# I only want the sentiment column in my new Dataframe 21\n",
      "\n",
      "daily_tweets 3\n",
      "head()\n",
      "That's looking better 6\n",
      "Now, each row represents a single day and the sentiment score \n",
      "column  is showing us an average  sentiment for the day 25\n",
      "Let's see how many days' worth of\n",
      "tweets we have:\n",
      "daily_tweets 16\n",
      "shape\n",
      "(23, 1)\n",
      "OK, so we went from over 50,000 tweets to only 23 days 25\n",
      "Now, let's take a look at the\n",
      "progression of sentiment over several days :\n",
      "# plot the sentiment as a line graph\n",
      "daily_tweets 29\n",
      "sentiment 1\n",
      "plot(kind='line')\n",
      "\n",
      "\n",
      "Average daily sentiment in regard to a speciﬁc company for 23 days in May 2015\n",
      "import pandas as pd\n",
      "import pandas_datareader as pdr\n",
      "import datetime\n",
      "historical_prices = pdr 50\n",
      "get_data_yahoo('AAPL',\n",
      "                          start=datetime 11\n",
      "datetime(2015, 5, 2),\n",
      "                          end=datetime 14\n",
      "datetime(2015, 5, 25))\n",
      "prices = pd 14\n",
      "DataFrame(historical_prices)\n",
      "prices 6\n",
      "head()\n",
      "\n",
      "\n",
      "Now, two things are primarily of interest to us here:\n",
      "We are really only interested in the Close  column, which is the final price set for\n",
      "the trading day 36\n",
      "\n",
      "We also need to set the index  of this DataFrame to be datetimes,  so that we can\n",
      "merge the sentiment and the price DataFrames together:\n",
      "prices 34\n",
      "info() #the columns aren't numbers 8\n",
      "\n",
      "<class 'pandas 5\n",
      "core 1\n",
      "frame 1\n",
      "DataFrame'>\n",
      "DatetimeIndex: 15 entries, 2015-05-22 to 2015-05-04\n",
      "Data columns (total 8 columns):\n",
      "Adj_Close    15 non-null object\n",
      "Close        15 non-null object        # NOT A NUMBER\n",
      "Date         15 non-null object\n",
      "High         15 non-null object\n",
      "Low          15 non-null object\n",
      "Open         15 non-null object\n",
      "Symbol       15 non-null object\n",
      "Volume       15 non-null object\n",
      "dtypes: object(8)\n",
      "Let's fix that 114\n",
      "While we're at it, let's also fix Volume , which represents t he number of stocks\n",
      "traded on that day:\n",
      "# cast the column as numbers\n",
      "prices 34\n",
      "Close= not_null_close 5\n",
      "Close 1\n",
      "astype('float')\n",
      "prices 6\n",
      "Volume = not_null_close 5\n",
      "Volume 1\n",
      "astype('float')\n",
      "Now, let's try to plot both the volume and price of AAPL in the same graph:\n",
      "# plot both volume and close as line graphs in the same graph, what do you\n",
      "notice is the problem 47\n",
      "\n",
      "prices[[\"Volume\", 'Close']] 9\n",
      "plot()\n",
      "\n",
      "\n",
      "Trade volume versus date\n",
      "Woah, what's wrong here 14\n",
      "Well, if we look carefully, Volume  and Close  are on very\n",
      "different scales 18\n",
      "\n",
      "prices[[\"Volume\", 'Close']] 9\n",
      "describe()\n",
      "\n",
      "\n",
      "And by a lot 6\n",
      "The Volume  column has a mean in the tens of millions, while the average\n",
      "closing price is merely 125 23\n",
      "\n",
      "from sklearn 3\n",
      "preprocessing import StandardScaler\n",
      "# scale the columns by z scores using StandardScaler\n",
      "# Then plot the scaled data\n",
      "s = StandardScaler()\n",
      "only_prices_and_volumes = prices[[\"Volume\", 'Close']]\n",
      "price_volume_scaled = s 47\n",
      "fit_transform(only_prices_and_volumes)\n",
      "pd 10\n",
      "DataFrame(price_volume_scaled, columns=[\"Volume\", 'Close']) 12\n",
      "plot()\n",
      "Correlation of volume and closing prices \n",
      "That looks much better 14\n",
      "You can see how, as the price of AAPL went down somewhere in\n",
      "the middle, the volume of trading also went up 26\n",
      "This is actually fairly common:\n",
      "# concatinate prices 10\n",
      "Close, and daily_tweets 5\n",
      "sentiment\n",
      "merged = pd 5\n",
      "concat([prices 3\n",
      "Close, daily_tweets 4\n",
      "sentiment], axis=1)\n",
      "merged 7\n",
      "head()\n",
      "\n",
      "\n",
      "Hmm, why are there some null Close  values 12\n",
      "Well, if you look up May 2, 2015 on a\n",
      "calendar, you will see that it is a Saturday and the markets are closed on Saturdays,\n",
      "meaning there cannot be a closing price 41\n",
      "So, we need to make a decision on whether or not\n",
      "to remove these rows because we still have sentiment for that day 25\n",
      "Eventually, we will be\n",
      "attempting to predict the next day's closing price and whether the price increased or not, so\n",
      "let's go ahead and remove any null values in our dataset:\n",
      "# Delete any rows with missing values in any column\n",
      "merged 51\n",
      "dropna(inplace=True)\n",
      "Now, let's attempt to graph our plot:\n",
      "merged 16\n",
      "plot()\n",
      "# wow that looks awful\n",
      "Closing price/sentiment score versus date\n",
      "\n",
      "Wow, that's terrible 22\n",
      "Once again, we must scale our features in order to gain any valuable\n",
      "insight:\n",
      "# scale the columns by z scores using StandardScaler\n",
      "from sklearn 31\n",
      "preprocessing import StandardScaler\n",
      "s = StandardScaler()\n",
      "merged_scaled = s 14\n",
      "fit_transform(merged)\n",
      "pd 6\n",
      "DataFrame(merged_scaled, columns=merged 8\n",
      "columns) 2\n",
      "plot()\n",
      "# notice how sentiment seems to follow the closing price\n",
      "Closing price versus sentiment score \n",
      "Much better 21\n",
      "You can start to see how the closing price of the stock actually does seem to\n",
      "move with our sentiment 21\n",
      "Let's take this one step further and attempt to apply a supervised\n",
      "learning model 16\n",
      "For this to work, we need to define our features and our response 14\n",
      "Recall\n",
      "that our response is the value that we wish to predict, and our features are values that we\n",
      "will use to predict the response 28\n",
      "\n",
      "If we look at each row of our data, we have a sentiment and closing price for that day 21\n",
      "\n",
      "However, we wish to use today's sentiment to predict tomorrow's stock price and whether\n",
      "it increased or not 23\n",
      "Think about it; it would be kind of cheating because today's sentiment\n",
      "will include tweets from after the closing price was finalized 25\n",
      "To simplify this, we will\n",
      "ignore any tweet as a feature for the prediction of today's price 20\n",
      "\n",
      "\n",
      "So, for each row, our response should be today's closing price, while our feature should be\n",
      "yesterday's sentiment of the stock 29\n",
      "To do this, I will use a built-in function in Pandas called\n",
      "shift  to shift our sentiment column one item backward:\n",
      "# Shift the sentiment column backwards one item\n",
      "merged['yesterday_sentiment'] = merged['sentiment'] 49\n",
      "shift(1)\n",
      "merged 5\n",
      "head()\n",
      "Dataframe with yesterday's sentiment included\n",
      "Ah good, so now, for each  day we have  our true feature, which is yesterday_sentiment 32\n",
      "\n",
      "Note that in our heads (first five rows), we have a new null value 17\n",
      "This is because, on the\n",
      "first day, we don't have a value from yesterday, so we will have to remove it 26\n",
      "But before we\n",
      "do, let's define our response column 12\n",
      "\n",
      "We have two options:\n",
      "Keep our response quantitative and use a regression analysis\n",
      "Convert our response to a qualitative state and use classification\n",
      "Which route to choose is up to the data scientist and depends on the situation 42\n",
      "If you\n",
      "merely wish to associate sentiment with a movement in price, then I recommend using the\n",
      "classification route 23\n",
      "If you wish to associate sentiment with the amount of movement, I\n",
      "recommend a regression 17\n",
      "I will do both 4\n",
      "\n",
      "\n",
      "Regression route\n",
      "We are already good to go on this front 13\n",
      "We have our response and our single feature 8\n",
      "We\n",
      "will first have to remove  that one null value  before continuing:\n",
      "# Make a new dataframe for our regression and drop the null values\n",
      "regression_df = merged[['yesterday_sentiment', 'Close']]\n",
      "regression_df 47\n",
      "dropna(inplace=True)\n",
      "regression_df 8\n",
      "head()\n",
      "Let's use both a random forest  and a linear regression and see which performs better, using\n",
      "root-mean-square error  (RMSE ) as our metric:\n",
      "# Imports for our regression\n",
      "from sklearn 43\n",
      "linear_model import LinearRegression\n",
      "from sklearn 8\n",
      "ensemble import RandomForestRegressor\n",
      "from sklearn 7\n",
      "cross_validation import cross_val_score\n",
      "import numpy as np\n",
      "We will use a cross-validated RMSE in order to compare our two models:\n",
      "# Our RMSE as a result of cross validation linear regression\n",
      "linreg = LinearRegression()\n",
      "rmse_cv = np 53\n",
      "sqrt(abs(cross_val_score(linreg,\n",
      "regression_df[['yesterday_sentiment']], regression_df['Close'], cv=3,\n",
      "scoring='mean_squared_error') 35\n",
      "mean()))\n",
      "rmse_cv\n",
      "3 7\n",
      "49837\n",
      "# Our RMSE as a result of cross validation random forest\n",
      "rf = RandomForestRegressor()\n",
      "\n",
      "rmse_cv = np 27\n",
      "sqrt(abs(cross_val_score(rf,\n",
      "regression_df[['yesterday_sentiment']], regression_df['Close'], cv=3,\n",
      "scoring='mean_squared_error') 34\n",
      "mean()))\n",
      "rmse_cv\n",
      "3 7\n",
      "30603\n",
      "Look at our RMSE; it's about 3 15\n",
      "5 for both models, meaning that on average, our model is off\n",
      "by about 3 20\n",
      "5 dollars, which is actually a big deal considering our stock price likely doesn't\n",
      "move that much:\n",
      "regression_df['Close'] 28\n",
      "describe()\n",
      "count     14 6\n",
      "000000\n",
      "mean     128 8\n",
      "132858\n",
      "std        2 8\n",
      "471810    # Our standard deviation is less than our RMSE (bad\n",
      "sign)\n",
      "min      125 23\n",
      "010002\n",
      "25%      125 9\n",
      "905003\n",
      "50%      128 9\n",
      "195003\n",
      "75%      130 9\n",
      "067505\n",
      "max      132 8\n",
      "539993\n",
      "Another way to test the validity of our model is by comparing our RMSE to the null\n",
      "model's RMSE 27\n",
      "The null model for a regression model is predicting the average value for\n",
      "each value:\n",
      "# null model for regression\n",
      "mean_close = regression_df['Close'] 31\n",
      "mean()\n",
      "preds = [mean_close]*regression_df 12\n",
      "shape[0]\n",
      "preds\n",
      "from sklearn 9\n",
      "metrics import mean_squared_error\n",
      "null_rmse = np 11\n",
      "sqrt(mean_squared_error(preds, regression_df['Close']))\n",
      "null_rmse\n",
      "2 17\n",
      "381895\n",
      "Because our model did not beat the null model, perhaps regression isn't the best way to go 23\n",
      "\n",
      "\n",
      "Classification route\n",
      "For classification, we have a bit more  work to do because  we don't have a categorical\n",
      "response yet 27\n",
      "To make one, we need to transform the closing column into some categorical\n",
      "option 16\n",
      "I will choose to make the following response 8\n",
      "I will make a new column called\n",
      "change_close_big_deal , defined as follows:\n",
      "So, our response will be 1 if our response changed significantly, and 0 if the change in stock\n",
      "was negligible:\n",
      "# Imports for our classification\n",
      "from sklearn 52\n",
      "linear_model import LogisticRegression\n",
      "from sklearn 8\n",
      "ensemble import RandomForestClassifier\n",
      "from sklearn 7\n",
      "cross_validation import cross_val_score\n",
      "import numpy as np\n",
      "# Make a new dataframe for our classification and drop the null values\n",
      "classification_df = merged[['yesterday_sentiment', 'Close']]\n",
      "# variable to represent yesterday's closing price\n",
      "classification_df['yesterday_close'] = classification_df['Close'] 61\n",
      "shift(1)\n",
      "# column that represents the precent change in price since yesterday\n",
      "classification_df['percent_change_in_price'] = (classification_df['Close']-\n",
      "classification_df['yesterday_close']) /\n",
      "classification_df['yesterday_close']\n",
      "# drop any null values\n",
      "classification_df 56\n",
      "dropna(inplace=True)\n",
      "classification_df 7\n",
      "head()\n",
      "# Our new classification response\n",
      "classification_df['change_close_big_deal'] =\n",
      "abs(classification_df['percent_change_in_price'] ) > 30\n",
      "01\n",
      "classification_df 5\n",
      "head()\n",
      "\n",
      "\n",
      "Our DataFrame with a new column called change_close_big_deal  is either True  or\n",
      "False 22\n",
      "\n",
      "Let's now perform the same cross-validation as we did with our regression, but this time,\n",
      "we will be using the accuracy  feature of our cross-validation module and, instead of a \n",
      "regression  module, we will be using  two classification machine learning algorithms:\n",
      "# Our accuracy as a result of cross validation random forest\n",
      "rf = RandomForestClassifier()\n",
      "accuracy_cv = cross_val_score(rf,\n",
      "classification_df[['yesterday_sentiment']],\n",
      "classification_df['change_close_big_deal'], cv=3,\n",
      "scoring='accuracy') 107\n",
      "mean()\n",
      "accuracy_cv\n",
      "0 6\n",
      "1777777\n",
      "Gosh 7\n",
      "Not so good, so let's try logistic regression instead:\n",
      "# Our accuracy as a result of cross validation logistic regression\n",
      "logreg = LogisticRegression()\n",
      "accuracy_cv = cross_val_score(logreg,\n",
      "classification_df[['yesterday_sentiment']],\n",
      "classification_df['change_close_big_deal'], cv=3,\n",
      "scoring='accuracy') 65\n",
      "mean()\n",
      "accuracy_cv\n",
      "0 6\n",
      "5888\n",
      "\n",
      "Better 5\n",
      "But, of course, we should check  it with our null model's accuracy:\n",
      "# null model for classification\n",
      "null_accuracy = 1 - classification_df['change_close_big_deal'] 38\n",
      "mean()\n",
      "null_accuracy\n",
      "0 6\n",
      "5833333\n",
      "Whoa, our model can beat the null accuracy,  meaning that our machine learning algorithm\n",
      "can predict the movement of a stock price using social media sentiment better than just\n",
      "randomly guessing 43\n",
      "\n",
      "Going beyond with this example\n",
      "There are many ways that we could have  enhanced this example to make a more robust\n",
      "prediction 26\n",
      "We could have included more features, including a moving average of\n",
      "sentiment, instead of looking simply at the previous day's sentiment 26\n",
      "We could have also\n",
      "brought in more examples to enhance our idea of sentiment 16\n",
      "We could have looked at\n",
      "Facebook, the media, and so on, for more information on how we believe the stock will\n",
      "perform in the future 30\n",
      "\n",
      "We really only had 14 data points, which is far from sufficient to make a production-ready\n",
      "algorithm 22\n",
      "Of course, for the purposes of this book, this is enough, but if we are serious\n",
      "about making a financial algorithm that can effectively predict stock price movement, we\n",
      "will have to obtain many more days of media coverage and prices 47\n",
      "\n",
      "We could have spent more time optimizing our parameters in our models by utilizing the\n",
      "gridsearchCV  module in the sklearn  package to get the most out of our models 35\n",
      "There\n",
      "are other models  that exist that deal specifically with time series data (data that changes\n",
      "over time), including a model called AutoRegressive Integrated Moving Average\n",
      "(ARIMA ) 38\n",
      "Models such as ARIMA and similar ones attempt to focus and zero in on specific\n",
      "time series features 20\n",
      "\n",
      "\n",
      "Case study 2 – Why do some people cheat\n",
      "on their spouses 15\n",
      "\n",
      "In 1978, a survey was conducted  on housewives in order to discern factors that led them to\n",
      "pursue extramarital affairs 31\n",
      "This study became the basis for many future studies of both\n",
      "men and women, all attempting to focus on features of people and marriages that led either\n",
      "partner to seek partners elsewhere behind their spouse's back 40\n",
      "\n",
      "Supervised learning is not always about prediction 9\n",
      "In this case study, we will purely\n",
      "attempt to identify a few factors of the many that we believe might be the most important\n",
      "factors that lead someone to pursue an affair 36\n",
      "\n",
      "First, let's read in the data:\n",
      "# Using dataset of a 1978 survey conducted to measure likliehood of women\n",
      "to perform extramarital affairs\n",
      "# http://statsmodels 40\n",
      "sourceforge 2\n",
      "net/stable/datasets/generated/fair 8\n",
      "html\n",
      "import statsmodels 5\n",
      "api as sm\n",
      "affairs_df = sm 9\n",
      "datasets 1\n",
      "fair 1\n",
      "load_pandas() 4\n",
      "data\n",
      "affairs_df 5\n",
      "head()\n",
      "The statsmodels  website provides a data dictionary, as follows:\n",
      "rate_marriage : The rating given to the marriage (given by the wife); 1 = very\n",
      "poor, 2 = poor , 3 = fair , 4 = good , 5 = very good ; ordinal level\n",
      "age: Age of the wife; ratio level\n",
      "yrs_married : Number of years married: ratio level\n",
      "\n",
      "children : Number of children between husband and wife: ratio level\n",
      "religious : How religious the wife is; 1 = not , 2 = mildly , 3 = fairly , 4 = strongly ;\n",
      "ordinal level\n",
      "educ : Level of education; 9 = grade school , 12 = high school , 14 = some college , 16 =\n",
      "college graduate , 17 = some graduate school , 20 = advanced degree ; ratio level\n",
      "occupation : 1 = student;  2 = farming, agriculture; semi-skilled, or unskilled worker ; 3\n",
      "= white-collar ; 4 = teacher, counselor, social worker, nurse; artist, writer; technician,\n",
      "skilled worker;  5 = managerial, administrative, business;  6 = professional with advanced\n",
      "degree ; nominal level\n",
      "occupation_husb : Husband's occupation 260\n",
      "Same as occupation; nominal level\n",
      "affairs : Measure of time spent in extramarital affairs; ratio level\n",
      "Okay, so we have a quantitative response, but my question is simply what factors cause\n",
      "someone to have an affair 46\n",
      "The exact number of minutes or hours does not really matter\n",
      "that much 14\n",
      "For this reason, let's make a new categorical variable called affair_binary ,\n",
      "which is either true  (they had an affair for more than 0 minutes) or false  (they had an\n",
      "affair for 0 minutes):\n",
      "# Create a categorical variable\n",
      "affairs_df['affair_binary'] = (affairs_df['affairs'] > 0)\n",
      "Again, this column has either a true  or a false  value 88\n",
      "The value is true  if the person had\n",
      "an extramarital affair for more than 0 minutes 21\n",
      "The value is false  otherwise 6\n",
      "From now on,\n",
      "let's use this binary  response as our primary response 15\n",
      "Now, we are trying to find which of\n",
      "these variables are associated with our response, so let's begin 22\n",
      "\n",
      "Let's start with a simple correlation matrix 9\n",
      "Recall that this matrix shows us linear\n",
      "correlations between our quantitative variables and our response 17\n",
      "I will show the\n",
      "correlation matrix as both a matrix of decimals and also as a heat map 20\n",
      "Let's see the\n",
      "numbers first:\n",
      "# find linear correlations between variables and affair_binary\n",
      "affairs_df 21\n",
      "corr()\n",
      "\n",
      "\n",
      "Correlation matrix for extramarital aﬀairs data from a Likert survey conducted in 1978\n",
      "Remember that we ignore the diagonal series of 1s because they are merely telling us that\n",
      "every quantitative variable is correlated with itself 52\n",
      "Note the other correlated variables,\n",
      "which are the values closest to 1 and -1 in the last row or column (the matrix is always\n",
      "symmetrical across the diagonal) 35\n",
      "\n",
      "We can see a few standout variables:\n",
      "affairs\n",
      "age\n",
      "yrs_married\n",
      "children\n",
      "These are the top four variables with the largest magnitude (absolute value) 35\n",
      "However, one\n",
      "of these variables is cheating 9\n",
      "The affairs  variable is the largest in magnitude, but is\n",
      "obviously correlated to affair_binary  because we made the affair_binary\n",
      "variable directly based on affairs 32\n",
      "So let's ignore that one 6\n",
      "\n",
      "\n",
      "Let's take a look at our correlation heat map to see whether our views can be seen there:\n",
      "import seaborn as sns\n",
      "sns 27\n",
      "heatmap(affairs_df 5\n",
      "corr())\n",
      "Correlation matrix\n",
      "The same correlation matrix, but this time  as a heat map 19\n",
      "Note the colors close to dark red\n",
      "and dark blue (excluding the diagonal) 16\n",
      "\n",
      "We are looking for the dark red and dark blue areas of the heat map 16\n",
      "These colors are\n",
      "associated with the most correlated features 10\n",
      "\n",
      "Remember correlations are not the only way to identify which features are associated with\n",
      "our response 18\n",
      "This method shows us how linearly correlated the variables are with each\n",
      "other 15\n",
      "We may find another variable that affects affairs by evaluating the coefficients of a\n",
      "decision tree classifier 18\n",
      "These methods might reveal new variables that are associated with\n",
      "our variables, but not in a linear fashion 20\n",
      "\n",
      "\n",
      "Also notice that there are two variables here that don't actually belong …\n",
      "Can you spot them 20\n",
      "These are the occupation  and occupation_husb\n",
      "variables 11\n",
      "Recall earlier that we deemed them as nominal and therefore\n",
      "have no right to be included in this correlation matrix 21\n",
      "This is because\n",
      "Pandas, unknowingly, casts them as integers and now considers them as\n",
      "quantitative variables 23\n",
      "Don't worry, we will fix this soon 9\n",
      "\n",
      "First, let's make ourselves an X and a y DataFrame:\n",
      "affairs_X = affairs_df 20\n",
      "drop(['affairs', 'affair_binary'], axis=1)\n",
      "# data without the affairs or affair_binary column\n",
      "affairs_y = affairs_df['affair_binary']\n",
      "Now, we will instantiate a decision tree classifier and cross-validate our model in order to\n",
      "determine whether or not the model is doing an okay job at fitting our data:\n",
      "from sklearn 72\n",
      "tree import DecisionTreeClassifier\n",
      "model = DecisionTreeClassifier()\n",
      "# instantiate the model\n",
      "from sklearn 19\n",
      "cross_validation import cross_val_score\n",
      "# import our cross validation module\n",
      "# check the accuracy on the training set\n",
      "scores = cross_val_score(model, affairs_X, affairs_y, cv=10)\n",
      "print( scores 43\n",
      "mean(), \"average accuracy\" )\n",
      "0 8\n",
      "659756806845 average accuracy\n",
      "print( scores 11\n",
      "std(), \"standard deviation\") # very low, meaning variance of\n",
      "the model is low\n",
      "0 20\n",
      "0204081732291 standard deviation\n",
      "# Looks ok on the cross validation side\n",
      "Because our standard deviation is low, we may make  the assumption that the variance of\n",
      "our model is low (because variance is the square of standard deviation) 50\n",
      "This is good\n",
      "because that means that our model is not fitting wildly differently on each fold of the cross\n",
      "validation and that it is generally a reliable model 31\n",
      "\n",
      "\n",
      "Because we agree that our decision tree classifier is generally a reliable model, we can fit the\n",
      "tree to our entire dataset and use the importance metric to identify which variables our tree\n",
      "deems the most important:\n",
      "# Explore individual features that make the biggest impact\n",
      "# rate_marriage, yrs_married, and occupation_husb 66\n",
      "But one of these\n",
      "variables doesn't quite make sense right 12\n",
      "\n",
      "# Its the occupation variable, because they are nominal, their\n",
      "interpretations\n",
      "model 18\n",
      "fit(affairs_X, affairs_y)\n",
      "pd 10\n",
      "DataFrame({'feature':affairs_X 7\n",
      "columns,\n",
      "'importance':model 7\n",
      "feature_importances_}) 5\n",
      "sort_values('importance') 6\n",
      "tail(3)\n",
      "So, yrs_married  and rate_marriage  both are important, but the most important\n",
      "variable is occupation_husb 29\n",
      "But that doesn't make sense because that variable is\n",
      "nominal 13\n",
      "So, let's apply our dummy variable technique wherein we create new columns\n",
      "that represent each option for occupation_husb  and also for occupation 28\n",
      "\n",
      "Firstly, for the occupation  column:\n",
      "# Dummy Variables:\n",
      "# Encoding qualitiative (nominal) data using separate columns (see slides\n",
      "for linear regression for more)\n",
      "occuptation_dummies = pd 44\n",
      "get_dummies(affairs_df['occupation'],\n",
      "prefix='occ_') 15\n",
      "iloc[:, 1:]\n",
      "# concatenate the dummy variable columns onto the original DataFrame\n",
      "(axis=0 means rows, axis=1 means columns)\n",
      "affairs_df = pd 34\n",
      "concat([affairs_df, occuptation_dummies], axis=1)\n",
      "affairs_df 19\n",
      "head()\n",
      "\n",
      "This new DataFrame has many new columns:\n",
      "Remember, these new columns, occ_2 19\n",
      "0 , occ_4 6\n",
      "0 , and so on, represent a binary variable\n",
      "that represents whether  or not the wife holds job 2, or 4, and so on:\n",
      "# Now for the husband's job\n",
      "occuptation_dummies = pd 48\n",
      "get_dummies(affairs_df['occupation_husb'],\n",
      "prefix='occ_husb_') 19\n",
      "iloc[:, 1:]\n",
      "# concatenate the dummy variable columns onto the original DataFrame\n",
      "(axis=0 means rows, axis=1 means columns)\n",
      "affairs_df = pd 34\n",
      "concat([affairs_df, occuptation_dummies], axis=1)\n",
      "affairs_df 19\n",
      "head()\n",
      "(6366, 15)\n",
      "Now we have 15 new columns 16\n",
      "Let's run our tree again and find the most important\n",
      "variables:\n",
      "# remove appropiate columns for feature set affairs_X =\n",
      "affairs_df 29\n",
      "drop(['affairs', 'affair_binary', 'occupation',\n",
      "'occupation_husb'], axis=1) affairs_y = affairs_df['affair_binary'] model =\n",
      "DecisionTreeClassifier() from sklearn 40\n",
      "cross_validation import\n",
      "cross_val_score # check the accuracy on the training set scores =\n",
      "cross_val_score(model, affairs_X, affairs_y, cv=10) print scores 34\n",
      "mean(),\n",
      "\"average accuracy\"\n",
      "print (scores 9\n",
      "std(), \"standard deviation\") # very low, meaning variance of\n",
      "the model is low # Still looks ok # Explore individual features that make\n",
      "the biggest impact model 33\n",
      "fit(affairs_X, affairs_y)\n",
      "\n",
      "pd 10\n",
      "DataFrame({'feature':affairs_X 7\n",
      "columns,\n",
      "'importance':model 7\n",
      "feature_importances_}) 5\n",
      "sort_values('importance') 6\n",
      "tail(10\n",
      ")\n",
      "And there you have it:\n",
      "rate_marriage : The rating of the marriage, as told by the decision tree\n",
      "children : The number of children they had, as told by the decision tree and our\n",
      "correlation matrix\n",
      "yrs_married : The number of years they  had been married, as told by the\n",
      "decision tree and our correlation matrix\n",
      "educ : The level of education the women had, as told by the decision tree\n",
      "age: The age of the women, as told by the decision tree and our correlation\n",
      "matrix\n",
      "These seem to be the top five most important variables in determining whether or not a\n",
      "woman from the 1978 survey would be involved in an extramarital affair 147\n",
      "\n",
      "Case study 3 – Using TensorFlow\n",
      "I would like to finish off our time  together by looking at a somewhat more modern machine\n",
      "learning module called TensorFlow 32\n",
      "\n",
      "\n",
      "TensorFlow is an open source machine learning module that is used primarily for its\n",
      "simplified deep learning and neural network abilities 25\n",
      "I would like to take some time  to\n",
      "introduce the module and solve a few quick problems using TensorFlow 22\n",
      "The syntax for\n",
      "TensorFlow (like PyBrain in Chapter 12 , Beyond the Essentials ) is a bit different than our\n",
      "normal scikit-learn  syntax, so I will be going over it step by step 44\n",
      "Let's start with some\n",
      "imports:\n",
      "from sklearn import datasets, metrics\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "from sklearn 27\n",
      "cross_validation import train_test_split\n",
      "%matplotlib inline\n",
      "Our imports from sklearn  include train_test_split , datasets , and metrics 25\n",
      "We will\n",
      "be utilizing our train test splits to reduce overfitting, we will use datasets in order to import\n",
      "our iris  classification data, and we'll use the metrics module in order to calculate some\n",
      "simple metrics for our learning models 49\n",
      "\n",
      "TensorFlow learns in a different way in that it is always trying to minimize an error\n",
      "function 20\n",
      "It does this by iteratively going through our entire dataset and, every so often,\n",
      "updates our model to better fit the data 25\n",
      "\n",
      "It is important to note that TensorFlow doesn't just implement neural networks, but can\n",
      "implement even simpler models as well 24\n",
      "For example, let's implement a classic logistic\n",
      "regression using TensorFlow:\n",
      "# Our data set of iris flowers\n",
      "iris = datasets 26\n",
      "load_iris()\n",
      "# Load datasets and split them for training and testing\n",
      "X_train, X_test, y_train, y_test = train_test_split(iris 32\n",
      "data, iris 3\n",
      "target)\n",
      "####### TENSORFLOW #######\n",
      "# Here is tensorflow's syntax for defining features 17\n",
      "\n",
      "# We must specify that all features have real-value data\n",
      "feature_columns = [tf 18\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\", dimension=4)]\n",
      "# notice the dimension is set to four because we have four columns\n",
      "# We set our \"learning rate\" which is a decimal that tells the network\n",
      "# how quickly to learn\n",
      "optimizer = tf 49\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1)\n",
      "# A learning rate closer to 0 means the network will learn slower\n",
      "\n",
      "# Build a linear classifier (logistic regression)\n",
      "# note we have to tell tensorflow the number of classes we are looking for\n",
      "# which are 3 classes of iris\n",
      "classifier =\n",
      "tf 56\n",
      "contrib 1\n",
      "learn 1\n",
      "LinearClassifier(feature_columns=feature_columns,\n",
      "                                             optimizer=optimizer,\n",
      "                                                      n_classes=3)\n",
      "# Fit model 22\n",
      "Uses error optimization techniques like stochastic gradient\n",
      "descent\n",
      "classifier 12\n",
      "fit(x=X_train,\n",
      "               y=y_train,\n",
      "               steps=1000)  # number of iterations\n",
      "I will point out the key lines of code from the preceding snippet to really solidify what is\n",
      "happening during training:\n",
      "feature_columns = [tf 53\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\",1 6\n",
      "\n",
      "dimension=4)] :\n",
      "Here, I am creating four input  columns that we know correlate to the\n",
      "flowers' sepal length, sepal width, petal length, and petal width 40\n",
      "\n",
      "optimizer =2 4\n",
      "\n",
      "tf 2\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1) :\n",
      "Here, I am telling TensorFlow to optimize using something called\n",
      "gradient descent , which means that we will define an error function\n",
      "(which will happen in the next step) and, little by little, we will work\n",
      "our way to minimize this error function 54\n",
      "\n",
      "Our learning rate should hover close to 0 because we want our model\n",
      "to learn slowly 19\n",
      "If our model learns too quickly, it might \"skip over\" the\n",
      "correct answer 17\n",
      "\n",
      "classifier =3 4\n",
      "\n",
      "tf 2\n",
      "contrib 1\n",
      "learn 1\n",
      "LinearClassifier(feature_columns=feature_colum\n",
      "ns, optimizer=optimizer, n_classes=3) :\n",
      "When we specify LinearClassifier , we are denoting the same error\n",
      "function that logistic regression is minimizing, meaning that this\n",
      "classifier is attempting to work as a logistic regression classifier 56\n",
      "\n",
      "We give the model our feature_columns  as defined in Step 1 15\n",
      "\n",
      "The optimizer  is the method of minimizing our error function; in this\n",
      "case, we chose gradient descent 22\n",
      "\n",
      "We must also specify our number of classes as being 3 13\n",
      "We know that\n",
      "we have three different iris flowers that the model could choose from 16\n",
      "\n",
      "\n",
      "classifier 2\n",
      "fit(x=X_train, y=y_train, steps=1000) : 4 17\n",
      "\n",
      "The training looks similar to a scikit-learn model, with an added\n",
      "parameter called steps 20\n",
      "Steps tell us how many times we would like\n",
      "to go over our dataset 15\n",
      "So, when we specify 1000 , we are iterating over\n",
      "our dataset 16\n",
      "The more steps we take, the more the model gets a chance\n",
      "to learn 16\n",
      "\n",
      "Phew 3\n",
      "When we run the preceding  code, a linear classifier (logistic regression) model is\n",
      "being fit, and when it is done, it is ready to be tested:\n",
      "# Evaluate accuracy 38\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=X_test,\n",
      "                                     y=y_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 20\n",
      "format(accuracy_score))\n",
      "Accuracy: 0 9\n",
      "973684\n",
      "Excellent 5\n",
      "It is worth noting that when using TensorFlow, we may also utilize a similarly\n",
      "simple predict  function:\n",
      "# Classify two new flower samples 28\n",
      "\n",
      "new_samples = np 5\n",
      "array(\n",
      "    [[6 5\n",
      "4, 3 5\n",
      "2, 4 5\n",
      "5, 1 5\n",
      "5], [5 5\n",
      "8, 3 5\n",
      "1, 5 5\n",
      "0, 1 5\n",
      "7]], dtype=float)\n",
      "y = classifier 9\n",
      "predict(new_samples)\n",
      "print('Predictions: {}' 11\n",
      "format(str(y)))\n",
      "Predictions: [1 2]\n",
      "Now, let's compare this with a standard scikit-learn logistic regression to see who won:\n",
      "from sklearn 34\n",
      "linear_model import LogisticRegression\n",
      "# compare our result above to a simple scikit-learn logistic regression\n",
      "logreg = LogisticRegression()\n",
      "# instantiate the model\n",
      "logreg 34\n",
      "fit(X_train, y_train)\n",
      "# fit it to our training set\n",
      "y_predicted = logreg 20\n",
      "predict(X_test)\n",
      "# predict on our test set, to avoid overfitting 16\n",
      "\n",
      "accuracy = metrics 4\n",
      "accuracy_score(y_predicted, y_test)\n",
      "# get our accuracy score\n",
      "\n",
      "accuracy\n",
      "# It's the same thing 22\n",
      "\n",
      "Wow, so it seems that with 1,000 steps, a gradient descent-optimized TensorFlow model is\n",
      "no better than a simple sklearn logistic regression 31\n",
      "OK, that's fine, but what if we allowed\n",
      "the model to iterate over the iris  dataset even more 23\n",
      "\n",
      "feature_columns = [tf 6\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\", dimension=4)]\n",
      "optimizer = tf 12\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1)\n",
      "classifier =\n",
      "tf 6\n",
      "contrib 1\n",
      "learn 1\n",
      "LinearClassifier(feature_columns=feature_columns,\n",
      "                                               optimizer=optimizer,\n",
      "                                            n_classes=3)\n",
      "classifier 20\n",
      "fit(x=X_train,\n",
      "               y=y_train,\n",
      "               steps=2000)  # number of iterations is 2000 now\n",
      "Our code is exactly the same as before, but now we have 2000  steps instead of 1000 :\n",
      "# Evaluate accuracy 54\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=X_test,\n",
      "                                     y=y_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 20\n",
      "format(accuracy_score))\n",
      "Accuracy: 0 9\n",
      "973684\n",
      "And now we have even better accuracy 11\n",
      "\n",
      "Note that you need to be very careful in choosing the number of steps 15\n",
      "As\n",
      "you increase this number, you increase the number of times your model\n",
      "sees the exact same training points over and over again 27\n",
      "We do have a\n",
      "chance of overfitting 11\n",
      "To remedy this, I would recommend choosing\n",
      "multiple train/test splits and running the model on each one ( k-fold cross\n",
      "validation ) 27\n",
      "\n",
      "It is also worth mentioning that TensorFlow implements very low bias, high-variance\n",
      "models, meaning that running  the preceding code again for TensorFlow might result in a\n",
      "different answer 36\n",
      "This is one of the caveats of deep learning 10\n",
      "They might converge to a very\n",
      "great low bias model, but that model will have a high variance and, therefore, amazingly\n",
      "might not generalize to all of the sample data 35\n",
      "As mentioned before, cross validation would\n",
      "be helpful in order to mitigate this 15\n",
      "\n",
      "\n",
      "TensorFlow and neural networks\n",
      "Now, let's point a more powerful model  at our iris  dataset 22\n",
      "Let's create a neural network \n",
      "whose  goal it is to classify iris flowers (because why not 20\n",
      ":\n",
      "# Specify that all features have real-value data\n",
      "feature_columns = [tf 16\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\", dimension=4)]\n",
      "optimizer = tf 12\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1)\n",
      "# Build 3 layer DNN with 10, 20, 10 units respectively 21\n",
      "\n",
      "classifier =\n",
      "tf 4\n",
      "contrib 1\n",
      "learn 1\n",
      "DNNClassifier(feature_columns=feature_columns,\n",
      "                                        hidden_units=[10, 20, 10],\n",
      "                                        optimizer=optimizer,\n",
      "                                        n_classes=3)\n",
      "# Fit model 35\n",
      "\n",
      "classifier 2\n",
      "fit(x=X_train,\n",
      "               y=y_train,\n",
      "               steps=2000)\n",
      "Notice that our code  really hasn't changed  from the last segment 30\n",
      "We still have our\n",
      "feature_columns  from before, but now we introduce, instead of a linear classifier, a\n",
      "DNNClassifier , which stands for Deep Neural Network Classifier 35\n",
      "\n",
      "This is TensorFlow's syntax for implementing a neural network 11\n",
      "Let's take a closer look:\n",
      "tf 8\n",
      "contrib 1\n",
      "learn 1\n",
      "DNNClassifier(feature_columns=feature_columns,\n",
      "                                     hidden_units=[10, 20, 10],\n",
      "                                     optimizer=optimizer,\n",
      "                                     n_classes=3)\n",
      "We see that we are inputting the same feature_columns , n_classes , and optimizer ,\n",
      "but see how we have a new parameter called hidden_units 61\n",
      "This list represents the\n",
      "number of nodes to have in each layer between the input and the output layer 20\n",
      "\n",
      "\n",
      "All in all, this neural network will have five layers:\n",
      "The first layer will have four nodes, one for each of the iris feature variables, and\n",
      "this layer is the input layer\n",
      "A hidden layer of 10 nodes\n",
      "A hidden layer of 20 nodes\n",
      "A hidden layer of 10 nodes\n",
      "The final layer will have three nodes, one for each possible outcome of the\n",
      "network, and this is called our output layer\n",
      "Now that we've trained  our model, let's evaluate  it on our test set:\n",
      "# Evaluate accuracy 111\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=X_test,\n",
      "                                     y=y_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 20\n",
      "format(accuracy_score))\n",
      "Accuracy: 0 9\n",
      "921053\n",
      "Hmm, our neural network didn't do so well on this dataset, but perhaps it is because the\n",
      "network is a bit too complicated for such a simple dataset 36\n",
      "Let's introduce a new dataset\n",
      "that has a bit more to it 14\n",
      " 1\n",
      " 1\n",
      "\n",
      "The mnist  dataset consists of over 50,000 handwritten digits (0-9) and the goal is to\n",
      "recognize the handwritten digits and output which letter they are writing 37\n",
      "TensorFlow has\n",
      "a built-in mechanism for downloading and loading these images 13\n",
      "We've seen these images\n",
      "before, but on a much smaller scale in Chapter 12 , Beyond the Essentials :\n",
      "from tensorflow 25\n",
      "examples 1\n",
      "tutorials 1\n",
      "mnist import input_data\n",
      "mnist = input_data 10\n",
      "read_data_sets(\"MNIST_data/\", one_hot=False)\n",
      "Extracting MNIST_data/train-images-idx3-ubyte 24\n",
      "gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte 15\n",
      "gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte 16\n",
      "gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte 17\n",
      "gz\n",
      "Notice that one of our inputs for downloading mnist  is called one_hot 16\n",
      "This parameter\n",
      "either brings in the dataset's target variable (which is the digit itself) as a single number, or\n",
      "has a dummy variable 29\n",
      "\n",
      "\n",
      "For example, if the first digit were a 7, the target would be either of these:\n",
      "7: If one_hot  was false\n",
      "0 0 0 0 0 0 0 1 0 0 : If one_hot  was true  (notice that starting from 0, the seventh\n",
      "index is a 1)\n",
      "We will encode our target the former way, as this is what our TensorFlow neural network\n",
      "and our sklearn logistic regression will expect 99\n",
      "\n",
      "The dataset is split up already into a training and test set, so let's create new variables to\n",
      "hold them:\n",
      "x_mnist = mnist 30\n",
      "train 1\n",
      "images\n",
      "y_mnist = mnist 7\n",
      "train 1\n",
      "labels 1\n",
      "astype(int)\n",
      "For the y_mnist  variable, I specifically cast every target as an integer (by default, they\n",
      "come in as floats) because otherwise, TensorFlow would throw an error at us 41\n",
      "\n",
      "Out of curiosity, let's take a look  at a single image:\n",
      "import matplotlib 18\n",
      "pyplot as plt\n",
      "plt 5\n",
      "imshow(x_mnist[10] 7\n",
      "reshape(28, 28))\n",
      "The number 0 in the MNIST dataset\n",
      "\n",
      "And, hopefully, our target variable matches at the 10th index as well:\n",
      "y_mnist[10]\n",
      "0\n",
      "Excellent 43\n",
      "Let's now take a peek at how big our dataset is:\n",
      "x_mnist 16\n",
      "shape\n",
      "(55000, 784)\n",
      "y_mnist 12\n",
      "shape\n",
      "(55000,)\n",
      "Our training size then is 55000  images and target variables 19\n",
      "\n",
      "Let's fit a deep neural network to our images and see whether it will be able to pick up on\n",
      "the patterns in our inputs:\n",
      "# Specify that all features have real-value data feature_columns =\n",
      "[tf 43\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\", dimension=784)] optimizer =\n",
      "tf 12\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1) # Build 3 layer DNN\n",
      "with 10, 20, 10 units respectively 22\n",
      "classifier =\n",
      "tf 3\n",
      "contrib 1\n",
      "learn 1\n",
      "DNNClassifier(feature_columns=feature_columns,\n",
      "hidden_units=[10, 20, 10], optimizer=optimizer, n_classes=10) # Fit model 32\n",
      "\n",
      "classifier 2\n",
      "fit(x=x_mnist, y=y_mnist, steps=1000) # Warning this is\n",
      "veryyyyyyyy slow\n",
      "This code is very similar to our previous  segment using DNNClassifier ; however, look\n",
      "how, in our first line of code, I have changed the number of columns to be 784 while, in the\n",
      "classifier itself, I changed the number of output classes to be 10 84\n",
      "These are manual  inputs\n",
      "that TensorFlow must be given to work 13\n",
      "\n",
      "The preceding code runs very slowly 7\n",
      "It is adjusting itself, little by little,  in order to get the\n",
      "best possible performance from our training set 23\n",
      "Of course, we know that the ultimate test\n",
      "here is testing our network on an unknown test set, which is also given to us by\n",
      "TensorFlow:\n",
      "x_mnist_test = mnist 38\n",
      "test 1\n",
      "images\n",
      "y_mnist_test = mnist 8\n",
      "test 1\n",
      "labels 1\n",
      "astype(int)\n",
      "x_mnist_test 8\n",
      "shape\n",
      "(10000, 784)\n",
      "y_mnist_test 13\n",
      "shape\n",
      "(10000,)\n",
      "\n",
      "So we have 10,000 images to test on; let's see how our network was able to adapt to the\n",
      "dataset:\n",
      "# Evaluate accuracy 37\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=x_mnist_test,\n",
      "                                     y=y_mnist_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 24\n",
      "format(accuracy_score))\n",
      "Accuracy: 0 9\n",
      "920600\n",
      "Not bad, 92% accuracy on our dataset 14\n",
      "Let's take a second and compare this\n",
      "performance to a standard sklearn logistic regression now:\n",
      "logreg = LogisticRegression()\n",
      "logreg 26\n",
      "fit(x_mnist, y_mnist)\n",
      "# Warning this is slow\n",
      "y_predicted = logreg 20\n",
      "predict(x_mnist_test)\n",
      "from sklearn 8\n",
      "metrics import accuracy_score\n",
      "# predict on our test set, to avoid overfitting 17\n",
      "\n",
      "accuracy = accuracy_score(y_predicted, y_mnist_test)\n",
      "# get our accuracy score\n",
      "accuracy\n",
      "0 22\n",
      "91969\n",
      "Success 5\n",
      "Our neural network performed better  than the standard logistic regression 11\n",
      "This is\n",
      "likely because the network is attempting to find relationships between the pixels\n",
      "themselves and using these relationships to map them to what digit we are writing down 33\n",
      "\n",
      "In logistic regression, the model assumes that every single input is independent of one\n",
      "another, and therefore has a tough time finding relationships between them 29\n",
      "\n",
      "\n",
      "There are ways of making our neural network learn differently:\n",
      "We could make our network wider, that is, increase the number of nodes in the\n",
      "hidden layers instead of having several layers of a smaller number of nodes:\n",
      "Artiﬁcial neural network (Source: http://electronicdesign 59\n",
      "com/site-ﬁles/electronicdesign 10\n",
      "com/ﬁles/uploads/2015/02/0816_Development_Tools_F1_0 22\n",
      "gif)\n",
      "# A wider network\n",
      "feature_columns = [tf 12\n",
      "contrib 1\n",
      "layers 1\n",
      "real_valued_column(\"\",\n",
      "dimension=784)]\n",
      "optimizer = tf 13\n",
      "train 1\n",
      "GradientDescentOptimizer(learning_rate= 8\n",
      "1)\n",
      "# Build 3 layer DNN with 10, 20, 10 units respectively 21\n",
      "\n",
      "classifier =\n",
      "tf 4\n",
      "contrib 1\n",
      "learn 1\n",
      "DNNClassifier(feature_columns=feature_columns,\n",
      "                                     hidden_units=[1500],\n",
      "                                     optimizer=optimizer,\n",
      "                                            n_classes=10)\n",
      "# Fit model 30\n",
      "\n",
      "classifier 2\n",
      "fit(x=x_mnist,\n",
      "               y=y_mnist,\n",
      "               steps=100)\n",
      "# Warning this is veryyyyyyyy slow\n",
      "# Evaluate accuracy 29\n",
      "\n",
      "accuracy_score = classifier 5\n",
      "evaluate(x=x_mnist_test,\n",
      "                                     y=y_mnist_test)[\"accuracy\"]\n",
      "print('Accuracy: {0:f}' 24\n",
      "format(accuracy_score))\n",
      "      Accuracy: 0 10\n",
      "898400\n",
      "\n",
      "We could increase our learning rate, forcing the network to attempt to converge\n",
      "on an answer faster 23\n",
      "As I mentioned before, we run the risk of the model\n",
      "skipping the answer entirely if we go down this route 24\n",
      "It is usually better  to stick\n",
      "with a smaller learning rate 13\n",
      "\n",
      "We can change the method  of optimization 9\n",
      "Gradient descent is very popular;\n",
      "however, there are other algorithms for doing this 15\n",
      "One example is called the\n",
      "Adam Optimizer 9\n",
      "The difference is in the way they traverse the error function,\n",
      "and therefore the way that they approach the optimization point 22\n",
      "Different\n",
      "problems in different domains call for different optimizers 11\n",
      "\n",
      "There is no replacement for a good old fashioned feature selection phase, instead\n",
      "of attempting to let the network figure everything out for us 27\n",
      "We can take the\n",
      "time to find relevant and meaningful features that actually will allow our\n",
      "network to find an answer quicker 24\n",
      "\n",
      "Summary\n",
      "In this chapter, we've seen three different case studies from three different domains using\n",
      "many different statistical and machine learning methods 27\n",
      "However, what all of them have\n",
      "in common is that in order to solve them properly, we had to implement a data science\n",
      "mindset 29\n",
      "We had to solve problems in an interesting way, obtain data, clean the data,\n",
      "visualize the data, and finally model the data and evaluate our thinking process 32\n",
      "\n",
      "I do hope that you have found the contents of this book to be interesting and not just the\n",
      "final chapter 23\n",
      "I leave it unto you to keep exploring the world of data science 13\n",
      "Keep learning\n",
      "Python 4\n",
      "Keep learning statistics and probability 5\n",
      "Keep your minds open 4\n",
      "It is my hope that\n",
      "this book has been a catalyst for you to go out and find even more on the subject 24\n",
      "\n",
      "For further reading beyond this book, I highly recommend looking into well-known data\n",
      "science books and blogs, such as:\n",
      "Dataschool 27\n",
      "io, a blog by Kevin Markham\n",
      "Python for Data Scientists,  by Packt\n",
      "If you would like to contact me for any reason, please feel free to reach out to\n",
      "sinan 40\n",
      "u 1\n",
      "ozdemir@gmail 4\n",
      "com 1\n",
      "\n",
      "\n",
      "4\n",
      "Microsoft Azure Databricks\n",
      "The main purpose of this chapter is to highlight the Microsoft Data Environment and how\n",
      "we can utilize the many tools provided to us, especially Azure Databricks: a fullyfledged,\n",
      "powerful analytics platform powered by Apache Spark 54\n",
      "\n",
      "We have three main case studies in this chapter that join together the principles of data\n",
      "science and machine learning that we have learned about in this book with the ease and\n",
      "power of the Microsoft Data Environment, specifically Azure Databricks 47\n",
      "Each case study\n",
      "will highlight different features of using Azure Databricks, as well as aspects of machine\n",
      "learning that we have learned in this text 30\n",
      "\n",
      "The Microsoft data science environment\n",
      "Microsoft offers many tools and environments to the data scientist, and this offering\n",
      "consists of many parts 27\n",
      "The three main components are as follows:\n",
      "Microsoft Azure : This is an  enterprise-grade cloud computing platform that\n",
      "provides a great  deal of access and support for production-ready scalable\n",
      "systems 38\n",
      "\n",
      "Microsoft Azure Machine Learning Studio : This is a GUI-based environment for\n",
      "creating  and operationalizing a machine learning workflow that is optimized for\n",
      "Azure 30\n",
      "\n",
      "Azure Databricks : This is an Apache Spark-based analytics and machine\n",
      "learning platform  optimized for the Microsoft Azure platform 25\n",
      "\n",
      "\n",
      "This text will focus heavily  on the Azure  Databricks  environment as it provides access to\n",
      "many tools, including these:\n",
      "Spark DataFrames : Spark DataFrames are distributed collections of data\n",
      "organized into rows and columns 46\n",
      "They are conceptually equivalent to a data\n",
      "frame in Python 12\n",
      "\n",
      "Notebooks : Lik e Jupyter, the Azure Databricks notebook tool provides a cell-\n",
      "based code environment for writing logically separated code that is easy to read\n",
      "and replicate 36\n",
      "\n",
      "Clusters/workers : Using Microsoft Azure (or another cloud computing\n",
      "platform), we can spin  up resources in order to massively optimize and\n",
      "parallelize our machine learning and data analysis to allow for faster iteration 42\n",
      "\n",
      "MLib : This is a scalable machine  learning library consisting of common learning\n",
      "algorithms and utilities, including classification, regression, and more 29\n",
      "\n",
      "Azure Databricks provides many more capabilities other than what we have listed here 16\n",
      "We\n",
      "will focus on the tools that are relevant to us as data scientists and machine learning\n",
      "engineers 21\n",
      "\n",
      "For more information on Azure Databricks, check out https:/ ​/​docs 18\n",
      "\n",
      "microsoft 3\n",
      "​com/ ​en-​us/​azure/ ​azure- ​databricks/ ​what- ​is-​azure-\n",
      "databricks 32\n",
      "\n",
      "What exactly are Spark and PySpark 8\n",
      "\n",
      "The backbone of Azure Databricks  is Apache Spark 12\n",
      "Spark  is an analytics engine for big\n",
      "data processing 11\n",
      "It has built-in modules for data streaming, SQL, machine learning, and\n",
      "more 17\n",
      "PySpark  is the Python API for Spark 9\n",
      "It allows us to invoke the power  of Spark using\n",
      "Python code 14\n",
      "Azure Databricks brings all of this at our fingertips and makes setting up\n",
      "clusters running Spark and PySpark within seconds possible 25\n",
      "Let's see it in action 6\n",
      "\n",
      "Basic Azure Databricks use\n",
      "Before we begin with our case studies, it is important to get our bearings in  Azure\n",
      "Databricks 30\n",
      "We will begin by setting up our first cluster and some notebooks to write our\n",
      "Python code in 19\n",
      "\n",
      "\n",
      "Setting up our first cluster\n",
      "To get started in Azure Databricks, we have to set up our first cluster 24\n",
      "This will spin up\n",
      "(initialize) resources running the Azure Databricks Runtime Environment (including\n",
      "Spark) 22\n",
      "This is where all of the action takes place 9\n",
      "Whenever we run code in our notebooks,\n",
      "the code is sent to our cluster to actually run it 19\n",
      "This means that no code will ever actually\n",
      "run on our local machine 14\n",
      "This is great for many reasons, one of the main ones being that it\n",
      "provides data scientists with sub-optimal  equipment at home/work a chance to use\n",
      "production-quality resources for a fraction of the cost 43\n",
      "\n",
      "To set up a cluster, navigate to the Clusters  option on the left-hand pane and click on\n",
      "Create cluster 25\n",
      "There, you will see a form with three basic pieces of information: Cluster\n",
      "Name , Azure Databricks Runtime Version , and Python Version 28\n",
      "Make your cluster name\n",
      "whatever your heart desires 9\n",
      "Try to use the most recent Azure Databricks version (this is\n",
      "the default) and select Python version 3 24\n",
      "Once that's finished, click Create cluster  again and\n",
      "you are done 15\n",
      "\n",
      "When spinning up a cluster, there are many optional advanced settings\n",
      "that we have the ability to set 21\n",
      "For our purposes, we will not need to 9\n",
      "\n",
      "The page should look something like this:\n",
      "\n",
      "\n",
      "Once we have a cluster, it is time to set up a notebook 23\n",
      "This can be done from the home\n",
      "page 9\n",
      "When creating a new notebook, all we have to do is select the language we wish to\n",
      "use (Python for now, but more are  available) 31\n",
      "The notebooks in Azure Databricks are nearly\n",
      "identical to Jupyter notebooks in functionality and use (nifty 23\n",
      " 1\n",
      "This comes in handy for\n",
      "data scientists who are used to this environment :\n",
      "Creating a notebook is even easier than spinning up a new cluster 27\n",
      "All of the code that we write in here will be run on our cluster and not on our local machine\n",
      "Once we have our cluster up and running and we are able to create notebooks, it is time to\n",
      "jump right into using the Azure Databricks environment and seeing first-hand how the\n",
      "power of Apache Spark and the Microsoft Data Environment will affect the way that we\n",
      "write data-driven code 80\n",
      "\n",
      "Case study 1 – bike-sharing usage prediction\n",
      "using parallelization in Azure Databricks\n",
      "Our first case study will focus on setting up a simple  notebook in Azure Databricks and\n",
      "running some basic data visualization and machine learning code in order to get used to\n",
      "the Azure Databricks environment 62\n",
      "Azure Databricks comes with a built-in filesystem that\n",
      "is preloaded with data for us to use 21\n",
      "We can upload our own files to the system (which we\n",
      "will do in the second case study),  but for now, we will import a dataset that came pre-\n",
      "loaded with Azure Databricks 41\n",
      "We will also make heavy use of the built-in Spark-based\n",
      "visualization tools to analyze our data to the fullest extent that we can 26\n",
      "\n",
      "\n",
      "The aspects of Azure Databricks that we will be highlighting in this case study include the\n",
      "following:\n",
      "The collection of open data that is easily accessible by the Azure Databricks\n",
      "filesystem\n",
      "Converting our pandas  DataFrames to Spark equivalents and generating\n",
      "visualizations\n",
      "Parallelizing some simple hyperparameter tuning\n",
      "Broadcasting variables to workers to enhance parallelization further\n",
      "The data that we will be using involves predicting the amount of bikes being rented via a\n",
      "bike-share system 96\n",
      "Our goal  is to predict the usage of the system based on daily/hourly\n",
      "corresponding weather, time-based, and seasonal information 29\n",
      "Let's get right to it and see\n",
      "our first Azure Databricks-specific programming 17\n",
      "We can access this through the dbutils\n",
      "module:\n",
      "# display is a reserved function in Databricks that allows us to view and\n",
      "manipulate Dataframes inline\n",
      "# dbutils is a library full of ready-to-use datasets for us to use\n",
      "# Let's start by displaying all of the directories in the main data folder\n",
      "# \"databricks-datasets\"\n",
      "display(dbutils 79\n",
      "fs 1\n",
      "ls(\"/databricks-datasets\"))\n",
      "The output is a DataFrame of available folders to look in for data 21\n",
      "Here is a snippet:\n",
      "path                                         name                 size\n",
      "dbfs:/databricks-datasets/README 21\n",
      "md          README 3\n",
      "md            976\n",
      "dbfs:/databricks-datasets/Rdatasets/         Rdatasets/           0\n",
      "dbfs:/databricks-datasets/SPARK_README 37\n",
      "md    SPARK_README 6\n",
      "md      3359\n",
      "dbfs:/databricks-datasets/adult/             adult/               0\n",
      "dbfs:/databricks-datasets/airlines/          airlines/            0\n",
      "dbfs:/databricks-datasets/amazon/            amazon/              0\n",
      "dbfs:/databricks-datasets/asa/               asa/                 0\n",
      "dbfs:/databricks-datasets/atlas_higgs/       atlas_higgs/         0\n",
      "dbfs:/databricks-datasets/bikeSharing/       bikeSharing/         0\n",
      "dbfs:/databricks-datasets/cctvVideos/        cctvVideos/          0\n",
      "dbfs:/databricks-datasets/credit-card-fraud/ credit-card-fraud/   0\n",
      "dbfs:/databricks-datasets/cs100/             cs100/               0\n",
      "dbfs:/databricks-datasets/cs110x/            cs110x/              0\n",
      " 208\n",
      " 1\n",
      " 1\n",
      "\n",
      "\n",
      "Every time we run a command in our notebook, the time that it took our cluster to execute\n",
      "that code is shown at the bottom 28\n",
      "For example, this code block took my cluster 0 11\n",
      "88 seconds 3\n",
      "\n",
      "In general, the format for this statement is as follows:\n",
      "Command took  <<elapsed_time>>  -- by <<username/email>>  at <<date>>,\n",
      "<<time>>  on <<cluster_name>>\n",
      "We can view the contents of a particular file:\n",
      "# Let's check out the general README of the date folder by opening the\n",
      "markdown folder and printing out the result of \"readlines\"\n",
      "# which is a way to print out the contents of a file\n",
      "with open(\"/dbfs/databricks-datasets/README 105\n",
      "md\") as f:\n",
      " x = '' 8\n",
      "join(f 2\n",
      "readlines())\n",
      "print(x)\n",
      "Databricks Hosted Datasets\n",
      "==========================\n",
      "The data contained within this directory is hosted for users to build data\n",
      "pipelines using Apache Spark and Databricks 39\n",
      "\n",
      "License\n",
      "-------\n",
      " 4\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "Let's take a deeper look at the bikeSharing  directory:\n",
      "# Let's list the contents of the directory corresponding to the data that\n",
      "we want to import\n",
      "display(dbutils 37\n",
      "fs 1\n",
      "ls(\"dbfs:/databricks-datasets/bikeSharing/\"))\n",
      "Running the preceding code will list out the contents of the bikeSharing  directory:\n",
      "path                                                name       size\n",
      "dbfs:/databricks-datasets/bikeSharing/README 50\n",
      "md     README 3\n",
      "md  5016\n",
      "dbfs:/databricks-datasets/bikeSharing/data-001/     data-001/  0\n",
      "We have a README file in markdown format and another sub-directory 42\n",
      "In the  data-001\n",
      "folder, we can list out the contents and see that there are two CSV files:\n",
      "# the data is given to use in both an hourly and a daily format 39\n",
      "\n",
      "display(dbutils 4\n",
      "fs 1\n",
      "ls(\"dbfs:/databricks-datasets/bikeSharing/data-001/\"))\n",
      "\n",
      "The output is as follows:\n",
      "path                                                     name       size\n",
      "dbfs:/databricks-datasets/bikeSharing/data-001/day 45\n",
      "csv   day 3\n",
      "csv    57569\n",
      "dbfs:/databricks-datasets/bikeSharing/data-001/hour 22\n",
      "csv  hour 3\n",
      "csv   1156736\n",
      "We will use the hourly format for our study 16\n",
      "Before we do, note that there is a README in\n",
      "the directory that will give us context and information about the data 24\n",
      "We can view the file\n",
      "by opening it and printing our the line contents as we did before:\n",
      "# Note that we had to change the format from dbfs:/ to /dbfs/\n",
      "with open(\"/dbfs/databricks-datasets/bikeSharing/README 53\n",
      "md\") as f:\n",
      " x = '' 8\n",
      "join(f 2\n",
      "readlines())\n",
      "print(x)\n",
      "## Dataset\n",
      "Bike-sharing rental process is highly correlated to the environmental and\n",
      "seasonal settings 25\n",
      "For instance, weather conditions, precipitation, day of\n",
      "week, season, hour of the day, etc 21\n",
      "can affect the rental behaviors 5\n",
      "The\n",
      "core data set is related to the two-year historical log corresponding to\n",
      "years 2011 and 2012 from Capital Bikeshare system, Washington D 33\n",
      "C 1\n",
      ", USA\n",
      "which is publicly available in http://capitalbikeshare 15\n",
      "com/system-data 3\n",
      "We\n",
      "aggregated the data on two hourly and daily basis and then extracted and\n",
      "added the corresponding weather and seasonal information 24\n",
      "Weather\n",
      "information are extracted from http://www 9\n",
      "freemeteo 4\n",
      "com 1\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "The README goes on to say that this dataset is primarily used to do regression (predicting\n",
      "a continuous response) 24\n",
      "We will treat the problem as a classification by bucketing our\n",
      "response to classes 16\n",
      "More on this later 4\n",
      "Let's import our data into a good old-fashioned\n",
      "pandas DataFrame:\n",
      "# Load data into a Pandas dataframe (note that pandas, sklearn, etc come\n",
      "with the environment 36\n",
      "That's pretty neat 4\n",
      "\n",
      "import pandas\n",
      "bike_data = pandas 8\n",
      "read_csv(\"/dbfs/databricks-\n",
      "datasets/bikeSharing/data-001/hour 18\n",
      "csv\") 2\n",
      "iloc[:,1:] # remove line number\n",
      "# view the dataframe\n",
      "#look at the first row\n",
      "bike_data 24\n",
      "loc[0]\n",
      "\n",
      "The output is as follows:\n",
      "dteday 2011-01-01\n",
      "season 1\n",
      "yr 0 mnth 1 hr 0 holiday 0 weekday 6 workingday 0 weathersit 1 temp 0 53\n",
      "24\n",
      "atemp 0 7\n",
      "2879 hum 0 6\n",
      "81 windspeed 0 casual 3 registered 13 cnt 16\n",
      "Descriptions for each variable are in the README:\n",
      "holiday : A Boolean that  is 1 (True ) means the  day is a holiday and 0 (False )\n",
      "means it is not  (extracted from http:/ ​/​dchr 66\n",
      "​dc 2\n",
      "​gov/ ​page/ ​holiday- ​schedule )\n",
      "workingday : A Boolean that is 1 (True ) means  the day is neither a weekend nor\n",
      "a holiday, otherwise 0 (False )\n",
      "cnt: Count of total rental bikes, including both casual and registered\n",
      "The README included in the Azure Databricks filesystem is very helpful for context and\n",
      "information about the datasets 78\n",
      "Let's see how many observations we have:\n",
      "# 17,379 observations\n",
      "bike_data 18\n",
      "shape\n",
      "(17379, 16)\n",
      "It is nice to stop and notice that everything that we have done so far (except for the Azure\n",
      "Databricks filesystem module) would be exactly the same if we were to do it in a normal\n",
      "Jupyter environment 54\n",
      "Let's see whether our dataset has any missing data that we would\n",
      "have to deal with:\n",
      "# no missing data, great 25\n",
      "\n",
      "bike_data 3\n",
      "isnull() 3\n",
      "sum()\n",
      "dteday 0\n",
      "season 0\n",
      "yr 0\n",
      "mnth 0 hr 0\n",
      "holiday 0\n",
      "weekday 0\n",
      "workingday 0\n",
      "weathersit 0\n",
      "temp 0\n",
      "atemp 0\n",
      "hum 0\n",
      "windspeed 0\n",
      "casual 0\n",
      "registered 0\n",
      "cnt 0\n",
      "\n",
      "No missing data 78\n",
      "Excellent 1\n",
      "Let's see a quick histogram of the cnt column, as it represents\n",
      "the total count of bikes reserved in that hour:\n",
      "# not everything will carry over, but that is ok\n",
      "%matplotlib inline\n",
      "bike_data 43\n",
      "hist(\"cnt\")\n",
      "# matplotlib inline is not supported in Databricks 14\n",
      "\n",
      "# You can display matplotlib figures using display() 10\n",
      "For an example, see #\n",
      "https://docs 9\n",
      "databricks 2\n",
      "com/user-guide/visualizations/matplotlib-and-ggplot\n",
      " 13\n",
      "html\n",
      "We get an error when we run this code, even though we have run this command  in this\n",
      "book before 25\n",
      "Unfortunately, matplotlib  does not work exactly the same in this\n",
      "environment as it does in Jupyter 20\n",
      "That is fine, though, because Azure Databricks provides\n",
      "a visualization tool built on top of Spark's version of a DataFrame that puts even more\n",
      "capabilities at our fingertips 35\n",
      "To get access to these visualizations, we will have to first\n",
      "convert our pandas DataFrame to its Spark equivalent:\n",
      "# converting to a Spark DataFrame allows us to make multiple types of plots\n",
      "using either a sample of data or the entire data population as the source\n",
      "sparkDataframe = spark 58\n",
      "createDataFrame(bike_data)\n",
      "# display is a simple way to view your data either via a table or through\n",
      "graphs\n",
      "display(sparkDataframe)\n",
      "Running the preceding code yields a snippet of the Dataframe for us to inspect:\n",
      "\n",
      "\n",
      "The display  command shows the Spark DataFrame inline with our notebook 59\n",
      "\n",
      "Azure Databricks allows for extremely powerful graphing capabilities thanks to\n",
      "Spark 16\n",
      "When using the display  command, on the bottom-left of the cell, a widget will\n",
      "appear, allowing us to graph using the data 28\n",
      "Let's click on the Histogram :\n",
      "Azure Databricks oﬀers a multitude of graphing options to get the best picture of our data\n",
      "Another button will appear, called Plot Options 39\n",
      " 1\n",
      " 1\n",
      ", which allows us to customize our graph 8\n",
      "\n",
      "We can drag and drop our columns into one of three fields:\n",
      "Keys  will, in general, represent our x-axis 25\n",
      "\n",
      "Values  will, in general, represent our y-axis 12\n",
      "\n",
      "Series groupings  will separate our data into groupings in order to get a bigger\n",
      "picture 20\n",
      "\n",
      "\n",
      "By default, display and visualizations will only aggregate over the first\n",
      "1,000 rows for convenience 21\n",
      "We can force Azure Databricks to utilize the\n",
      "entire dataset in our graph by setting the appropriate option 22\n",
      "\n",
      "Creating a simple histogram in Azure Databricks couldn't be simpler 14\n",
      "\n",
      "We can see how easy to use the system is and that our distribution is right-skewed 19\n",
      "\n",
      "Interesting 2\n",
      "Let's try something else now 6\n",
      "Let's say we also want to visualize the hourly\n",
      "usage, separated by the binary variable workingday ; the idea being that we are curious to\n",
      "see how the total amount of bikes reserved changes by the hour and if that distribution\n",
      "changes depending on whether it's a working day or not 58\n",
      "We can achieve this by selecting\n",
      "cnt as our value, hr as our key, and workingday  as our grouping 24\n",
      "\n",
      "\n",
      "It should look something like this:\n",
      "Hitting Apply  will apply this to the entire dataset, instead of the first-1,000-row sample that\n",
      "it shows us on the right:\n",
      "\n",
      "\n",
      "It is extremely easy to use Azure Databricks to generate beautiful and interpretable graphs\n",
      "based on our data 61\n",
      "From this, we can see that bike sharing appears to be somewhat\n",
      "normally distributed on weekends while workdays have large spikes in the morning and\n",
      "evening (which makes sense) 36\n",
      "\n",
      "In this dataset, there are three possible  regression response candidates: casual ,\n",
      "registered , and cnt 20\n",
      "We will turn our problem into a classification problem by bucketing\n",
      "the cnt column into one of two buckets 21\n",
      "Our response will either be 100 or fewer bikes\n",
      "reserved that hour ( False ) or over 100 ( True ):\n",
      "# Seperate into X, and y (features and label)\n",
      "# we will turn our regression problem into a classification problem by\n",
      "bucketing the column \"cnt\" as being either over 100 / 100 or under 68\n",
      "\n",
      "features, labels = bike_data[[\"season\", \"yr\", \"mnth\", \"hr\", \"holiday\",\n",
      "\"weekday\", \"workingday\", \"weathersit\", \"atemp\", \"hum\", \"windspeed\"]],\n",
      "bike_data[\"cnt\"] > 100\n",
      "# See the distribution of our labels\n",
      "labels 66\n",
      "value_counts()\n",
      "True 10344\n",
      "False 7035\n",
      "Let's now create a simple function that will do a few things: take in a param choice for a\n",
      "random forest classifier, fit and test our model on our data, and return the results:\n",
      "from sklearn 55\n",
      "ensemble import RandomForestClassifier\n",
      "from sklearn 7\n",
      "model_selection import train_test_split\n",
      "from sklearn 9\n",
      "metrics import accuracy_score\n",
      "# Create a function that will\n",
      "# 0 15\n",
      "Take in a parameter for our Random Forest's n_estimators param\n",
      "# 1 16\n",
      "Instantiate a Random Forest algorithm\n",
      "# 2 9\n",
      "Split our data into a training and testing split\n",
      "# 3 13\n",
      "Fit on our training data\n",
      "# 4 9\n",
      "evaluate on our testing data\n",
      "# 5 9\n",
      "return a tuple with the n_estimators param and corresponding accuracy\n",
      "def runRandomForest(c):\n",
      " rf = RandomForestClassifier(n_estimators=c)\n",
      " # Split into train and test using\n",
      "sklearn 36\n",
      "cross_validation 2\n",
      "train_test_split\n",
      " X_train, X_test, y_train, y_test = train_test_split(features, labels,\n",
      "test_size=0 27\n",
      "2, random_state=1)\n",
      "\n",
      " # Build the model\n",
      " rf 14\n",
      "fit(X_train, y_train)\n",
      " # Calculate predictions and accuracy\n",
      " predictions = rf 16\n",
      "predict(X_test)\n",
      " accuracy = accuracy_score(predictions, y_test)\n",
      " # return param and accuracy score\n",
      " return (c, accuracy)\n",
      "This function takes in a single number as an input, sets that number as a random forest's\n",
      "n_estimator  parameter, and returns the accuracy associated with that param choice on a\n",
      "train-test split:\n",
      "runRandomForest(1)\n",
      "(1, 0 78\n",
      "90218642117376291)\n",
      "Command took  0 13\n",
      "13 seconds\n",
      "This took a very short amount of time because we were only training a single decision tree\n",
      "in our forest 25\n",
      "Let's now iteratively try a varying number of estimators and see how this\n",
      "affects our accuracy:\n",
      "for i in [1, 10, 100, 1000, 10000]:\n",
      "  print(runRandomForest(i))\n",
      "(1, 0 54\n",
      "89700805523590332)\n",
      "(10, 0 13\n",
      "939873417721519)\n",
      "(100, 0 12\n",
      "94677790563866515)\n",
      "(1000, 0 14\n",
      "94677790563866515)\n",
      "(10000, 0 14\n",
      "94792865362485612)\n",
      "Command took  4 13\n",
      "16 minutes\n",
      "It took my cluster over 4 minutes to try these five n_estimator  options 20\n",
      "Most of the time\n",
      "was taken up by the final two options as it took a very long time to train thousands of\n",
      "decision trees 27\n",
      "\n",
      "As we add more combinations of parameters  and more parameter\n",
      "options, the time it will take to iteratively go through these options will\n",
      "explode 30\n",
      "We will see how we can utilize Databrick's environment to\n",
      "optimize this in the third case study 21\n",
      "\n",
      "\n",
      "Let's make use of Spark to parallelize our for loop 13\n",
      "Every notebook has a special variable\n",
      "called sc that represents Spark:\n",
      "# every notebook has a variable called \"sc\" that represents the Spark\n",
      "context in our cluster\n",
      "sc\n",
      "There are a few ways of performing this parallelization, but in general, it will look like this:\n",
      "Create a dataset that will be sent to our cluster (in this case, parameter options) 74\n",
      "1 2\n",
      "\n",
      "Map a function to each element of the dataset (our runRandomForest  function) 18\n",
      "2 2\n",
      "\n",
      "Collect the results:3 6\n",
      "\n",
      "# 1 4\n",
      "set up 5 tasks in our Spark Cluster by parallelizing a\n",
      "dataset (list) of five elements (n_estimator options)\n",
      "k = sc 29\n",
      "parallelize([1, 10, 100, 1000, 10000])\n",
      "# 2 22\n",
      "map our function to our 5 tasks\n",
      "# The code will not be sent to our cluster until we run the\n",
      "next command\n",
      "results = k 30\n",
      "map(runRandomForest)\n",
      "Command took  0 10\n",
      "13 seconds\n",
      "Here, we are introduced to our first Apache Spark-specific syntax 16\n",
      "Step 1  will return a\n",
      "distributed dataset that is optimized for parallel computation, and we will call that dataset\n",
      "k 25\n",
      "The values of k represent different possible arguments for our function,\n",
      "runRandomForest 15\n",
      "Step 2  tells our cluster to run the function across our distributed\n",
      "dataset 16\n",
      "\n",
      "It is important to note that while Step 1  and Step 2  are Spark-specific commands, up until\n",
      "now, our function has not actually been sent to our cluster for execution 39\n",
      "We have just set\n",
      "the stage to do so by setting up the appropriate variables 16\n",
      "Step 3  will collect our results by\n",
      "running the function in parallel across the different values in k:\n",
      "# 3 25\n",
      "the collect method actually sends the five tasks to our cluster for\n",
      "execution\n",
      "# Faster (1 19\n",
      "5x) because we aren't doing each task one after the other 15\n",
      "We\n",
      "are doing them in parallel\n",
      "# This becomes much more noticeable when doing more params (we will get to\n",
      "this in a later case study)\n",
      "results 32\n",
      "collect()\n",
      "Command took  2 7\n",
      "73 minutes\n",
      "\n",
      "Immediately, we can see the value of parallelizing functions using Spark 17\n",
      "By doing nothing\n",
      "more than relying on Azure Databricks and Spark, we are able to perform our for loop 1 25\n",
      "5x\n",
      "faster 6\n",
      "If we use a variable in a function (like our dataset), Spark will automatically send the\n",
      "dataset to the workers 23\n",
      "This is usually fine 4\n",
      "We can send it to workers  more efficiently by\n",
      "broadcasting  it 15\n",
      "By broadcasting data, a copy of the data is sent to our workers, which are\n",
      "used when running tasks 22\n",
      "This is much more efficient when dealing with extremely large\n",
      "datasets with large values in them 17\n",
      "We can rewrite our previous function using broadcast\n",
      "variables:\n",
      "# Broadcast dataset\n",
      "# If we use a variable in a function, Spark will automatically send the\n",
      "dataset to the workers 35\n",
      "This is usually fine 4\n",
      "\n",
      "# We can send it to workers more efficiently by broadcasting it 13\n",
      "By\n",
      "broadasting data, a copy is sent to our workers which are used when running\n",
      "tasks 21\n",
      "\n",
      "# For more info on broadcast variables, see the Spark programming guide 14\n",
      "\n",
      "You can create a Broadcast variable using sc 9\n",
      "broadcast() 2\n",
      "\n",
      "# To access the value of a broadcast variable, you need to use 15\n",
      "value\n",
      "# broadcast the variables to our workers\n",
      "featuresBroadcast = sc 14\n",
      "broadcast(features)\n",
      "labelsBroadcast = sc 7\n",
      "broadcast(labels)\n",
      "# reboot of the previous function\n",
      "def runRandomForestBroadcast(c):\n",
      "  rf = RandomForestClassifier(n_estimators=c)\n",
      "  # Split into train and test using\n",
      "sklearn 37\n",
      "cross_validation 2\n",
      "train_test_split\n",
      "  # ** This part of the function is the only difference from the previous\n",
      "version **\n",
      "  X_train, X_test, y_train, y_test =\n",
      "train_test_split(featuresBroadcast 40\n",
      "value, labelsBroadcast 4\n",
      "value,\n",
      "test_size=0 6\n",
      "2, random_state=1)\n",
      "  # Build the model\n",
      "  rf 16\n",
      "fit(X_train, y_train)\n",
      "  # Calculate predictions and accuracy\n",
      "  predictions = rf 18\n",
      "predict(X_test)\n",
      "  accuracy = accuracy_score(predictions, y_test)\n",
      "  return (c, accuracy)\n",
      "\n",
      "Once our new function using broadcast variables is complete, running it in parallel is no\n",
      "different:\n",
      "# set up 5 tasks in our Spark Cluster\n",
      "k = sc 54\n",
      "parallelize([1, 10, 100, 1000, 10000])\n",
      "# map our function to our five tasks\n",
      "results = k 31\n",
      "map(runRandomForestBroadcast)\n",
      "# the real work begins here 12\n",
      "\n",
      "results 2\n",
      "collect()\n",
      "The timing is not very different from  our last run with non-broadcast data 18\n",
      "This is because\n",
      "our data and logic are not large enough to see a noticeable difference yet 18\n",
      "Once done with\n",
      "our variables, we can unpersist (unbroadcast) them like so:\n",
      "# Since we are done with our broadcast variables, we can clean them up 34\n",
      "\n",
      "# (This will happen automatically, but we can make it happen earlier by\n",
      "explicitly unpersisting the broadcast variables 25\n",
      "\n",
      "featuresBroadcast 3\n",
      "unpersist()\n",
      "labelsBroadcast 5\n",
      "unpersist()\n",
      "Hyperparameter tuning would be very difficult to do if you wanted to do a grid search\n",
      "across multiple parameters and multiple options 28\n",
      "We could enhance our function in order\n",
      "to accommodate this; however, it would be better to rely on existing frameworks in scikit-\n",
      "learn to do so 31\n",
      "We will see how to remedy this conundrum in a later case study 15\n",
      "\n",
      "We can see how we can utilize Databrick's easy-to-use environment, clusters, and\n",
      "notebooks in order to enhance our data analysis and machine learning with minimal\n",
      "changes to our coding style 41\n",
      "In the next case study, we will examine Spark's scalable\n",
      "machine learning library, MLlib, for optimized machine learning speed 25\n",
      "\n",
      "Case study 2 – Using MLlib in Azure Databricks\n",
      "to predict credit card fraud\n",
      "Our second case study will focus  on predicting credit card fraud and will make use of\n",
      "MLlib, Apache Spark's scalable machine learning library 49\n",
      "MLlib comes standard with our\n",
      "Azure Databricks environment and allows us to write scalable machine learning code 21\n",
      "This\n",
      "case study will focus on MLlib syntax while we draw parallels (no pun intended) to its\n",
      "scikit-learn cousins 27\n",
      "\n",
      "\n",
      "The aspects of Azure Databricks that we will be highlighting in this case study include\n",
      "these:\n",
      "Importing a CSV that is uploaded to the Azure Databricks filesystem\n",
      "Using MLlib's pipeline, feature pre-processing, and machine learning library to\n",
      "write scalable machine learning code\n",
      "Using MLlib metric evaluation modules that mirror scikit-learn's components\n",
      "The data in question is predicting credit card fraud 82\n",
      "The data represents credit card\n",
      "transactions and contains 28 anonymized continuous features (called V1 19\n",
      " 1\n",
      "V28) plus the\n",
      "amount that the transaction was for and the time at which the transaction occurred 20\n",
      "We will\n",
      "not dive too deeply into feature engineering in this case study, but will focus  mainly on\n",
      "using the MLlib modules that come with Azure Databricks to predict the response label\n",
      "(which is a binary variable) 46\n",
      "Let's get right into it and start by importing a CSV that we have\n",
      "uploaded using the Azure Databricks data import feature:\n",
      "# File location and type\n",
      "# This dataset also exists in the DBFS of Databricks 46\n",
      "This is just to show\n",
      "how to import a CSV that has been previously uploaded so that\n",
      "# you can upload your own 25\n",
      "\n",
      "file_location = \"/FileStore/tables/creditcard 12\n",
      "csv\"\n",
      "file_type = \"csv\"\n",
      "# CSV options\n",
      "# will automatically cast columns as appropiate types (float, string, etc)\n",
      "infer_schema = \"true\"\n",
      "first_row_is_header = \"true\"\n",
      "delimiter = \",\"\n",
      "# read a csv file and convert to a Spark dataframe\n",
      "df = spark 61\n",
      "read 1\n",
      "format(file_type) \\\n",
      " 5\n",
      "option(\"inferSchema\", infer_schema) \\\n",
      " 9\n",
      "option(\"header\", first_row_is_header) \\\n",
      " 10\n",
      "option(\"sep\", delimiter) \\\n",
      " 7\n",
      "load(file_location)\n",
      "# show us the Spark Dataframe\n",
      "display(df)\n",
      "\n",
      "Our goal is to build a pipeline that will do this:\n",
      "Assemble the columns that we wish to use as features1 39\n",
      "\n",
      "Scale our features using a standard z-score function2 11\n",
      "\n",
      "Encode our label as 0 or 1 (it already is but it is good to see this functionality)3 24\n",
      "\n",
      "Run a logistic regression across the training data to fit coefficients4 13\n",
      "\n",
      "Evaluate binary metrics on the testing set5 9\n",
      "\n",
      "Use an MLlib cross-validating grid searching module to find the best parameters6 17\n",
      "\n",
      "for our logistic regression model\n",
      "Phew 9\n",
      "That's a lot, so let's go one step at a time 14\n",
      "The following code block will handle Step 1 ,\n",
      "Step 2 , and Step 3  by setting up a pipeline with three steps in it:\n",
      "# import the pipeline module from pyspark\n",
      "from pyspark 42\n",
      "ml import Pipeline\n",
      "from pyspark 7\n",
      "ml 1\n",
      "feature import VectorAssembler, StandardScaler,\n",
      "StringIndexer\n",
      "# will hold the steps of our pipeline\n",
      "stages = []\n",
      "The first stage of our pipeline will assemble the features that we want to use in our machine\n",
      "learning procedure 46\n",
      "Let's use all 28 entries from the anonymized data (V1 - V28) plus the\n",
      "amount  column 25\n",
      "For this case study, we will not use the time  column:\n",
      "# Transform all features into a vector using VectorAssembler\n",
      "# create list of [\"V1\", \"V2\", \"V3\", 41\n",
      " 1\n",
      " 1\n",
      "\n",
      "numericCols = [\"V{}\" 8\n",
      "format(i) for i in range(1, 29)]\n",
      "# Add \"Amount\" to the list of features\n",
      "assemblerInputs = numericCols + [\"Amount\"]\n",
      "# VectorAssembler acts like scikit-learn's \"FeatureUnion\" to put together\n",
      "the feature columns and adding the label \"features\"\n",
      "assembler = VectorAssembler(inputCols=assemblerInputs,\n",
      "outputCol=\"features\")\n",
      "# add the VectorAssembler to the stages of our MLLib Pipeline\n",
      "stages 96\n",
      "append(assembler)\n",
      "The second stage of our pipeline will take the assembled features and scale them:\n",
      "# MLLib's StandardScaler acts like scikit-learn's StandardScaler\n",
      "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
      " withStd=True, withMean=True)\n",
      "# add StandardScaler to our pipeline\n",
      "stages 70\n",
      "append(scaler)\n",
      "\n",
      "Once we've gathered our features and scaled them, the final step of our pipeline encodes\n",
      "our class label  to ensure consistency:\n",
      "# Convert label into label indices using the StringIndexer (like scikit-\n",
      "learn's LabelEncoder)\n",
      "label_stringIdx = StringIndexer(inputCol=\"Class\", outputCol=\"label\")\n",
      "# add the StringIndexer to our pipeline\n",
      "stages 81\n",
      "append(label_stringIdx)\n",
      "That was a lot, but note that every step of the pipeline has a scikit-learn equivalent 25\n",
      "As a\n",
      "data scientist, the most important thing to remember is that as long as you know the theory \n",
      "and the proper steps of data science and machine learning, you can transfer your skills\n",
      "across many languages, platforms, and technologies 48\n",
      "Now that we have created our list of\n",
      "three stages, let's instantiate a pipeline  object:\n",
      "# create our pipeline with three stages\n",
      "pipeline = Pipeline(stages=stages)\n",
      "To make sure that we are training a model that is able to predict unseen cases, we should\n",
      "split up our data into a training and testing set, and fit our pipeline to the training set while\n",
      "using the trained pipeline to transform the testing set:\n",
      "# We need to split our data into training and test sets 100\n",
      "This is like\n",
      "scikit-learn's train_test_split function\n",
      "# We will also set a random number seed for reproducibility\n",
      "(trainingData, testData) = df 35\n",
      "randomSplit([0 4\n",
      "7, 0 5\n",
      "3], seed=1)\n",
      "print(trainingData 10\n",
      "count())\n",
      "print(testData 5\n",
      "count())\n",
      "199470  # elements in the training set\n",
      "85337  # elements in the testing set\n",
      "Let's now fit our pipeline to the training set 32\n",
      "This will learn the features as well as the\n",
      "parameters to scale future unseen testing data:\n",
      "# fit and transform to our training data\n",
      "pipelineModel = pipeline 31\n",
      "fit(trainingData)\n",
      "trainingDataTransformed = pipelineModel 11\n",
      "transform(trainingData)\n",
      "\n",
      "If we take a look at our DataFrame, we will notice that we have three new columns:\n",
      "features , scaledFeatures , and label 30\n",
      "These three columns were added by our pipeline\n",
      "and those names can be found exactly in the preceding code where we set the three stages\n",
      "of the pipeline 30\n",
      "Note that the data types of the features and scaledFeatures  column\n",
      "are vector 16\n",
      "This indicates that they represent observations to be learned by our machine\n",
      "learning model in the future:\n",
      "# note the new columns \"features\", \"scaledFeatures\", and \"label\" at the end\n",
      "trainingDataTransformed\n",
      "Time:decimal(10,0)\n",
      "V1:double\n",
      "V2:double\n",
      "V3:double\n",
      "V4:double\n",
      "V5:double\n",
      "V6:double\n",
      "V7:double\n",
      "V8:double\n",
      "V9:double\n",
      "V10:double\n",
      "V11:double\n",
      "V12:double\n",
      "V13:double\n",
      "V14:double\n",
      "V15:double\n",
      "V16:double\n",
      "V17:double\n",
      "V18:double\n",
      "V19:double\n",
      "V20:double\n",
      "V21:double\n",
      "V22:double\n",
      "V23:double\n",
      "V24:double\n",
      "V25:double\n",
      "V26:double\n",
      "V27:double\n",
      "V28:double\n",
      "Amount:double\n",
      "Class:integer\n",
      "features: udt\n",
      "scaledFeatures: udt\n",
      "label:double\n",
      "\n",
      "Just like we do in scikit-learn, we will have to import out logistic regression model,\n",
      "instantiate it, and fit it to our training data:\n",
      "# Import the logistic regression module from pyspark\n",
      "from pyspark 260\n",
      "ml 1\n",
      "classification import LogisticRegression\n",
      "# Create initial LogisticRegression model\n",
      "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"scaledFeatures\")\n",
      "# Train model with Training Data\n",
      "lrModel = lr 38\n",
      "fit(trainingDataTransformed)\n",
      "This process should look very familiar to us as it is nearly identical to scikit-learn 24\n",
      "The main\n",
      "difference is that when we instantiate our model, we have to tell the object the names of the\n",
      "label and features, instead of feeding the features and label separately into the fit method\n",
      "(like we do in scikit-learn) 50\n",
      "\n",
      "Once we fit our model, we can transform and gather predictions from our testing data:\n",
      "# transform (not fit) to the testing data\n",
      "testDataTransformed = pipelineModel 36\n",
      "transform(testData)\n",
      "# run our logistic regression over the transformed testing set\n",
      "predictions = lrModel 19\n",
      "transform(testDataTransformed)\n",
      "Transforming our testing data using logistic regression will actually add three new columns\n",
      "(like the pipeline did) 27\n",
      "We can see this by running this:\n",
      "predictions:\n",
      "Time: decimal(10,0)\n",
      "V1: double\n",
      "V2: double\n",
      "V3: double\n",
      "V4: double\n",
      "V5: double\n",
      "V6: double\n",
      "V7: double\n",
      "V8: double\n",
      "V9: double\n",
      "V10: double\n",
      "V11: double\n",
      "V12: double\n",
      "V13: double\n",
      "V14: double\n",
      "V15: double\n",
      "V16: double\n",
      "V17: double\n",
      "V18: double\n",
      "\n",
      "V19: double\n",
      "V20: double\n",
      "V21: double\n",
      "V22: double\n",
      "V23: double\n",
      "V24: double\n",
      "V25: double\n",
      "V26: double\n",
      "V27: double\n",
      "V28: double\n",
      "Amount: double\n",
      "Class: integer\n",
      "features: udt\n",
      "scaledFeatures: udt\n",
      "label: double\n",
      "rawPrediction: udt\n",
      "probability: udt\n",
      "prediction: double\n",
      "Let's take a look at the label  column as well  as two of the new columns,  probability  and\n",
      "prediction 222\n",
      "We can do this by invoking the filter method of the Spark DataFrame:\n",
      "selected = predictions 17\n",
      "select(\"label\", \"prediction\", \"probability\")\n",
      "display(selected)\n",
      "The output is as follows:\n",
      "label prediction probability\n",
      "0     0          [1,2,[],[0 35\n",
      "9998645390717447,0 9\n",
      "000135460928255428]]\n",
      "0     0          [1,2,[],[0 20\n",
      "9998821292706751,0 9\n",
      "00011787072932487872]]\n",
      "0     0          [1,2,[],[0 21\n",
      "9994714991454193,0 9\n",
      "000528500854580697]]\n",
      "0     0          [1,2,[],[0 20\n",
      "9991193503498385,0 9\n",
      "0008806496501614437]]\n",
      "0     0          [1,2,[],[0 21\n",
      "9997043818469743,0 9\n",
      "00029561815302580084]]\n",
      "0     0          [1,2,[],[0 21\n",
      "9998106820888389,0 9\n",
      "00018931791116114655]]\n",
      "0     0          [1,2,[],[0 21\n",
      "9995735877526569,0 9\n",
      "0004264122473429876]]\n",
      " 9\n",
      " 1\n",
      " 1\n",
      "\n",
      "The label  and prediction  columns show each observation's ground truth and our\n",
      "model's estimate, while the probability  column holds a vector that contains the\n",
      "predicted probability (think scikit-learn's predict_proba  functionality) 47\n",
      "Let's now bring in\n",
      "PySpark's metric evaluation module in order to get some basic metrics 19\n",
      "we will start with\n",
      "BinaryClassificationEvaluator , which can tell us the testing AUC (area under the\n",
      "ROC curve):\n",
      "# like scikit-learn's metric module\n",
      "from pyspark 38\n",
      "ml 1\n",
      "evaluation import BinaryClassificationEvaluator\n",
      "\n",
      "# Evaluate model using either area under Precision Recall curev or the area\n",
      "under the ROC\n",
      "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n",
      "metricName=\"areaUnderROC\")\n",
      "evaluator 47\n",
      "evaluate(predictions)\n",
      "0 4\n",
      "984986959421\n",
      "This is helpful, but we also want some of the more familiar and interpretable metrics such\n",
      "as accuracy , precision , recall , and more 34\n",
      "We can get these using\n",
      "PySpark's MulticlassMetrics  module:\n",
      "# Get some deeper metrics out of our predictions\n",
      "from pyspark 28\n",
      "mllib 2\n",
      "evaluation import MulticlassMetrics\n",
      "# must turn DF into RDD (Resilient Distributed Dataset)\n",
      "predictionAndLabels = predictions 24\n",
      "select(\"label\",\n",
      "\"prediction\") 7\n",
      "rdd 2\n",
      "map(tuple)\n",
      "# instantiate a MulticlassMetrics object\n",
      "metrics = MulticlassMetrics(predictionAndLabels)\n",
      "# Overall statistics\n",
      "accuracy = metrics 27\n",
      "accuracy\n",
      "precision = metrics 5\n",
      "precision()\n",
      "recall = metrics 5\n",
      "recall()\n",
      "f1Score = metrics 7\n",
      "fMeasure()\n",
      "print(\"Summary Stats\")\n",
      "print(\"Accuracy = %s\" % accuracy)\n",
      "print(\"Precision = %s\" % precision)\n",
      "print(\"Recall = %s\" % recall)\n",
      "print(\"F1 Score = %s\" % f1Score)\n",
      "The output is as follows:\n",
      "Summary Stats\n",
      "Accuracy = 0 66\n",
      "999203159239\n",
      "Precision = 0 10\n",
      "999203159239\n",
      "Recall = 0 11\n",
      "999203159239\n",
      "F1 Score = 0 12\n",
      "999203159239\n",
      "Note that accuracy is an attribute of the MulticlassMetrics  object and\n",
      "not a method, so we do not need the parentheses 32\n",
      "\n",
      "\n",
      "Great 2\n",
      "We can also calculate our true positive rate, false negative rate, and other by using\n",
      "the predictions vector 21\n",
      "This will give us a better sense of how our machine learning model\n",
      "performs on particular examples of positive and negative fraud cases:\n",
      "tp = predictions[(predictions 31\n",
      "label == 1) & (predictions 8\n",
      "prediction ==\n",
      "1)] 4\n",
      "count()\n",
      "tn = predictions[(predictions 7\n",
      "label == 0) & (predictions 8\n",
      "prediction ==\n",
      "0)] 4\n",
      "count()\n",
      "fp = predictions[(predictions 7\n",
      "label == 0) & (predictions 8\n",
      "prediction ==\n",
      "1)] 4\n",
      "count()\n",
      "fn = predictions[(predictions 7\n",
      "label == 1) & (predictions 8\n",
      "prediction ==\n",
      "0)] 4\n",
      "count()\n",
      "print (\"True Positives:\", tp)\n",
      "print (\"True Negatives:\", tn)\n",
      "print (\"False Positives:\", fp)\n",
      "print (\"False Negatives:\", fn)\n",
      "The output is as follows:\n",
      "True Positives: 93\n",
      "True Negatives: 85176\n",
      "False Positives: 18\n",
      "False Negatives: 50\n",
      "Let's consider this our baseline logistic regression model and try to optimize our results by\n",
      "tweaking our logistic regression parameters 93\n",
      "\n",
      "Using the MLlib Grid Search module to tune\n",
      "hyperparameters\n",
      "To optimize our parameters, it would help  to know exactly what they were and how to use\n",
      "them 35\n",
      "Luckily, we can see the README in each model by running the explainParams\n",
      "method 17\n",
      "This will generate a list of the available parameters, a description as to what they\n",
      "represent, and usually a guide to the acceptable values:\n",
      "# explain the parameters that are included in MLLib's Logistic Regression\n",
      "print(lr 45\n",
      "explainParams())\n",
      "The output is as follows:\n",
      "aggregationDepth : suggested depth for treeAggregate (>= 2) 23\n",
      "(default: 2)\n",
      "elasticNetParam : the ElasticNet mixing parameter, in range [0, 1] 24\n",
      "For\n",
      "alpha = 0, the penalty is an L2 penalty 14\n",
      "For alpha = 1, it is an L1\n",
      "\n",
      "penalty 14\n",
      "(default: 0 5\n",
      "0)\n",
      "family: The name of family which is a description of the label distribution\n",
      "to be used in the model 24\n",
      "Supported options: auto, binomial, multinomial\n",
      "(default: auto) featuresCol : features column name 21\n",
      "(default: features,\n",
      "current: scaledFeatures) fitIntercept : whether to fit an intercept term 20\n",
      "\n",
      "(default: True)\n",
      "labelCol : label column name 11\n",
      "(default: label, current: label)\n",
      " 9\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "\n",
      "Let's build a parameter grid for Spark to gridsearch  across 14\n",
      "Let's choose three parameters\n",
      "to start with: maxIter , regParam,  and elasticNetParam 21\n",
      "We first need to build a\n",
      "parameter grid using PySpark's version of scikit-learn's GridSearchCV :\n",
      "# pyspark's version of GridSearchCV\n",
      "from pyspark 37\n",
      "ml 1\n",
      "tuning import ParamGridBuilder, CrossValidator\n",
      "# Create ParamGrid for Cross Validation\n",
      "paramGrid = (ParamGridBuilder()\n",
      " 25\n",
      "addGrid(lr 3\n",
      "regParam, [0 5\n",
      "0, 0 5\n",
      "01, 0 5\n",
      "5, 2 5\n",
      "0])\n",
      " 3\n",
      "addGrid(lr 3\n",
      "elasticNetParam, [0 6\n",
      "0, 0 5\n",
      "5, 1 5\n",
      "0])\n",
      " 3\n",
      "addGrid(lr 3\n",
      "maxIter, [5, 10])\n",
      " 9\n",
      "build())\n",
      "# Create 5-fold CrossValidator that can also test multiple parameters (like\n",
      "scikit-learn's GridSearchCV)\n",
      "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid,\n",
      "evaluator=evaluator, numFolds=5)\n",
      "Let's send our grid search to the cluster for the efficient parallelization of tasks:\n",
      "# Run cross validations on our cluster\n",
      "cvModel = cv 84\n",
      "fit(trainingDataTransformed)\n",
      "# this will likely take a fair amount of time because of the amount of\n",
      "models that we're creating and testing\n",
      "The variable, cvModel , holds the logistic regression model with the optimized parameters 45\n",
      "\n",
      "From our optimized model, we can extract the learned coefficients to gain a deeper\n",
      "understanding as to how the features correlate to the label:\n",
      "# extract the weights from our model\n",
      "weights = cvModel 41\n",
      "bestModel 2\n",
      "coefficients\n",
      "# convert from numpy type to float\n",
      "weights = [[float(w)] for w in weights]\n",
      "weightsDF = sqlContext 26\n",
      "createDataFrame(weights, [\"Feature Weight\"])\n",
      "display(weightsDF)\n",
      "\n",
      "Running the preceding code yields a single-column DataFrame with the logistic regression\n",
      "coefficients 28\n",
      "As we've seen before, the weights represent the importance and correlation\n",
      "between the features and the response:\n",
      "Feature Weight\n",
      "-0 26\n",
      "009310428799214699\n",
      "0 9\n",
      "024089617982041803\n",
      "-0 10\n",
      "07660344812402071\n",
      "0 9\n",
      "13879375587420806\n",
      "0 9\n",
      "03389644146602658\n",
      "-0 10\n",
      "03197804822203382\n",
      "-0 10\n",
      "026671134727093863\n",
      "-0 10\n",
      "05963860645699601\n",
      "-0 10\n",
      "07157334685249503\n",
      "-0 10\n",
      "12739634200985744\n",
      "0 9\n",
      "11271203988538568\n",
      "-0 10\n",
      "16991941687681994\n",
      "-0 10\n",
      "022382161065846975\n",
      "-0 10\n",
      "2967372927422323\n",
      "-0 10\n",
      "002525484797586701\n",
      "-0 10\n",
      "08661888759753078\n",
      "-0 10\n",
      "09428351861530046\n",
      "-0 10\n",
      "010145267312697291\n",
      "0 9\n",
      "000474661023205239\n",
      "0 9\n",
      "003985147929831393\n",
      "0 9\n",
      "031230406955806467\n",
      "0 9\n",
      "011362000753207976\n",
      "-0 10\n",
      "014548646956536248\n",
      "-0 10\n",
      "011270335506048019\n",
      "-0 10\n",
      "004390342109545349\n",
      "0 9\n",
      "008722583938741943\n",
      "0 9\n",
      "01390573346423987\n",
      "0 9\n",
      "014176539525542918\n",
      "0 9\n",
      "021489763526114244\n",
      "We can grab our predictions in the same way we did previously to compare our results:\n",
      "# Use test set to get the best params\n",
      "predictions = cvModel 39\n",
      "transform(testDataTransformed)\n",
      "# must turn DF into RDD (Resilient Distributed Dataset)\n",
      "predictionAndLabels = predictions 24\n",
      "select(\"label\",\n",
      "\"prediction\") 7\n",
      "rdd 2\n",
      "map(tuple)\n",
      "metrics = MulticlassMetrics(predictionAndLabels)\n",
      "# Overall statistics\n",
      "\n",
      "accuracy = metrics 19\n",
      "accuracy\n",
      "precision = metrics 5\n",
      "precision()\n",
      "recall = metrics 5\n",
      "recall()\n",
      "f1Score = metrics 7\n",
      "fMeasure()\n",
      "print(\"Summary Stats\")\n",
      "print(\"Accuracy = %s\" % accuracy)\n",
      "print(\"Precision = %s\" % precision)\n",
      "print(\"Recall = %s\" % recall)\n",
      "print(\"F1 Score = %s\" % f1Score)\n",
      "The output is as follows:\n",
      "Summary Stats\n",
      "Accuracy = 0 66\n",
      "998839893598\n",
      "Precision = 0 10\n",
      "998839893598\n",
      "Recall = 0 11\n",
      "998839893598\n",
      "F1 Score = 0 12\n",
      "998839893598\n",
      "Run the following code:\n",
      "tp = predictions[(predictions 16\n",
      "label == 1) & (predictions 8\n",
      "prediction ==\n",
      "1)] 4\n",
      "count()\n",
      "tn = predictions[(predictions 7\n",
      "label == 0) & (predictions 8\n",
      "prediction ==\n",
      "0)] 4\n",
      "count()\n",
      "fp = predictions[(predictions 7\n",
      "label == 0) & (predictions 8\n",
      "prediction ==\n",
      "1)] 4\n",
      "count()\n",
      "fn = predictions[(predictions 7\n",
      "label == 1) & (predictions 8\n",
      "prediction ==\n",
      "0)] 4\n",
      "count()\n",
      "print \"True Positives:\", tp\n",
      "print \"True Negatives:\", tn\n",
      "print \"False Positives:\", fp\n",
      "print \"False Negatives:\", fn\n",
      "# False positive went from 18 to 16 (win) but False Negative jumped to 83\n",
      "(opposite of win)\n",
      "The output is as follows:\n",
      "True Positives: 67\n",
      "True Negatives: 85178\n",
      "False Positives: 16\n",
      "False Negatives: 76\n",
      "It's easy to see how Azure Databricks' environment makes it easy to utilize Spark and\n",
      "MLlib to create scalable machine  learning pipelines that are similar in construction and\n",
      "usage to scikit-learn 137\n",
      "\n",
      "\n",
      "Case study 3 – Using Azure Databricks to\n",
      "optimize our hyperparameter tuning\n",
      "Our final case study is the shortest and will showcase how we can combine the best of\n",
      "scikit-learn, Spark, and Azure Databricks to build simple yet powerful machine learning\n",
      "models 58\n",
      "We will be using the MNIST dataset, which we used earlier, and we will be fitting\n",
      "a fairly simple RandomForestClassifier  to the data 29\n",
      "The interesting bit will come when\n",
      "we import a third-party tool called spark_sklearn  to help us out 22\n",
      "\n",
      "The aspects of Azure Databricks that we will be highlighting in this case study include\n",
      "these:\n",
      "Importing third-party packages into our Azure Databricks environment\n",
      "Enabling Spark's parallelization within scikit-learn's easy-to-use syntax\n",
      "How to add Python libraries to your cluster\n",
      "Up until now, all of the packages that we have used come with the Azure Databricks\n",
      "environment 81\n",
      "Now we need to add a new package, called spark_sklearn 13\n",
      "To add a third-\n",
      "party package to our Azure Databricks cluster, we simply click on Import Library  from our\n",
      "main dashboard, and we will see a window like the following:\n",
      "Type in any package that you wish to use in the PyPi Name ﬁeld\n",
      "\n",
      "We simply type in spark_sklearn , hit Install Library , and we are done 72\n",
      "The cluster will\n",
      "now let us import the library from any existing or new notebook 16\n",
      "Now, spark_sklearn  is\n",
      "a handy tool that allows the use of some  scikit-learn packages with the backend swapped\n",
      "out for PySpark 32\n",
      "This means that we can use existing code that we have already written,\n",
      "and only have to tweak it slightly to make it compatible with Spark 27\n",
      "\n",
      "Using spark_sklearn to build an MNIST classifier\n",
      "We have already seen that MNIST is a handwritten digit detection dataset 25\n",
      "Without\n",
      "spending too much time  on the data itself, let's jump right into how we can use\n",
      "spark_sklearn  for our benefit 30\n",
      "Let's bring in our data using the standard scikit-learn\n",
      "dataset module:\n",
      "from sklearn import datasets\n",
      "from sklearn 24\n",
      "ensemble import RandomForestClassifier\n",
      "from sklearn 7\n",
      "model_selection import GridSearchCV as original_grid_search\n",
      "digits = datasets 14\n",
      "load_digits()\n",
      "X, y = digits 8\n",
      "data, digits 3\n",
      "target\n",
      "From here, we can import our handy GridSearchCV  module to do some hyperparameter\n",
      "tuning:\n",
      "param_grid = {\"max_depth\": [3, None],\n",
      " \"max_features\": [1, 3, 10],\n",
      " \"min_samples_leaf\": [1, 3, 10],\n",
      " \"bootstrap\": [True, False],\n",
      " \"criterion\": [\"gini\", \"entropy\"],\n",
      " \"n_estimators\": [10, 100, 1000]}\n",
      "gs = original_grid_search(RandomForestClassifier(), param_grid=param_grid)\n",
      "gs 112\n",
      "fit(X, y)\n",
      "gs 6\n",
      "best_params_, gs 4\n",
      "best score\n",
      "({'bootstrap': False,\n",
      "'criterion': 'gini',\n",
      "'max_depth': None,\n",
      "'max_features': 3,\n",
      "'min_samples_leaf': 1,\n",
      "'n_estimators': 1000},\n",
      "0 45\n",
      "95436839176405119)\n",
      "Command took 24 12\n",
      "69 minutes\n",
      "\n",
      "The preceding code is a standard grid search that takes nearly 25 minutes to run 20\n",
      "As in our\n",
      "first case study, we could write a custom function to run this in parallel, but that would\n",
      "take a lot of custom code 30\n",
      "As in our second case study, we use could MLlib to write a\n",
      "scalable grid search using the MLlib standard models, but that would  take a while as well 36\n",
      "\n",
      "spark_sklearn  provides a third option 9\n",
      "We can import their GridSearchCV  and swap\n",
      "out our module for theirs to get extreme gains in speed with minimal code intervention:\n",
      "# the new gridsearch module\n",
      "from spark_sklearn import GridSearchCV as spark_grid_search\n",
      "# the only difference is passing in the SparkContext objecr as the first\n",
      "parameter of the grid search\n",
      "gs = spark_grid_search(sc, RandomForestClassifier(), param_grid=param_grid)\n",
      "gs 87\n",
      "fit(X, y)\n",
      "gs 6\n",
      "best_params_, gs 4\n",
      "bestscore\n",
      "({'bootstrap': False,\n",
      "'criterion': 'gini',\n",
      "'max_depth': None,\n",
      "'max_features': 3,\n",
      "'min_samples_leaf': 1,\n",
      "'n_estimators': 100},\n",
      "0 44\n",
      "95436839176405119)\n",
      "Command took  5 13\n",
      "29 minute\n",
      "By doing nothing more than importing a new grid search module and passing the\n",
      "SparkContext  variable into the new  module, we can get a 5x speed boost 38\n",
      "Always be on\n",
      "the lookout for third-party modules that can be used to enhance the already easy-to-use\n",
      "Azure Databricks environment 27\n",
      "\n",
      "More about spark_sklearn  can be found on their GitHub page: https:/\n",
      "/​github 21\n",
      "​com/ ​databricks/ ​spark- ​sklearn 14\n",
      "\n",
      "\n",
      "Summary\n",
      "Azure Databricks provides an environment that allows us to create scalable machine\n",
      "learning pipelines with ease 22\n",
      "By using the notebooks in Azure Databricks, we can run our\n",
      "data analytics and machine learning code in the cloud with powerful Azure resources in\n",
      "the backend 32\n",
      "By implementing MLlib, we can create scalable machine learning pipelines\n",
      "behind Apache Spark 17\n",
      "Utilizing third-party tools is a great way to bring everything together\n",
      "to build extremely powerful learning algorithms and train them much faster than on our\n",
      "local machines 31\n",
      "\n",
      "\n",
      "ther Books You May Enjoy\n",
      "If you enjoyed this book, you may be interested in these other books by Packt:\n",
      "Data Science Algorithms in a Week, Second Edition\n",
      "Dávid Natingga\n",
      "ISBN:  978-1-78980-607-6\n",
      "Understand how to identify a data science problem correctly\n",
      "Implement well-known machine learning algorithms efficiently using Python\n",
      "Classify your datasets using Naive Bayes, decision trees, and random forest with\n",
      "accuracy\n",
      "Devise an appropriate prediction solution using regression\n",
      "Work with time series data to identify relevant data events and trends\n",
      "Cluster your data using the k-means algorithm\n",
      "\n",
      "\n",
      "Practical Data Science Cookbook, Second Edition\n",
      "Tony Ojeda, Sean Patrick Murphy, Et al\n",
      "ISBN: 978-1-78712-962-7\n",
      "Learn and understand the installation procedure and environment required for R\n",
      "and Python on various platforms\n",
      "Prepare data for analysis by implement various data science concepts such as\n",
      "acquisition, cleaning and munging through R and Python\n",
      "Build a predictive model and an exploratory model\n",
      "Analyze the results of your model and create reports on the acquired data\n",
      "Build various tree-based methods and Build random forest\n",
      "\n",
      "Leave a review - let other readers know what\n",
      "you think\n",
      "Please share your thoughts on this book with others by leaving a review on the site that you\n",
      "bought it from 280\n",
      "If you purchased the book from Amazon, please leave us an honest review\n",
      "on this book's Amazon page 21\n",
      "This is vital so that other potential readers can see and use\n",
      "your unbiased opinion to make purchasing decisions, we can understand what our\n",
      "customers think about our products, and our authors can see your feedback on the title that\n",
      "they have worked with Packt to create 53\n",
      "It will only take a few minutes of your time, but is\n",
      "valuable to other potential customers, our authors, and Packt 27\n",
      "Thank you 2\n",
      "\n",
      "\n",
      "ndex\n",
      "A\n",
      "Adam Optimizer  374\n",
      "arithmetic mean  48, 144\n",
      "arithmetic symbols\n",
      "   about  78\n",
      "   dot product  79, 81\n",
      "   proportional  79\n",
      "   summation  78\n",
      "AutoRegressive Integrated Moving Average\n",
      "(ARIMA)  355\n",
      "average observation  44\n",
      "B\n",
      "back-propagation  332\n",
      "bar charts  193\n",
      "Bayes' formula  79\n",
      "Bayes' theorem  112, 113, 115\n",
      "   applications  117\n",
      "   example  117, 119\n",
      "Bayesian approach\n",
      "   about  97\n",
      "   versus Frequentist approach  97\n",
      "Bayesian\n",
      "   about  113\n",
      "bias/variance tradeoff\n",
      "   about  298\n",
      "   error functions  309, 310\n",
      "   errors, due to bias  298\n",
      "   errors, due to variance  298, 299\n",
      "   example  299, 300, 301, 302, 303, 304, 306,\n",
      "307\n",
      "   overfitting  308\n",
      "   underfitting  308\n",
      "big data  21\n",
      "binary classifier  110\n",
      "binomial random variable  127, 129Bootstrap aggregation (bagging)  323\n",
      "box plots  197, 199\n",
      "C\n",
      "Cartesian graph  82\n",
      "causation  201\n",
      "chi-square goodness of fit test\n",
      "   about  182\n",
      "   assumptions  182\n",
      "   example  183\n",
      "   for association/independence  185\n",
      "chi-square independence test\n",
      "   assumptions  185\n",
      "classification  219\n",
      "classification route  353, 354, 355\n",
      "classification tree\n",
      "   fitting  266, 268, 270, 271\n",
      "cluster validation\n",
      "   optimal number, selecting for  282\n",
      "clustering  221\n",
      "coefficient of variation  149\n",
      "collectively exhaustive\n",
      "   about  112\n",
      "   events  112\n",
      "comma separated value (CSV)  39\n",
      "communication matter  189\n",
      "compound events  101\n",
      "conditional probability  104\n",
      "confidence intervals  170, 172, 174\n",
      "confounding factor  142\n",
      "confusion matrices  110\n",
      "confusion matrix  110\n",
      "continuous random variable  133, 136\n",
      "correlation  201\n",
      "correlation coefficients  157\n",
      "correlation\n",
      "   causation, lacking  206\n",
      "\n",
      "   versus causation  201, 203\n",
      "cross-validation error\n",
      "   versus training error  318, 320\n",
      "D\n",
      "data levels\n",
      "   about  42\n",
      "   checking  46\n",
      "   interval level  47\n",
      "   nominal level  43\n",
      "   ordinal level  44\n",
      "   ratio level  51\n",
      "data pre-processing\n",
      "   example  34\n",
      "   special characters  35\n",
      "   text, relative length  36\n",
      "   topics, picking out  36\n",
      "   word/phrase counts  35\n",
      "data science, case studies\n",
      "   about  22\n",
      "   dollars, marketing  25\n",
      "   government paper pushing, automating  23\n",
      "   job description  27, 29\n",
      "   performance  24\n",
      "data science\n",
      "   about  9, 55\n",
      "   computer programming  12, 16\n",
      "   consideration  10\n",
      "   data mining  21\n",
      "   data, exploring  56\n",
      "   data, modeling  57\n",
      "   data, obtaining  56\n",
      "   domain knowledge  12, 20\n",
      "   exploratory data analysis (EDA)  21\n",
      "   machine learning  21\n",
      "   math  13\n",
      "   math/statistics  12\n",
      "   organized data  9\n",
      "   overview  56\n",
      "   Python  16\n",
      "   question  56\n",
      "   results, communicating  57\n",
      "   results, visualizing  57\n",
      "   terminology  9, 21\n",
      "   unorganized data  9   Venn diagram  12, 13\n",
      "   xyz123 Technologies  11\n",
      "data\n",
      "   about  42, 53\n",
      "   experimental  139\n",
      "   experimentation  139\n",
      "   exploring  57, 58\n",
      "   observational  139\n",
      "   obtaining  139\n",
      "   probability sampling  142\n",
      "   random sampling  142\n",
      "   sampling  139, 141\n",
      "   titanic dataset  68\n",
      "   types  32, 33\n",
      "   unequal probability sampling  143\n",
      "   yelp dataset  59\n",
      "DataFrame  61\n",
      "decision trees\n",
      "   about  264, 266\n",
      "   classification tree, fitting  266, 268, 270, 271\n",
      "   comparing, with random forests  329\n",
      "   regression tree, building  266\n",
      "Deep Neural Network Classifier  368\n",
      "dimension reduction  221\n",
      "discrete random variable\n",
      "   about  121, 124, 126\n",
      "   binomial random variables  127, 129\n",
      "   continuous random variable  133, 136\n",
      "   geometric random variable  129\n",
      "   poisson random variable  131\n",
      "   types  127\n",
      "DNNClassifier  368\n",
      "domain knowledge  20\n",
      "dummy variables  248, 249, 250, 251, 252\n",
      "E\n",
      "effective visualizations\n",
      "   identifying  189\n",
      "empirical rule  159, 160\n",
      "ensembling\n",
      "   about  320, 321, 322\n",
      "   random forests  323, 324, 325, 328\n",
      "error functions  309, 310\n",
      "Euler's number  241\n",
      "event  96\n",
      "\n",
      "exploratory data analysis (EDA)  338, 345, 349,\n",
      "350\n",
      "   about  339, 340, 341, 343\n",
      "   classification route  353, 354, 355\n",
      "   exploratory data analysis  339\n",
      "   regression route  351\n",
      "exponent  83, 84\n",
      "F\n",
      "false positive  181\n",
      "feature extraction  284, 286, 288, 291, 293, 295\n",
      "Frequentist approach\n",
      "   about  97, 98\n",
      "   law of large num  99\n",
      "   versus Bayesian approach  97\n",
      "G\n",
      "geometric mean  51\n",
      "geometric random variable  130\n",
      "gradient descent  365\n",
      "graphs  82, 201\n",
      "grid searching\n",
      "   about  315, 316, 317, 318\n",
      "   training error, versus cross-validation error  318,\n",
      "320\n",
      "H\n",
      "histograms  195\n",
      "hypothesis test  175\n",
      "   conducting  176\n",
      "   one sample t-tests  177\n",
      "hypothesis tests\n",
      "   chi-square goodness of fit test  182\n",
      "   for categorical variables  182\n",
      "   type I errors  181\n",
      "   type II errors  181\n",
      "I\n",
      "ineffective visualizations\n",
      "   identifying  189\n",
      "interval level\n",
      "   about  47\n",
      "   example  47\n",
      "   mathematical operations  48\n",
      "   measures of center  48   measures of variation  49\n",
      "J\n",
      "Jaccard measure  89\n",
      "K\n",
      "k folds cross-validation\n",
      "   about  310, 311, 313, 314\n",
      "   features  312\n",
      "k-fold cross validation  367\n",
      "k-means clustering\n",
      "   about  272, 274\n",
      "   beer, illustrative example  279, 281\n",
      "   data points, illustrative example  274, 276, 278\n",
      "K-Nearest Neighbors (KNN) algorithm  310\n",
      "K\n",
      "   optimal number, selecting for  282\n",
      "key performance indicator (KPI)  206\n",
      "L\n",
      "labeled data  215\n",
      "likelihood  256\n",
      "likert  44\n",
      "Likert scale  124\n",
      "line graphs  191\n",
      "linear algebra  82\n",
      "   about  90\n",
      "   matrix multiplication  90\n",
      "linear regression  226, 227, 228, 229, 230, 231\n",
      "   predictors, adding  231, 232, 233, 234\n",
      "   regression metrics  234, 235, 237, 239, 240\n",
      "log odds  242, 243, 244, 245\n",
      "logarithm  83, 84\n",
      "logistic regression  240, 241, 242\n",
      "   math  245, 247, 248\n",
      "M\n",
      "machine learning\n",
      "   about  15, 211\n",
      "   data  220\n",
      "   facial recognition  211\n",
      "   limitations  213\n",
      "   predictions  218\n",
      "   probabilistic model  21\n",
      "\n",
      "   reinforcement learning  225\n",
      "   statistical model  21\n",
      "   supervised learning  215\n",
      "   supervised learning, example  216, 217\n",
      "   supervised learning, types  219\n",
      "   supervised machine learning  224\n",
      "   types  214, 215\n",
      "   types, overview  224\n",
      "   unsupervised learning  221, 222\n",
      "   unsupervised machine learning  225\n",
      "   working  214\n",
      "math  74\n",
      "   about  14\n",
      "   example  14\n",
      "mathematics  74, 75\n",
      "   symbols  75\n",
      "   terminology  75\n",
      "matrices\n",
      "   about  75\n",
      "   answers  78\n",
      "   multiplication  91, 93, 94\n",
      "matrix  76\n",
      "matrix multiplication  90\n",
      "mean  48\n",
      "Mean Squared Error (MSE)  266\n",
      "measures of center  43\n",
      "measures of relative standing  151\n",
      "   correlations, in data  156\n",
      "measures of variation  49, 151\n",
      "measures of variation, interval level\n",
      "   standard deviation  49\n",
      "measures of variation, statistics\n",
      "   defining  149\n",
      "   example  150\n",
      "median  45, 145\n",
      "microsoft data science environment\n",
      "   about  375\n",
      "   Microsoft Azure  375\n",
      "   Microsoft Azure Machine Learning Studio  375\n",
      "   Microsoft Databricks  375\n",
      "Microsoft Databricks\n",
      "   about  376\n",
      "   bike-sharing usage prediction, parallelization in\n",
      "Databricks  378, 379, 380, 381, 382, 383,\n",
      "385, 386, 387, 389, 390, 391   cluster, setting up  377\n",
      "   clusters/workers  376\n",
      "   MLib  376\n",
      "   MLlib Grid Search module, used to tune\n",
      "hyperparameters  399, 402\n",
      "   MLlib, used in Databricks to predict credit card \n",
      "391, 392, 393, 394, 395, 397, 399\n",
      "   Notebooks  376\n",
      "   Python libraries, adding to cluster  403\n",
      "   reference link  376\n",
      "   Spark DataFrames  376\n",
      "   spark_sklearn, used to build MNIST classifier \n",
      "404, 405\n",
      "   used, to optimize hyperparameter tuning  403,\n",
      "404\n",
      "   using  376\n",
      "model coefficients  229\n",
      "models  13\n",
      "multilayer perceptron (MLP)  332\n",
      "mutually exhaustive  112\n",
      "N\n",
      "Naive Bayes\n",
      "   classification  255, 259, 262, 263\n",
      "Naïve Bayes algorithm  112\n",
      "neural networks\n",
      "   about  329\n",
      "   basic structure  330\n",
      "   for anomaly detection  330\n",
      "   for entity movement  330\n",
      "   for pattern recognition  330\n",
      "   structure  331, 332, 333, 335, 336\n",
      "nominal level  60\n",
      "   about  43\n",
      "   data  44\n",
      "   mathematical operations  43\n",
      "   measure of center  43\n",
      "normalizing constant  256\n",
      "notation  96\n",
      "null model  239\n",
      "O\n",
      "odds  242, 243, 244, 245\n",
      "one sample t-tests\n",
      "   about  177\n",
      "\n",
      "   assumptions  178, 180\n",
      "   example  178\n",
      "ordinal level  44, 60\n",
      "   examples  44\n",
      "   mathematical operations  44\n",
      "   measure of center  45\n",
      "out-of-sample (OOS) error  311\n",
      "overfitting  308\n",
      "P\n",
      "parameter  138\n",
      "perceptrons  330\n",
      "point estimates  162, 165, 166\n",
      "poisson random variable  131\n",
      "pre-processing  34\n",
      "prediction  217\n",
      "predictive analytics models  215\n",
      "presentation\n",
      "   strategy  208\n",
      "principal component analysis  284, 286, 288, 291,\n",
      "293, 295\n",
      "Principal Component Analysis (PCA)  288\n",
      "prior probability  256\n",
      "probability  96, 242, 243, 244, 245\n",
      "probability density function (PDF)  133\n",
      "probability mass function (PMF)  122, 127\n",
      "probability\n",
      "   addition rule  104\n",
      "   complementary events  108\n",
      "   event independence  108\n",
      "   multiplication rule  107\n",
      "   mutual exclusivity  106\n",
      "   rules  104\n",
      "procedure  95\n",
      "proper subset  88\n",
      "PySpark  376\n",
      "Python\n",
      "   about  16\n",
      "   example  18\n",
      "   practices  17\n",
      "   single tweet, parsing  19\n",
      "Q\n",
      "qualitative data\n",
      "   about  37   example  37, 39\n",
      "   exploration tips  62\n",
      "   filtering in pandas  64\n",
      "   nominal level columns  62\n",
      "   ordinal level columns  67\n",
      "   versus quantitative data  36\n",
      "quantitative data\n",
      "   about  37\n",
      "   continuous data  41\n",
      "   discrete data  41\n",
      "   example  37, 39\n",
      "   overview  41\n",
      "R\n",
      "random forests\n",
      "   about  323, 324, 325, 328\n",
      "   comparing, with decision trees  329\n",
      "random variable  120\n",
      "   discrete random variable  121, 124, 126\n",
      "random variables  112\n",
      "ratio level\n",
      "   about  51\n",
      "   example  51\n",
      "   measures of center  51\n",
      "   problems  52\n",
      "regression  219\n",
      "regression metrics  234, 235, 237, 239, 240\n",
      "regression route  351\n",
      "regression tree\n",
      "   building  266\n",
      "reinforcement learning  223, 224, 225\n",
      "relative frequency  98\n",
      "root-mean-square error (RMSE)  351\n",
      "S\n",
      "sample space  96\n",
      "sampling bias  142\n",
      "sampling distributions  167, 169\n",
      "scalar  80\n",
      "scatter plots  190\n",
      "set theory  86\n",
      "Silhouette Coefficient  282, 284\n",
      "simpson's paradox  204\n",
      "social media\n",
      "   exploratory data analysis (EDA)  340, 341, 343,\n",
      "\n",
      "45, 349, 350\n",
      "   survey  356, 357, 359, 360, 362, 363\n",
      "Spark  376\n",
      "spark_sklearn\n",
      "   reference link  405\n",
      "square matrix  77\n",
      "standard deviation  146\n",
      "standard normal distribution  133\n",
      "statistical modeling  226\n",
      "statistics  137, 201\n",
      "   measures of center  144\n",
      "   measures of relative standing  150, 153, 156\n",
      "   measures of variation  145, 148\n",
      "   measuring  144\n",
      "stock price\n",
      "   example  355\n",
      "stock prices\n",
      "   predicting, on social media  338\n",
      "   text sentiment analysis  338, 339\n",
      "structured data\n",
      "   about  33\n",
      "   versus unstructured data  33\n",
      "superset  87\n",
      "supervised learning  215\n",
      "supervised learning models  271\n",
      "supervised learning\n",
      "   classification  219\n",
      "   regression  219\n",
      "   types  219\n",
      "supervised machine learning  224\n",
      "T\n",
      "TensorFlow\n",
      "   about  368, 369, 370, 371, 372, 374\n",
      "   neural networks  368, 369, 370, 371, 372, 374   using  363, 364, 366, 367\n",
      "titanic dataset  68\n",
      "training error\n",
      "   versus cross-validation error  318, 320\n",
      "U\n",
      "underfitting  308\n",
      "unstructured data\n",
      "   about  33\n",
      "   versus structured data  33\n",
      "unsupervised learning  222\n",
      "   about  221, 271\n",
      "   reinforcement learning  223, 224\n",
      "   using  271\n",
      "unsupervised machine learning  225\n",
      "V\n",
      "vectors\n",
      "   about  75\n",
      "   answers  78\n",
      "   exercises  78\n",
      "verbal communication\n",
      "   about  206\n",
      "   data findings, presentation  207\n",
      "W\n",
      "World Health Organization (WHO)  39\n",
      "Y\n",
      "yelp dataset\n",
      "   about  59\n",
      "   DataFrame  61\n",
      "   qualitative data, exploration tips  62\n",
      "   series  62 3657\n"
     ]
    }
   ],
   "source": [
    "# 각 Token에 5개의 중첩 문장 포함\n",
    "split = overlapping_chunks(principles_of_ds)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
